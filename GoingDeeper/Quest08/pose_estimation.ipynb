{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 02:56:35.772115: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-19 02:56:35.782772: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-19 02:56:35.795639: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-19 02:56:35.795682: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-19 02:56:35.804737: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-19 02:56:36.203533: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 주의! ray를 tensorflow보다 먼저 import하면 오류가 발생할 수 있습니다\n",
    "import io, json, os, math\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Add, Concatenate, Lambda\n",
    "from tensorflow.keras.layers import Input, Conv2D, ReLU, MaxPool2D\n",
    "from tensorflow.keras.layers import UpSampling2D, ZeroPadding2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import ray\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PROJECT_PATH = '/home/minho/Desktop/aiffel/pose_estimation'\n",
    "IMAGE_PATH = os.path.join(PROJECT_PATH, 'images')\n",
    "MODEL_PATH = os.path.join(PROJECT_PATH, 'models')\n",
    "TFRECORD_PATH = os.path.join(PROJECT_PATH, 'tfrecords_mpii')\n",
    "TRAIN_JSON = os.path.join(PROJECT_PATH, 'mpii_human_pose_v1_u12_2', 'train.json')\n",
    "VALID_JSON = os.path.join(PROJECT_PATH, 'mpii_human_pose_v1_u12_2', 'validation.json')\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"joints_vis\": [\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"joints\": [\n",
      "    [\n",
      "      620.0,\n",
      "      394.0\n",
      "    ],\n",
      "    [\n",
      "      616.0,\n",
      "      269.0\n",
      "    ],\n",
      "    [\n",
      "      573.0,\n",
      "      185.0\n",
      "    ],\n",
      "    [\n",
      "      647.0,\n",
      "      188.0\n",
      "    ],\n",
      "    [\n",
      "      661.0,\n",
      "      221.0\n",
      "    ],\n",
      "    [\n",
      "      656.0,\n",
      "      231.0\n",
      "    ],\n",
      "    [\n",
      "      610.0,\n",
      "      187.0\n",
      "    ],\n",
      "    [\n",
      "      647.0,\n",
      "      176.0\n",
      "    ],\n",
      "    [\n",
      "      637.0201,\n",
      "      189.8183\n",
      "    ],\n",
      "    [\n",
      "      695.9799,\n",
      "      108.1817\n",
      "    ],\n",
      "    [\n",
      "      606.0,\n",
      "      217.0\n",
      "    ],\n",
      "    [\n",
      "      553.0,\n",
      "      161.0\n",
      "    ],\n",
      "    [\n",
      "      601.0,\n",
      "      167.0\n",
      "    ],\n",
      "    [\n",
      "      692.0,\n",
      "      185.0\n",
      "    ],\n",
      "    [\n",
      "      693.0,\n",
      "      240.0\n",
      "    ],\n",
      "    [\n",
      "      688.0,\n",
      "      313.0\n",
      "    ]\n",
      "  ],\n",
      "  \"image\": \"015601864.jpg\",\n",
      "  \"scale\": 3.021046,\n",
      "  \"center\": [\n",
      "    594.0,\n",
      "    257.0\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(TRAIN_JSON) as train_json:\n",
    "    train_annos = json.load(train_json)\n",
    "    json_formatted_str = json.dumps(train_annos[0], indent=2)\n",
    "    print(json_formatted_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def parse_one_annotation(anno, image_dir):\n",
    "    filename = anno['image']\n",
    "    joints = anno['joints']\n",
    "    joints_visibility = anno['joints_vis']\n",
    "    annotation = {\n",
    "        'filename': filename,\n",
    "        'filepath': os.path.join(image_dir, filename),\n",
    "        'joints_visibility': joints_visibility,\n",
    "        'joints': joints,\n",
    "        'center': anno['center'],\n",
    "        'scale' : anno['scale']\n",
    "    }\n",
    "    return annotation\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filename': '015601864.jpg', 'filepath': '/home/minho/Desktop/aiffel/pose_estimation/images/015601864.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[620.0, 394.0], [616.0, 269.0], [573.0, 185.0], [647.0, 188.0], [661.0, 221.0], [656.0, 231.0], [610.0, 187.0], [647.0, 176.0], [637.0201, 189.8183], [695.9799, 108.1817], [606.0, 217.0], [553.0, 161.0], [601.0, 167.0], [692.0, 185.0], [693.0, 240.0], [688.0, 313.0]], 'center': [594.0, 257.0], 'scale': 3.021046}\n"
     ]
    }
   ],
   "source": [
    "with open(TRAIN_JSON) as train_json:\n",
    "    train_annos = json.load(train_json)\n",
    "    test = parse_one_annotation(train_annos[0], IMAGE_PATH)\n",
    "    print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def generate_tfexample(anno):\n",
    "\n",
    "    # byte 인코딩을 위한 함수\n",
    "    def _bytes_feature(value):\n",
    "        if isinstance(value, type(tf.constant(0))):\n",
    "            value = value.numpy()\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "    filename = anno['filename']\n",
    "    filepath = anno['filepath']\n",
    "    with open(filepath, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "    image = Image.open(filepath)\n",
    "    if image.format != 'JPEG' or image.mode != 'RGB':\n",
    "        image_rgb = image.convert('RGB')\n",
    "        with io.BytesIO() as output:\n",
    "            image_rgb.save(output, format=\"JPEG\", quality=95)\n",
    "            content = output.getvalue()\n",
    "\n",
    "    width, height = image.size\n",
    "    depth = 3\n",
    "\n",
    "    c_x = int(anno['center'][0])\n",
    "    c_y = int(anno['center'][1])\n",
    "    scale = anno['scale']\n",
    "\n",
    "    x = [\n",
    "        int(joint[0]) if joint[0] >= 0 else int(joint[0]) \n",
    "        for joint in anno['joints']\n",
    "    ]\n",
    "    y = [\n",
    "        int(joint[1]) if joint[1] >= 0 else int(joint[0]) \n",
    "        for joint in anno['joints']\n",
    "    ]\n",
    "\n",
    "    v = [0 if joint_v == 0 else 2 for joint_v in anno['joints_visibility']]\n",
    "\n",
    "    feature = {\n",
    "        'image/height':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),\n",
    "        'image/width':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[width])),\n",
    "        'image/depth':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[depth])),\n",
    "        'image/object/parts/x':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=x)),\n",
    "        'image/object/parts/y':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=y)),\n",
    "        'image/object/center/x': \n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[c_x])),\n",
    "        'image/object/center/y': \n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[c_y])),\n",
    "        'image/object/scale':\n",
    "        tf.train.Feature(float_list=tf.train.FloatList(value=[scale])),\n",
    "        'image/object/parts/v':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=v)),\n",
    "        'image/encoded':\n",
    "        _bytes_feature(content),\n",
    "        'image/filename':\n",
    "        _bytes_feature(filename.encode())\n",
    "    }\n",
    "\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def chunkify(l, n):\n",
    "    size = len(l) // n\n",
    "    start = 0\n",
    "    results = []\n",
    "    for i in range(n):\n",
    "        results.append(l[start:start + size])\n",
    "        start += size\n",
    "    return results\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "64\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "test_chunks = chunkify([0] * 1000, 64)\n",
    "print(test_chunks)\n",
    "print(len(test_chunks))\n",
    "print(len(test_chunks[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "@ray.remote\n",
    "def build_single_tfrecord(chunk, path):\n",
    "    print('start to build tf records for ' + path)\n",
    "\n",
    "    with tf.io.TFRecordWriter(path) as writer:\n",
    "        for anno in chunk:\n",
    "            tf_example = generate_tfexample(anno)\n",
    "            writer.write(tf_example.SerializeToString())\n",
    "\n",
    "    print('finished building tf records for ' + path)\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def build_tf_records(annotations, total_shards, split):\n",
    "    chunks = chunkify(annotations, total_shards)\n",
    "    futures = [\n",
    "        build_single_tfrecord.remote(\n",
    "            chunk, '{}/{}_{}_of_{}.tfrecords'.format(\n",
    "                TFRECORD_PATH,\n",
    "                split,\n",
    "                str(i + 1).zfill(4),\n",
    "                str(total_shards).zfill(4),\n",
    "            )) for i, chunk in enumerate(chunks)\n",
    "    ]\n",
    "    ray.get(futures)\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 02:56:49,593\tINFO worker.py:1841 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to parse annotations.\n",
      "First train annotation:  {'filename': '015601864.jpg', 'filepath': '/home/minho/Desktop/aiffel/pose_estimation/images/015601864.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[620.0, 394.0], [616.0, 269.0], [573.0, 185.0], [647.0, 188.0], [661.0, 221.0], [656.0, 231.0], [610.0, 187.0], [647.0, 176.0], [637.0201, 189.8183], [695.9799, 108.1817], [606.0, 217.0], [553.0, 161.0], [601.0, 167.0], [692.0, 185.0], [693.0, 240.0], [688.0, 313.0]], 'center': [594.0, 257.0], 'scale': 3.021046}\n",
      "First val annotation:  {'filename': '005808361.jpg', 'filepath': '/home/minho/Desktop/aiffel/pose_estimation/images/005808361.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[804.0, 711.0], [816.0, 510.0], [908.0, 438.0], [1040.0, 454.0], [906.0, 528.0], [883.0, 707.0], [974.0, 446.0], [985.0, 253.0], [982.7591, 235.9694], [962.2409, 80.0306], [869.0, 214.0], [798.0, 340.0], [902.0, 253.0], [1067.0, 253.0], [1167.0, 353.0], [1142.0, 478.0]], 'center': [966.0, 340.0], 'scale': 4.718488}\n",
      "Start to build TF Records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=85228)\u001b[0m 2025-03-19 02:56:50.784208: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[36m(pid=85228)\u001b[0m 2025-03-19 02:56:50.798232: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(pid=85228)\u001b[0m 2025-03-19 02:56:50.817806: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=85228)\u001b[0m 2025-03-19 02:56:50.817833: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=85228)\u001b[0m 2025-03-19 02:56:50.828486: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[36m(pid=85228)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(pid=85226)\u001b[0m 2025-03-19 02:56:51.498285: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(build_single_tfrecord pid=85227)\u001b[0m start to build tf records for /home/minho/Desktop/aiffel/pose_estimation/tfrecords_mpii/train_0010_of_0064.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(build_single_tfrecord pid=85227)\u001b[0m 2025-03-19 02:56:52.506006: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:282] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "\u001b[36m(build_single_tfrecord pid=85227)\u001b[0m 2025-03-19 02:56:52.506030: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:134] retrieving CUDA diagnostic information for host: minho-MS-7D75\n",
      "\u001b[36m(build_single_tfrecord pid=85227)\u001b[0m 2025-03-19 02:56:52.506039: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:141] hostname: minho-MS-7D75\n",
      "\u001b[36m(build_single_tfrecord pid=85227)\u001b[0m 2025-03-19 02:56:52.506112: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:165] libcuda reported version is: 550.120.0\n",
      "\u001b[36m(build_single_tfrecord pid=85227)\u001b[0m 2025-03-19 02:56:52.506131: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:169] kernel reported version is: 550.120.0\n",
      "\u001b[36m(build_single_tfrecord pid=85227)\u001b[0m 2025-03-19 02:56:52.506140: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:248] kernel version seems to match DSO: 550.120.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(build_single_tfrecord pid=85232)\u001b[0m finished building tf records for /home/minho/Desktop/aiffel/pose_estimation/tfrecords_mpii/train_0003_of_0064.tfrecords\n",
      "\u001b[36m(build_single_tfrecord pid=85229)\u001b[0m start to build tf records for /home/minho/Desktop/aiffel/pose_estimation/tfrecords_mpii/train_0062_of_0064.tfrecords\u001b[32m [repeated 61x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(build_single_tfrecord pid=85228)\u001b[0m finished building tf records for /home/minho/Desktop/aiffel/pose_estimation/tfrecords_mpii/val_0007_of_0008.tfrecords\u001b[32m [repeated 64x across cluster]\u001b[0m\n",
      "Successfully wrote 25204 annotations to TF Records.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(build_single_tfrecord pid=85237)\u001b[0m finished building tf records for /home/minho/Desktop/aiffel/pose_estimation/tfrecords_mpii/val_0006_of_0008.tfrecords\n"
     ]
    }
   ],
   "source": [
    "num_train_shards = 64\n",
    "num_val_shards = 8\n",
    "\n",
    "ray.init()\n",
    "\n",
    "print('Start to parse annotations.')\n",
    "if not os.path.exists(TFRECORD_PATH):\n",
    "    os.makedirs(TFRECORD_PATH)\n",
    "\n",
    "with open(TRAIN_JSON) as train_json:\n",
    "    train_annos = json.load(train_json)\n",
    "    train_annotations = [\n",
    "        parse_one_annotation(anno, IMAGE_PATH)\n",
    "        for anno in train_annos\n",
    "    ]\n",
    "    print('First train annotation: ', train_annotations[0])\n",
    "\n",
    "with open(VALID_JSON) as val_json:\n",
    "    val_annos = json.load(val_json)\n",
    "    val_annotations = [\n",
    "        parse_one_annotation(anno, IMAGE_PATH) \n",
    "        for anno in val_annos\n",
    "    ]\n",
    "    print('First val annotation: ', val_annotations[0])\n",
    "    \n",
    "print('Start to build TF Records.')\n",
    "build_tf_records(train_annotations, num_train_shards, 'train')\n",
    "build_tf_records(val_annotations, num_val_shards, 'val')\n",
    "\n",
    "print('Successfully wrote {} annotations to TF Records.'.format(\n",
    "    len(train_annotations) + len(val_annotations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def parse_tfexample(example):\n",
    "    image_feature_description = {\n",
    "        'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/object/parts/x': tf.io.VarLenFeature(tf.int64),\n",
    "        'image/object/parts/y': tf.io.VarLenFeature(tf.int64),\n",
    "        'image/object/parts/v': tf.io.VarLenFeature(tf.int64),\n",
    "        'image/object/center/x': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/object/center/y': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/object/scale': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "    return tf.io.parse_single_example(example, image_feature_description)\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def crop_roi(image, features, margin=0.2):\n",
    "    img_shape = tf.shape(image)\n",
    "    img_height = img_shape[0]\n",
    "    img_width = img_shape[1]\n",
    "    img_depth = img_shape[2]\n",
    "\n",
    "    keypoint_x = tf.cast(tf.sparse.to_dense(features['image/object/parts/x']), dtype=tf.int32)\n",
    "    keypoint_y = tf.cast(tf.sparse.to_dense(features['image/object/parts/y']), dtype=tf.int32)\n",
    "    center_x = features['image/object/center/x']\n",
    "    center_y = features['image/object/center/y']\n",
    "    body_height = features['image/object/scale'] * 200.0\n",
    "\n",
    "    # keypoint 중 유효한값(visible = 1) 만 사용합니다.\n",
    "    masked_keypoint_x = tf.boolean_mask(keypoint_x, keypoint_x > 0)\n",
    "    masked_keypoint_y = tf.boolean_mask(keypoint_y, keypoint_y > 0)\n",
    "\n",
    "    # min, max 값을 찾습니다.\n",
    "    keypoint_xmin = tf.reduce_min(masked_keypoint_x)\n",
    "    keypoint_xmax = tf.reduce_max(masked_keypoint_x)\n",
    "    keypoint_ymin = tf.reduce_min(masked_keypoint_y)\n",
    "    keypoint_ymax = tf.reduce_max(masked_keypoint_y)\n",
    "\n",
    "    # 높이 값을 이용해서 x, y 위치를 재조정 합니다. 박스를 정사각형으로 사용하기 위해 아래와 같이 사용합니다.\n",
    "    xmin = keypoint_xmin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "    xmax = keypoint_xmax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "    ymin = keypoint_ymin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "    ymax = keypoint_ymax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "\n",
    "    # 이미지 크기를 벗어나는 점을 재조정 해줍니다.\n",
    "    effective_xmin = xmin if xmin > 0 else 0\n",
    "    effective_ymin = ymin if ymin > 0 else 0\n",
    "    effective_xmax = xmax if xmax < img_width else img_width\n",
    "    effective_ymax = ymax if ymax < img_height else img_height\n",
    "    effective_height = effective_ymax - effective_ymin\n",
    "    effective_width = effective_xmax - effective_xmin\n",
    "\n",
    "    image = image[effective_ymin:effective_ymax, effective_xmin:effective_xmax, :]\n",
    "    new_shape = tf.shape(image)\n",
    "    new_height = new_shape[0]\n",
    "    new_width = new_shape[1]\n",
    "\n",
    "    effective_keypoint_x = (keypoint_x - effective_xmin) / new_width\n",
    "    effective_keypoint_y = (keypoint_y - effective_ymin) / new_height\n",
    "\n",
    "    return image, effective_keypoint_x, effective_keypoint_y\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def generate_2d_guassian(height, width, y0, x0, visibility=2, sigma=1, scale=12):\n",
    "    heatmap = tf.zeros((height, width))\n",
    "\n",
    "    xmin = x0 - 3 * sigma\n",
    "    ymin = y0 - 3 * sigma\n",
    "    xmax = x0 + 3 * sigma\n",
    "    ymax = y0 + 3 * sigma\n",
    "    \n",
    "    if xmin >= width or ymin >= height or xmax < 0 or ymax < 0 or visibility == 0:\n",
    "        return heatmap\n",
    "\n",
    "    size = 6 * sigma + 1\n",
    "    x, y = tf.meshgrid(tf.range(0, 6 * sigma + 1, 1), tf.range(0, 6 * sigma + 1, 1), indexing='xy')\n",
    "\n",
    "    center_x = size // 2\n",
    "    center_y = size // 2\n",
    "\n",
    "    gaussian_patch = tf.cast(tf.math.exp(\n",
    "        -(tf.math.square(x - center_x) + tf.math.square(y - center_y)) / (tf.math.square(sigma) * 2)) * scale,\n",
    "                             dtype=tf.float32)\n",
    "\n",
    "    patch_xmin = tf.math.maximum(0, -xmin)\n",
    "    patch_ymin = tf.math.maximum(0, -ymin)\n",
    "    patch_xmax = tf.math.minimum(xmax, width) - xmin\n",
    "    patch_ymax = tf.math.minimum(ymax, height) - ymin\n",
    "\n",
    "    heatmap_xmin = tf.math.maximum(0, xmin)\n",
    "    heatmap_ymin = tf.math.maximum(0, ymin)\n",
    "    heatmap_xmax = tf.math.minimum(xmax, width)\n",
    "    heatmap_ymax = tf.math.minimum(ymax, height)\n",
    "\n",
    "    indices = tf.TensorArray(tf.int32, 1, dynamic_size=True)\n",
    "    updates = tf.TensorArray(tf.float32, 1, dynamic_size=True)\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for j in tf.range(patch_ymin, patch_ymax):\n",
    "        for i in tf.range(patch_xmin, patch_xmax):\n",
    "            indices = indices.write(count, [heatmap_ymin + j, heatmap_xmin + i])\n",
    "            updates = updates.write(count, gaussian_patch[j][i])\n",
    "            count += 1\n",
    "\n",
    "    heatmap = tf.tensor_scatter_nd_update(heatmap, indices.stack(), updates.stack())\n",
    "\n",
    "    return heatmap\n",
    "\n",
    "def make_heatmaps(features, keypoint_x, keypoint_y, heatmap_shape):\n",
    "    v = tf.cast(tf.sparse.to_dense(features['image/object/parts/v']), dtype=tf.float32)\n",
    "    x = tf.cast(tf.math.round(keypoint_x * heatmap_shape[0]), dtype=tf.int32)\n",
    "    y = tf.cast(tf.math.round(keypoint_y * heatmap_shape[1]), dtype=tf.int32)\n",
    "\n",
    "    num_heatmap = heatmap_shape[2]\n",
    "    heatmap_array = tf.TensorArray(tf.float32, 16)\n",
    "\n",
    "    for i in range(num_heatmap):\n",
    "        gaussian = self.generate_2d_guassian(heatmap_shape[1], heatmap_shape[0], y[i], x[i], v[i])\n",
    "        heatmap_array = heatmap_array.write(i, gaussian)\n",
    "\n",
    "    heatmaps = heatmap_array.stack()\n",
    "    heatmaps = tf.transpose(heatmaps, perm=[1, 2, 0])  # change to (64, 64, 16)\n",
    "\n",
    "    return heatmaps\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class Preprocessor(object):\n",
    "    def __init__(self,\n",
    "                 image_shape=(256, 256, 3),\n",
    "                 heatmap_shape=(64, 64, 16),\n",
    "                 is_train=False):\n",
    "        self.is_train = is_train\n",
    "        self.image_shape = image_shape\n",
    "        self.heatmap_shape = heatmap_shape\n",
    "\n",
    "    def __call__(self, example):\n",
    "        features = self.parse_tfexample(example)\n",
    "        image = tf.io.decode_jpeg(features['image/encoded'])\n",
    "\n",
    "        if self.is_train:\n",
    "            random_margin = tf.random.uniform([1], 0.1, 0.3)[0]\n",
    "            image, keypoint_x, keypoint_y = self.crop_roi(image, features, margin=random_margin)\n",
    "            image = tf.image.resize(image, self.image_shape[0:2])\n",
    "        else:\n",
    "            image, keypoint_x, keypoint_y = self.crop_roi(image, features)\n",
    "            image = tf.image.resize(image, self.image_shape[0:2])\n",
    "\n",
    "        image = tf.cast(image, tf.float32) / 127.5 - 1\n",
    "        heatmaps = self.make_heatmaps(features, keypoint_x, keypoint_y, self.heatmap_shape)\n",
    "\n",
    "        return image, heatmaps\n",
    "\n",
    "        \n",
    "    def crop_roi(self, image, features, margin=0.2):\n",
    "        img_shape = tf.shape(image)\n",
    "        img_height = img_shape[0]\n",
    "        img_width = img_shape[1]\n",
    "        img_depth = img_shape[2]\n",
    "\n",
    "        keypoint_x = tf.cast(tf.sparse.to_dense(features['image/object/parts/x']), dtype=tf.int32)\n",
    "        keypoint_y = tf.cast(tf.sparse.to_dense(features['image/object/parts/y']), dtype=tf.int32)\n",
    "        center_x = features['image/object/center/x']\n",
    "        center_y = features['image/object/center/y']\n",
    "        body_height = features['image/object/scale'] * 200.0\n",
    "        \n",
    "        masked_keypoint_x = tf.boolean_mask(keypoint_x, keypoint_x > 0)\n",
    "        masked_keypoint_y = tf.boolean_mask(keypoint_y, keypoint_y > 0)\n",
    "        \n",
    "        keypoint_xmin = tf.reduce_min(masked_keypoint_x)\n",
    "        keypoint_xmax = tf.reduce_max(masked_keypoint_x)\n",
    "        keypoint_ymin = tf.reduce_min(masked_keypoint_y)\n",
    "        keypoint_ymax = tf.reduce_max(masked_keypoint_y)\n",
    "        \n",
    "        xmin = keypoint_xmin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        xmax = keypoint_xmax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        ymin = keypoint_ymin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        ymax = keypoint_ymax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        \n",
    "        effective_xmin = xmin if xmin > 0 else 0\n",
    "        effective_ymin = ymin if ymin > 0 else 0\n",
    "        effective_xmax = xmax if xmax < img_width else img_width\n",
    "        effective_ymax = ymax if ymax < img_height else img_height\n",
    "        effective_height = effective_ymax - effective_ymin\n",
    "        effective_width = effective_xmax - effective_xmin\n",
    "\n",
    "        image = image[effective_ymin:effective_ymax, effective_xmin:effective_xmax, :]\n",
    "        new_shape = tf.shape(image)\n",
    "        new_height = new_shape[0]\n",
    "        new_width = new_shape[1]\n",
    "        \n",
    "        effective_keypoint_x = (keypoint_x - effective_xmin) / new_width\n",
    "        effective_keypoint_y = (keypoint_y - effective_ymin) / new_height\n",
    "        \n",
    "        return image, effective_keypoint_x, effective_keypoint_y\n",
    "        \n",
    "    \n",
    "    def generate_2d_guassian(self, height, width, y0, x0, visibility=2, sigma=1, scale=12):\n",
    "        \n",
    "        heatmap = tf.zeros((height, width))\n",
    "\n",
    "        xmin = x0 - 3 * sigma\n",
    "        ymin = y0 - 3 * sigma\n",
    "        xmax = x0 + 3 * sigma\n",
    "        ymax = y0 + 3 * sigma\n",
    "\n",
    "        if xmin >= width or ymin >= height or xmax < 0 or ymax <0 or visibility == 0:\n",
    "            return heatmap\n",
    "\n",
    "        size = 6 * sigma + 1\n",
    "        x, y = tf.meshgrid(tf.range(0, 6*sigma+1, 1), tf.range(0, 6*sigma+1, 1), indexing='xy')\n",
    "\n",
    "        center_x = size // 2\n",
    "        center_y = size // 2\n",
    "\n",
    "        gaussian_patch = tf.cast(tf.math.exp(-(tf.square(x - center_x) + tf.math.square(y - center_y)) / (tf.math.square(sigma) * 2)) * scale, dtype=tf.float32)\n",
    "\n",
    "        patch_xmin = tf.math.maximum(0, -xmin)\n",
    "        patch_ymin = tf.math.maximum(0, -ymin)\n",
    "        patch_xmax = tf.math.minimum(xmax, width) - xmin\n",
    "        patch_ymax = tf.math.minimum(ymax, height) - ymin\n",
    "\n",
    "        heatmap_xmin = tf.math.maximum(0, xmin)\n",
    "        heatmap_ymin = tf.math.maximum(0, ymin)\n",
    "        heatmap_xmax = tf.math.minimum(xmax, width)\n",
    "        heatmap_ymax = tf.math.minimum(ymax, height)\n",
    "\n",
    "        indices = tf.TensorArray(tf.int32, 1, dynamic_size=True)\n",
    "        updates = tf.TensorArray(tf.float32, 1, dynamic_size=True)\n",
    "\n",
    "        count = 0\n",
    "\n",
    "        for j in tf.range(patch_ymin, patch_ymax):\n",
    "            for i in tf.range(patch_xmin, patch_xmax):\n",
    "                indices = indices.write(count, [heatmap_ymin+j, heatmap_xmin+i])\n",
    "                updates = updates.write(count, gaussian_patch[j][i])\n",
    "                count += 1\n",
    "                \n",
    "        heatmap = tf.tensor_scatter_nd_update(heatmap, indices.stack(), updates.stack())\n",
    "\n",
    "        return heatmap\n",
    "\n",
    "\n",
    "    def make_heatmaps(self, features, keypoint_x, keypoint_y, heatmap_shape):\n",
    "        v = tf.cast(tf.sparse.to_dense(features['image/object/parts/v']), dtype=tf.float32)\n",
    "        x = tf.cast(tf.math.round(keypoint_x * heatmap_shape[0]), dtype=tf.int32)\n",
    "        y = tf.cast(tf.math.round(keypoint_y * heatmap_shape[1]), dtype=tf.int32)\n",
    "        \n",
    "        num_heatmap = heatmap_shape[2]\n",
    "        heatmap_array = tf.TensorArray(tf.float32, 16)\n",
    "\n",
    "        for i in range(num_heatmap):\n",
    "            gaussian = self.generate_2d_guassian(heatmap_shape[1], heatmap_shape[0], y[i], x[i], v[i])\n",
    "            heatmap_array = heatmap_array.write(i, gaussian)\n",
    "        \n",
    "        heatmaps = heatmap_array.stack()\n",
    "        heatmaps = tf.transpose(heatmaps, perm=[1, 2, 0]) # change to (64, 64, 16)\n",
    "        \n",
    "        return heatmaps\n",
    "\n",
    "    def parse_tfexample(self, example):\n",
    "        image_feature_description = {\n",
    "            'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/parts/x': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/parts/y': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/parts/v': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/center/x': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/center/y': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/scale': tf.io.FixedLenFeature([], tf.float32),\n",
    "            'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "            'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "        }\n",
    "        return tf.io.parse_single_example(example,\n",
    "                                          image_feature_description)\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def BottleneckBlock(inputs, filters, strides=1, downsample=False, name=None):\n",
    "    identity = inputs\n",
    "    if downsample:\n",
    "        identity = Conv2D(\n",
    "            filters=filters,\n",
    "            kernel_size=1,\n",
    "            strides=strides,\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal')(inputs)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(inputs)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters // 2,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters // 2,\n",
    "        kernel_size=3,\n",
    "        strides=strides,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = Add()([identity, x])\n",
    "    return x\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def HourglassModule(inputs, order, filters, num_residual):\n",
    "    \n",
    "    up1 = BottleneckBlock(inputs, filters, downsample=False)\n",
    "    for i in range(num_residual):\n",
    "        up1 = BottleneckBlock(up1, filters, downsample=False)\n",
    "\n",
    "    low1 = MaxPool2D(pool_size=2, strides=2)(inputs)\n",
    "    for i in range(num_residual):\n",
    "        low1 = BottleneckBlock(low1, filters, downsample=False)\n",
    "\n",
    "    low2 = low1\n",
    "    if order > 1:\n",
    "        low2 = HourglassModule(low1, order - 1, filters, num_residual)\n",
    "    else:\n",
    "        for i in range(num_residual):\n",
    "            low2 = BottleneckBlock(low2, filters, downsample=False)\n",
    "\n",
    "    low3 = low2\n",
    "    for i in range(num_residual):\n",
    "        low3 = BottleneckBlock(low3, filters, downsample=False)\n",
    "\n",
    "    up2 = UpSampling2D(size=2)(low3)\n",
    "\n",
    "    return up2 + up1\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def LinearLayer(inputs, filters):\n",
    "    x = Conv2D(\n",
    "        filters=filters,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(inputs)\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    return x\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def StackedHourglassNetwork(\n",
    "        input_shape=(256, 256, 3), \n",
    "        num_stack=4, \n",
    "        num_residual=1,\n",
    "        num_heatmap=16):\n",
    "    \n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=7,\n",
    "        strides=2,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(inputs)\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BottleneckBlock(x, 128, downsample=True)\n",
    "    x = MaxPool2D(pool_size=2, strides=2)(x)\n",
    "    x = BottleneckBlock(x, 128, downsample=False)\n",
    "    x = BottleneckBlock(x, 256, downsample=True)\n",
    "\n",
    "    ys = []\n",
    "    for i in range(num_stack):\n",
    "        x = HourglassModule(x, order=4, filters=256, num_residual=num_residual)\n",
    "        for i in range(num_residual):\n",
    "            x = BottleneckBlock(x, 256, downsample=False)\n",
    "\n",
    "        x = LinearLayer(x, 256)\n",
    "\n",
    "        y = Conv2D(\n",
    "            filters=num_heatmap,\n",
    "            kernel_size=1,\n",
    "            strides=1,\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal')(x)\n",
    "        ys.append(y)\n",
    "\n",
    "        if i < num_stack - 1:\n",
    "            y_intermediate_1 = Conv2D(filters=256, kernel_size=1, strides=1)(x)\n",
    "            y_intermediate_2 = Conv2D(filters=256, kernel_size=1, strides=1)(y)\n",
    "            x = Add()([y_intermediate_1, y_intermediate_2])\n",
    "\n",
    "    return tf.keras.Model(inputs, ys, name='stacked_hourglass')\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SHAPE = (256, 256, 3)\n",
    "HEATMAP_SIZE = (64, 64)\n",
    "\n",
    "def create_dataset(tfrecords, batch_size, num_heatmap, is_train):\n",
    "    preprocess = Preprocessor(\n",
    "        IMAGE_SHAPE, (HEATMAP_SIZE[0], HEATMAP_SIZE[1], num_heatmap), is_train)\n",
    "\n",
    "    dataset = tf.data.Dataset.list_files(tfrecords)\n",
    "    dataset = tf.data.TFRecordDataset(dataset)\n",
    "    dataset = dataset.map(\n",
    "        preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    if is_train:\n",
    "        dataset = dataset.shuffle(batch_size)\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 epochs,\n",
    "                 global_batch_size,\n",
    "                 strategy,\n",
    "                 initial_learning_rate):\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.strategy = strategy\n",
    "        self.global_batch_size = global_batch_size\n",
    "        self.loss_object = tf.keras.losses.MeanSquaredError(\n",
    "            reduction=tf.keras.losses.Reduction.NONE)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=initial_learning_rate)\n",
    "\n",
    "        self.current_learning_rate = initial_learning_rate\n",
    "        self.last_val_loss = math.inf\n",
    "        self.lowest_val_loss = math.inf\n",
    "        self.patience_count = 0\n",
    "        self.max_patience = 10\n",
    "        self.best_model = None\n",
    "        \n",
    "        # history 객체: 에포크별 train_loss와 val_loss 기록\n",
    "        self.history = {\"train_loss\": [], \"val_loss\": []}\n",
    "\n",
    "    def lr_decay(self):\n",
    "        if self.patience_count >= self.max_patience:\n",
    "            self.current_learning_rate /= 10.0\n",
    "            self.patience_count = 0\n",
    "        elif self.last_val_loss == self.lowest_val_loss:\n",
    "            self.patience_count = 0\n",
    "        self.patience_count += 1\n",
    "        self.optimizer.learning_rate = self.current_learning_rate\n",
    "\n",
    "    def lr_decay_step(self, epoch):\n",
    "        if epoch in [25, 50, 75]:\n",
    "            self.current_learning_rate /= 10.0\n",
    "        self.optimizer.learning_rate = self.current_learning_rate\n",
    "\n",
    "    def compute_loss(self, labels, outputs):\n",
    "        # labels를 float32로 캐스팅합니다.\n",
    "        labels = tf.cast(labels, tf.float32)\n",
    "        # 초기 loss 값을 float32 0.0으로 선언합니다.\n",
    "        loss = tf.constant(0.0, dtype=tf.float32)\n",
    "        for output in outputs:\n",
    "            weights = tf.cast(labels > 0, dtype=tf.float32) * 81 + 1\n",
    "            loss += tf.math.reduce_mean(tf.math.square(labels - output) * weights) * (1.0 / self.global_batch_size)\n",
    "        return loss\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = self.model(images, training=True)\n",
    "            loss = self.compute_loss(labels, outputs)\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "    def val_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        outputs = self.model(images, training=False)\n",
    "        loss = self.compute_loss(labels, outputs)\n",
    "        return loss\n",
    "\n",
    "    def run(self, train_dist_dataset, val_dist_dataset):\n",
    "        @tf.function\n",
    "        def distributed_train_epoch(dataset):\n",
    "            tf.print('Start distributed training...')\n",
    "            total_loss = 0.0\n",
    "            num_train_batches = 0.0\n",
    "            for one_batch in dataset:\n",
    "                per_replica_loss = self.strategy.run(self.train_step, args=(one_batch,))\n",
    "                batch_loss = self.strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "                total_loss += batch_loss\n",
    "                num_train_batches += 1\n",
    "                tf.print('Trained batch', num_train_batches, 'batch loss', batch_loss,\n",
    "                         'epoch total loss', total_loss / num_train_batches)\n",
    "            return total_loss, num_train_batches\n",
    "\n",
    "        @tf.function\n",
    "        def distributed_val_epoch(dataset):\n",
    "            total_loss = 0.0\n",
    "            num_val_batches = 0.0\n",
    "            for one_batch in dataset:\n",
    "                per_replica_loss = self.strategy.run(self.val_step, args=(one_batch,))\n",
    "                num_val_batches += 1\n",
    "                batch_loss = self.strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "                tf.print('Validated batch', num_val_batches, 'batch loss', batch_loss)\n",
    "                if not tf.math.is_nan(batch_loss):\n",
    "                    total_loss += batch_loss\n",
    "                else:\n",
    "                    num_val_batches -= 1\n",
    "            return total_loss, num_val_batches\n",
    "\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            self.lr_decay()\n",
    "            print('Start epoch {} with learning rate {}'.format(epoch, self.current_learning_rate))\n",
    "\n",
    "            train_total_loss, num_train_batches = distributed_train_epoch(train_dist_dataset)\n",
    "            train_loss = train_total_loss / num_train_batches\n",
    "            print('Epoch {} train loss {}'.format(epoch, train_loss))\n",
    "            self.history[\"train_loss\"].append(train_loss.numpy())\n",
    "\n",
    "            val_total_loss, num_val_batches = distributed_val_epoch(val_dist_dataset)\n",
    "            val_loss = val_total_loss / num_val_batches\n",
    "            print('Epoch {} val loss {}'.format(epoch, val_loss))\n",
    "            self.history[\"val_loss\"].append(val_loss.numpy())\n",
    "\n",
    "            if val_loss < self.lowest_val_loss:\n",
    "                self.save_model(epoch, val_loss)\n",
    "                self.lowest_val_loss = val_loss\n",
    "            self.last_val_loss = val_loss\n",
    "\n",
    "        return self.history\n",
    "\n",
    "    def save_model(self, epoch, loss):\n",
    "        model_name = MODEL_PATH + '/model-epoch-{}-loss-{:.4f}.weights.h5'.format(epoch, loss)\n",
    "        self.model.save_weights(model_name)\n",
    "        self.best_model = model_name\n",
    "        print(\"Model {} saved.\".format(model_name))\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 같은 train 메서드를 이용하여 학습하기 위해, 로스 계산 시 float로 캐스팅하는 부분을 추가했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def train(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords):\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    global_batch_size = strategy.num_replicas_in_sync * batch_size\n",
    "    train_dataset = create_dataset(\n",
    "        train_tfrecords, global_batch_size, num_heatmap, is_train=True)\n",
    "    val_dataset = create_dataset(\n",
    "        val_tfrecords, global_batch_size, num_heatmap, is_train=False)\n",
    "\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        os.makedirs(MODEL_PATH)\n",
    "\n",
    "    with strategy.scope():\n",
    "        train_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            train_dataset)\n",
    "        val_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            val_dataset)\n",
    "\n",
    "        model = StackedHourglassNetwork(IMAGE_SHAPE, 4, 1, num_heatmap)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model,\n",
    "            epochs,\n",
    "            global_batch_size,\n",
    "            strategy,\n",
    "            initial_learning_rate=learning_rate)\n",
    "\n",
    "        print('Start training...')\n",
    "        return trainer.run(train_dist_dataset, val_dist_dataset)\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_simple_baseline(input_shape, num_heatmaps):\n",
    "    \"\"\"\n",
    "    SimpleBaseline 모델 구성 (출력 해상도는 TFRecord로부터 추출된 label과 동일해야 함)\n",
    "    예를 들어, 입력이 256x256이면 downsampling 후 (64,64) 크기의 feature map에 대해 \n",
    "    마지막 1x1 Conv로 num_heatmaps 채널을 예측하도록 구성합니다.\n",
    "    \"\"\"\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    \n",
    "    # 초기 다운샘플링: 7x7 conv, strides=2, 그리고 maxpooling\n",
    "    x = tf.keras.layers.Conv2D(64, kernel_size=7, strides=2, padding='same', activation='relu')(inputs)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.MaxPooling2D(pool_size=2, strides=2, padding='same')(x)  # 결과: (64,64,64) if input is 256x256\n",
    "    \n",
    "    # 여러 Conv2D 레이어 (해상도 유지)\n",
    "    for filters in [64, 128, 256]:\n",
    "        x = tf.keras.layers.Conv2D(filters, kernel_size=3, padding='same', activation='relu')(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    # 출력: 1x1 Conv로 num_heatmaps 채널 예측 (출력 해상도: (64,64,num_heatmaps))\n",
    "    outputs = tf.keras.layers.Conv2D(num_heatmaps, kernel_size=1, activation='linear')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "def train_sb(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords):\n",
    "    # GPU 분산 훈련 전략 (MirroredStrategy 사용)\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    global_batch_size = strategy.num_replicas_in_sync * batch_size\n",
    "    \n",
    "    # TFRecord 파일들을 와일드카드 패턴으로 로드 (예: \"train_*.tfrecords\")\n",
    "    train_dataset = create_dataset(train_tfrecords, global_batch_size, num_heatmap, is_train=True)\n",
    "    val_dataset = create_dataset(val_tfrecords, global_batch_size, num_heatmap, is_train=False)\n",
    "    \n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        os.makedirs(MODEL_PATH)\n",
    "    \n",
    "    with strategy.scope():\n",
    "        train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)\n",
    "        val_dist_dataset = strategy.experimental_distribute_dataset(val_dataset)\n",
    "        \n",
    "        # SimpleBaseline 모델 생성\n",
    "        model = build_simple_baseline(IMAGE_SHAPE, num_heatmap)\n",
    "        \n",
    "        # Trainer 클래스는 앞서 구현한 대로 history 객체를 반환하도록 수정되어 있음\n",
    "        trainer = Trainer(model, epochs, global_batch_size, strategy, learning_rate)\n",
    "        print('Start training SimpleBaseline model...')\n",
    "        history = trainer.run(train_dist_dataset, val_dist_dataset)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_HEATMAP = 16\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# 생성된 TFRecord 파일 경로 설정\n",
    "train_tfrecords = os.path.join(TFRECORD_PATH, \"train_*.tfrecords\")\n",
    "val_tfrecords = os.path.join(TFRECORD_PATH, \"val_*.tfrecords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Start training...\n",
      "Start epoch 1 with learning rate 0.001\n",
      "Start distributed training...\n",
      "Trained batch 1 batch loss 2.48266673 epoch total loss 2.48266673\n",
      "Trained batch 2 batch loss 2.52182937 epoch total loss 2.50224805\n",
      "Trained batch 3 batch loss 2.36522341 epoch total loss 2.45657325\n",
      "Trained batch 4 batch loss 2.3984704 epoch total loss 2.4420476\n",
      "Trained batch 5 batch loss 2.31956124 epoch total loss 2.41755033\n",
      "Trained batch 6 batch loss 2.42834234 epoch total loss 2.41934896\n",
      "Trained batch 7 batch loss 2.25516605 epoch total loss 2.39589429\n",
      "Trained batch 8 batch loss 2.00303841 epoch total loss 2.34678721\n",
      "Trained batch 9 batch loss 2.04049921 epoch total loss 2.31275511\n",
      "Trained batch 10 batch loss 2.08256984 epoch total loss 2.28973651\n",
      "Trained batch 11 batch loss 2.04707837 epoch total loss 2.26767683\n",
      "Trained batch 12 batch loss 2.00087047 epoch total loss 2.24544287\n",
      "Trained batch 13 batch loss 2.07939625 epoch total loss 2.23267\n",
      "Trained batch 14 batch loss 2.06190753 epoch total loss 2.22047281\n",
      "Trained batch 15 batch loss 2.03754067 epoch total loss 2.20827746\n",
      "Trained batch 16 batch loss 2.03923559 epoch total loss 2.19771218\n",
      "Trained batch 17 batch loss 2.01298428 epoch total loss 2.18684578\n",
      "Trained batch 18 batch loss 2.04114628 epoch total loss 2.17875147\n",
      "Trained batch 19 batch loss 2.01852202 epoch total loss 2.17031813\n",
      "Trained batch 20 batch loss 1.99184513 epoch total loss 2.1613946\n",
      "Trained batch 21 batch loss 1.89991808 epoch total loss 2.14894319\n",
      "Trained batch 22 batch loss 1.88651705 epoch total loss 2.13701463\n",
      "Trained batch 23 batch loss 1.75128412 epoch total loss 2.12024379\n",
      "Trained batch 24 batch loss 1.70128465 epoch total loss 2.10278726\n",
      "Trained batch 25 batch loss 1.75303757 epoch total loss 2.08879733\n",
      "Trained batch 26 batch loss 1.71103024 epoch total loss 2.07426786\n",
      "Trained batch 27 batch loss 1.60198617 epoch total loss 2.05677581\n",
      "Trained batch 28 batch loss 1.67366934 epoch total loss 2.04309344\n",
      "Trained batch 29 batch loss 1.5727756 epoch total loss 2.0268755\n",
      "Trained batch 30 batch loss 1.60311556 epoch total loss 2.01275015\n",
      "Trained batch 31 batch loss 1.72303653 epoch total loss 2.00340462\n",
      "Trained batch 32 batch loss 1.8444972 epoch total loss 1.99843884\n",
      "Trained batch 33 batch loss 1.85169637 epoch total loss 1.99399221\n",
      "Trained batch 34 batch loss 1.70957708 epoch total loss 1.98562717\n",
      "Trained batch 35 batch loss 1.63835788 epoch total loss 1.97570515\n",
      "Trained batch 36 batch loss 1.69520187 epoch total loss 1.96791327\n",
      "Trained batch 37 batch loss 1.72933316 epoch total loss 1.96146512\n",
      "Trained batch 38 batch loss 1.71301198 epoch total loss 1.95492697\n",
      "Trained batch 39 batch loss 1.76927447 epoch total loss 1.95016658\n",
      "Trained batch 40 batch loss 1.70245194 epoch total loss 1.94397378\n",
      "Trained batch 41 batch loss 1.78801262 epoch total loss 1.94016969\n",
      "Trained batch 42 batch loss 1.77002954 epoch total loss 1.93611872\n",
      "Trained batch 43 batch loss 1.76540244 epoch total loss 1.93214858\n",
      "Trained batch 44 batch loss 1.8342241 epoch total loss 1.92992294\n",
      "Trained batch 45 batch loss 1.7772367 epoch total loss 1.92653\n",
      "Trained batch 46 batch loss 1.77233446 epoch total loss 1.92317784\n",
      "Trained batch 47 batch loss 1.76598835 epoch total loss 1.91983342\n",
      "Trained batch 48 batch loss 1.58882868 epoch total loss 1.91293752\n",
      "Trained batch 49 batch loss 1.53214097 epoch total loss 1.90516615\n",
      "Trained batch 50 batch loss 1.60963452 epoch total loss 1.89925551\n",
      "Trained batch 51 batch loss 1.75887728 epoch total loss 1.89650309\n",
      "Trained batch 52 batch loss 1.79922688 epoch total loss 1.89463234\n",
      "Trained batch 53 batch loss 1.75756896 epoch total loss 1.89204621\n",
      "Trained batch 54 batch loss 1.78540063 epoch total loss 1.89007127\n",
      "Trained batch 55 batch loss 1.78372109 epoch total loss 1.8881377\n",
      "Trained batch 56 batch loss 1.82640886 epoch total loss 1.88703537\n",
      "Trained batch 57 batch loss 1.78383923 epoch total loss 1.88522482\n",
      "Trained batch 58 batch loss 1.78816414 epoch total loss 1.88355136\n",
      "Trained batch 59 batch loss 1.6456883 epoch total loss 1.87951982\n",
      "Trained batch 60 batch loss 1.75454938 epoch total loss 1.877437\n",
      "Trained batch 61 batch loss 1.75560319 epoch total loss 1.87543964\n",
      "Trained batch 62 batch loss 1.78579962 epoch total loss 1.87399375\n",
      "Trained batch 63 batch loss 1.74137 epoch total loss 1.87188864\n",
      "Trained batch 64 batch loss 1.73663688 epoch total loss 1.8697753\n",
      "Trained batch 65 batch loss 1.70069146 epoch total loss 1.86717403\n",
      "Trained batch 66 batch loss 1.71405482 epoch total loss 1.8648541\n",
      "Trained batch 67 batch loss 1.68045163 epoch total loss 1.86210179\n",
      "Trained batch 68 batch loss 1.57059348 epoch total loss 1.85781491\n",
      "Trained batch 69 batch loss 1.680035 epoch total loss 1.85523832\n",
      "Trained batch 70 batch loss 1.72060847 epoch total loss 1.85331511\n",
      "Trained batch 71 batch loss 1.65913725 epoch total loss 1.8505801\n",
      "Trained batch 72 batch loss 1.66444921 epoch total loss 1.84799492\n",
      "Trained batch 73 batch loss 1.71663237 epoch total loss 1.84619534\n",
      "Trained batch 74 batch loss 1.72149897 epoch total loss 1.84451032\n",
      "Trained batch 75 batch loss 1.74036908 epoch total loss 1.84312177\n",
      "Trained batch 76 batch loss 1.77942133 epoch total loss 1.84228361\n",
      "Trained batch 77 batch loss 1.66958249 epoch total loss 1.84004068\n",
      "Trained batch 78 batch loss 1.68163252 epoch total loss 1.83800972\n",
      "Trained batch 79 batch loss 1.51798725 epoch total loss 1.83395886\n",
      "Trained batch 80 batch loss 1.74992013 epoch total loss 1.83290839\n",
      "Trained batch 81 batch loss 1.54178166 epoch total loss 1.82931423\n",
      "Trained batch 82 batch loss 1.65634322 epoch total loss 1.82720482\n",
      "Trained batch 83 batch loss 1.62843406 epoch total loss 1.82481\n",
      "Trained batch 84 batch loss 1.67060935 epoch total loss 1.82297421\n",
      "Trained batch 85 batch loss 1.67278171 epoch total loss 1.82120717\n",
      "Trained batch 86 batch loss 1.72064114 epoch total loss 1.82003784\n",
      "Trained batch 87 batch loss 1.72898579 epoch total loss 1.8189913\n",
      "Trained batch 88 batch loss 1.73953128 epoch total loss 1.81808841\n",
      "Trained batch 89 batch loss 1.74466801 epoch total loss 1.81726348\n",
      "Trained batch 90 batch loss 1.75516498 epoch total loss 1.81657338\n",
      "Trained batch 91 batch loss 1.68919301 epoch total loss 1.81517363\n",
      "Trained batch 92 batch loss 1.63841665 epoch total loss 1.81325233\n",
      "Trained batch 93 batch loss 1.60327566 epoch total loss 1.81099451\n",
      "Trained batch 94 batch loss 1.72406161 epoch total loss 1.81006968\n",
      "Trained batch 95 batch loss 1.69172192 epoch total loss 1.80882394\n",
      "Trained batch 96 batch loss 1.75583792 epoch total loss 1.808272\n",
      "Trained batch 97 batch loss 1.71336436 epoch total loss 1.80729365\n",
      "Trained batch 98 batch loss 1.70551276 epoch total loss 1.80625498\n",
      "Trained batch 99 batch loss 1.68295228 epoch total loss 1.80500948\n",
      "Trained batch 100 batch loss 1.69280708 epoch total loss 1.80388749\n",
      "Trained batch 101 batch loss 1.71847308 epoch total loss 1.80304182\n",
      "Trained batch 102 batch loss 1.67213559 epoch total loss 1.80175841\n",
      "Trained batch 103 batch loss 1.76483166 epoch total loss 1.8014\n",
      "Trained batch 104 batch loss 1.54732621 epoch total loss 1.79895699\n",
      "Trained batch 105 batch loss 1.66153359 epoch total loss 1.79764807\n",
      "Trained batch 106 batch loss 1.60483146 epoch total loss 1.79582906\n",
      "Trained batch 107 batch loss 1.50731111 epoch total loss 1.79313266\n",
      "Trained batch 108 batch loss 1.47007453 epoch total loss 1.79014134\n",
      "Trained batch 109 batch loss 1.54679871 epoch total loss 1.78790879\n",
      "Trained batch 110 batch loss 1.58450055 epoch total loss 1.78605974\n",
      "Trained batch 111 batch loss 1.70502353 epoch total loss 1.78532958\n",
      "Trained batch 112 batch loss 1.72887993 epoch total loss 1.78482556\n",
      "Trained batch 113 batch loss 1.66610086 epoch total loss 1.78377497\n",
      "Trained batch 114 batch loss 1.68840337 epoch total loss 1.78293836\n",
      "Trained batch 115 batch loss 1.68919659 epoch total loss 1.78212321\n",
      "Trained batch 116 batch loss 1.67012334 epoch total loss 1.78115761\n",
      "Trained batch 117 batch loss 1.73032546 epoch total loss 1.78072321\n",
      "Trained batch 118 batch loss 1.71960378 epoch total loss 1.78020525\n",
      "Trained batch 119 batch loss 1.68303633 epoch total loss 1.77938867\n",
      "Trained batch 120 batch loss 1.56088412 epoch total loss 1.77756774\n",
      "Trained batch 121 batch loss 1.49788189 epoch total loss 1.77525628\n",
      "Trained batch 122 batch loss 1.47529125 epoch total loss 1.77279758\n",
      "Trained batch 123 batch loss 1.48569167 epoch total loss 1.77046335\n",
      "Trained batch 124 batch loss 1.63070226 epoch total loss 1.76933634\n",
      "Trained batch 125 batch loss 1.53898168 epoch total loss 1.76749349\n",
      "Trained batch 126 batch loss 1.56697655 epoch total loss 1.76590204\n",
      "Trained batch 127 batch loss 1.4747895 epoch total loss 1.76360989\n",
      "Trained batch 128 batch loss 1.60832191 epoch total loss 1.76239669\n",
      "Trained batch 129 batch loss 1.50705481 epoch total loss 1.76041722\n",
      "Trained batch 130 batch loss 1.56121874 epoch total loss 1.75888491\n",
      "Trained batch 131 batch loss 1.62294745 epoch total loss 1.75784719\n",
      "Trained batch 132 batch loss 1.56696463 epoch total loss 1.75640118\n",
      "Trained batch 133 batch loss 1.73364615 epoch total loss 1.75623012\n",
      "Trained batch 134 batch loss 1.6164999 epoch total loss 1.75518727\n",
      "Trained batch 135 batch loss 1.524086 epoch total loss 1.75347543\n",
      "Trained batch 136 batch loss 1.54645956 epoch total loss 1.75195324\n",
      "Trained batch 137 batch loss 1.50830424 epoch total loss 1.75017476\n",
      "Trained batch 138 batch loss 1.55386734 epoch total loss 1.74875224\n",
      "Trained batch 139 batch loss 1.54934514 epoch total loss 1.74731767\n",
      "Trained batch 140 batch loss 1.57326794 epoch total loss 1.74607444\n",
      "Trained batch 141 batch loss 1.46266222 epoch total loss 1.74406445\n",
      "Trained batch 142 batch loss 1.44289792 epoch total loss 1.7419436\n",
      "Trained batch 143 batch loss 1.43837249 epoch total loss 1.73982072\n",
      "Trained batch 144 batch loss 1.56701529 epoch total loss 1.73862064\n",
      "Trained batch 145 batch loss 1.67487466 epoch total loss 1.73818111\n",
      "Trained batch 146 batch loss 1.62543499 epoch total loss 1.73740888\n",
      "Trained batch 147 batch loss 1.69028544 epoch total loss 1.73708832\n",
      "Trained batch 148 batch loss 1.6692102 epoch total loss 1.73662984\n",
      "Trained batch 149 batch loss 1.72407722 epoch total loss 1.73654568\n",
      "Trained batch 150 batch loss 1.72055173 epoch total loss 1.73643899\n",
      "Trained batch 151 batch loss 1.69196236 epoch total loss 1.73614442\n",
      "Trained batch 152 batch loss 1.54018438 epoch total loss 1.73485529\n",
      "Trained batch 153 batch loss 1.67858863 epoch total loss 1.73448753\n",
      "Trained batch 154 batch loss 1.67475808 epoch total loss 1.73409951\n",
      "Trained batch 155 batch loss 1.62851727 epoch total loss 1.73341835\n",
      "Trained batch 156 batch loss 1.6095593 epoch total loss 1.73262429\n",
      "Trained batch 157 batch loss 1.64849544 epoch total loss 1.73208857\n",
      "Trained batch 158 batch loss 1.70856524 epoch total loss 1.73193955\n",
      "Trained batch 159 batch loss 1.66058028 epoch total loss 1.73149085\n",
      "Trained batch 160 batch loss 1.67448771 epoch total loss 1.73113465\n",
      "Trained batch 161 batch loss 1.61147106 epoch total loss 1.73039138\n",
      "Trained batch 162 batch loss 1.58686101 epoch total loss 1.72950542\n",
      "Trained batch 163 batch loss 1.45854175 epoch total loss 1.72784293\n",
      "Trained batch 164 batch loss 1.67756367 epoch total loss 1.72753632\n",
      "Trained batch 165 batch loss 1.62057757 epoch total loss 1.72688806\n",
      "Trained batch 166 batch loss 1.62466598 epoch total loss 1.72627223\n",
      "Trained batch 167 batch loss 1.59885049 epoch total loss 1.72550917\n",
      "Trained batch 168 batch loss 1.66655982 epoch total loss 1.72515833\n",
      "Trained batch 169 batch loss 1.60412419 epoch total loss 1.72444212\n",
      "Trained batch 170 batch loss 1.60117877 epoch total loss 1.72371697\n",
      "Trained batch 171 batch loss 1.64871 epoch total loss 1.7232784\n",
      "Trained batch 172 batch loss 1.68779898 epoch total loss 1.72307217\n",
      "Trained batch 173 batch loss 1.63138103 epoch total loss 1.72254217\n",
      "Trained batch 174 batch loss 1.61964309 epoch total loss 1.72195065\n",
      "Trained batch 175 batch loss 1.65289569 epoch total loss 1.72155607\n",
      "Trained batch 176 batch loss 1.66912532 epoch total loss 1.72125816\n",
      "Trained batch 177 batch loss 1.61688471 epoch total loss 1.72066844\n",
      "Trained batch 178 batch loss 1.66809011 epoch total loss 1.72037303\n",
      "Trained batch 179 batch loss 1.67264664 epoch total loss 1.72010636\n",
      "Trained batch 180 batch loss 1.59822798 epoch total loss 1.71942937\n",
      "Trained batch 181 batch loss 1.57116818 epoch total loss 1.71861017\n",
      "Trained batch 182 batch loss 1.62900579 epoch total loss 1.71811783\n",
      "Trained batch 183 batch loss 1.69317377 epoch total loss 1.71798158\n",
      "Trained batch 184 batch loss 1.65438831 epoch total loss 1.71763599\n",
      "Trained batch 185 batch loss 1.53679764 epoch total loss 1.71665847\n",
      "Trained batch 186 batch loss 1.51764166 epoch total loss 1.71558845\n",
      "Trained batch 187 batch loss 1.54964828 epoch total loss 1.71470118\n",
      "Trained batch 188 batch loss 1.64372659 epoch total loss 1.71432364\n",
      "Trained batch 189 batch loss 1.66663027 epoch total loss 1.71407127\n",
      "Trained batch 190 batch loss 1.60870433 epoch total loss 1.71351671\n",
      "Trained batch 191 batch loss 1.55081058 epoch total loss 1.71266484\n",
      "Trained batch 192 batch loss 1.6173588 epoch total loss 1.71216857\n",
      "Trained batch 193 batch loss 1.63634944 epoch total loss 1.71177566\n",
      "Trained batch 194 batch loss 1.55560851 epoch total loss 1.71097064\n",
      "Trained batch 195 batch loss 1.53319192 epoch total loss 1.71005905\n",
      "Trained batch 196 batch loss 1.50602233 epoch total loss 1.70901799\n",
      "Trained batch 197 batch loss 1.46382213 epoch total loss 1.70777345\n",
      "Trained batch 198 batch loss 1.61365747 epoch total loss 1.70729804\n",
      "Trained batch 199 batch loss 1.6001668 epoch total loss 1.70675969\n",
      "Trained batch 200 batch loss 1.59053469 epoch total loss 1.70617855\n",
      "Trained batch 201 batch loss 1.61862767 epoch total loss 1.70574296\n",
      "Trained batch 202 batch loss 1.64602137 epoch total loss 1.70544732\n",
      "Trained batch 203 batch loss 1.679075 epoch total loss 1.7053175\n",
      "Trained batch 204 batch loss 1.69144034 epoch total loss 1.70524943\n",
      "Trained batch 205 batch loss 1.65435815 epoch total loss 1.70500112\n",
      "Trained batch 206 batch loss 1.6103673 epoch total loss 1.7045418\n",
      "Trained batch 207 batch loss 1.63286185 epoch total loss 1.70419562\n",
      "Trained batch 208 batch loss 1.64644861 epoch total loss 1.70391798\n",
      "Trained batch 209 batch loss 1.6797173 epoch total loss 1.70380223\n",
      "Trained batch 210 batch loss 1.69187 epoch total loss 1.70374537\n",
      "Trained batch 211 batch loss 1.64465058 epoch total loss 1.70346534\n",
      "Trained batch 212 batch loss 1.60885525 epoch total loss 1.70301902\n",
      "Trained batch 213 batch loss 1.63252175 epoch total loss 1.70268798\n",
      "Trained batch 214 batch loss 1.58246338 epoch total loss 1.70212615\n",
      "Trained batch 215 batch loss 1.6500361 epoch total loss 1.70188391\n",
      "Trained batch 216 batch loss 1.63809288 epoch total loss 1.70158851\n",
      "Trained batch 217 batch loss 1.63605285 epoch total loss 1.70128644\n",
      "Trained batch 218 batch loss 1.63369954 epoch total loss 1.70097649\n",
      "Trained batch 219 batch loss 1.62883401 epoch total loss 1.70064712\n",
      "Trained batch 220 batch loss 1.63088751 epoch total loss 1.70033\n",
      "Trained batch 221 batch loss 1.60786211 epoch total loss 1.69991159\n",
      "Trained batch 222 batch loss 1.62236583 epoch total loss 1.69956231\n",
      "Trained batch 223 batch loss 1.60437346 epoch total loss 1.69913542\n",
      "Trained batch 224 batch loss 1.68470645 epoch total loss 1.69907093\n",
      "Trained batch 225 batch loss 1.58055 epoch total loss 1.69854414\n",
      "Trained batch 226 batch loss 1.63872743 epoch total loss 1.6982795\n",
      "Trained batch 227 batch loss 1.68249726 epoch total loss 1.69820988\n",
      "Trained batch 228 batch loss 1.69753885 epoch total loss 1.69820702\n",
      "Trained batch 229 batch loss 1.59986281 epoch total loss 1.69777751\n",
      "Trained batch 230 batch loss 1.68635762 epoch total loss 1.69772792\n",
      "Trained batch 231 batch loss 1.56385326 epoch total loss 1.69714832\n",
      "Trained batch 232 batch loss 1.68090689 epoch total loss 1.69707835\n",
      "Trained batch 233 batch loss 1.63680792 epoch total loss 1.69681966\n",
      "Trained batch 234 batch loss 1.71466327 epoch total loss 1.69689584\n",
      "Trained batch 235 batch loss 1.60880172 epoch total loss 1.69652104\n",
      "Trained batch 236 batch loss 1.58813918 epoch total loss 1.69606173\n",
      "Trained batch 237 batch loss 1.62064695 epoch total loss 1.69574344\n",
      "Trained batch 238 batch loss 1.70573652 epoch total loss 1.69578552\n",
      "Trained batch 239 batch loss 1.64006579 epoch total loss 1.69555247\n",
      "Trained batch 240 batch loss 1.65766418 epoch total loss 1.69539452\n",
      "Trained batch 241 batch loss 1.50454855 epoch total loss 1.69460261\n",
      "Trained batch 242 batch loss 1.34281993 epoch total loss 1.69314909\n",
      "Trained batch 243 batch loss 1.48890913 epoch total loss 1.69230855\n",
      "Trained batch 244 batch loss 1.58879328 epoch total loss 1.6918844\n",
      "Trained batch 245 batch loss 1.56441903 epoch total loss 1.69136417\n",
      "Trained batch 246 batch loss 1.65486646 epoch total loss 1.69121587\n",
      "Trained batch 247 batch loss 1.58566082 epoch total loss 1.69078851\n",
      "Trained batch 248 batch loss 1.63838542 epoch total loss 1.69057727\n",
      "Trained batch 249 batch loss 1.66837811 epoch total loss 1.69048798\n",
      "Trained batch 250 batch loss 1.56837177 epoch total loss 1.68999946\n",
      "Trained batch 251 batch loss 1.46514189 epoch total loss 1.68910372\n",
      "Trained batch 252 batch loss 1.5510217 epoch total loss 1.68855572\n",
      "Trained batch 253 batch loss 1.58641231 epoch total loss 1.68815207\n",
      "Trained batch 254 batch loss 1.52816367 epoch total loss 1.68752217\n",
      "Trained batch 255 batch loss 1.61502123 epoch total loss 1.68723786\n",
      "Trained batch 256 batch loss 1.61282635 epoch total loss 1.68694723\n",
      "Trained batch 257 batch loss 1.63035023 epoch total loss 1.68672693\n",
      "Trained batch 258 batch loss 1.60071528 epoch total loss 1.68639362\n",
      "Trained batch 259 batch loss 1.64630604 epoch total loss 1.68623877\n",
      "Trained batch 260 batch loss 1.63204598 epoch total loss 1.68603039\n",
      "Trained batch 261 batch loss 1.617226 epoch total loss 1.6857667\n",
      "Trained batch 262 batch loss 1.65417659 epoch total loss 1.68564606\n",
      "Trained batch 263 batch loss 1.7876662 epoch total loss 1.68603396\n",
      "Trained batch 264 batch loss 1.69796205 epoch total loss 1.68607914\n",
      "Trained batch 265 batch loss 1.71662164 epoch total loss 1.68619442\n",
      "Trained batch 266 batch loss 1.66468596 epoch total loss 1.68611348\n",
      "Trained batch 267 batch loss 1.60299599 epoch total loss 1.68580222\n",
      "Trained batch 268 batch loss 1.4938004 epoch total loss 1.68508577\n",
      "Trained batch 269 batch loss 1.67442298 epoch total loss 1.68504608\n",
      "Trained batch 270 batch loss 1.64679027 epoch total loss 1.68490446\n",
      "Trained batch 271 batch loss 1.6558162 epoch total loss 1.68479717\n",
      "Trained batch 272 batch loss 1.63042426 epoch total loss 1.68459725\n",
      "Trained batch 273 batch loss 1.57234478 epoch total loss 1.6841861\n",
      "Trained batch 274 batch loss 1.57107592 epoch total loss 1.68377328\n",
      "Trained batch 275 batch loss 1.6024214 epoch total loss 1.6834774\n",
      "Trained batch 276 batch loss 1.65369105 epoch total loss 1.68336952\n",
      "Trained batch 277 batch loss 1.74221623 epoch total loss 1.68358195\n",
      "Trained batch 278 batch loss 1.67594028 epoch total loss 1.68355441\n",
      "Trained batch 279 batch loss 1.62575531 epoch total loss 1.68334734\n",
      "Trained batch 280 batch loss 1.67043054 epoch total loss 1.68330121\n",
      "Trained batch 281 batch loss 1.69863164 epoch total loss 1.68335581\n",
      "Trained batch 282 batch loss 1.72040153 epoch total loss 1.68348718\n",
      "Trained batch 283 batch loss 1.58884513 epoch total loss 1.68315268\n",
      "Trained batch 284 batch loss 1.58143806 epoch total loss 1.68279457\n",
      "Trained batch 285 batch loss 1.53198028 epoch total loss 1.6822654\n",
      "Trained batch 286 batch loss 1.59635353 epoch total loss 1.68196499\n",
      "Trained batch 287 batch loss 1.60552192 epoch total loss 1.68169868\n",
      "Trained batch 288 batch loss 1.57097936 epoch total loss 1.68131423\n",
      "Trained batch 289 batch loss 1.59539104 epoch total loss 1.68101692\n",
      "Trained batch 290 batch loss 1.51282859 epoch total loss 1.68043697\n",
      "Trained batch 291 batch loss 1.52779114 epoch total loss 1.67991245\n",
      "Trained batch 292 batch loss 1.52694857 epoch total loss 1.67938864\n",
      "Trained batch 293 batch loss 1.55940127 epoch total loss 1.67897904\n",
      "Trained batch 294 batch loss 1.61751473 epoch total loss 1.67877\n",
      "Trained batch 295 batch loss 1.54250932 epoch total loss 1.67830813\n",
      "Trained batch 296 batch loss 1.48914111 epoch total loss 1.67766905\n",
      "Trained batch 297 batch loss 1.50298643 epoch total loss 1.67708087\n",
      "Trained batch 298 batch loss 1.61237776 epoch total loss 1.67686367\n",
      "Trained batch 299 batch loss 1.57055545 epoch total loss 1.67650819\n",
      "Trained batch 300 batch loss 1.69770837 epoch total loss 1.67657888\n",
      "Trained batch 301 batch loss 1.65395045 epoch total loss 1.67650378\n",
      "Trained batch 302 batch loss 1.53866899 epoch total loss 1.67604733\n",
      "Trained batch 303 batch loss 1.50015235 epoch total loss 1.67546678\n",
      "Trained batch 304 batch loss 1.44394183 epoch total loss 1.67470515\n",
      "Trained batch 305 batch loss 1.5731082 epoch total loss 1.6743722\n",
      "Trained batch 306 batch loss 1.50615561 epoch total loss 1.67382228\n",
      "Trained batch 307 batch loss 1.60645986 epoch total loss 1.67360282\n",
      "Trained batch 308 batch loss 1.5460788 epoch total loss 1.67318881\n",
      "Trained batch 309 batch loss 1.39378572 epoch total loss 1.67228472\n",
      "Trained batch 310 batch loss 1.52921009 epoch total loss 1.67182326\n",
      "Trained batch 311 batch loss 1.59564 epoch total loss 1.67157829\n",
      "Trained batch 312 batch loss 1.62116933 epoch total loss 1.67141664\n",
      "Trained batch 313 batch loss 1.6020267 epoch total loss 1.67119503\n",
      "Trained batch 314 batch loss 1.56319213 epoch total loss 1.67085099\n",
      "Trained batch 315 batch loss 1.55624866 epoch total loss 1.67048728\n",
      "Trained batch 316 batch loss 1.53345835 epoch total loss 1.6700536\n",
      "Trained batch 317 batch loss 1.532094 epoch total loss 1.66961849\n",
      "Trained batch 318 batch loss 1.43141198 epoch total loss 1.66886926\n",
      "Trained batch 319 batch loss 1.56923163 epoch total loss 1.66855693\n",
      "Trained batch 320 batch loss 1.51160717 epoch total loss 1.66806638\n",
      "Trained batch 321 batch loss 1.54498076 epoch total loss 1.66768301\n",
      "Trained batch 322 batch loss 1.60201621 epoch total loss 1.66747892\n",
      "Trained batch 323 batch loss 1.57753026 epoch total loss 1.66720045\n",
      "Trained batch 324 batch loss 1.57425964 epoch total loss 1.66691363\n",
      "Trained batch 325 batch loss 1.60259032 epoch total loss 1.66671574\n",
      "Trained batch 326 batch loss 1.58275497 epoch total loss 1.66645825\n",
      "Trained batch 327 batch loss 1.55142188 epoch total loss 1.66610634\n",
      "Trained batch 328 batch loss 1.63665915 epoch total loss 1.66601658\n",
      "Trained batch 329 batch loss 1.63938951 epoch total loss 1.66593564\n",
      "Trained batch 330 batch loss 1.43551111 epoch total loss 1.66523731\n",
      "Trained batch 331 batch loss 1.50818968 epoch total loss 1.66476285\n",
      "Trained batch 332 batch loss 1.53993 epoch total loss 1.66438675\n",
      "Trained batch 333 batch loss 1.59089434 epoch total loss 1.66416609\n",
      "Trained batch 334 batch loss 1.57233763 epoch total loss 1.66389108\n",
      "Trained batch 335 batch loss 1.53755677 epoch total loss 1.6635139\n",
      "Trained batch 336 batch loss 1.58001459 epoch total loss 1.66326547\n",
      "Trained batch 337 batch loss 1.52941179 epoch total loss 1.66286826\n",
      "Trained batch 338 batch loss 1.46984112 epoch total loss 1.66229713\n",
      "Trained batch 339 batch loss 1.51842427 epoch total loss 1.66187274\n",
      "Trained batch 340 batch loss 1.49188805 epoch total loss 1.66137278\n",
      "Trained batch 341 batch loss 1.60273802 epoch total loss 1.66120088\n",
      "Trained batch 342 batch loss 1.67586255 epoch total loss 1.66124368\n",
      "Trained batch 343 batch loss 1.65385509 epoch total loss 1.6612221\n",
      "Trained batch 344 batch loss 1.66904473 epoch total loss 1.66124499\n",
      "Trained batch 345 batch loss 1.67131674 epoch total loss 1.66127419\n",
      "Trained batch 346 batch loss 1.6305232 epoch total loss 1.66118515\n",
      "Trained batch 347 batch loss 1.54669285 epoch total loss 1.66085529\n",
      "Trained batch 348 batch loss 1.68715 epoch total loss 1.66093075\n",
      "Trained batch 349 batch loss 1.6664474 epoch total loss 1.66094661\n",
      "Trained batch 350 batch loss 1.57373452 epoch total loss 1.66069734\n",
      "Trained batch 351 batch loss 1.60737586 epoch total loss 1.66054547\n",
      "Trained batch 352 batch loss 1.65569127 epoch total loss 1.66053164\n",
      "Trained batch 353 batch loss 1.62447536 epoch total loss 1.66042948\n",
      "Trained batch 354 batch loss 1.59468484 epoch total loss 1.66024363\n",
      "Trained batch 355 batch loss 1.5483768 epoch total loss 1.65992856\n",
      "Trained batch 356 batch loss 1.48348284 epoch total loss 1.65943289\n",
      "Trained batch 357 batch loss 1.62104428 epoch total loss 1.65932536\n",
      "Trained batch 358 batch loss 1.6203053 epoch total loss 1.65921628\n",
      "Trained batch 359 batch loss 1.65002429 epoch total loss 1.65919077\n",
      "Trained batch 360 batch loss 1.56488717 epoch total loss 1.65892875\n",
      "Trained batch 361 batch loss 1.60644221 epoch total loss 1.65878344\n",
      "Trained batch 362 batch loss 1.65450716 epoch total loss 1.65877151\n",
      "Trained batch 363 batch loss 1.61510146 epoch total loss 1.65865123\n",
      "Trained batch 364 batch loss 1.59788752 epoch total loss 1.65848434\n",
      "Trained batch 365 batch loss 1.54662228 epoch total loss 1.65817785\n",
      "Trained batch 366 batch loss 1.4898988 epoch total loss 1.65771818\n",
      "Trained batch 367 batch loss 1.46368277 epoch total loss 1.65718949\n",
      "Trained batch 368 batch loss 1.49406362 epoch total loss 1.65674627\n",
      "Trained batch 369 batch loss 1.44731808 epoch total loss 1.65617871\n",
      "Trained batch 370 batch loss 1.51164174 epoch total loss 1.65578806\n",
      "Trained batch 371 batch loss 1.66430783 epoch total loss 1.65581107\n",
      "Trained batch 372 batch loss 1.68083251 epoch total loss 1.65587842\n",
      "Trained batch 373 batch loss 1.67719042 epoch total loss 1.65593553\n",
      "Trained batch 374 batch loss 1.69105554 epoch total loss 1.65602934\n",
      "Trained batch 375 batch loss 1.63649404 epoch total loss 1.65597725\n",
      "Trained batch 376 batch loss 1.69210565 epoch total loss 1.65607321\n",
      "Trained batch 377 batch loss 1.45323229 epoch total loss 1.65553522\n",
      "Trained batch 378 batch loss 1.49989426 epoch total loss 1.65512347\n",
      "Trained batch 379 batch loss 1.59092617 epoch total loss 1.65495408\n",
      "Trained batch 380 batch loss 1.57603645 epoch total loss 1.65474641\n",
      "Trained batch 381 batch loss 1.60875618 epoch total loss 1.65462577\n",
      "Trained batch 382 batch loss 1.68697238 epoch total loss 1.65471041\n",
      "Trained batch 383 batch loss 1.59658849 epoch total loss 1.65455866\n",
      "Trained batch 384 batch loss 1.53613019 epoch total loss 1.65425026\n",
      "Trained batch 385 batch loss 1.55469322 epoch total loss 1.6539917\n",
      "Trained batch 386 batch loss 1.74278498 epoch total loss 1.65422177\n",
      "Trained batch 387 batch loss 1.662714 epoch total loss 1.65424371\n",
      "Trained batch 388 batch loss 1.59347522 epoch total loss 1.65408707\n",
      "Trained batch 389 batch loss 1.67403293 epoch total loss 1.65413821\n",
      "Trained batch 390 batch loss 1.64597547 epoch total loss 1.65411735\n",
      "Trained batch 391 batch loss 1.66384768 epoch total loss 1.65414214\n",
      "Trained batch 392 batch loss 1.64603353 epoch total loss 1.65412152\n",
      "Trained batch 393 batch loss 1.60199761 epoch total loss 1.65398896\n",
      "Trained batch 394 batch loss 1.56927848 epoch total loss 1.6537739\n",
      "Trained batch 395 batch loss 1.53920603 epoch total loss 1.65348375\n",
      "Trained batch 396 batch loss 1.55283225 epoch total loss 1.65322971\n",
      "Trained batch 397 batch loss 1.54739225 epoch total loss 1.65296304\n",
      "Trained batch 398 batch loss 1.58354545 epoch total loss 1.65278864\n",
      "Trained batch 399 batch loss 1.55357981 epoch total loss 1.65254\n",
      "Trained batch 400 batch loss 1.49007082 epoch total loss 1.65213382\n",
      "Trained batch 401 batch loss 1.58010674 epoch total loss 1.65195405\n",
      "Trained batch 402 batch loss 1.60956275 epoch total loss 1.65184867\n",
      "Trained batch 403 batch loss 1.67036414 epoch total loss 1.65189457\n",
      "Trained batch 404 batch loss 1.65984726 epoch total loss 1.65191424\n",
      "Trained batch 405 batch loss 1.60439456 epoch total loss 1.65179682\n",
      "Trained batch 406 batch loss 1.65821671 epoch total loss 1.65181267\n",
      "Trained batch 407 batch loss 1.4597224 epoch total loss 1.6513406\n",
      "Trained batch 408 batch loss 1.30831623 epoch total loss 1.65049982\n",
      "Trained batch 409 batch loss 1.59372723 epoch total loss 1.65036106\n",
      "Trained batch 410 batch loss 1.68287301 epoch total loss 1.65044034\n",
      "Trained batch 411 batch loss 1.70117807 epoch total loss 1.65056384\n",
      "Trained batch 412 batch loss 1.57379138 epoch total loss 1.65037739\n",
      "Trained batch 413 batch loss 1.61987615 epoch total loss 1.6503036\n",
      "Trained batch 414 batch loss 1.63240099 epoch total loss 1.65026033\n",
      "Trained batch 415 batch loss 1.59852397 epoch total loss 1.65013564\n",
      "Trained batch 416 batch loss 1.53236032 epoch total loss 1.64985251\n",
      "Trained batch 417 batch loss 1.55193281 epoch total loss 1.64961767\n",
      "Trained batch 418 batch loss 1.47927403 epoch total loss 1.6492101\n",
      "Trained batch 419 batch loss 1.60377121 epoch total loss 1.64910161\n",
      "Trained batch 420 batch loss 1.64058948 epoch total loss 1.64908123\n",
      "Trained batch 421 batch loss 1.70940256 epoch total loss 1.64922452\n",
      "Trained batch 422 batch loss 1.71940064 epoch total loss 1.64939094\n",
      "Trained batch 423 batch loss 1.66290975 epoch total loss 1.64942288\n",
      "Trained batch 424 batch loss 1.60749507 epoch total loss 1.64932394\n",
      "Trained batch 425 batch loss 1.57218671 epoch total loss 1.6491425\n",
      "Trained batch 426 batch loss 1.501454 epoch total loss 1.64879584\n",
      "Trained batch 427 batch loss 1.51296902 epoch total loss 1.64847767\n",
      "Trained batch 428 batch loss 1.57604468 epoch total loss 1.6483084\n",
      "Trained batch 429 batch loss 1.6505537 epoch total loss 1.64831376\n",
      "Trained batch 430 batch loss 1.60265338 epoch total loss 1.64820755\n",
      "Trained batch 431 batch loss 1.60571337 epoch total loss 1.64810896\n",
      "Trained batch 432 batch loss 1.56167829 epoch total loss 1.64790893\n",
      "Trained batch 433 batch loss 1.54566586 epoch total loss 1.64767277\n",
      "Trained batch 434 batch loss 1.62544525 epoch total loss 1.64762151\n",
      "Trained batch 435 batch loss 1.5493418 epoch total loss 1.64739561\n",
      "Trained batch 436 batch loss 1.64383554 epoch total loss 1.64738739\n",
      "Trained batch 437 batch loss 1.56240177 epoch total loss 1.64719296\n",
      "Trained batch 438 batch loss 1.56490541 epoch total loss 1.64700496\n",
      "Trained batch 439 batch loss 1.61951959 epoch total loss 1.64694238\n",
      "Trained batch 440 batch loss 1.59616208 epoch total loss 1.64682698\n",
      "Trained batch 441 batch loss 1.59583449 epoch total loss 1.64671135\n",
      "Trained batch 442 batch loss 1.46297777 epoch total loss 1.64629567\n",
      "Trained batch 443 batch loss 1.44577217 epoch total loss 1.64584303\n",
      "Trained batch 444 batch loss 1.32431018 epoch total loss 1.64511883\n",
      "Trained batch 445 batch loss 1.44821858 epoch total loss 1.64467633\n",
      "Trained batch 446 batch loss 1.44228065 epoch total loss 1.6442225\n",
      "Trained batch 447 batch loss 1.30653763 epoch total loss 1.64346707\n",
      "Trained batch 448 batch loss 1.27212739 epoch total loss 1.64263821\n",
      "Trained batch 449 batch loss 1.26686335 epoch total loss 1.64180124\n",
      "Trained batch 450 batch loss 1.24982786 epoch total loss 1.64093018\n",
      "Trained batch 451 batch loss 1.54566789 epoch total loss 1.64071894\n",
      "Trained batch 452 batch loss 1.66107726 epoch total loss 1.640764\n",
      "Trained batch 453 batch loss 1.60348976 epoch total loss 1.64068174\n",
      "Trained batch 454 batch loss 1.53900433 epoch total loss 1.64045775\n",
      "Trained batch 455 batch loss 1.50233185 epoch total loss 1.64015412\n",
      "Trained batch 456 batch loss 1.47276032 epoch total loss 1.63978708\n",
      "Trained batch 457 batch loss 1.52998805 epoch total loss 1.63954675\n",
      "Trained batch 458 batch loss 1.57061255 epoch total loss 1.63939631\n",
      "Trained batch 459 batch loss 1.52967668 epoch total loss 1.63915718\n",
      "Trained batch 460 batch loss 1.52557695 epoch total loss 1.63891029\n",
      "Trained batch 461 batch loss 1.4612931 epoch total loss 1.63852501\n",
      "Trained batch 462 batch loss 1.52753222 epoch total loss 1.6382848\n",
      "Trained batch 463 batch loss 1.57852912 epoch total loss 1.63815582\n",
      "Trained batch 464 batch loss 1.50229168 epoch total loss 1.63786304\n",
      "Trained batch 465 batch loss 1.46249163 epoch total loss 1.63748586\n",
      "Trained batch 466 batch loss 1.34811175 epoch total loss 1.63686478\n",
      "Trained batch 467 batch loss 1.36186242 epoch total loss 1.63627601\n",
      "Trained batch 468 batch loss 1.41813469 epoch total loss 1.6358099\n",
      "Trained batch 469 batch loss 1.4421792 epoch total loss 1.63539708\n",
      "Trained batch 470 batch loss 1.56442344 epoch total loss 1.63524616\n",
      "Trained batch 471 batch loss 1.48680604 epoch total loss 1.63493097\n",
      "Trained batch 472 batch loss 1.56505847 epoch total loss 1.63478291\n",
      "Trained batch 473 batch loss 1.49314773 epoch total loss 1.63448358\n",
      "Trained batch 474 batch loss 1.57257307 epoch total loss 1.63435292\n",
      "Trained batch 475 batch loss 1.55143023 epoch total loss 1.6341784\n",
      "Trained batch 476 batch loss 1.77790165 epoch total loss 1.63448036\n",
      "Trained batch 477 batch loss 1.69770503 epoch total loss 1.6346128\n",
      "Trained batch 478 batch loss 1.67054832 epoch total loss 1.63468802\n",
      "Trained batch 479 batch loss 1.62262976 epoch total loss 1.63466275\n",
      "Trained batch 480 batch loss 1.55211687 epoch total loss 1.63449085\n",
      "Trained batch 481 batch loss 1.60346842 epoch total loss 1.63442636\n",
      "Trained batch 482 batch loss 1.58741117 epoch total loss 1.63432872\n",
      "Trained batch 483 batch loss 1.55996847 epoch total loss 1.63417482\n",
      "Trained batch 484 batch loss 1.57518589 epoch total loss 1.63405299\n",
      "Trained batch 485 batch loss 1.62471974 epoch total loss 1.63403368\n",
      "Trained batch 486 batch loss 1.56552339 epoch total loss 1.63389277\n",
      "Trained batch 487 batch loss 1.56789196 epoch total loss 1.63375723\n",
      "Trained batch 488 batch loss 1.56916916 epoch total loss 1.63362479\n",
      "Trained batch 489 batch loss 1.54528391 epoch total loss 1.63344419\n",
      "Trained batch 490 batch loss 1.54879701 epoch total loss 1.63327134\n",
      "Trained batch 491 batch loss 1.54906297 epoch total loss 1.63309991\n",
      "Trained batch 492 batch loss 1.57986474 epoch total loss 1.63299179\n",
      "Trained batch 493 batch loss 1.50616705 epoch total loss 1.63273454\n",
      "Trained batch 494 batch loss 1.56152582 epoch total loss 1.63259029\n",
      "Trained batch 495 batch loss 1.57848608 epoch total loss 1.6324811\n",
      "Trained batch 496 batch loss 1.58011758 epoch total loss 1.63237548\n",
      "Trained batch 497 batch loss 1.56438315 epoch total loss 1.63223875\n",
      "Trained batch 498 batch loss 1.54523587 epoch total loss 1.63206398\n",
      "Trained batch 499 batch loss 1.58287 epoch total loss 1.63196552\n",
      "Trained batch 500 batch loss 1.59298301 epoch total loss 1.63188744\n",
      "Trained batch 501 batch loss 1.62680221 epoch total loss 1.6318773\n",
      "Trained batch 502 batch loss 1.53485096 epoch total loss 1.63168406\n",
      "Trained batch 503 batch loss 1.38266277 epoch total loss 1.63118911\n",
      "Trained batch 504 batch loss 1.54955256 epoch total loss 1.6310271\n",
      "Trained batch 505 batch loss 1.49231505 epoch total loss 1.63075244\n",
      "Trained batch 506 batch loss 1.43957686 epoch total loss 1.63037455\n",
      "Trained batch 507 batch loss 1.26827216 epoch total loss 1.62966037\n",
      "Trained batch 508 batch loss 1.2714678 epoch total loss 1.62895525\n",
      "Trained batch 509 batch loss 1.43349171 epoch total loss 1.62857127\n",
      "Trained batch 510 batch loss 1.46790767 epoch total loss 1.6282562\n",
      "Trained batch 511 batch loss 1.30523157 epoch total loss 1.62762403\n",
      "Trained batch 512 batch loss 1.48327041 epoch total loss 1.6273421\n",
      "Trained batch 513 batch loss 1.44556546 epoch total loss 1.6269877\n",
      "Trained batch 514 batch loss 1.53065801 epoch total loss 1.6268003\n",
      "Trained batch 515 batch loss 1.35851562 epoch total loss 1.62627935\n",
      "Trained batch 516 batch loss 1.45511544 epoch total loss 1.62594771\n",
      "Trained batch 517 batch loss 1.50696325 epoch total loss 1.62571752\n",
      "Trained batch 518 batch loss 1.57002699 epoch total loss 1.62561\n",
      "Trained batch 519 batch loss 1.58086252 epoch total loss 1.62552381\n",
      "Trained batch 520 batch loss 1.59942651 epoch total loss 1.62547362\n",
      "Trained batch 521 batch loss 1.56815672 epoch total loss 1.62536359\n",
      "Trained batch 522 batch loss 1.55691707 epoch total loss 1.62523258\n",
      "Trained batch 523 batch loss 1.45946276 epoch total loss 1.6249156\n",
      "Trained batch 524 batch loss 1.56593919 epoch total loss 1.62480307\n",
      "Trained batch 525 batch loss 1.50665653 epoch total loss 1.624578\n",
      "Trained batch 526 batch loss 1.58136833 epoch total loss 1.62449586\n",
      "Trained batch 527 batch loss 1.54802728 epoch total loss 1.62435079\n",
      "Trained batch 528 batch loss 1.48148251 epoch total loss 1.62408018\n",
      "Trained batch 529 batch loss 1.57185495 epoch total loss 1.62398148\n",
      "Trained batch 530 batch loss 1.56620324 epoch total loss 1.62387252\n",
      "Trained batch 531 batch loss 1.62151659 epoch total loss 1.62386799\n",
      "Trained batch 532 batch loss 1.57489169 epoch total loss 1.62377596\n",
      "Trained batch 533 batch loss 1.61828184 epoch total loss 1.62376571\n",
      "Trained batch 534 batch loss 1.58715737 epoch total loss 1.62369716\n",
      "Trained batch 535 batch loss 1.61476195 epoch total loss 1.62368035\n",
      "Trained batch 536 batch loss 1.56470013 epoch total loss 1.62357032\n",
      "Trained batch 537 batch loss 1.5303843 epoch total loss 1.62339687\n",
      "Trained batch 538 batch loss 1.60709214 epoch total loss 1.62336659\n",
      "Trained batch 539 batch loss 1.68476152 epoch total loss 1.62348044\n",
      "Trained batch 540 batch loss 1.62230897 epoch total loss 1.62347829\n",
      "Trained batch 541 batch loss 1.57014382 epoch total loss 1.62337971\n",
      "Trained batch 542 batch loss 1.57156432 epoch total loss 1.6232841\n",
      "Trained batch 543 batch loss 1.51270723 epoch total loss 1.62308049\n",
      "Trained batch 544 batch loss 1.41899061 epoch total loss 1.62270534\n",
      "Trained batch 545 batch loss 1.54312646 epoch total loss 1.62255943\n",
      "Trained batch 546 batch loss 1.40314674 epoch total loss 1.62215745\n",
      "Trained batch 547 batch loss 1.437621 epoch total loss 1.62182009\n",
      "Trained batch 548 batch loss 1.57551289 epoch total loss 1.62173557\n",
      "Trained batch 549 batch loss 1.71213567 epoch total loss 1.62190032\n",
      "Trained batch 550 batch loss 1.60231793 epoch total loss 1.62186468\n",
      "Trained batch 551 batch loss 1.55613089 epoch total loss 1.62174547\n",
      "Trained batch 552 batch loss 1.54304564 epoch total loss 1.62160277\n",
      "Trained batch 553 batch loss 1.61584973 epoch total loss 1.6215924\n",
      "Trained batch 554 batch loss 1.50060391 epoch total loss 1.62137401\n",
      "Trained batch 555 batch loss 1.53143251 epoch total loss 1.62121201\n",
      "Trained batch 556 batch loss 1.52734566 epoch total loss 1.62104309\n",
      "Trained batch 557 batch loss 1.49478304 epoch total loss 1.62081647\n",
      "Trained batch 558 batch loss 1.50008965 epoch total loss 1.6206001\n",
      "Trained batch 559 batch loss 1.56724477 epoch total loss 1.62050474\n",
      "Trained batch 560 batch loss 1.54580724 epoch total loss 1.62037134\n",
      "Trained batch 561 batch loss 1.54290247 epoch total loss 1.6202333\n",
      "Trained batch 562 batch loss 1.50768971 epoch total loss 1.62003303\n",
      "Trained batch 563 batch loss 1.48189843 epoch total loss 1.61978757\n",
      "Trained batch 564 batch loss 1.48549044 epoch total loss 1.61954951\n",
      "Trained batch 565 batch loss 1.47856939 epoch total loss 1.6193\n",
      "Trained batch 566 batch loss 1.43565142 epoch total loss 1.61897552\n",
      "Trained batch 567 batch loss 1.28797626 epoch total loss 1.61839175\n",
      "Trained batch 568 batch loss 1.37769938 epoch total loss 1.61796796\n",
      "Trained batch 569 batch loss 1.47118008 epoch total loss 1.61771\n",
      "Trained batch 570 batch loss 1.46998239 epoch total loss 1.61745083\n",
      "Trained batch 571 batch loss 1.41005564 epoch total loss 1.6170876\n",
      "Trained batch 572 batch loss 1.35243988 epoch total loss 1.61662483\n",
      "Trained batch 573 batch loss 1.62508202 epoch total loss 1.61663961\n",
      "Trained batch 574 batch loss 1.43764091 epoch total loss 1.61632764\n",
      "Trained batch 575 batch loss 1.45494723 epoch total loss 1.61604702\n",
      "Trained batch 576 batch loss 1.51640391 epoch total loss 1.61587405\n",
      "Trained batch 577 batch loss 1.48309779 epoch total loss 1.61564398\n",
      "Trained batch 578 batch loss 1.48049271 epoch total loss 1.61541009\n",
      "Trained batch 579 batch loss 1.24074554 epoch total loss 1.6147629\n",
      "Trained batch 580 batch loss 1.30103374 epoch total loss 1.61422205\n",
      "Trained batch 581 batch loss 1.28273618 epoch total loss 1.61365139\n",
      "Trained batch 582 batch loss 1.6152395 epoch total loss 1.61365414\n",
      "Trained batch 583 batch loss 1.67393982 epoch total loss 1.61375761\n",
      "Trained batch 584 batch loss 1.59924185 epoch total loss 1.6137327\n",
      "Trained batch 585 batch loss 1.62596858 epoch total loss 1.61375368\n",
      "Trained batch 586 batch loss 1.51187205 epoch total loss 1.61357987\n",
      "Trained batch 587 batch loss 1.61100841 epoch total loss 1.61357546\n",
      "Trained batch 588 batch loss 1.56775582 epoch total loss 1.61349761\n",
      "Trained batch 589 batch loss 1.50260305 epoch total loss 1.61330938\n",
      "Trained batch 590 batch loss 1.48565841 epoch total loss 1.61309302\n",
      "Trained batch 591 batch loss 1.50273085 epoch total loss 1.61290622\n",
      "Trained batch 592 batch loss 1.5353713 epoch total loss 1.61277533\n",
      "Trained batch 593 batch loss 1.50611043 epoch total loss 1.61259544\n",
      "Trained batch 594 batch loss 1.46408486 epoch total loss 1.61234546\n",
      "Trained batch 595 batch loss 1.51075315 epoch total loss 1.61217475\n",
      "Trained batch 596 batch loss 1.52063489 epoch total loss 1.61202109\n",
      "Trained batch 597 batch loss 1.58595121 epoch total loss 1.61197746\n",
      "Trained batch 598 batch loss 1.51517427 epoch total loss 1.61181557\n",
      "Trained batch 599 batch loss 1.63306034 epoch total loss 1.6118511\n",
      "Trained batch 600 batch loss 1.64249229 epoch total loss 1.61190212\n",
      "Trained batch 601 batch loss 1.42655051 epoch total loss 1.61159384\n",
      "Trained batch 602 batch loss 1.52700508 epoch total loss 1.61145329\n",
      "Trained batch 603 batch loss 1.50604272 epoch total loss 1.61127841\n",
      "Trained batch 604 batch loss 1.4506489 epoch total loss 1.61101246\n",
      "Trained batch 605 batch loss 1.43520379 epoch total loss 1.61072183\n",
      "Trained batch 606 batch loss 1.48467541 epoch total loss 1.61051381\n",
      "Trained batch 607 batch loss 1.52752 epoch total loss 1.61037707\n",
      "Trained batch 608 batch loss 1.49273014 epoch total loss 1.6101836\n",
      "Trained batch 609 batch loss 1.50668669 epoch total loss 1.61001372\n",
      "Trained batch 610 batch loss 1.57734418 epoch total loss 1.60996008\n",
      "Trained batch 611 batch loss 1.40156507 epoch total loss 1.60961902\n",
      "Trained batch 612 batch loss 1.34256756 epoch total loss 1.60918272\n",
      "Trained batch 613 batch loss 1.45073211 epoch total loss 1.60892427\n",
      "Trained batch 614 batch loss 1.41626084 epoch total loss 1.60861051\n",
      "Trained batch 615 batch loss 1.49646592 epoch total loss 1.60842812\n",
      "Trained batch 616 batch loss 1.60824311 epoch total loss 1.60842776\n",
      "Trained batch 617 batch loss 1.61430693 epoch total loss 1.6084373\n",
      "Trained batch 618 batch loss 1.54074645 epoch total loss 1.60832787\n",
      "Trained batch 619 batch loss 1.49442291 epoch total loss 1.60814381\n",
      "Trained batch 620 batch loss 1.49036527 epoch total loss 1.60795391\n",
      "Trained batch 621 batch loss 1.48725867 epoch total loss 1.60775948\n",
      "Trained batch 622 batch loss 1.41116166 epoch total loss 1.60744333\n",
      "Trained batch 623 batch loss 1.42071676 epoch total loss 1.60714364\n",
      "Trained batch 624 batch loss 1.53207278 epoch total loss 1.60702324\n",
      "Trained batch 625 batch loss 1.51027489 epoch total loss 1.60686851\n",
      "Trained batch 626 batch loss 1.49170136 epoch total loss 1.60668445\n",
      "Trained batch 627 batch loss 1.50291586 epoch total loss 1.60651898\n",
      "Trained batch 628 batch loss 1.47542071 epoch total loss 1.60631025\n",
      "Trained batch 629 batch loss 1.43879926 epoch total loss 1.60604393\n",
      "Trained batch 630 batch loss 1.49163437 epoch total loss 1.60586226\n",
      "Trained batch 631 batch loss 1.52488756 epoch total loss 1.60573399\n",
      "Trained batch 632 batch loss 1.57217085 epoch total loss 1.60568082\n",
      "Trained batch 633 batch loss 1.50330913 epoch total loss 1.60551906\n",
      "Trained batch 634 batch loss 1.48110199 epoch total loss 1.60532284\n",
      "Trained batch 635 batch loss 1.46149659 epoch total loss 1.60509634\n",
      "Trained batch 636 batch loss 1.42203248 epoch total loss 1.60480845\n",
      "Trained batch 637 batch loss 1.50253069 epoch total loss 1.60464787\n",
      "Trained batch 638 batch loss 1.56105733 epoch total loss 1.60457957\n",
      "Trained batch 639 batch loss 1.47788119 epoch total loss 1.6043812\n",
      "Trained batch 640 batch loss 1.48206329 epoch total loss 1.60419011\n",
      "Trained batch 641 batch loss 1.51781 epoch total loss 1.60405529\n",
      "Trained batch 642 batch loss 1.48676658 epoch total loss 1.60387266\n",
      "Trained batch 643 batch loss 1.46488404 epoch total loss 1.60365653\n",
      "Trained batch 644 batch loss 1.37492418 epoch total loss 1.60330129\n",
      "Trained batch 645 batch loss 1.46161175 epoch total loss 1.6030817\n",
      "Trained batch 646 batch loss 1.44336653 epoch total loss 1.60283446\n",
      "Trained batch 647 batch loss 1.46737599 epoch total loss 1.60262513\n",
      "Trained batch 648 batch loss 1.46716523 epoch total loss 1.60241604\n",
      "Trained batch 649 batch loss 1.48898959 epoch total loss 1.60224128\n",
      "Trained batch 650 batch loss 1.47521043 epoch total loss 1.60204589\n",
      "Trained batch 651 batch loss 1.54915404 epoch total loss 1.60196471\n",
      "Trained batch 652 batch loss 1.50470066 epoch total loss 1.60181558\n",
      "Trained batch 653 batch loss 1.44263196 epoch total loss 1.6015718\n",
      "Trained batch 654 batch loss 1.53257275 epoch total loss 1.60146642\n",
      "Trained batch 655 batch loss 1.53720391 epoch total loss 1.60136831\n",
      "Trained batch 656 batch loss 1.54952943 epoch total loss 1.60128927\n",
      "Trained batch 657 batch loss 1.50479245 epoch total loss 1.60114241\n",
      "Trained batch 658 batch loss 1.55070198 epoch total loss 1.60106564\n",
      "Trained batch 659 batch loss 1.43971193 epoch total loss 1.60082078\n",
      "Trained batch 660 batch loss 1.36402345 epoch total loss 1.60046196\n",
      "Trained batch 661 batch loss 1.52535558 epoch total loss 1.60034847\n",
      "Trained batch 662 batch loss 1.6142379 epoch total loss 1.60036945\n",
      "Trained batch 663 batch loss 1.46319759 epoch total loss 1.60016263\n",
      "Trained batch 664 batch loss 1.55192828 epoch total loss 1.60008991\n",
      "Trained batch 665 batch loss 1.53479218 epoch total loss 1.5999918\n",
      "Trained batch 666 batch loss 1.52917182 epoch total loss 1.59988546\n",
      "Trained batch 667 batch loss 1.541399 epoch total loss 1.59979773\n",
      "Trained batch 668 batch loss 1.34845805 epoch total loss 1.5994215\n",
      "Trained batch 669 batch loss 1.51043463 epoch total loss 1.59928846\n",
      "Trained batch 670 batch loss 1.49478626 epoch total loss 1.59913242\n",
      "Trained batch 671 batch loss 1.50553894 epoch total loss 1.59899282\n",
      "Trained batch 672 batch loss 1.54992437 epoch total loss 1.59891987\n",
      "Trained batch 673 batch loss 1.44258296 epoch total loss 1.59868765\n",
      "Trained batch 674 batch loss 1.38876438 epoch total loss 1.59837615\n",
      "Trained batch 675 batch loss 1.46941733 epoch total loss 1.59818506\n",
      "Trained batch 676 batch loss 1.48815084 epoch total loss 1.59802222\n",
      "Trained batch 677 batch loss 1.52552819 epoch total loss 1.59791517\n",
      "Trained batch 678 batch loss 1.54953301 epoch total loss 1.59784389\n",
      "Trained batch 679 batch loss 1.50122106 epoch total loss 1.59770155\n",
      "Trained batch 680 batch loss 1.54679835 epoch total loss 1.59762669\n",
      "Trained batch 681 batch loss 1.5541749 epoch total loss 1.59756291\n",
      "Trained batch 682 batch loss 1.46870339 epoch total loss 1.59737396\n",
      "Trained batch 683 batch loss 1.52563035 epoch total loss 1.59726894\n",
      "Trained batch 684 batch loss 1.42460895 epoch total loss 1.59701645\n",
      "Trained batch 685 batch loss 1.41562414 epoch total loss 1.59675169\n",
      "Trained batch 686 batch loss 1.36633134 epoch total loss 1.59641576\n",
      "Trained batch 687 batch loss 1.46371877 epoch total loss 1.59622264\n",
      "Trained batch 688 batch loss 1.4843986 epoch total loss 1.59606016\n",
      "Trained batch 689 batch loss 1.41354311 epoch total loss 1.59579527\n",
      "Trained batch 690 batch loss 1.62064958 epoch total loss 1.59583116\n",
      "Trained batch 691 batch loss 1.49233079 epoch total loss 1.59568143\n",
      "Trained batch 692 batch loss 1.33266902 epoch total loss 1.59530127\n",
      "Trained batch 693 batch loss 1.49444389 epoch total loss 1.59515572\n",
      "Trained batch 694 batch loss 1.57192254 epoch total loss 1.5951221\n",
      "Trained batch 695 batch loss 1.513834 epoch total loss 1.59500515\n",
      "Trained batch 696 batch loss 1.49825859 epoch total loss 1.59486616\n",
      "Trained batch 697 batch loss 1.6140244 epoch total loss 1.59489369\n",
      "Trained batch 698 batch loss 1.54598701 epoch total loss 1.5948236\n",
      "Trained batch 699 batch loss 1.61973262 epoch total loss 1.59485924\n",
      "Trained batch 700 batch loss 1.5598942 epoch total loss 1.59480941\n",
      "Trained batch 701 batch loss 1.52651381 epoch total loss 1.5947119\n",
      "Trained batch 702 batch loss 1.45922065 epoch total loss 1.5945189\n",
      "Trained batch 703 batch loss 1.55593562 epoch total loss 1.59446406\n",
      "Trained batch 704 batch loss 1.61694515 epoch total loss 1.59449589\n",
      "Trained batch 705 batch loss 1.55070603 epoch total loss 1.59443378\n",
      "Trained batch 706 batch loss 1.578933 epoch total loss 1.59441185\n",
      "Trained batch 707 batch loss 1.5317502 epoch total loss 1.59432328\n",
      "Trained batch 708 batch loss 1.5793376 epoch total loss 1.59430206\n",
      "Trained batch 709 batch loss 1.51687992 epoch total loss 1.59419286\n",
      "Trained batch 710 batch loss 1.47689366 epoch total loss 1.59402764\n",
      "Trained batch 711 batch loss 1.58612752 epoch total loss 1.59401667\n",
      "Trained batch 712 batch loss 1.60032511 epoch total loss 1.59402549\n",
      "Trained batch 713 batch loss 1.5180912 epoch total loss 1.59391904\n",
      "Trained batch 714 batch loss 1.43537807 epoch total loss 1.59369695\n",
      "Trained batch 715 batch loss 1.34096169 epoch total loss 1.5933435\n",
      "Trained batch 716 batch loss 1.46619391 epoch total loss 1.59316587\n",
      "Trained batch 717 batch loss 1.46623683 epoch total loss 1.59298885\n",
      "Trained batch 718 batch loss 1.45950532 epoch total loss 1.59280288\n",
      "Trained batch 719 batch loss 1.35529208 epoch total loss 1.59247255\n",
      "Trained batch 720 batch loss 1.29466 epoch total loss 1.59205902\n",
      "Trained batch 721 batch loss 1.37815583 epoch total loss 1.5917623\n",
      "Trained batch 722 batch loss 1.38819802 epoch total loss 1.59148037\n",
      "Trained batch 723 batch loss 1.43969297 epoch total loss 1.59127045\n",
      "Trained batch 724 batch loss 1.53486812 epoch total loss 1.5911926\n",
      "Trained batch 725 batch loss 1.56639099 epoch total loss 1.59115839\n",
      "Trained batch 726 batch loss 1.43830585 epoch total loss 1.59094799\n",
      "Trained batch 727 batch loss 1.54495704 epoch total loss 1.59088457\n",
      "Trained batch 728 batch loss 1.50130761 epoch total loss 1.59076166\n",
      "Trained batch 729 batch loss 1.50377512 epoch total loss 1.59064233\n",
      "Trained batch 730 batch loss 1.42281318 epoch total loss 1.5904125\n",
      "Trained batch 731 batch loss 1.5531776 epoch total loss 1.5903616\n",
      "Trained batch 732 batch loss 1.43097603 epoch total loss 1.59014392\n",
      "Trained batch 733 batch loss 1.43318248 epoch total loss 1.58992982\n",
      "Trained batch 734 batch loss 1.44539666 epoch total loss 1.589733\n",
      "Trained batch 735 batch loss 1.44277632 epoch total loss 1.58953297\n",
      "Trained batch 736 batch loss 1.39308262 epoch total loss 1.58926606\n",
      "Trained batch 737 batch loss 1.41632867 epoch total loss 1.58903146\n",
      "Trained batch 738 batch loss 1.3725841 epoch total loss 1.5887382\n",
      "Trained batch 739 batch loss 1.49045718 epoch total loss 1.58860517\n",
      "Trained batch 740 batch loss 1.43832994 epoch total loss 1.58840215\n",
      "Trained batch 741 batch loss 1.42838502 epoch total loss 1.58818614\n",
      "Trained batch 742 batch loss 1.47145391 epoch total loss 1.58802879\n",
      "Trained batch 743 batch loss 1.46699703 epoch total loss 1.58786595\n",
      "Trained batch 744 batch loss 1.61200523 epoch total loss 1.58789849\n",
      "Trained batch 745 batch loss 1.49267888 epoch total loss 1.5877707\n",
      "Trained batch 746 batch loss 1.46942186 epoch total loss 1.58761215\n",
      "Trained batch 747 batch loss 1.41565275 epoch total loss 1.58738196\n",
      "Trained batch 748 batch loss 1.44167495 epoch total loss 1.58718705\n",
      "Trained batch 749 batch loss 1.4276495 epoch total loss 1.58697402\n",
      "Trained batch 750 batch loss 1.51714861 epoch total loss 1.5868808\n",
      "Trained batch 751 batch loss 1.44262695 epoch total loss 1.58668876\n",
      "Trained batch 752 batch loss 1.45215487 epoch total loss 1.58650982\n",
      "Trained batch 753 batch loss 1.46182084 epoch total loss 1.58634424\n",
      "Trained batch 754 batch loss 1.50800085 epoch total loss 1.58624041\n",
      "Trained batch 755 batch loss 1.46834373 epoch total loss 1.58608437\n",
      "Trained batch 756 batch loss 1.5630213 epoch total loss 1.58605373\n",
      "Trained batch 757 batch loss 1.50126386 epoch total loss 1.58594167\n",
      "Trained batch 758 batch loss 1.48466563 epoch total loss 1.58580804\n",
      "Trained batch 759 batch loss 1.46717286 epoch total loss 1.58565176\n",
      "Trained batch 760 batch loss 1.45837092 epoch total loss 1.58548427\n",
      "Trained batch 761 batch loss 1.41200161 epoch total loss 1.58525622\n",
      "Trained batch 762 batch loss 1.4821651 epoch total loss 1.58512092\n",
      "Trained batch 763 batch loss 1.51506114 epoch total loss 1.58502913\n",
      "Trained batch 764 batch loss 1.32015204 epoch total loss 1.58468246\n",
      "Trained batch 765 batch loss 1.43913388 epoch total loss 1.58449209\n",
      "Trained batch 766 batch loss 1.46580112 epoch total loss 1.58433723\n",
      "Trained batch 767 batch loss 1.43130219 epoch total loss 1.58413768\n",
      "Trained batch 768 batch loss 1.53492308 epoch total loss 1.58407354\n",
      "Trained batch 769 batch loss 1.45085907 epoch total loss 1.58390021\n",
      "Trained batch 770 batch loss 1.50737429 epoch total loss 1.58380079\n",
      "Trained batch 771 batch loss 1.52240682 epoch total loss 1.58372128\n",
      "Trained batch 772 batch loss 1.36220515 epoch total loss 1.58343422\n",
      "Trained batch 773 batch loss 1.48927283 epoch total loss 1.58331239\n",
      "Trained batch 774 batch loss 1.43491352 epoch total loss 1.5831207\n",
      "Trained batch 775 batch loss 1.53457737 epoch total loss 1.583058\n",
      "Trained batch 776 batch loss 1.58365846 epoch total loss 1.58305883\n",
      "Trained batch 777 batch loss 1.49401212 epoch total loss 1.58294415\n",
      "Trained batch 778 batch loss 1.42458367 epoch total loss 1.58274066\n",
      "Trained batch 779 batch loss 1.37426674 epoch total loss 1.58247304\n",
      "Trained batch 780 batch loss 1.63254 epoch total loss 1.58253717\n",
      "Trained batch 781 batch loss 1.61806488 epoch total loss 1.58258271\n",
      "Trained batch 782 batch loss 1.71857786 epoch total loss 1.58275664\n",
      "Trained batch 783 batch loss 1.62101197 epoch total loss 1.5828054\n",
      "Trained batch 784 batch loss 1.36056399 epoch total loss 1.58252203\n",
      "Trained batch 785 batch loss 1.35430884 epoch total loss 1.58223128\n",
      "Trained batch 786 batch loss 1.41847563 epoch total loss 1.58202291\n",
      "Trained batch 787 batch loss 1.45385027 epoch total loss 1.58186007\n",
      "Trained batch 788 batch loss 1.44236577 epoch total loss 1.58168304\n",
      "Trained batch 789 batch loss 1.52899647 epoch total loss 1.58161628\n",
      "Trained batch 790 batch loss 1.46718383 epoch total loss 1.58147144\n",
      "Trained batch 791 batch loss 1.5116663 epoch total loss 1.58138323\n",
      "Trained batch 792 batch loss 1.51831365 epoch total loss 1.5813036\n",
      "Trained batch 793 batch loss 1.51053047 epoch total loss 1.58121431\n",
      "Trained batch 794 batch loss 1.52938795 epoch total loss 1.5811491\n",
      "Trained batch 795 batch loss 1.45423746 epoch total loss 1.58098948\n",
      "Trained batch 796 batch loss 1.45792341 epoch total loss 1.58083475\n",
      "Trained batch 797 batch loss 1.44088435 epoch total loss 1.58065927\n",
      "Trained batch 798 batch loss 1.5197022 epoch total loss 1.58058274\n",
      "Trained batch 799 batch loss 1.41046238 epoch total loss 1.58037\n",
      "Trained batch 800 batch loss 1.37390184 epoch total loss 1.58011186\n",
      "Trained batch 801 batch loss 1.42234564 epoch total loss 1.57991493\n",
      "Trained batch 802 batch loss 1.35485911 epoch total loss 1.57963431\n",
      "Trained batch 803 batch loss 1.25462139 epoch total loss 1.57922959\n",
      "Trained batch 804 batch loss 1.21136129 epoch total loss 1.57877195\n",
      "Trained batch 805 batch loss 1.23203588 epoch total loss 1.57834125\n",
      "Trained batch 806 batch loss 1.48048508 epoch total loss 1.57821977\n",
      "Trained batch 807 batch loss 1.62559712 epoch total loss 1.57827854\n",
      "Trained batch 808 batch loss 1.75920475 epoch total loss 1.57850242\n",
      "Trained batch 809 batch loss 1.58101177 epoch total loss 1.57850552\n",
      "Trained batch 810 batch loss 1.30017674 epoch total loss 1.57816195\n",
      "Trained batch 811 batch loss 1.54088283 epoch total loss 1.57811594\n",
      "Trained batch 812 batch loss 1.65274775 epoch total loss 1.57820785\n",
      "Trained batch 813 batch loss 1.54501414 epoch total loss 1.57816708\n",
      "Trained batch 814 batch loss 1.52598023 epoch total loss 1.57810295\n",
      "Trained batch 815 batch loss 1.51867163 epoch total loss 1.57803\n",
      "Trained batch 816 batch loss 1.53529763 epoch total loss 1.57797766\n",
      "Trained batch 817 batch loss 1.49767649 epoch total loss 1.57787931\n",
      "Trained batch 818 batch loss 1.46284723 epoch total loss 1.57773876\n",
      "Trained batch 819 batch loss 1.51454508 epoch total loss 1.57766163\n",
      "Trained batch 820 batch loss 1.4086597 epoch total loss 1.57745552\n",
      "Trained batch 821 batch loss 1.51674068 epoch total loss 1.57738161\n",
      "Trained batch 822 batch loss 1.57901442 epoch total loss 1.57738352\n",
      "Trained batch 823 batch loss 1.4030993 epoch total loss 1.57717168\n",
      "Trained batch 824 batch loss 1.30190897 epoch total loss 1.57683766\n",
      "Trained batch 825 batch loss 1.27777314 epoch total loss 1.57647514\n",
      "Trained batch 826 batch loss 1.3268466 epoch total loss 1.57617307\n",
      "Trained batch 827 batch loss 1.52046454 epoch total loss 1.57610571\n",
      "Trained batch 828 batch loss 1.51321292 epoch total loss 1.57602978\n",
      "Trained batch 829 batch loss 1.47965312 epoch total loss 1.57591343\n",
      "Trained batch 830 batch loss 1.53481674 epoch total loss 1.57586396\n",
      "Trained batch 831 batch loss 1.37169135 epoch total loss 1.57561827\n",
      "Trained batch 832 batch loss 1.338943 epoch total loss 1.57533383\n",
      "Trained batch 833 batch loss 1.47059464 epoch total loss 1.57520807\n",
      "Trained batch 834 batch loss 1.54111469 epoch total loss 1.57516718\n",
      "Trained batch 835 batch loss 1.44654334 epoch total loss 1.57501316\n",
      "Trained batch 836 batch loss 1.43193007 epoch total loss 1.57484198\n",
      "Trained batch 837 batch loss 1.39513683 epoch total loss 1.57462728\n",
      "Trained batch 838 batch loss 1.48893023 epoch total loss 1.574525\n",
      "Trained batch 839 batch loss 1.58031476 epoch total loss 1.57453191\n",
      "Trained batch 840 batch loss 1.55830169 epoch total loss 1.5745126\n",
      "Trained batch 841 batch loss 1.43511748 epoch total loss 1.57434678\n",
      "Trained batch 842 batch loss 1.42543745 epoch total loss 1.57416987\n",
      "Trained batch 843 batch loss 1.31790781 epoch total loss 1.57386589\n",
      "Trained batch 844 batch loss 1.34038138 epoch total loss 1.57358921\n",
      "Trained batch 845 batch loss 1.40979755 epoch total loss 1.57339537\n",
      "Trained batch 846 batch loss 1.39890313 epoch total loss 1.57318914\n",
      "Trained batch 847 batch loss 1.38666654 epoch total loss 1.57296896\n",
      "Trained batch 848 batch loss 1.44830465 epoch total loss 1.57282197\n",
      "Trained batch 849 batch loss 1.51493883 epoch total loss 1.57275379\n",
      "Trained batch 850 batch loss 1.58063233 epoch total loss 1.57276309\n",
      "Trained batch 851 batch loss 1.50398076 epoch total loss 1.57268238\n",
      "Trained batch 852 batch loss 1.47629142 epoch total loss 1.57256925\n",
      "Trained batch 853 batch loss 1.4752152 epoch total loss 1.57245505\n",
      "Trained batch 854 batch loss 1.45748818 epoch total loss 1.57232046\n",
      "Trained batch 855 batch loss 1.5732162 epoch total loss 1.57232153\n",
      "Trained batch 856 batch loss 1.43846154 epoch total loss 1.57216525\n",
      "Trained batch 857 batch loss 1.48284352 epoch total loss 1.57206094\n",
      "Trained batch 858 batch loss 1.44655824 epoch total loss 1.57191467\n",
      "Trained batch 859 batch loss 1.34844422 epoch total loss 1.57165444\n",
      "Trained batch 860 batch loss 1.36300015 epoch total loss 1.57141185\n",
      "Trained batch 861 batch loss 1.33290339 epoch total loss 1.57113481\n",
      "Trained batch 862 batch loss 1.32308936 epoch total loss 1.57084703\n",
      "Trained batch 863 batch loss 1.25082958 epoch total loss 1.57047629\n",
      "Trained batch 864 batch loss 1.54518652 epoch total loss 1.57044697\n",
      "Trained batch 865 batch loss 1.55990946 epoch total loss 1.57043481\n",
      "Trained batch 866 batch loss 1.542454 epoch total loss 1.57040262\n",
      "Trained batch 867 batch loss 1.66290283 epoch total loss 1.57050931\n",
      "Trained batch 868 batch loss 1.56770313 epoch total loss 1.5705061\n",
      "Trained batch 869 batch loss 1.52371061 epoch total loss 1.57045221\n",
      "Trained batch 870 batch loss 1.40966344 epoch total loss 1.57026744\n",
      "Trained batch 871 batch loss 1.4339757 epoch total loss 1.57011092\n",
      "Trained batch 872 batch loss 1.54638588 epoch total loss 1.57008374\n",
      "Trained batch 873 batch loss 1.50554407 epoch total loss 1.57000983\n",
      "Trained batch 874 batch loss 1.40838182 epoch total loss 1.56982481\n",
      "Trained batch 875 batch loss 1.47211683 epoch total loss 1.56971312\n",
      "Trained batch 876 batch loss 1.38398719 epoch total loss 1.56950116\n",
      "Trained batch 877 batch loss 1.42418838 epoch total loss 1.56933546\n",
      "Trained batch 878 batch loss 1.40099645 epoch total loss 1.56914377\n",
      "Trained batch 879 batch loss 1.41081738 epoch total loss 1.56896365\n",
      "Trained batch 880 batch loss 1.5588131 epoch total loss 1.56895208\n",
      "Trained batch 881 batch loss 1.57617354 epoch total loss 1.56896031\n",
      "Trained batch 882 batch loss 1.59950984 epoch total loss 1.56899488\n",
      "Trained batch 883 batch loss 1.61442363 epoch total loss 1.56904626\n",
      "Trained batch 884 batch loss 1.53986931 epoch total loss 1.56901336\n",
      "Trained batch 885 batch loss 1.53124285 epoch total loss 1.56897068\n",
      "Trained batch 886 batch loss 1.3995384 epoch total loss 1.56877947\n",
      "Trained batch 887 batch loss 1.23388457 epoch total loss 1.56840193\n",
      "Trained batch 888 batch loss 1.1698997 epoch total loss 1.56795311\n",
      "Trained batch 889 batch loss 1.34250355 epoch total loss 1.56769955\n",
      "Trained batch 890 batch loss 1.41117013 epoch total loss 1.56752372\n",
      "Trained batch 891 batch loss 1.48239219 epoch total loss 1.56742811\n",
      "Trained batch 892 batch loss 1.43576574 epoch total loss 1.56728053\n",
      "Trained batch 893 batch loss 1.42564535 epoch total loss 1.56712198\n",
      "Trained batch 894 batch loss 1.40692556 epoch total loss 1.56694281\n",
      "Trained batch 895 batch loss 1.26806951 epoch total loss 1.56660891\n",
      "Trained batch 896 batch loss 1.36811686 epoch total loss 1.56638741\n",
      "Trained batch 897 batch loss 1.35050631 epoch total loss 1.56614673\n",
      "Trained batch 898 batch loss 1.42176318 epoch total loss 1.56598592\n",
      "Trained batch 899 batch loss 1.38707113 epoch total loss 1.56578696\n",
      "Trained batch 900 batch loss 1.43226147 epoch total loss 1.56563854\n",
      "Trained batch 901 batch loss 1.37389994 epoch total loss 1.56542575\n",
      "Trained batch 902 batch loss 1.45952654 epoch total loss 1.56530833\n",
      "Trained batch 903 batch loss 1.60651994 epoch total loss 1.56535399\n",
      "Trained batch 904 batch loss 1.44527602 epoch total loss 1.56522119\n",
      "Trained batch 905 batch loss 1.38434422 epoch total loss 1.5650214\n",
      "Trained batch 906 batch loss 1.40275335 epoch total loss 1.56484222\n",
      "Trained batch 907 batch loss 1.49294055 epoch total loss 1.56476295\n",
      "Trained batch 908 batch loss 1.39307547 epoch total loss 1.56457388\n",
      "Trained batch 909 batch loss 1.51609302 epoch total loss 1.5645206\n",
      "Trained batch 910 batch loss 1.6252476 epoch total loss 1.56458724\n",
      "Trained batch 911 batch loss 1.5208199 epoch total loss 1.56453931\n",
      "Trained batch 912 batch loss 1.42678022 epoch total loss 1.56438816\n",
      "Trained batch 913 batch loss 1.43064713 epoch total loss 1.56424177\n",
      "Trained batch 914 batch loss 1.44466043 epoch total loss 1.56411099\n",
      "Trained batch 915 batch loss 1.3960743 epoch total loss 1.56392741\n",
      "Trained batch 916 batch loss 1.39702463 epoch total loss 1.56374514\n",
      "Trained batch 917 batch loss 1.46613503 epoch total loss 1.56363869\n",
      "Trained batch 918 batch loss 1.34612298 epoch total loss 1.5634017\n",
      "Trained batch 919 batch loss 1.37292778 epoch total loss 1.56319439\n",
      "Trained batch 920 batch loss 1.21429038 epoch total loss 1.56281507\n",
      "Trained batch 921 batch loss 1.19884324 epoch total loss 1.56241989\n",
      "Trained batch 922 batch loss 1.30200124 epoch total loss 1.56213748\n",
      "Trained batch 923 batch loss 1.54279 epoch total loss 1.56211662\n",
      "Trained batch 924 batch loss 1.55289102 epoch total loss 1.56210661\n",
      "Trained batch 925 batch loss 1.45633769 epoch total loss 1.56199217\n",
      "Trained batch 926 batch loss 1.5335865 epoch total loss 1.56196153\n",
      "Trained batch 927 batch loss 1.41798341 epoch total loss 1.5618062\n",
      "Trained batch 928 batch loss 1.39693105 epoch total loss 1.56162858\n",
      "Trained batch 929 batch loss 1.37780559 epoch total loss 1.56143069\n",
      "Trained batch 930 batch loss 1.44214761 epoch total loss 1.56130242\n",
      "Trained batch 931 batch loss 1.40960109 epoch total loss 1.56113935\n",
      "Trained batch 932 batch loss 1.40717244 epoch total loss 1.56097424\n",
      "Trained batch 933 batch loss 1.21043766 epoch total loss 1.56059861\n",
      "Trained batch 934 batch loss 1.30760145 epoch total loss 1.56032765\n",
      "Trained batch 935 batch loss 1.46398687 epoch total loss 1.56022465\n",
      "Trained batch 936 batch loss 1.44173086 epoch total loss 1.56009805\n",
      "Trained batch 937 batch loss 1.53171122 epoch total loss 1.56006789\n",
      "Trained batch 938 batch loss 1.56781971 epoch total loss 1.56007612\n",
      "Trained batch 939 batch loss 1.60998988 epoch total loss 1.56012928\n",
      "Trained batch 940 batch loss 1.58271039 epoch total loss 1.56015337\n",
      "Trained batch 941 batch loss 1.44870138 epoch total loss 1.56003499\n",
      "Trained batch 942 batch loss 1.5758214 epoch total loss 1.56005168\n",
      "Trained batch 943 batch loss 1.57724977 epoch total loss 1.56007\n",
      "Trained batch 944 batch loss 1.55692565 epoch total loss 1.56006658\n",
      "Trained batch 945 batch loss 1.53073978 epoch total loss 1.56003559\n",
      "Trained batch 946 batch loss 1.44155228 epoch total loss 1.5599103\n",
      "Trained batch 947 batch loss 1.32086873 epoch total loss 1.55965793\n",
      "Trained batch 948 batch loss 1.36696064 epoch total loss 1.55945468\n",
      "Trained batch 949 batch loss 1.40064681 epoch total loss 1.55928731\n",
      "Trained batch 950 batch loss 1.51533735 epoch total loss 1.55924106\n",
      "Trained batch 951 batch loss 1.50867093 epoch total loss 1.55918789\n",
      "Trained batch 952 batch loss 1.55087495 epoch total loss 1.55917919\n",
      "Trained batch 953 batch loss 1.61821 epoch total loss 1.55924118\n",
      "Trained batch 954 batch loss 1.44736862 epoch total loss 1.55912387\n",
      "Trained batch 955 batch loss 1.5134778 epoch total loss 1.55907607\n",
      "Trained batch 956 batch loss 1.37677515 epoch total loss 1.55888534\n",
      "Trained batch 957 batch loss 1.54144239 epoch total loss 1.5588671\n",
      "Trained batch 958 batch loss 1.46267021 epoch total loss 1.5587666\n",
      "Trained batch 959 batch loss 1.37630272 epoch total loss 1.55857646\n",
      "Trained batch 960 batch loss 1.47459745 epoch total loss 1.55848897\n",
      "Trained batch 961 batch loss 1.43210721 epoch total loss 1.55835748\n",
      "Trained batch 962 batch loss 1.41549969 epoch total loss 1.55820906\n",
      "Trained batch 963 batch loss 1.46353054 epoch total loss 1.55811071\n",
      "Trained batch 964 batch loss 1.44042122 epoch total loss 1.55798864\n",
      "Trained batch 965 batch loss 1.37198746 epoch total loss 1.55779576\n",
      "Trained batch 966 batch loss 1.48081231 epoch total loss 1.55771613\n",
      "Trained batch 967 batch loss 1.5393846 epoch total loss 1.55769718\n",
      "Trained batch 968 batch loss 1.44958901 epoch total loss 1.55758548\n",
      "Trained batch 969 batch loss 1.47041798 epoch total loss 1.55749559\n",
      "Trained batch 970 batch loss 1.3542738 epoch total loss 1.55728614\n",
      "Trained batch 971 batch loss 1.41144276 epoch total loss 1.55713594\n",
      "Trained batch 972 batch loss 1.4058578 epoch total loss 1.55698037\n",
      "Trained batch 973 batch loss 1.36357713 epoch total loss 1.55678153\n",
      "Trained batch 974 batch loss 1.45925951 epoch total loss 1.55668139\n",
      "Trained batch 975 batch loss 1.45785499 epoch total loss 1.55658007\n",
      "Trained batch 976 batch loss 1.48589993 epoch total loss 1.55650759\n",
      "Trained batch 977 batch loss 1.51700115 epoch total loss 1.55646706\n",
      "Trained batch 978 batch loss 1.46658254 epoch total loss 1.55637515\n",
      "Trained batch 979 batch loss 1.45313048 epoch total loss 1.55626965\n",
      "Trained batch 980 batch loss 1.49620485 epoch total loss 1.55620837\n",
      "Trained batch 981 batch loss 1.50772238 epoch total loss 1.5561589\n",
      "Trained batch 982 batch loss 1.45194507 epoch total loss 1.5560528\n",
      "Trained batch 983 batch loss 1.48486948 epoch total loss 1.55598032\n",
      "Trained batch 984 batch loss 1.41167819 epoch total loss 1.5558337\n",
      "Trained batch 985 batch loss 1.43544412 epoch total loss 1.55571139\n",
      "Trained batch 986 batch loss 1.50259924 epoch total loss 1.55565751\n",
      "Trained batch 987 batch loss 1.53272963 epoch total loss 1.55563426\n",
      "Trained batch 988 batch loss 1.56514955 epoch total loss 1.55564392\n",
      "Trained batch 989 batch loss 1.5396111 epoch total loss 1.5556277\n",
      "Trained batch 990 batch loss 1.51673234 epoch total loss 1.55558836\n",
      "Trained batch 991 batch loss 1.51075757 epoch total loss 1.55554307\n",
      "Trained batch 992 batch loss 1.60330343 epoch total loss 1.55559123\n",
      "Trained batch 993 batch loss 1.50669944 epoch total loss 1.55554199\n",
      "Trained batch 994 batch loss 1.45861661 epoch total loss 1.55544448\n",
      "Trained batch 995 batch loss 1.45743191 epoch total loss 1.55534589\n",
      "Trained batch 996 batch loss 1.4447403 epoch total loss 1.55523491\n",
      "Trained batch 997 batch loss 1.4838798 epoch total loss 1.55516326\n",
      "Trained batch 998 batch loss 1.31447721 epoch total loss 1.5549221\n",
      "Trained batch 999 batch loss 1.1920768 epoch total loss 1.55455887\n",
      "Trained batch 1000 batch loss 1.34163523 epoch total loss 1.55434597\n",
      "Trained batch 1001 batch loss 1.34184933 epoch total loss 1.55413365\n",
      "Trained batch 1002 batch loss 1.36805975 epoch total loss 1.55394793\n",
      "Trained batch 1003 batch loss 1.48084152 epoch total loss 1.55387497\n",
      "Trained batch 1004 batch loss 1.4318006 epoch total loss 1.55375338\n",
      "Trained batch 1005 batch loss 1.43842018 epoch total loss 1.5536387\n",
      "Trained batch 1006 batch loss 1.47042704 epoch total loss 1.55355597\n",
      "Trained batch 1007 batch loss 1.39433205 epoch total loss 1.55339777\n",
      "Trained batch 1008 batch loss 1.51351452 epoch total loss 1.55335832\n",
      "Trained batch 1009 batch loss 1.3626188 epoch total loss 1.55316925\n",
      "Trained batch 1010 batch loss 1.32946181 epoch total loss 1.55294776\n",
      "Trained batch 1011 batch loss 1.37774873 epoch total loss 1.55277455\n",
      "Trained batch 1012 batch loss 1.48336089 epoch total loss 1.552706\n",
      "Trained batch 1013 batch loss 1.48358679 epoch total loss 1.55263782\n",
      "Trained batch 1014 batch loss 1.50687265 epoch total loss 1.55259264\n",
      "Trained batch 1015 batch loss 1.59128392 epoch total loss 1.55263078\n",
      "Trained batch 1016 batch loss 1.58181 epoch total loss 1.55265951\n",
      "Trained batch 1017 batch loss 1.38907075 epoch total loss 1.55249858\n",
      "Trained batch 1018 batch loss 1.3513335 epoch total loss 1.55230105\n",
      "Trained batch 1019 batch loss 1.40893209 epoch total loss 1.55216026\n",
      "Trained batch 1020 batch loss 1.49362946 epoch total loss 1.55210292\n",
      "Trained batch 1021 batch loss 1.42149556 epoch total loss 1.55197501\n",
      "Trained batch 1022 batch loss 1.46352839 epoch total loss 1.55188847\n",
      "Trained batch 1023 batch loss 1.39858067 epoch total loss 1.55173862\n",
      "Trained batch 1024 batch loss 1.4257319 epoch total loss 1.5516156\n",
      "Trained batch 1025 batch loss 1.57530737 epoch total loss 1.55163872\n",
      "Trained batch 1026 batch loss 1.74585927 epoch total loss 1.55182803\n",
      "Trained batch 1027 batch loss 1.56346774 epoch total loss 1.55183935\n",
      "Trained batch 1028 batch loss 1.57271016 epoch total loss 1.55185974\n",
      "Trained batch 1029 batch loss 1.50669765 epoch total loss 1.55181587\n",
      "Trained batch 1030 batch loss 1.52515876 epoch total loss 1.55178988\n",
      "Trained batch 1031 batch loss 1.54264379 epoch total loss 1.55178106\n",
      "Trained batch 1032 batch loss 1.53949618 epoch total loss 1.55176914\n",
      "Trained batch 1033 batch loss 1.44769168 epoch total loss 1.55166841\n",
      "Trained batch 1034 batch loss 1.45329893 epoch total loss 1.55157316\n",
      "Trained batch 1035 batch loss 1.52166581 epoch total loss 1.55154419\n",
      "Trained batch 1036 batch loss 1.57626426 epoch total loss 1.55156815\n",
      "Trained batch 1037 batch loss 1.5254308 epoch total loss 1.55154288\n",
      "Trained batch 1038 batch loss 1.45630121 epoch total loss 1.55145109\n",
      "Trained batch 1039 batch loss 1.4967556 epoch total loss 1.5513984\n",
      "Trained batch 1040 batch loss 1.42805779 epoch total loss 1.5512799\n",
      "Trained batch 1041 batch loss 1.36254311 epoch total loss 1.55109859\n",
      "Trained batch 1042 batch loss 1.47192407 epoch total loss 1.55102253\n",
      "Trained batch 1043 batch loss 1.37974119 epoch total loss 1.55085838\n",
      "Trained batch 1044 batch loss 1.35290587 epoch total loss 1.55066872\n",
      "Trained batch 1045 batch loss 1.50445187 epoch total loss 1.55062449\n",
      "Trained batch 1046 batch loss 1.54192114 epoch total loss 1.55061615\n",
      "Trained batch 1047 batch loss 1.51226985 epoch total loss 1.55057955\n",
      "Trained batch 1048 batch loss 1.48297691 epoch total loss 1.55051506\n",
      "Trained batch 1049 batch loss 1.40985429 epoch total loss 1.55038106\n",
      "Trained batch 1050 batch loss 1.44249535 epoch total loss 1.55027831\n",
      "Trained batch 1051 batch loss 1.5501852 epoch total loss 1.55027819\n",
      "Trained batch 1052 batch loss 1.4987545 epoch total loss 1.55022931\n",
      "Trained batch 1053 batch loss 1.47587037 epoch total loss 1.55015862\n",
      "Trained batch 1054 batch loss 1.4452734 epoch total loss 1.55005908\n",
      "Trained batch 1055 batch loss 1.43237531 epoch total loss 1.54994762\n",
      "Trained batch 1056 batch loss 1.36724234 epoch total loss 1.54977453\n",
      "Trained batch 1057 batch loss 1.32607198 epoch total loss 1.54956281\n",
      "Trained batch 1058 batch loss 1.4343 epoch total loss 1.54945397\n",
      "Trained batch 1059 batch loss 1.4992758 epoch total loss 1.54940653\n",
      "Trained batch 1060 batch loss 1.44541514 epoch total loss 1.54930842\n",
      "Trained batch 1061 batch loss 1.40371358 epoch total loss 1.54917121\n",
      "Trained batch 1062 batch loss 1.40803766 epoch total loss 1.54903841\n",
      "Trained batch 1063 batch loss 1.33815968 epoch total loss 1.54883993\n",
      "Trained batch 1064 batch loss 1.46093059 epoch total loss 1.54875731\n",
      "Trained batch 1065 batch loss 1.4980489 epoch total loss 1.54870975\n",
      "Trained batch 1066 batch loss 1.57782626 epoch total loss 1.54873705\n",
      "Trained batch 1067 batch loss 1.76636744 epoch total loss 1.54894102\n",
      "Trained batch 1068 batch loss 1.59045243 epoch total loss 1.54897988\n",
      "Trained batch 1069 batch loss 1.57120872 epoch total loss 1.54900062\n",
      "Trained batch 1070 batch loss 1.57249117 epoch total loss 1.54902267\n",
      "Trained batch 1071 batch loss 1.45519066 epoch total loss 1.54893506\n",
      "Trained batch 1072 batch loss 1.29759288 epoch total loss 1.54870057\n",
      "Trained batch 1073 batch loss 1.43407321 epoch total loss 1.54859376\n",
      "Trained batch 1074 batch loss 1.47880769 epoch total loss 1.54852879\n",
      "Trained batch 1075 batch loss 1.5596664 epoch total loss 1.54853916\n",
      "Trained batch 1076 batch loss 1.47069836 epoch total loss 1.5484668\n",
      "Trained batch 1077 batch loss 1.41546822 epoch total loss 1.5483433\n",
      "Trained batch 1078 batch loss 1.33285058 epoch total loss 1.54814351\n",
      "Trained batch 1079 batch loss 1.34232056 epoch total loss 1.54795265\n",
      "Trained batch 1080 batch loss 1.4416728 epoch total loss 1.5478543\n",
      "Trained batch 1081 batch loss 1.42051125 epoch total loss 1.54773653\n",
      "Trained batch 1082 batch loss 1.34656978 epoch total loss 1.54755056\n",
      "Trained batch 1083 batch loss 1.41638923 epoch total loss 1.54742944\n",
      "Trained batch 1084 batch loss 1.46254849 epoch total loss 1.54735112\n",
      "Trained batch 1085 batch loss 1.53174126 epoch total loss 1.5473367\n",
      "Trained batch 1086 batch loss 1.64855456 epoch total loss 1.54742992\n",
      "Trained batch 1087 batch loss 1.58119404 epoch total loss 1.54746103\n",
      "Trained batch 1088 batch loss 1.46187484 epoch total loss 1.54738235\n",
      "Trained batch 1089 batch loss 1.45997393 epoch total loss 1.54730213\n",
      "Trained batch 1090 batch loss 1.43027163 epoch total loss 1.54719472\n",
      "Trained batch 1091 batch loss 1.4256736 epoch total loss 1.54708338\n",
      "Trained batch 1092 batch loss 1.45959425 epoch total loss 1.54700327\n",
      "Trained batch 1093 batch loss 1.4439292 epoch total loss 1.54690897\n",
      "Trained batch 1094 batch loss 1.53553629 epoch total loss 1.54689848\n",
      "Trained batch 1095 batch loss 1.45588589 epoch total loss 1.54681551\n",
      "Trained batch 1096 batch loss 1.36982381 epoch total loss 1.54665399\n",
      "Trained batch 1097 batch loss 1.49403548 epoch total loss 1.54660606\n",
      "Trained batch 1098 batch loss 1.5073967 epoch total loss 1.54657042\n",
      "Trained batch 1099 batch loss 1.43720245 epoch total loss 1.54647088\n",
      "Trained batch 1100 batch loss 1.50308526 epoch total loss 1.54643142\n",
      "Trained batch 1101 batch loss 1.42367709 epoch total loss 1.54632\n",
      "Trained batch 1102 batch loss 1.38459885 epoch total loss 1.54617321\n",
      "Trained batch 1103 batch loss 1.36781907 epoch total loss 1.54601157\n",
      "Trained batch 1104 batch loss 1.38806605 epoch total loss 1.54586852\n",
      "Trained batch 1105 batch loss 1.44181275 epoch total loss 1.54577422\n",
      "Trained batch 1106 batch loss 1.55631244 epoch total loss 1.54578376\n",
      "Trained batch 1107 batch loss 1.56968307 epoch total loss 1.54580534\n",
      "Trained batch 1108 batch loss 1.459746 epoch total loss 1.54572773\n",
      "Trained batch 1109 batch loss 1.47036457 epoch total loss 1.54565966\n",
      "Trained batch 1110 batch loss 1.50885785 epoch total loss 1.54562664\n",
      "Trained batch 1111 batch loss 1.47158909 epoch total loss 1.54555988\n",
      "Trained batch 1112 batch loss 1.60163951 epoch total loss 1.54561043\n",
      "Trained batch 1113 batch loss 1.39898646 epoch total loss 1.54547858\n",
      "Trained batch 1114 batch loss 1.41772294 epoch total loss 1.5453639\n",
      "Trained batch 1115 batch loss 1.48439121 epoch total loss 1.54530919\n",
      "Trained batch 1116 batch loss 1.417485 epoch total loss 1.54519463\n",
      "Trained batch 1117 batch loss 1.46983528 epoch total loss 1.54512727\n",
      "Trained batch 1118 batch loss 1.49892497 epoch total loss 1.54508591\n",
      "Trained batch 1119 batch loss 1.44174147 epoch total loss 1.54499352\n",
      "Trained batch 1120 batch loss 1.41414893 epoch total loss 1.54487669\n",
      "Trained batch 1121 batch loss 1.39952564 epoch total loss 1.54474711\n",
      "Trained batch 1122 batch loss 1.31915832 epoch total loss 1.54454613\n",
      "Trained batch 1123 batch loss 1.38447869 epoch total loss 1.54440355\n",
      "Trained batch 1124 batch loss 1.38012731 epoch total loss 1.5442574\n",
      "Trained batch 1125 batch loss 1.3872124 epoch total loss 1.54411781\n",
      "Trained batch 1126 batch loss 1.2671901 epoch total loss 1.54387188\n",
      "Trained batch 1127 batch loss 1.28478122 epoch total loss 1.54364204\n",
      "Trained batch 1128 batch loss 1.37588894 epoch total loss 1.54349327\n",
      "Trained batch 1129 batch loss 1.38807726 epoch total loss 1.54335558\n",
      "Trained batch 1130 batch loss 1.46001077 epoch total loss 1.54328179\n",
      "Trained batch 1131 batch loss 1.39809585 epoch total loss 1.54315341\n",
      "Trained batch 1132 batch loss 1.56591725 epoch total loss 1.54317355\n",
      "Trained batch 1133 batch loss 1.61415 epoch total loss 1.54323614\n",
      "Trained batch 1134 batch loss 1.57440782 epoch total loss 1.54326367\n",
      "Trained batch 1135 batch loss 1.58098912 epoch total loss 1.54329693\n",
      "Trained batch 1136 batch loss 1.41740429 epoch total loss 1.54318607\n",
      "Trained batch 1137 batch loss 1.37190425 epoch total loss 1.54303539\n",
      "Trained batch 1138 batch loss 1.41396976 epoch total loss 1.54292202\n",
      "Trained batch 1139 batch loss 1.35354936 epoch total loss 1.54275572\n",
      "Trained batch 1140 batch loss 1.40733147 epoch total loss 1.54263687\n",
      "Trained batch 1141 batch loss 1.42213953 epoch total loss 1.54253125\n",
      "Trained batch 1142 batch loss 1.41819978 epoch total loss 1.54242241\n",
      "Trained batch 1143 batch loss 1.39641404 epoch total loss 1.54229462\n",
      "Trained batch 1144 batch loss 1.32327724 epoch total loss 1.54210317\n",
      "Trained batch 1145 batch loss 1.33281231 epoch total loss 1.5419203\n",
      "Trained batch 1146 batch loss 1.42583025 epoch total loss 1.54181898\n",
      "Trained batch 1147 batch loss 1.44407785 epoch total loss 1.54173374\n",
      "Trained batch 1148 batch loss 1.31648064 epoch total loss 1.54153764\n",
      "Trained batch 1149 batch loss 1.40346694 epoch total loss 1.54141748\n",
      "Trained batch 1150 batch loss 1.36982739 epoch total loss 1.54126823\n",
      "Trained batch 1151 batch loss 1.32911289 epoch total loss 1.54108393\n",
      "Trained batch 1152 batch loss 1.35468042 epoch total loss 1.54092216\n",
      "Trained batch 1153 batch loss 1.47272122 epoch total loss 1.54086304\n",
      "Trained batch 1154 batch loss 1.54575157 epoch total loss 1.54086733\n",
      "Trained batch 1155 batch loss 1.71011961 epoch total loss 1.54101384\n",
      "Trained batch 1156 batch loss 1.56986141 epoch total loss 1.54103875\n",
      "Trained batch 1157 batch loss 1.37980485 epoch total loss 1.5408994\n",
      "Trained batch 1158 batch loss 1.47626603 epoch total loss 1.54084361\n",
      "Trained batch 1159 batch loss 1.47115326 epoch total loss 1.54078352\n",
      "Trained batch 1160 batch loss 1.42948461 epoch total loss 1.54068756\n",
      "Trained batch 1161 batch loss 1.43798518 epoch total loss 1.54059911\n",
      "Trained batch 1162 batch loss 1.49280131 epoch total loss 1.54055786\n",
      "Trained batch 1163 batch loss 1.51496 epoch total loss 1.54053593\n",
      "Trained batch 1164 batch loss 1.60764778 epoch total loss 1.54059362\n",
      "Trained batch 1165 batch loss 1.63407516 epoch total loss 1.54067385\n",
      "Trained batch 1166 batch loss 1.60200334 epoch total loss 1.54072642\n",
      "Trained batch 1167 batch loss 1.50167489 epoch total loss 1.54069304\n",
      "Trained batch 1168 batch loss 1.60156798 epoch total loss 1.54074514\n",
      "Trained batch 1169 batch loss 1.52325058 epoch total loss 1.54073012\n",
      "Trained batch 1170 batch loss 1.5066098 epoch total loss 1.54070091\n",
      "Trained batch 1171 batch loss 1.50455523 epoch total loss 1.54067\n",
      "Trained batch 1172 batch loss 1.44600391 epoch total loss 1.54058933\n",
      "Trained batch 1173 batch loss 1.41603851 epoch total loss 1.54048312\n",
      "Trained batch 1174 batch loss 1.44515133 epoch total loss 1.54040194\n",
      "Trained batch 1175 batch loss 1.47565484 epoch total loss 1.54034686\n",
      "Trained batch 1176 batch loss 1.46639383 epoch total loss 1.54028404\n",
      "Trained batch 1177 batch loss 1.43840146 epoch total loss 1.54019749\n",
      "Trained batch 1178 batch loss 1.35535121 epoch total loss 1.54004049\n",
      "Trained batch 1179 batch loss 1.49653792 epoch total loss 1.54000366\n",
      "Trained batch 1180 batch loss 1.51485419 epoch total loss 1.53998232\n",
      "Trained batch 1181 batch loss 1.56161308 epoch total loss 1.54000068\n",
      "Trained batch 1182 batch loss 1.44111884 epoch total loss 1.53991711\n",
      "Trained batch 1183 batch loss 1.34504974 epoch total loss 1.53975236\n",
      "Trained batch 1184 batch loss 1.35788167 epoch total loss 1.53959882\n",
      "Trained batch 1185 batch loss 1.44621992 epoch total loss 1.53952\n",
      "Trained batch 1186 batch loss 1.55024457 epoch total loss 1.53952909\n",
      "Trained batch 1187 batch loss 1.46404874 epoch total loss 1.53946543\n",
      "Trained batch 1188 batch loss 1.51830018 epoch total loss 1.53944767\n",
      "Trained batch 1189 batch loss 1.5564208 epoch total loss 1.53946185\n",
      "Trained batch 1190 batch loss 1.55951715 epoch total loss 1.53947878\n",
      "Trained batch 1191 batch loss 1.44552648 epoch total loss 1.53939986\n",
      "Trained batch 1192 batch loss 1.42656374 epoch total loss 1.53930521\n",
      "Trained batch 1193 batch loss 1.371696 epoch total loss 1.53916466\n",
      "Trained batch 1194 batch loss 1.35221088 epoch total loss 1.53900814\n",
      "Trained batch 1195 batch loss 1.53991389 epoch total loss 1.53900886\n",
      "Trained batch 1196 batch loss 1.40855169 epoch total loss 1.53889978\n",
      "Trained batch 1197 batch loss 1.39068937 epoch total loss 1.53877604\n",
      "Trained batch 1198 batch loss 1.37627459 epoch total loss 1.53864038\n",
      "Trained batch 1199 batch loss 1.36843181 epoch total loss 1.5384984\n",
      "Trained batch 1200 batch loss 1.43558478 epoch total loss 1.53841257\n",
      "Trained batch 1201 batch loss 1.40837622 epoch total loss 1.53830421\n",
      "Trained batch 1202 batch loss 1.55548775 epoch total loss 1.53831863\n",
      "Trained batch 1203 batch loss 1.50750601 epoch total loss 1.53829288\n",
      "Trained batch 1204 batch loss 1.41270626 epoch total loss 1.5381887\n",
      "Trained batch 1205 batch loss 1.45050383 epoch total loss 1.53811598\n",
      "Trained batch 1206 batch loss 1.37407124 epoch total loss 1.53797984\n",
      "Trained batch 1207 batch loss 1.38388467 epoch total loss 1.53785217\n",
      "Trained batch 1208 batch loss 1.47545505 epoch total loss 1.53780055\n",
      "Trained batch 1209 batch loss 1.59279788 epoch total loss 1.53784609\n",
      "Trained batch 1210 batch loss 1.48932767 epoch total loss 1.53780603\n",
      "Trained batch 1211 batch loss 1.42548108 epoch total loss 1.53771329\n",
      "Trained batch 1212 batch loss 1.32203519 epoch total loss 1.53753531\n",
      "Trained batch 1213 batch loss 1.42856109 epoch total loss 1.53744555\n",
      "Trained batch 1214 batch loss 1.53785825 epoch total loss 1.53744578\n",
      "Trained batch 1215 batch loss 1.42351139 epoch total loss 1.53735197\n",
      "Trained batch 1216 batch loss 1.40190458 epoch total loss 1.53724062\n",
      "Trained batch 1217 batch loss 1.42904508 epoch total loss 1.53715169\n",
      "Trained batch 1218 batch loss 1.42983949 epoch total loss 1.5370636\n",
      "Trained batch 1219 batch loss 1.50459027 epoch total loss 1.53703701\n",
      "Trained batch 1220 batch loss 1.43484807 epoch total loss 1.53695321\n",
      "Trained batch 1221 batch loss 1.37482309 epoch total loss 1.53682041\n",
      "Trained batch 1222 batch loss 1.39250839 epoch total loss 1.53670228\n",
      "Trained batch 1223 batch loss 1.43687606 epoch total loss 1.53662074\n",
      "Trained batch 1224 batch loss 1.30190229 epoch total loss 1.53642893\n",
      "Trained batch 1225 batch loss 1.35475516 epoch total loss 1.53628063\n",
      "Trained batch 1226 batch loss 1.32786322 epoch total loss 1.53611064\n",
      "Trained batch 1227 batch loss 1.32549834 epoch total loss 1.53593886\n",
      "Trained batch 1228 batch loss 1.33866954 epoch total loss 1.53577828\n",
      "Trained batch 1229 batch loss 1.25368202 epoch total loss 1.53554869\n",
      "Trained batch 1230 batch loss 1.4322958 epoch total loss 1.53546464\n",
      "Trained batch 1231 batch loss 1.3664031 epoch total loss 1.53532743\n",
      "Trained batch 1232 batch loss 1.37931728 epoch total loss 1.53520072\n",
      "Trained batch 1233 batch loss 1.41493511 epoch total loss 1.5351032\n",
      "Trained batch 1234 batch loss 1.46719539 epoch total loss 1.53504813\n",
      "Trained batch 1235 batch loss 1.42239535 epoch total loss 1.53495693\n",
      "Trained batch 1236 batch loss 1.43450904 epoch total loss 1.53487551\n",
      "Trained batch 1237 batch loss 1.41324234 epoch total loss 1.53477716\n",
      "Trained batch 1238 batch loss 1.34615636 epoch total loss 1.53462493\n",
      "Trained batch 1239 batch loss 1.36666584 epoch total loss 1.53448939\n",
      "Trained batch 1240 batch loss 1.39557588 epoch total loss 1.53437734\n",
      "Trained batch 1241 batch loss 1.42666626 epoch total loss 1.53429055\n",
      "Trained batch 1242 batch loss 1.28489542 epoch total loss 1.5340898\n",
      "Trained batch 1243 batch loss 1.31083107 epoch total loss 1.53391016\n",
      "Trained batch 1244 batch loss 1.40917361 epoch total loss 1.5338099\n",
      "Trained batch 1245 batch loss 1.34863234 epoch total loss 1.53366113\n",
      "Trained batch 1246 batch loss 1.36576629 epoch total loss 1.5335263\n",
      "Trained batch 1247 batch loss 1.33936453 epoch total loss 1.53337061\n",
      "Trained batch 1248 batch loss 1.33612621 epoch total loss 1.53321266\n",
      "Trained batch 1249 batch loss 1.2779547 epoch total loss 1.53300822\n",
      "Trained batch 1250 batch loss 1.59486604 epoch total loss 1.53305769\n",
      "Trained batch 1251 batch loss 1.59620571 epoch total loss 1.53310823\n",
      "Trained batch 1252 batch loss 1.42094016 epoch total loss 1.53301859\n",
      "Trained batch 1253 batch loss 1.50858986 epoch total loss 1.53299904\n",
      "Trained batch 1254 batch loss 1.6216476 epoch total loss 1.53306973\n",
      "Trained batch 1255 batch loss 1.65000832 epoch total loss 1.53316295\n",
      "Trained batch 1256 batch loss 1.52704072 epoch total loss 1.53315818\n",
      "Trained batch 1257 batch loss 1.36272705 epoch total loss 1.53302252\n",
      "Trained batch 1258 batch loss 1.42320406 epoch total loss 1.53293526\n",
      "Trained batch 1259 batch loss 1.4603734 epoch total loss 1.53287756\n",
      "Trained batch 1260 batch loss 1.38041079 epoch total loss 1.53275645\n",
      "Trained batch 1261 batch loss 1.36595082 epoch total loss 1.53262424\n",
      "Trained batch 1262 batch loss 1.50484586 epoch total loss 1.53260231\n",
      "Trained batch 1263 batch loss 1.33949447 epoch total loss 1.53244936\n",
      "Trained batch 1264 batch loss 1.36013901 epoch total loss 1.53231299\n",
      "Trained batch 1265 batch loss 1.33347225 epoch total loss 1.53215587\n",
      "Trained batch 1266 batch loss 1.37078285 epoch total loss 1.53202832\n",
      "Trained batch 1267 batch loss 1.41589308 epoch total loss 1.53193665\n",
      "Trained batch 1268 batch loss 1.40686846 epoch total loss 1.53183806\n",
      "Trained batch 1269 batch loss 1.39899671 epoch total loss 1.53173339\n",
      "Trained batch 1270 batch loss 1.32831383 epoch total loss 1.5315733\n",
      "Trained batch 1271 batch loss 1.35681438 epoch total loss 1.53143573\n",
      "Trained batch 1272 batch loss 1.50791264 epoch total loss 1.53141725\n",
      "Trained batch 1273 batch loss 1.51063585 epoch total loss 1.53140092\n",
      "Trained batch 1274 batch loss 1.43185711 epoch total loss 1.53132284\n",
      "Trained batch 1275 batch loss 1.46468616 epoch total loss 1.53127062\n",
      "Trained batch 1276 batch loss 1.29122794 epoch total loss 1.53108251\n",
      "Trained batch 1277 batch loss 1.33245122 epoch total loss 1.53092694\n",
      "Trained batch 1278 batch loss 1.45963836 epoch total loss 1.53087103\n",
      "Trained batch 1279 batch loss 1.2790072 epoch total loss 1.53067422\n",
      "Trained batch 1280 batch loss 1.32620788 epoch total loss 1.53051448\n",
      "Trained batch 1281 batch loss 1.36889052 epoch total loss 1.53038824\n",
      "Trained batch 1282 batch loss 1.35460162 epoch total loss 1.53025115\n",
      "Trained batch 1283 batch loss 1.41620541 epoch total loss 1.53016233\n",
      "Trained batch 1284 batch loss 1.33985162 epoch total loss 1.53001404\n",
      "Trained batch 1285 batch loss 1.44652331 epoch total loss 1.52994907\n",
      "Trained batch 1286 batch loss 1.4954834 epoch total loss 1.52992237\n",
      "Trained batch 1287 batch loss 1.40468824 epoch total loss 1.52982497\n",
      "Trained batch 1288 batch loss 1.30009353 epoch total loss 1.52964664\n",
      "Trained batch 1289 batch loss 1.29292154 epoch total loss 1.52946293\n",
      "Trained batch 1290 batch loss 1.28921187 epoch total loss 1.52927673\n",
      "Trained batch 1291 batch loss 1.39744496 epoch total loss 1.52917457\n",
      "Trained batch 1292 batch loss 1.4536531 epoch total loss 1.52911615\n",
      "Trained batch 1293 batch loss 1.43977928 epoch total loss 1.52904701\n",
      "Trained batch 1294 batch loss 1.43702626 epoch total loss 1.52897596\n",
      "Trained batch 1295 batch loss 1.40530789 epoch total loss 1.52888048\n",
      "Trained batch 1296 batch loss 1.40249264 epoch total loss 1.52878284\n",
      "Trained batch 1297 batch loss 1.41447711 epoch total loss 1.52869475\n",
      "Trained batch 1298 batch loss 1.41492724 epoch total loss 1.52860701\n",
      "Trained batch 1299 batch loss 1.31442487 epoch total loss 1.52844214\n",
      "Trained batch 1300 batch loss 1.49238396 epoch total loss 1.52841449\n",
      "Trained batch 1301 batch loss 1.45561802 epoch total loss 1.52835846\n",
      "Trained batch 1302 batch loss 1.55403447 epoch total loss 1.52837825\n",
      "Trained batch 1303 batch loss 1.4023304 epoch total loss 1.52828157\n",
      "Trained batch 1304 batch loss 1.40870082 epoch total loss 1.52818978\n",
      "Trained batch 1305 batch loss 1.30809557 epoch total loss 1.52802122\n",
      "Trained batch 1306 batch loss 1.39020431 epoch total loss 1.52791572\n",
      "Trained batch 1307 batch loss 1.38213599 epoch total loss 1.52780414\n",
      "Trained batch 1308 batch loss 1.31105173 epoch total loss 1.52763844\n",
      "Trained batch 1309 batch loss 1.34320092 epoch total loss 1.52749753\n",
      "Trained batch 1310 batch loss 1.48942113 epoch total loss 1.52746844\n",
      "Trained batch 1311 batch loss 1.45845306 epoch total loss 1.52741587\n",
      "Trained batch 1312 batch loss 1.41247439 epoch total loss 1.52732825\n",
      "Trained batch 1313 batch loss 1.40118933 epoch total loss 1.52723217\n",
      "Trained batch 1314 batch loss 1.55805194 epoch total loss 1.52725565\n",
      "Trained batch 1315 batch loss 1.45606577 epoch total loss 1.52720153\n",
      "Trained batch 1316 batch loss 1.43777263 epoch total loss 1.52713358\n",
      "Trained batch 1317 batch loss 1.4818933 epoch total loss 1.52709925\n",
      "Trained batch 1318 batch loss 1.47103906 epoch total loss 1.52705669\n",
      "Trained batch 1319 batch loss 1.412992 epoch total loss 1.52697027\n",
      "Trained batch 1320 batch loss 1.37491453 epoch total loss 1.52685499\n",
      "Trained batch 1321 batch loss 1.36464643 epoch total loss 1.52673221\n",
      "Trained batch 1322 batch loss 1.26211834 epoch total loss 1.52653205\n",
      "Trained batch 1323 batch loss 1.3928771 epoch total loss 1.52643096\n",
      "Trained batch 1324 batch loss 1.36686635 epoch total loss 1.52631044\n",
      "Trained batch 1325 batch loss 1.44750834 epoch total loss 1.52625096\n",
      "Trained batch 1326 batch loss 1.44774723 epoch total loss 1.52619171\n",
      "Trained batch 1327 batch loss 1.5028069 epoch total loss 1.52617407\n",
      "Trained batch 1328 batch loss 1.67652345 epoch total loss 1.52628732\n",
      "Trained batch 1329 batch loss 1.53709221 epoch total loss 1.52629542\n",
      "Trained batch 1330 batch loss 1.44953322 epoch total loss 1.52623773\n",
      "Trained batch 1331 batch loss 1.36600661 epoch total loss 1.52611732\n",
      "Trained batch 1332 batch loss 1.22126162 epoch total loss 1.52588856\n",
      "Trained batch 1333 batch loss 1.16279316 epoch total loss 1.52561617\n",
      "Trained batch 1334 batch loss 1.37782216 epoch total loss 1.52550542\n",
      "Trained batch 1335 batch loss 1.33810604 epoch total loss 1.525365\n",
      "Trained batch 1336 batch loss 1.13660133 epoch total loss 1.52507401\n",
      "Trained batch 1337 batch loss 1.14433169 epoch total loss 1.52478921\n",
      "Trained batch 1338 batch loss 1.19814301 epoch total loss 1.52454507\n",
      "Trained batch 1339 batch loss 1.24851167 epoch total loss 1.52433896\n",
      "Trained batch 1340 batch loss 1.35942078 epoch total loss 1.52421582\n",
      "Trained batch 1341 batch loss 1.4189167 epoch total loss 1.52413738\n",
      "Trained batch 1342 batch loss 1.49240327 epoch total loss 1.52411366\n",
      "Trained batch 1343 batch loss 1.56761789 epoch total loss 1.52414608\n",
      "Trained batch 1344 batch loss 1.45318985 epoch total loss 1.52409327\n",
      "Trained batch 1345 batch loss 1.45108366 epoch total loss 1.52403903\n",
      "Trained batch 1346 batch loss 1.48446727 epoch total loss 1.52400959\n",
      "Trained batch 1347 batch loss 1.39648926 epoch total loss 1.52391493\n",
      "Trained batch 1348 batch loss 1.37269318 epoch total loss 1.52380276\n",
      "Trained batch 1349 batch loss 1.37807858 epoch total loss 1.52369487\n",
      "Trained batch 1350 batch loss 1.3663559 epoch total loss 1.52357841\n",
      "Trained batch 1351 batch loss 1.30659282 epoch total loss 1.52341783\n",
      "Trained batch 1352 batch loss 1.30709064 epoch total loss 1.52325785\n",
      "Trained batch 1353 batch loss 1.37911987 epoch total loss 1.52315128\n",
      "Trained batch 1354 batch loss 1.3965205 epoch total loss 1.52305782\n",
      "Trained batch 1355 batch loss 1.3077116 epoch total loss 1.52289879\n",
      "Trained batch 1356 batch loss 1.36729681 epoch total loss 1.52278399\n",
      "Trained batch 1357 batch loss 1.29272509 epoch total loss 1.52261436\n",
      "Trained batch 1358 batch loss 1.39441967 epoch total loss 1.52252007\n",
      "Trained batch 1359 batch loss 1.45057988 epoch total loss 1.52246726\n",
      "Trained batch 1360 batch loss 1.45787072 epoch total loss 1.52241969\n",
      "Trained batch 1361 batch loss 1.4603107 epoch total loss 1.52237391\n",
      "Trained batch 1362 batch loss 1.48132968 epoch total loss 1.52234387\n",
      "Trained batch 1363 batch loss 1.37676585 epoch total loss 1.52223706\n",
      "Trained batch 1364 batch loss 1.48672926 epoch total loss 1.52221107\n",
      "Trained batch 1365 batch loss 1.40151167 epoch total loss 1.52212274\n",
      "Trained batch 1366 batch loss 1.37902701 epoch total loss 1.52201784\n",
      "Trained batch 1367 batch loss 1.42674971 epoch total loss 1.52194822\n",
      "Trained batch 1368 batch loss 1.42854738 epoch total loss 1.52187991\n",
      "Trained batch 1369 batch loss 1.20540488 epoch total loss 1.52164865\n",
      "Trained batch 1370 batch loss 1.22034264 epoch total loss 1.52142882\n",
      "Trained batch 1371 batch loss 1.20486128 epoch total loss 1.5211978\n",
      "Trained batch 1372 batch loss 1.32472849 epoch total loss 1.52105463\n",
      "Trained batch 1373 batch loss 1.4597435 epoch total loss 1.52100992\n",
      "Trained batch 1374 batch loss 1.46783328 epoch total loss 1.52097118\n",
      "Trained batch 1375 batch loss 1.55498743 epoch total loss 1.52099597\n",
      "Trained batch 1376 batch loss 1.4309715 epoch total loss 1.52093041\n",
      "Trained batch 1377 batch loss 1.47176027 epoch total loss 1.52089465\n",
      "Trained batch 1378 batch loss 1.47019958 epoch total loss 1.52085793\n",
      "Trained batch 1379 batch loss 1.4694066 epoch total loss 1.52082062\n",
      "Trained batch 1380 batch loss 1.42725682 epoch total loss 1.52075279\n",
      "Trained batch 1381 batch loss 1.48236036 epoch total loss 1.52072513\n",
      "Trained batch 1382 batch loss 1.49355435 epoch total loss 1.52070546\n",
      "Trained batch 1383 batch loss 1.58606493 epoch total loss 1.52075279\n",
      "Trained batch 1384 batch loss 1.52396381 epoch total loss 1.52075517\n",
      "Trained batch 1385 batch loss 1.45601165 epoch total loss 1.52070844\n",
      "Trained batch 1386 batch loss 1.40381145 epoch total loss 1.52062404\n",
      "Trained batch 1387 batch loss 1.30311513 epoch total loss 1.52046728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 03:25:14.863390: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "2025-03-19 03:25:14.863440: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "\t [[RemoteCall]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1388 batch loss 1.29290175 epoch total loss 1.52030337\n",
      "Epoch 1 train loss 1.5203033685684204\n",
      "Validated batch 1 batch loss 1.28493476\n",
      "Validated batch 2 batch loss 1.43440437\n",
      "Validated batch 3 batch loss 1.4480598\n",
      "Validated batch 4 batch loss 1.35865903\n",
      "Validated batch 5 batch loss 1.47185028\n",
      "Validated batch 6 batch loss 1.51156712\n",
      "Validated batch 7 batch loss 1.29794741\n",
      "Validated batch 8 batch loss 1.45064735\n",
      "Validated batch 9 batch loss 1.38685167\n",
      "Validated batch 10 batch loss 1.47258615\n",
      "Validated batch 11 batch loss 1.41917074\n",
      "Validated batch 12 batch loss 1.2795105\n",
      "Validated batch 13 batch loss 1.30917954\n",
      "Validated batch 14 batch loss 1.4421401\n",
      "Validated batch 15 batch loss 1.40162873\n",
      "Validated batch 16 batch loss 1.39026904\n",
      "Validated batch 17 batch loss 1.40413117\n",
      "Validated batch 18 batch loss 1.39431322\n",
      "Validated batch 19 batch loss 1.43902063\n",
      "Validated batch 20 batch loss 1.50886416\n",
      "Validated batch 21 batch loss 1.44692385\n",
      "Validated batch 22 batch loss 1.4403789\n",
      "Validated batch 23 batch loss 1.32293367\n",
      "Validated batch 24 batch loss 1.36399746\n",
      "Validated batch 25 batch loss 1.4270674\n",
      "Validated batch 26 batch loss 1.35991168\n",
      "Validated batch 27 batch loss 1.33019173\n",
      "Validated batch 28 batch loss 1.327\n",
      "Validated batch 29 batch loss 1.45931482\n",
      "Validated batch 30 batch loss 1.40561342\n",
      "Validated batch 31 batch loss 1.34790087\n",
      "Validated batch 32 batch loss 1.39459491\n",
      "Validated batch 33 batch loss 1.4172107\n",
      "Validated batch 34 batch loss 1.36732984\n",
      "Validated batch 35 batch loss 1.36787605\n",
      "Validated batch 36 batch loss 1.4390204\n",
      "Validated batch 37 batch loss 1.41271424\n",
      "Validated batch 38 batch loss 1.49004245\n",
      "Validated batch 39 batch loss 1.48118019\n",
      "Validated batch 40 batch loss 1.40480971\n",
      "Validated batch 41 batch loss 1.48329127\n",
      "Validated batch 42 batch loss 1.32683063\n",
      "Validated batch 43 batch loss 1.41069448\n",
      "Validated batch 44 batch loss 1.39171624\n",
      "Validated batch 45 batch loss 1.45800197\n",
      "Validated batch 46 batch loss 1.54855943\n",
      "Validated batch 47 batch loss 1.38743544\n",
      "Validated batch 48 batch loss 1.45724595\n",
      "Validated batch 49 batch loss 1.33971369\n",
      "Validated batch 50 batch loss 1.48618186\n",
      "Validated batch 51 batch loss 1.43757486\n",
      "Validated batch 52 batch loss 1.45874751\n",
      "Validated batch 53 batch loss 1.48835373\n",
      "Validated batch 54 batch loss 1.47029614\n",
      "Validated batch 55 batch loss 1.476264\n",
      "Validated batch 56 batch loss 1.42564762\n",
      "Validated batch 57 batch loss 1.49013984\n",
      "Validated batch 58 batch loss 1.47255707\n",
      "Validated batch 59 batch loss 1.42280054\n",
      "Validated batch 60 batch loss 1.52004087\n",
      "Validated batch 61 batch loss 1.44986141\n",
      "Validated batch 62 batch loss 1.41925704\n",
      "Validated batch 63 batch loss 1.55151343\n",
      "Validated batch 64 batch loss 1.22144306\n",
      "Validated batch 65 batch loss 1.43294525\n",
      "Validated batch 66 batch loss 1.33759069\n",
      "Validated batch 67 batch loss 1.4116025\n",
      "Validated batch 68 batch loss 1.51199687\n",
      "Validated batch 69 batch loss 1.36766994\n",
      "Validated batch 70 batch loss 1.49570155\n",
      "Validated batch 71 batch loss 1.38379633\n",
      "Validated batch 72 batch loss 1.40231776\n",
      "Validated batch 73 batch loss 1.33482754\n",
      "Validated batch 74 batch loss 1.4140594\n",
      "Validated batch 75 batch loss 1.5436033\n",
      "Validated batch 76 batch loss 1.34398055\n",
      "Validated batch 77 batch loss 1.43960142\n",
      "Validated batch 78 batch loss 1.44137692\n",
      "Validated batch 79 batch loss 1.44334316\n",
      "Validated batch 80 batch loss 1.44163871\n",
      "Validated batch 81 batch loss 1.296996\n",
      "Validated batch 82 batch loss 1.50471115\n",
      "Validated batch 83 batch loss 1.45263934\n",
      "Validated batch 84 batch loss 1.43883884\n",
      "Validated batch 85 batch loss 1.41838384\n",
      "Validated batch 86 batch loss 1.42815733\n",
      "Validated batch 87 batch loss 1.303931\n",
      "Validated batch 88 batch loss 1.38561702\n",
      "Validated batch 89 batch loss 1.35952592\n",
      "Validated batch 90 batch loss 1.35433161\n",
      "Validated batch 91 batch loss 1.42445457\n",
      "Validated batch 92 batch loss 1.4086678\n",
      "Validated batch 93 batch loss 1.39605403\n",
      "Validated batch 94 batch loss 1.33750629\n",
      "Validated batch 95 batch loss 1.40105438\n",
      "Validated batch 96 batch loss 1.39260674\n",
      "Validated batch 97 batch loss 1.36975086\n",
      "Validated batch 98 batch loss 1.4862541\n",
      "Validated batch 99 batch loss 1.41298413\n",
      "Validated batch 100 batch loss 1.48673832\n",
      "Validated batch 101 batch loss 1.48061085\n",
      "Validated batch 102 batch loss 1.44778597\n",
      "Validated batch 103 batch loss 1.40959024\n",
      "Validated batch 104 batch loss 1.50414228\n",
      "Validated batch 105 batch loss 1.46175456\n",
      "Validated batch 106 batch loss 1.49044478\n",
      "Validated batch 107 batch loss 1.48903453\n",
      "Validated batch 108 batch loss 1.49125719\n",
      "Validated batch 109 batch loss 1.49407816\n",
      "Validated batch 110 batch loss 1.30800581\n",
      "Validated batch 111 batch loss 1.41038156\n",
      "Validated batch 112 batch loss 1.47162461\n",
      "Validated batch 113 batch loss 1.50963569\n",
      "Validated batch 114 batch loss 1.44205737\n",
      "Validated batch 115 batch loss 1.39863634\n",
      "Validated batch 116 batch loss 1.55374908\n",
      "Validated batch 117 batch loss 1.38843012\n",
      "Validated batch 118 batch loss 1.42628431\n",
      "Validated batch 119 batch loss 1.44678903\n",
      "Validated batch 120 batch loss 1.37259924\n",
      "Validated batch 121 batch loss 1.43361664\n",
      "Validated batch 122 batch loss 1.41108501\n",
      "Validated batch 123 batch loss 1.34873986\n",
      "Validated batch 124 batch loss 1.3818661\n",
      "Validated batch 125 batch loss 1.39261889\n",
      "Validated batch 126 batch loss 1.42237\n",
      "Validated batch 127 batch loss 1.44880712\n",
      "Validated batch 128 batch loss 1.4228375\n",
      "Validated batch 129 batch loss 1.34151924\n",
      "Validated batch 130 batch loss 1.39374375\n",
      "Validated batch 131 batch loss 1.44853783\n",
      "Validated batch 132 batch loss 1.42282569\n",
      "Validated batch 133 batch loss 1.45749235\n",
      "Validated batch 134 batch loss 1.4895792\n",
      "Validated batch 135 batch loss 1.64335954\n",
      "Validated batch 136 batch loss 1.52904963\n",
      "Validated batch 137 batch loss 1.42753065\n",
      "Validated batch 138 batch loss 1.3286345\n",
      "Validated batch 139 batch loss 1.34802592\n",
      "Validated batch 140 batch loss 1.31553817\n",
      "Validated batch 141 batch loss 1.41277933\n",
      "Validated batch 142 batch loss 1.39662898\n",
      "Validated batch 143 batch loss 1.3537544\n",
      "Validated batch 144 batch loss 1.42859507\n",
      "Validated batch 145 batch loss 1.39483118\n",
      "Validated batch 146 batch loss 1.45735872\n",
      "Validated batch 147 batch loss 1.49462068\n",
      "Validated batch 148 batch loss 1.36546338\n",
      "Validated batch 149 batch loss 1.53990507\n",
      "Validated batch 150 batch loss 1.49453568\n",
      "Validated batch 151 batch loss 1.3329792\n",
      "Validated batch 152 batch loss 1.40899956\n",
      "Validated batch 153 batch loss 1.46709681\n",
      "Validated batch 154 batch loss 1.38521886\n",
      "Validated batch 155 batch loss 1.52448535\n",
      "Validated batch 156 batch loss 1.40381467\n",
      "Validated batch 157 batch loss 1.47525191\n",
      "Validated batch 158 batch loss 1.37755728\n",
      "Validated batch 159 batch loss 1.41888964\n",
      "Validated batch 160 batch loss 1.4404422\n",
      "Validated batch 161 batch loss 1.39480674\n",
      "Validated batch 162 batch loss 1.47886252\n",
      "Validated batch 163 batch loss 1.47637486\n",
      "Validated batch 164 batch loss 1.3608427\n",
      "Validated batch 165 batch loss 1.37907958\n",
      "Validated batch 166 batch loss 1.45612824\n",
      "Validated batch 167 batch loss 1.4000479\n",
      "Validated batch 168 batch loss 1.41233087\n",
      "Validated batch 169 batch loss 1.3707453\n",
      "Validated batch 170 batch loss 1.3498106\n",
      "Validated batch 171 batch loss 1.53090978\n",
      "Validated batch 172 batch loss 1.37359047\n",
      "Validated batch 173 batch loss 1.33100808\n",
      "Validated batch 174 batch loss 1.3576287\n",
      "Validated batch 175 batch loss 1.54610205\n",
      "Validated batch 176 batch loss 1.47015142\n",
      "Validated batch 177 batch loss 1.51056814\n",
      "Validated batch 178 batch loss 1.36020958\n",
      "Validated batch 179 batch loss 1.53290534\n",
      "Validated batch 180 batch loss 1.41619599\n",
      "Validated batch 181 batch loss 1.48000669\n",
      "Validated batch 182 batch loss 1.49287164\n",
      "Validated batch 183 batch loss 1.25457239\n",
      "Validated batch 184 batch loss 1.37154925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 03:25:36.161657: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "2025-03-19 03:25:36.161694: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "\t [[RemoteCall]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 185 batch loss 1.55904174\n",
      "Epoch 1 val loss 1.4224616289138794\n",
      "Model /home/minho/Desktop/aiffel/pose_estimation/models/model-epoch-1-loss-1.4225.weights.h5 saved.\n",
      "Start epoch 2 with learning rate 0.001\n",
      "Start distributed training...\n",
      "Trained batch 1 batch loss 1.41157377 epoch total loss 1.41157377\n",
      "Trained batch 2 batch loss 1.45175028 epoch total loss 1.43166208\n",
      "Trained batch 3 batch loss 1.30855966 epoch total loss 1.39062786\n",
      "Trained batch 4 batch loss 1.33849967 epoch total loss 1.37759578\n",
      "Trained batch 5 batch loss 1.34507692 epoch total loss 1.37109208\n",
      "Trained batch 6 batch loss 1.32381022 epoch total loss 1.36321175\n",
      "Trained batch 7 batch loss 1.2610333 epoch total loss 1.34861481\n",
      "Trained batch 8 batch loss 1.3336674 epoch total loss 1.34674644\n",
      "Trained batch 9 batch loss 1.29978776 epoch total loss 1.34152877\n",
      "Trained batch 10 batch loss 1.35966086 epoch total loss 1.34334207\n",
      "Trained batch 11 batch loss 1.29084635 epoch total loss 1.33856964\n",
      "Trained batch 12 batch loss 1.25731087 epoch total loss 1.33179808\n",
      "Trained batch 13 batch loss 1.30360711 epoch total loss 1.32962966\n",
      "Trained batch 14 batch loss 1.42871499 epoch total loss 1.33670712\n",
      "Trained batch 15 batch loss 1.36973047 epoch total loss 1.33890867\n",
      "Trained batch 16 batch loss 1.46350431 epoch total loss 1.3466959\n",
      "Trained batch 17 batch loss 1.40697563 epoch total loss 1.35024166\n",
      "Trained batch 18 batch loss 1.38405967 epoch total loss 1.35212052\n",
      "Trained batch 19 batch loss 1.37195897 epoch total loss 1.35316467\n",
      "Trained batch 20 batch loss 1.36676109 epoch total loss 1.3538444\n",
      "Trained batch 21 batch loss 1.43475819 epoch total loss 1.35769749\n",
      "Trained batch 22 batch loss 1.26896918 epoch total loss 1.35366428\n",
      "Trained batch 23 batch loss 1.42481482 epoch total loss 1.35675776\n",
      "Trained batch 24 batch loss 1.35406446 epoch total loss 1.35664558\n",
      "Trained batch 25 batch loss 1.45411587 epoch total loss 1.36054444\n",
      "Trained batch 26 batch loss 1.48794413 epoch total loss 1.36544442\n",
      "Trained batch 27 batch loss 1.48686075 epoch total loss 1.36994147\n",
      "Trained batch 28 batch loss 1.42131042 epoch total loss 1.37177598\n",
      "Trained batch 29 batch loss 1.34607863 epoch total loss 1.3708899\n",
      "Trained batch 30 batch loss 1.39335907 epoch total loss 1.37163889\n",
      "Trained batch 31 batch loss 1.44142461 epoch total loss 1.37389\n",
      "Trained batch 32 batch loss 1.35380399 epoch total loss 1.37326241\n",
      "Trained batch 33 batch loss 1.41983652 epoch total loss 1.37467384\n",
      "Trained batch 34 batch loss 1.40582645 epoch total loss 1.37559009\n",
      "Trained batch 35 batch loss 1.32623124 epoch total loss 1.37417984\n",
      "Trained batch 36 batch loss 1.30738568 epoch total loss 1.37232447\n",
      "Trained batch 37 batch loss 1.24732113 epoch total loss 1.36894596\n",
      "Trained batch 38 batch loss 1.19353151 epoch total loss 1.36432981\n",
      "Trained batch 39 batch loss 1.22068644 epoch total loss 1.36064661\n",
      "Trained batch 40 batch loss 1.46242201 epoch total loss 1.36319101\n",
      "Trained batch 41 batch loss 1.45142198 epoch total loss 1.36534309\n",
      "Trained batch 42 batch loss 1.54957438 epoch total loss 1.36972952\n",
      "Trained batch 43 batch loss 1.52201688 epoch total loss 1.37327111\n",
      "Trained batch 44 batch loss 1.60405385 epoch total loss 1.3785162\n",
      "Trained batch 45 batch loss 1.42287588 epoch total loss 1.37950194\n",
      "Trained batch 46 batch loss 1.4854064 epoch total loss 1.38180423\n",
      "Trained batch 47 batch loss 1.30330443 epoch total loss 1.38013399\n",
      "Trained batch 48 batch loss 1.36555409 epoch total loss 1.37983024\n",
      "Trained batch 49 batch loss 1.39804184 epoch total loss 1.38020182\n",
      "Trained batch 50 batch loss 1.37307417 epoch total loss 1.38005936\n",
      "Trained batch 51 batch loss 1.24778581 epoch total loss 1.37746584\n",
      "Trained batch 52 batch loss 1.27053916 epoch total loss 1.37540948\n",
      "Trained batch 53 batch loss 1.19954872 epoch total loss 1.37209129\n",
      "Trained batch 54 batch loss 1.32371569 epoch total loss 1.37119544\n",
      "Trained batch 55 batch loss 1.22843564 epoch total loss 1.36859989\n",
      "Trained batch 56 batch loss 1.25263739 epoch total loss 1.36652923\n",
      "Trained batch 57 batch loss 1.30951571 epoch total loss 1.36552894\n",
      "Trained batch 58 batch loss 1.40844274 epoch total loss 1.36626887\n",
      "Trained batch 59 batch loss 1.39402628 epoch total loss 1.36673927\n",
      "Trained batch 60 batch loss 1.29721332 epoch total loss 1.36558044\n",
      "Trained batch 61 batch loss 1.30313909 epoch total loss 1.36455691\n",
      "Trained batch 62 batch loss 1.34382486 epoch total loss 1.36422253\n",
      "Trained batch 63 batch loss 1.44892156 epoch total loss 1.36556697\n",
      "Trained batch 64 batch loss 1.46643591 epoch total loss 1.36714303\n",
      "Trained batch 65 batch loss 1.40425348 epoch total loss 1.36771393\n",
      "Trained batch 66 batch loss 1.37538505 epoch total loss 1.36783016\n",
      "Trained batch 67 batch loss 1.44589138 epoch total loss 1.36899519\n",
      "Trained batch 68 batch loss 1.38355553 epoch total loss 1.36920929\n",
      "Trained batch 69 batch loss 1.40028143 epoch total loss 1.36965966\n",
      "Trained batch 70 batch loss 1.3077085 epoch total loss 1.36877465\n",
      "Trained batch 71 batch loss 1.30122614 epoch total loss 1.36782324\n",
      "Trained batch 72 batch loss 1.35130608 epoch total loss 1.36759377\n",
      "Trained batch 73 batch loss 1.42420745 epoch total loss 1.36836934\n",
      "Trained batch 74 batch loss 1.42927754 epoch total loss 1.36919236\n",
      "Trained batch 75 batch loss 1.53518796 epoch total loss 1.3714056\n",
      "Trained batch 76 batch loss 1.5085994 epoch total loss 1.37321079\n",
      "Trained batch 77 batch loss 1.49131989 epoch total loss 1.37474465\n",
      "Trained batch 78 batch loss 1.42371547 epoch total loss 1.37537253\n",
      "Trained batch 79 batch loss 1.48629856 epoch total loss 1.37677658\n",
      "Trained batch 80 batch loss 1.42216468 epoch total loss 1.37734389\n",
      "Trained batch 81 batch loss 1.38601875 epoch total loss 1.37745106\n",
      "Trained batch 82 batch loss 1.41143942 epoch total loss 1.37786543\n",
      "Trained batch 83 batch loss 1.43826067 epoch total loss 1.37859321\n",
      "Trained batch 84 batch loss 1.56994605 epoch total loss 1.38087118\n",
      "Trained batch 85 batch loss 1.39904213 epoch total loss 1.38108492\n",
      "Trained batch 86 batch loss 1.39065731 epoch total loss 1.38119626\n",
      "Trained batch 87 batch loss 1.3273741 epoch total loss 1.38057756\n",
      "Trained batch 88 batch loss 1.24117982 epoch total loss 1.37899351\n",
      "Trained batch 89 batch loss 1.28673899 epoch total loss 1.37795699\n",
      "Trained batch 90 batch loss 1.42485189 epoch total loss 1.37847793\n",
      "Trained batch 91 batch loss 1.31573224 epoch total loss 1.37778854\n",
      "Trained batch 92 batch loss 1.38828361 epoch total loss 1.37790263\n",
      "Trained batch 93 batch loss 1.42461026 epoch total loss 1.37840486\n",
      "Trained batch 94 batch loss 1.51870227 epoch total loss 1.37989748\n",
      "Trained batch 95 batch loss 1.50193417 epoch total loss 1.38118207\n",
      "Trained batch 96 batch loss 1.49253321 epoch total loss 1.38234198\n",
      "Trained batch 97 batch loss 1.54510367 epoch total loss 1.38402\n",
      "Trained batch 98 batch loss 1.34305286 epoch total loss 1.3836019\n",
      "Trained batch 99 batch loss 1.33294332 epoch total loss 1.38309026\n",
      "Trained batch 100 batch loss 1.36945117 epoch total loss 1.38295376\n",
      "Trained batch 101 batch loss 1.50715876 epoch total loss 1.38418353\n",
      "Trained batch 102 batch loss 1.35518575 epoch total loss 1.38389921\n",
      "Trained batch 103 batch loss 1.3270824 epoch total loss 1.38334763\n",
      "Trained batch 104 batch loss 1.32153225 epoch total loss 1.38275325\n",
      "Trained batch 105 batch loss 1.32103181 epoch total loss 1.38216543\n",
      "Trained batch 106 batch loss 1.25312233 epoch total loss 1.38094807\n",
      "Trained batch 107 batch loss 1.39035392 epoch total loss 1.38103592\n",
      "Trained batch 108 batch loss 1.27668262 epoch total loss 1.38006973\n",
      "Trained batch 109 batch loss 1.29097295 epoch total loss 1.37925231\n",
      "Trained batch 110 batch loss 1.3735429 epoch total loss 1.37920046\n",
      "Trained batch 111 batch loss 1.41570306 epoch total loss 1.37952936\n",
      "Trained batch 112 batch loss 1.44576573 epoch total loss 1.38012087\n",
      "Trained batch 113 batch loss 1.44751883 epoch total loss 1.38071728\n",
      "Trained batch 114 batch loss 1.33804893 epoch total loss 1.38034296\n",
      "Trained batch 115 batch loss 1.33806038 epoch total loss 1.37997532\n",
      "Trained batch 116 batch loss 1.36297154 epoch total loss 1.37982869\n",
      "Trained batch 117 batch loss 1.3455801 epoch total loss 1.37953603\n",
      "Trained batch 118 batch loss 1.39197469 epoch total loss 1.37964141\n",
      "Trained batch 119 batch loss 1.36900496 epoch total loss 1.37955201\n",
      "Trained batch 120 batch loss 1.3095603 epoch total loss 1.37896872\n",
      "Trained batch 121 batch loss 1.2940011 epoch total loss 1.37826657\n",
      "Trained batch 122 batch loss 1.35013723 epoch total loss 1.37803602\n",
      "Trained batch 123 batch loss 1.4871248 epoch total loss 1.37892282\n",
      "Trained batch 124 batch loss 1.46448326 epoch total loss 1.3796128\n",
      "Trained batch 125 batch loss 1.38422871 epoch total loss 1.37964976\n",
      "Trained batch 126 batch loss 1.36006904 epoch total loss 1.37949431\n",
      "Trained batch 127 batch loss 1.24001718 epoch total loss 1.37839615\n",
      "Trained batch 128 batch loss 1.24422526 epoch total loss 1.37734795\n",
      "Trained batch 129 batch loss 1.28266978 epoch total loss 1.37661397\n",
      "Trained batch 130 batch loss 1.36170578 epoch total loss 1.3764993\n",
      "Trained batch 131 batch loss 1.42849755 epoch total loss 1.37689626\n",
      "Trained batch 132 batch loss 1.40275514 epoch total loss 1.37709224\n",
      "Trained batch 133 batch loss 1.31634891 epoch total loss 1.37663543\n",
      "Trained batch 134 batch loss 1.22524393 epoch total loss 1.37550569\n",
      "Trained batch 135 batch loss 1.32713151 epoch total loss 1.37514734\n",
      "Trained batch 136 batch loss 1.28294563 epoch total loss 1.3744694\n",
      "Trained batch 137 batch loss 1.35278475 epoch total loss 1.37431109\n",
      "Trained batch 138 batch loss 1.34371889 epoch total loss 1.37408948\n",
      "Trained batch 139 batch loss 1.48549342 epoch total loss 1.37489092\n",
      "Trained batch 140 batch loss 1.29570341 epoch total loss 1.37432528\n",
      "Trained batch 141 batch loss 1.44594598 epoch total loss 1.37483323\n",
      "Trained batch 142 batch loss 1.43645656 epoch total loss 1.37526727\n",
      "Trained batch 143 batch loss 1.52048814 epoch total loss 1.37628281\n",
      "Trained batch 144 batch loss 1.52694499 epoch total loss 1.37732911\n",
      "Trained batch 145 batch loss 1.50487733 epoch total loss 1.37820876\n",
      "Trained batch 146 batch loss 1.48475921 epoch total loss 1.37893856\n",
      "Trained batch 147 batch loss 1.35801184 epoch total loss 1.37879622\n",
      "Trained batch 148 batch loss 1.33756673 epoch total loss 1.37851763\n",
      "Trained batch 149 batch loss 1.43947101 epoch total loss 1.37892675\n",
      "Trained batch 150 batch loss 1.50859666 epoch total loss 1.37979114\n",
      "Trained batch 151 batch loss 1.61043382 epoch total loss 1.38131857\n",
      "Trained batch 152 batch loss 1.40758371 epoch total loss 1.3814913\n",
      "Trained batch 153 batch loss 1.45746768 epoch total loss 1.38198793\n",
      "Trained batch 154 batch loss 1.38489723 epoch total loss 1.38200688\n",
      "Trained batch 155 batch loss 1.40723681 epoch total loss 1.3821696\n",
      "Trained batch 156 batch loss 1.43237281 epoch total loss 1.38249147\n",
      "Trained batch 157 batch loss 1.35118306 epoch total loss 1.38229203\n",
      "Trained batch 158 batch loss 1.40573359 epoch total loss 1.38244045\n",
      "Trained batch 159 batch loss 1.45296 epoch total loss 1.38288391\n",
      "Trained batch 160 batch loss 1.36966872 epoch total loss 1.38280129\n",
      "Trained batch 161 batch loss 1.37592602 epoch total loss 1.38275862\n",
      "Trained batch 162 batch loss 1.44588435 epoch total loss 1.38314831\n",
      "Trained batch 163 batch loss 1.4487803 epoch total loss 1.38355088\n",
      "Trained batch 164 batch loss 1.43287253 epoch total loss 1.38385165\n",
      "Trained batch 165 batch loss 1.43694794 epoch total loss 1.38417351\n",
      "Trained batch 166 batch loss 1.40010357 epoch total loss 1.38426948\n",
      "Trained batch 167 batch loss 1.34476161 epoch total loss 1.38403285\n",
      "Trained batch 168 batch loss 1.53210092 epoch total loss 1.38491416\n",
      "Trained batch 169 batch loss 1.501 epoch total loss 1.38560116\n",
      "Trained batch 170 batch loss 1.34911895 epoch total loss 1.38538659\n",
      "Trained batch 171 batch loss 1.39577246 epoch total loss 1.38544726\n",
      "Trained batch 172 batch loss 1.35658646 epoch total loss 1.38527942\n",
      "Trained batch 173 batch loss 1.4445622 epoch total loss 1.38562214\n",
      "Trained batch 174 batch loss 1.54100668 epoch total loss 1.38651514\n",
      "Trained batch 175 batch loss 1.60232627 epoch total loss 1.38774836\n",
      "Trained batch 176 batch loss 1.48417985 epoch total loss 1.38829625\n",
      "Trained batch 177 batch loss 1.41147673 epoch total loss 1.38842726\n",
      "Trained batch 178 batch loss 1.39722323 epoch total loss 1.38847661\n",
      "Trained batch 179 batch loss 1.41258287 epoch total loss 1.38861132\n",
      "Trained batch 180 batch loss 1.35799932 epoch total loss 1.38844121\n",
      "Trained batch 181 batch loss 1.30931723 epoch total loss 1.38800406\n",
      "Trained batch 182 batch loss 1.39163828 epoch total loss 1.38802397\n",
      "Trained batch 183 batch loss 1.41526318 epoch total loss 1.38817286\n",
      "Trained batch 184 batch loss 1.33248591 epoch total loss 1.38787019\n",
      "Trained batch 185 batch loss 1.39230418 epoch total loss 1.38789427\n",
      "Trained batch 186 batch loss 1.43008244 epoch total loss 1.38812113\n",
      "Trained batch 187 batch loss 1.39441347 epoch total loss 1.38815475\n",
      "Trained batch 188 batch loss 1.31931043 epoch total loss 1.38778853\n",
      "Trained batch 189 batch loss 1.36083293 epoch total loss 1.38764596\n",
      "Trained batch 190 batch loss 1.3375169 epoch total loss 1.38738215\n",
      "Trained batch 191 batch loss 1.31712389 epoch total loss 1.38701439\n",
      "Trained batch 192 batch loss 1.31236637 epoch total loss 1.38662565\n",
      "Trained batch 193 batch loss 1.3389945 epoch total loss 1.38637877\n",
      "Trained batch 194 batch loss 1.32051301 epoch total loss 1.38603938\n",
      "Trained batch 195 batch loss 1.37217712 epoch total loss 1.38596821\n",
      "Trained batch 196 batch loss 1.20103431 epoch total loss 1.38502455\n",
      "Trained batch 197 batch loss 1.14578819 epoch total loss 1.38381016\n",
      "Trained batch 198 batch loss 1.24562013 epoch total loss 1.38311207\n",
      "Trained batch 199 batch loss 1.42841506 epoch total loss 1.38333976\n",
      "Trained batch 200 batch loss 1.63976896 epoch total loss 1.38462186\n",
      "Trained batch 201 batch loss 1.57618618 epoch total loss 1.38557482\n",
      "Trained batch 202 batch loss 1.5381124 epoch total loss 1.38633\n",
      "Trained batch 203 batch loss 1.32955158 epoch total loss 1.38605034\n",
      "Trained batch 204 batch loss 1.39556301 epoch total loss 1.38609707\n",
      "Trained batch 205 batch loss 1.42015696 epoch total loss 1.38626325\n",
      "Trained batch 206 batch loss 1.38572419 epoch total loss 1.38626051\n",
      "Trained batch 207 batch loss 1.37888288 epoch total loss 1.38622487\n",
      "Trained batch 208 batch loss 1.40260911 epoch total loss 1.38630366\n",
      "Trained batch 209 batch loss 1.44993663 epoch total loss 1.38660824\n",
      "Trained batch 210 batch loss 1.33446813 epoch total loss 1.38635993\n",
      "Trained batch 211 batch loss 1.42807388 epoch total loss 1.38655758\n",
      "Trained batch 212 batch loss 1.31606734 epoch total loss 1.3862251\n",
      "Trained batch 213 batch loss 1.39635134 epoch total loss 1.38627279\n",
      "Trained batch 214 batch loss 1.38648057 epoch total loss 1.38627362\n",
      "Trained batch 215 batch loss 1.34735441 epoch total loss 1.38609266\n",
      "Trained batch 216 batch loss 1.3553499 epoch total loss 1.38595033\n",
      "Trained batch 217 batch loss 1.21037555 epoch total loss 1.38514125\n",
      "Trained batch 218 batch loss 1.3067224 epoch total loss 1.3847816\n",
      "Trained batch 219 batch loss 1.36208522 epoch total loss 1.38467801\n",
      "Trained batch 220 batch loss 1.43481684 epoch total loss 1.38490582\n",
      "Trained batch 221 batch loss 1.36679697 epoch total loss 1.38482392\n",
      "Trained batch 222 batch loss 1.3017267 epoch total loss 1.3844496\n",
      "Trained batch 223 batch loss 1.32791233 epoch total loss 1.38419604\n",
      "Trained batch 224 batch loss 1.19542801 epoch total loss 1.38335335\n",
      "Trained batch 225 batch loss 1.33697832 epoch total loss 1.38314724\n",
      "Trained batch 226 batch loss 1.4232471 epoch total loss 1.38332462\n",
      "Trained batch 227 batch loss 1.33622825 epoch total loss 1.38311732\n",
      "Trained batch 228 batch loss 1.31088 epoch total loss 1.38280046\n",
      "Trained batch 229 batch loss 1.37090516 epoch total loss 1.38274848\n",
      "Trained batch 230 batch loss 1.29186094 epoch total loss 1.38235343\n",
      "Trained batch 231 batch loss 1.37311935 epoch total loss 1.38231337\n",
      "Trained batch 232 batch loss 1.29095852 epoch total loss 1.38191962\n",
      "Trained batch 233 batch loss 1.34452701 epoch total loss 1.38175905\n",
      "Trained batch 234 batch loss 1.31275558 epoch total loss 1.38146412\n",
      "Trained batch 235 batch loss 1.30304182 epoch total loss 1.38113034\n",
      "Trained batch 236 batch loss 1.32594109 epoch total loss 1.38089645\n",
      "Trained batch 237 batch loss 1.38273156 epoch total loss 1.3809042\n",
      "Trained batch 238 batch loss 1.39500201 epoch total loss 1.38096333\n",
      "Trained batch 239 batch loss 1.60526848 epoch total loss 1.38190186\n",
      "Trained batch 240 batch loss 1.66437209 epoch total loss 1.38307881\n",
      "Trained batch 241 batch loss 1.55124497 epoch total loss 1.38377655\n",
      "Trained batch 242 batch loss 1.3329277 epoch total loss 1.38356638\n",
      "Trained batch 243 batch loss 1.22357106 epoch total loss 1.38290799\n",
      "Trained batch 244 batch loss 1.33391142 epoch total loss 1.38270724\n",
      "Trained batch 245 batch loss 1.38283885 epoch total loss 1.38270772\n",
      "Trained batch 246 batch loss 1.30822337 epoch total loss 1.38240492\n",
      "Trained batch 247 batch loss 1.37083721 epoch total loss 1.38235819\n",
      "Trained batch 248 batch loss 1.38893867 epoch total loss 1.38238478\n",
      "Trained batch 249 batch loss 1.51701498 epoch total loss 1.38292551\n",
      "Trained batch 250 batch loss 1.5060755 epoch total loss 1.38341808\n",
      "Trained batch 251 batch loss 1.4264667 epoch total loss 1.38358951\n",
      "Trained batch 252 batch loss 1.42049968 epoch total loss 1.38373601\n",
      "Trained batch 253 batch loss 1.3137058 epoch total loss 1.38345933\n",
      "Trained batch 254 batch loss 1.39312685 epoch total loss 1.38349736\n",
      "Trained batch 255 batch loss 1.3216126 epoch total loss 1.38325465\n",
      "Trained batch 256 batch loss 1.30690777 epoch total loss 1.3829565\n",
      "Trained batch 257 batch loss 1.27610445 epoch total loss 1.3825407\n",
      "Trained batch 258 batch loss 1.27787399 epoch total loss 1.38213491\n",
      "Trained batch 259 batch loss 1.32341838 epoch total loss 1.3819083\n",
      "Trained batch 260 batch loss 1.32804716 epoch total loss 1.38170111\n",
      "Trained batch 261 batch loss 1.22383702 epoch total loss 1.38109624\n",
      "Trained batch 262 batch loss 1.27339172 epoch total loss 1.38068509\n",
      "Trained batch 263 batch loss 1.28614509 epoch total loss 1.38032556\n",
      "Trained batch 264 batch loss 1.25177419 epoch total loss 1.37983871\n",
      "Trained batch 265 batch loss 1.3111552 epoch total loss 1.37957942\n",
      "Trained batch 266 batch loss 1.42121863 epoch total loss 1.37973595\n",
      "Trained batch 267 batch loss 1.4178493 epoch total loss 1.37987864\n",
      "Trained batch 268 batch loss 1.41739964 epoch total loss 1.38001871\n",
      "Trained batch 269 batch loss 1.37962675 epoch total loss 1.38001728\n",
      "Trained batch 270 batch loss 1.36451268 epoch total loss 1.37995982\n",
      "Trained batch 271 batch loss 1.36507702 epoch total loss 1.37990487\n",
      "Trained batch 272 batch loss 1.33975422 epoch total loss 1.37975729\n",
      "Trained batch 273 batch loss 1.23397613 epoch total loss 1.37922323\n",
      "Trained batch 274 batch loss 1.340693 epoch total loss 1.37908268\n",
      "Trained batch 275 batch loss 1.22985446 epoch total loss 1.37854\n",
      "Trained batch 276 batch loss 1.47843504 epoch total loss 1.37890196\n",
      "Trained batch 277 batch loss 1.38127923 epoch total loss 1.37891054\n",
      "Trained batch 278 batch loss 1.3578918 epoch total loss 1.37883484\n",
      "Trained batch 279 batch loss 1.2734828 epoch total loss 1.37845719\n",
      "Trained batch 280 batch loss 1.49600625 epoch total loss 1.37887704\n",
      "Trained batch 281 batch loss 1.16589856 epoch total loss 1.37811911\n",
      "Trained batch 282 batch loss 1.3645252 epoch total loss 1.37807095\n",
      "Trained batch 283 batch loss 1.34599078 epoch total loss 1.37795746\n",
      "Trained batch 284 batch loss 1.53603458 epoch total loss 1.37851417\n",
      "Trained batch 285 batch loss 1.54762495 epoch total loss 1.37910759\n",
      "Trained batch 286 batch loss 1.62293279 epoch total loss 1.37996006\n",
      "Trained batch 287 batch loss 1.51722908 epoch total loss 1.38043845\n",
      "Trained batch 288 batch loss 1.40628695 epoch total loss 1.38052809\n",
      "Trained batch 289 batch loss 1.42432415 epoch total loss 1.38067961\n",
      "Trained batch 290 batch loss 1.33185947 epoch total loss 1.38051128\n",
      "Trained batch 291 batch loss 1.39536631 epoch total loss 1.38056231\n",
      "Trained batch 292 batch loss 1.37351513 epoch total loss 1.38053811\n",
      "Trained batch 293 batch loss 1.4474082 epoch total loss 1.38076639\n",
      "Trained batch 294 batch loss 1.33840632 epoch total loss 1.38062227\n",
      "Trained batch 295 batch loss 1.2930851 epoch total loss 1.38032556\n",
      "Trained batch 296 batch loss 1.24304569 epoch total loss 1.37986183\n",
      "Trained batch 297 batch loss 1.37447226 epoch total loss 1.37984371\n",
      "Trained batch 298 batch loss 1.53991485 epoch total loss 1.38038087\n",
      "Trained batch 299 batch loss 1.63505054 epoch total loss 1.3812325\n",
      "Trained batch 300 batch loss 1.5350405 epoch total loss 1.38174522\n",
      "Trained batch 301 batch loss 1.42664599 epoch total loss 1.38189435\n",
      "Trained batch 302 batch loss 1.51308453 epoch total loss 1.38232875\n",
      "Trained batch 303 batch loss 1.58554232 epoch total loss 1.38299942\n",
      "Trained batch 304 batch loss 1.42376816 epoch total loss 1.38313353\n",
      "Trained batch 305 batch loss 1.43554044 epoch total loss 1.38330543\n",
      "Trained batch 306 batch loss 1.33402944 epoch total loss 1.38314426\n",
      "Trained batch 307 batch loss 1.37479103 epoch total loss 1.38311708\n",
      "Trained batch 308 batch loss 1.40500784 epoch total loss 1.38318813\n",
      "Trained batch 309 batch loss 1.27290022 epoch total loss 1.38283122\n",
      "Trained batch 310 batch loss 1.3475554 epoch total loss 1.38271737\n",
      "Trained batch 311 batch loss 1.43991053 epoch total loss 1.38290131\n",
      "Trained batch 312 batch loss 1.53529549 epoch total loss 1.38338983\n",
      "Trained batch 313 batch loss 1.32190418 epoch total loss 1.38319337\n",
      "Trained batch 314 batch loss 1.25749612 epoch total loss 1.38279307\n",
      "Trained batch 315 batch loss 1.22472382 epoch total loss 1.38229132\n",
      "Trained batch 316 batch loss 1.32839322 epoch total loss 1.38212073\n",
      "Trained batch 317 batch loss 1.20488501 epoch total loss 1.38156164\n",
      "Trained batch 318 batch loss 1.22436345 epoch total loss 1.3810674\n",
      "Trained batch 319 batch loss 1.26355433 epoch total loss 1.38069892\n",
      "Trained batch 320 batch loss 1.26547766 epoch total loss 1.38033891\n",
      "Trained batch 321 batch loss 1.32083511 epoch total loss 1.38015354\n",
      "Trained batch 322 batch loss 1.39601183 epoch total loss 1.38020277\n",
      "Trained batch 323 batch loss 1.44493282 epoch total loss 1.38040316\n",
      "Trained batch 324 batch loss 1.37297297 epoch total loss 1.38038027\n",
      "Trained batch 325 batch loss 1.36806357 epoch total loss 1.38034248\n",
      "Trained batch 326 batch loss 1.44735622 epoch total loss 1.380548\n",
      "Trained batch 327 batch loss 1.31770587 epoch total loss 1.38035583\n",
      "Trained batch 328 batch loss 1.44765198 epoch total loss 1.38056111\n",
      "Trained batch 329 batch loss 1.34034288 epoch total loss 1.3804388\n",
      "Trained batch 330 batch loss 1.33050728 epoch total loss 1.38028753\n",
      "Trained batch 331 batch loss 1.39877868 epoch total loss 1.38034332\n",
      "Trained batch 332 batch loss 1.25695419 epoch total loss 1.37997174\n",
      "Trained batch 333 batch loss 1.26942134 epoch total loss 1.37963963\n",
      "Trained batch 334 batch loss 1.36433148 epoch total loss 1.37959385\n",
      "Trained batch 335 batch loss 1.42966902 epoch total loss 1.37974322\n",
      "Trained batch 336 batch loss 1.52202678 epoch total loss 1.38016677\n",
      "Trained batch 337 batch loss 1.46221638 epoch total loss 1.38041019\n",
      "Trained batch 338 batch loss 1.34504128 epoch total loss 1.38030553\n",
      "Trained batch 339 batch loss 1.44670892 epoch total loss 1.38050139\n",
      "Trained batch 340 batch loss 1.52153444 epoch total loss 1.38091624\n",
      "Trained batch 341 batch loss 1.38724661 epoch total loss 1.38093483\n",
      "Trained batch 342 batch loss 1.38426685 epoch total loss 1.38094461\n",
      "Trained batch 343 batch loss 1.35481453 epoch total loss 1.38086843\n",
      "Trained batch 344 batch loss 1.34714067 epoch total loss 1.38077044\n",
      "Trained batch 345 batch loss 1.30371284 epoch total loss 1.38054705\n",
      "Trained batch 346 batch loss 1.21430635 epoch total loss 1.38006651\n",
      "Trained batch 347 batch loss 1.38808858 epoch total loss 1.38008964\n",
      "Trained batch 348 batch loss 1.29345775 epoch total loss 1.37984073\n",
      "Trained batch 349 batch loss 1.30183756 epoch total loss 1.37961721\n",
      "Trained batch 350 batch loss 1.25593078 epoch total loss 1.37926388\n",
      "Trained batch 351 batch loss 1.31004167 epoch total loss 1.37906659\n",
      "Trained batch 352 batch loss 1.27087152 epoch total loss 1.37875926\n",
      "Trained batch 353 batch loss 1.28259158 epoch total loss 1.37848675\n",
      "Trained batch 354 batch loss 1.3128593 epoch total loss 1.37830138\n",
      "Trained batch 355 batch loss 1.29395497 epoch total loss 1.3780638\n",
      "Trained batch 356 batch loss 1.25151515 epoch total loss 1.37770832\n",
      "Trained batch 357 batch loss 1.26027012 epoch total loss 1.37737942\n",
      "Trained batch 358 batch loss 1.17577732 epoch total loss 1.37681627\n",
      "Trained batch 359 batch loss 1.21011829 epoch total loss 1.37635195\n",
      "Trained batch 360 batch loss 1.23149633 epoch total loss 1.37594962\n",
      "Trained batch 361 batch loss 1.25722098 epoch total loss 1.37562072\n",
      "Trained batch 362 batch loss 1.34683323 epoch total loss 1.37554121\n",
      "Trained batch 363 batch loss 1.342242 epoch total loss 1.37544954\n",
      "Trained batch 364 batch loss 1.3249414 epoch total loss 1.37531078\n",
      "Trained batch 365 batch loss 1.4327209 epoch total loss 1.37546802\n",
      "Trained batch 366 batch loss 1.47665358 epoch total loss 1.37574446\n",
      "Trained batch 367 batch loss 1.40678513 epoch total loss 1.3758291\n",
      "Trained batch 368 batch loss 1.33692837 epoch total loss 1.37572336\n",
      "Trained batch 369 batch loss 1.39699602 epoch total loss 1.37578106\n",
      "Trained batch 370 batch loss 1.29098821 epoch total loss 1.37555182\n",
      "Trained batch 371 batch loss 1.28536201 epoch total loss 1.37530875\n",
      "Trained batch 372 batch loss 1.25922728 epoch total loss 1.37499666\n",
      "Trained batch 373 batch loss 1.31261253 epoch total loss 1.37482953\n",
      "Trained batch 374 batch loss 1.43995166 epoch total loss 1.37500358\n",
      "Trained batch 375 batch loss 1.66404021 epoch total loss 1.37577438\n",
      "Trained batch 376 batch loss 1.58672786 epoch total loss 1.3763355\n",
      "Trained batch 377 batch loss 1.41862547 epoch total loss 1.37644768\n",
      "Trained batch 378 batch loss 1.56036282 epoch total loss 1.37693429\n",
      "Trained batch 379 batch loss 1.45286393 epoch total loss 1.37713456\n",
      "Trained batch 380 batch loss 1.48475051 epoch total loss 1.3774178\n",
      "Trained batch 381 batch loss 1.3867234 epoch total loss 1.37744224\n",
      "Trained batch 382 batch loss 1.4208473 epoch total loss 1.37755585\n",
      "Trained batch 383 batch loss 1.27192044 epoch total loss 1.37728\n",
      "Trained batch 384 batch loss 1.31977534 epoch total loss 1.37713015\n",
      "Trained batch 385 batch loss 1.39864385 epoch total loss 1.37718606\n",
      "Trained batch 386 batch loss 1.41232967 epoch total loss 1.37727714\n",
      "Trained batch 387 batch loss 1.39830589 epoch total loss 1.3773315\n",
      "Trained batch 388 batch loss 1.34102964 epoch total loss 1.3772378\n",
      "Trained batch 389 batch loss 1.35227966 epoch total loss 1.37717378\n",
      "Trained batch 390 batch loss 1.36309063 epoch total loss 1.37713766\n",
      "Trained batch 391 batch loss 1.15153015 epoch total loss 1.37656069\n",
      "Trained batch 392 batch loss 1.13796854 epoch total loss 1.37595201\n",
      "Trained batch 393 batch loss 1.29095888 epoch total loss 1.37573564\n",
      "Trained batch 394 batch loss 1.43105328 epoch total loss 1.37587607\n",
      "Trained batch 395 batch loss 1.55614352 epoch total loss 1.3763324\n",
      "Trained batch 396 batch loss 1.38778293 epoch total loss 1.37636125\n",
      "Trained batch 397 batch loss 1.51466966 epoch total loss 1.37670958\n",
      "Trained batch 398 batch loss 1.490242 epoch total loss 1.37699485\n",
      "Trained batch 399 batch loss 1.34716809 epoch total loss 1.3769201\n",
      "Trained batch 400 batch loss 1.41248405 epoch total loss 1.37700891\n",
      "Trained batch 401 batch loss 1.49168801 epoch total loss 1.37729502\n",
      "Trained batch 402 batch loss 1.44978464 epoch total loss 1.37747526\n",
      "Trained batch 403 batch loss 1.28597903 epoch total loss 1.37724817\n",
      "Trained batch 404 batch loss 1.30084133 epoch total loss 1.37705898\n",
      "Trained batch 405 batch loss 1.2250725 epoch total loss 1.37668383\n",
      "Trained batch 406 batch loss 1.24703503 epoch total loss 1.37636447\n",
      "Trained batch 407 batch loss 1.40511405 epoch total loss 1.37643504\n",
      "Trained batch 408 batch loss 1.42981923 epoch total loss 1.37656581\n",
      "Trained batch 409 batch loss 1.44615698 epoch total loss 1.37673604\n",
      "Trained batch 410 batch loss 1.45339513 epoch total loss 1.37692285\n",
      "Trained batch 411 batch loss 1.43393493 epoch total loss 1.37706172\n",
      "Trained batch 412 batch loss 1.36381483 epoch total loss 1.37702954\n",
      "Trained batch 413 batch loss 1.31844163 epoch total loss 1.37688768\n",
      "Trained batch 414 batch loss 1.34167147 epoch total loss 1.37680256\n",
      "Trained batch 415 batch loss 1.45590925 epoch total loss 1.3769933\n",
      "Trained batch 416 batch loss 1.37583256 epoch total loss 1.37699056\n",
      "Trained batch 417 batch loss 1.29889727 epoch total loss 1.37680328\n",
      "Trained batch 418 batch loss 1.38731205 epoch total loss 1.37682843\n",
      "Trained batch 419 batch loss 1.40268517 epoch total loss 1.37689018\n",
      "Trained batch 420 batch loss 1.40750277 epoch total loss 1.37696314\n",
      "Trained batch 421 batch loss 1.28609526 epoch total loss 1.37674725\n",
      "Trained batch 422 batch loss 1.20093989 epoch total loss 1.37633061\n",
      "Trained batch 423 batch loss 1.15669763 epoch total loss 1.37581134\n",
      "Trained batch 424 batch loss 1.22411048 epoch total loss 1.37545359\n",
      "Trained batch 425 batch loss 1.28774786 epoch total loss 1.37524712\n",
      "Trained batch 426 batch loss 1.28871238 epoch total loss 1.37504399\n",
      "Trained batch 427 batch loss 1.25455534 epoch total loss 1.37476182\n",
      "Trained batch 428 batch loss 1.31703126 epoch total loss 1.37462699\n",
      "Trained batch 429 batch loss 1.46212721 epoch total loss 1.37483084\n",
      "Trained batch 430 batch loss 1.42635989 epoch total loss 1.37495065\n",
      "Trained batch 431 batch loss 1.32806039 epoch total loss 1.37484181\n",
      "Trained batch 432 batch loss 1.59669018 epoch total loss 1.37535536\n",
      "Trained batch 433 batch loss 1.62222886 epoch total loss 1.37592554\n",
      "Trained batch 434 batch loss 1.31683111 epoch total loss 1.3757894\n",
      "Trained batch 435 batch loss 1.24573076 epoch total loss 1.37549043\n",
      "Trained batch 436 batch loss 1.17168915 epoch total loss 1.37502301\n",
      "Trained batch 437 batch loss 1.21248102 epoch total loss 1.37465096\n",
      "Trained batch 438 batch loss 1.31312239 epoch total loss 1.37451041\n",
      "Trained batch 439 batch loss 1.36100984 epoch total loss 1.37447977\n",
      "Trained batch 440 batch loss 1.23147511 epoch total loss 1.37415469\n",
      "Trained batch 441 batch loss 1.24424851 epoch total loss 1.37386012\n",
      "Trained batch 442 batch loss 1.38124633 epoch total loss 1.37387681\n",
      "Trained batch 443 batch loss 1.26312554 epoch total loss 1.37362683\n",
      "Trained batch 444 batch loss 1.43528545 epoch total loss 1.37376571\n",
      "Trained batch 445 batch loss 1.19486058 epoch total loss 1.37336373\n",
      "Trained batch 446 batch loss 1.26240087 epoch total loss 1.37311494\n",
      "Trained batch 447 batch loss 1.26513469 epoch total loss 1.37287331\n",
      "Trained batch 448 batch loss 1.32799101 epoch total loss 1.37277317\n",
      "Trained batch 449 batch loss 1.35708523 epoch total loss 1.37273812\n",
      "Trained batch 450 batch loss 1.46188068 epoch total loss 1.37293625\n",
      "Trained batch 451 batch loss 1.52610159 epoch total loss 1.37327588\n",
      "Trained batch 452 batch loss 1.53394139 epoch total loss 1.37363136\n",
      "Trained batch 453 batch loss 1.26578522 epoch total loss 1.3733933\n",
      "Trained batch 454 batch loss 1.34851575 epoch total loss 1.37333846\n",
      "Trained batch 455 batch loss 1.18718171 epoch total loss 1.37292933\n",
      "Trained batch 456 batch loss 1.47938538 epoch total loss 1.37316275\n",
      "Trained batch 457 batch loss 1.36538923 epoch total loss 1.37314582\n",
      "Trained batch 458 batch loss 1.32545018 epoch total loss 1.37304163\n",
      "Trained batch 459 batch loss 1.48220372 epoch total loss 1.37327945\n",
      "Trained batch 460 batch loss 1.3853898 epoch total loss 1.3733058\n",
      "Trained batch 461 batch loss 1.46208775 epoch total loss 1.37349832\n",
      "Trained batch 462 batch loss 1.30451882 epoch total loss 1.37334907\n",
      "Trained batch 463 batch loss 1.38183367 epoch total loss 1.37336731\n",
      "Trained batch 464 batch loss 1.30123496 epoch total loss 1.37321186\n",
      "Trained batch 465 batch loss 1.3383255 epoch total loss 1.37313676\n",
      "Trained batch 466 batch loss 1.37481141 epoch total loss 1.37314034\n",
      "Trained batch 467 batch loss 1.37324071 epoch total loss 1.37314057\n",
      "Trained batch 468 batch loss 1.44878292 epoch total loss 1.37330222\n",
      "Trained batch 469 batch loss 1.42483294 epoch total loss 1.37341201\n",
      "Trained batch 470 batch loss 1.38090825 epoch total loss 1.37342799\n",
      "Trained batch 471 batch loss 1.45837867 epoch total loss 1.37360835\n",
      "Trained batch 472 batch loss 1.4347682 epoch total loss 1.37373793\n",
      "Trained batch 473 batch loss 1.35344231 epoch total loss 1.37369502\n",
      "Trained batch 474 batch loss 1.34588528 epoch total loss 1.37363636\n",
      "Trained batch 475 batch loss 1.32373941 epoch total loss 1.37353134\n",
      "Trained batch 476 batch loss 1.33217633 epoch total loss 1.37344432\n",
      "Trained batch 477 batch loss 1.38594961 epoch total loss 1.37347054\n",
      "Trained batch 478 batch loss 1.31717551 epoch total loss 1.37335277\n",
      "Trained batch 479 batch loss 1.31744993 epoch total loss 1.37323606\n",
      "Trained batch 480 batch loss 1.33520389 epoch total loss 1.37315691\n",
      "Trained batch 481 batch loss 1.34408355 epoch total loss 1.37309635\n",
      "Trained batch 482 batch loss 1.31184435 epoch total loss 1.37296927\n",
      "Trained batch 483 batch loss 1.30313039 epoch total loss 1.37282455\n",
      "Trained batch 484 batch loss 1.17415953 epoch total loss 1.37241411\n",
      "Trained batch 485 batch loss 1.35978734 epoch total loss 1.37238812\n",
      "Trained batch 486 batch loss 1.28551745 epoch total loss 1.37220931\n",
      "Trained batch 487 batch loss 1.43808389 epoch total loss 1.37234461\n",
      "Trained batch 488 batch loss 1.36085176 epoch total loss 1.37232113\n",
      "Trained batch 489 batch loss 1.23477685 epoch total loss 1.37203991\n",
      "Trained batch 490 batch loss 1.28496146 epoch total loss 1.37186217\n",
      "Trained batch 491 batch loss 1.30543411 epoch total loss 1.37172687\n",
      "Trained batch 492 batch loss 1.40354633 epoch total loss 1.3717916\n",
      "Trained batch 493 batch loss 1.35388601 epoch total loss 1.37175524\n",
      "Trained batch 494 batch loss 1.39387012 epoch total loss 1.3718\n",
      "Trained batch 495 batch loss 1.24937224 epoch total loss 1.37155271\n",
      "Trained batch 496 batch loss 1.31556976 epoch total loss 1.37143981\n",
      "Trained batch 497 batch loss 1.25682163 epoch total loss 1.37120914\n",
      "Trained batch 498 batch loss 1.23110843 epoch total loss 1.37092781\n",
      "Trained batch 499 batch loss 1.27785504 epoch total loss 1.37074125\n",
      "Trained batch 500 batch loss 1.41032422 epoch total loss 1.3708204\n",
      "Trained batch 501 batch loss 1.31853914 epoch total loss 1.37071609\n",
      "Trained batch 502 batch loss 1.34171414 epoch total loss 1.3706584\n",
      "Trained batch 503 batch loss 1.32213116 epoch total loss 1.37056196\n",
      "Trained batch 504 batch loss 1.28560376 epoch total loss 1.37039328\n",
      "Trained batch 505 batch loss 1.34057593 epoch total loss 1.37033427\n",
      "Trained batch 506 batch loss 1.28899753 epoch total loss 1.37017357\n",
      "Trained batch 507 batch loss 1.32334054 epoch total loss 1.37008119\n",
      "Trained batch 508 batch loss 1.3813107 epoch total loss 1.37010324\n",
      "Trained batch 509 batch loss 1.43338609 epoch total loss 1.37022758\n",
      "Trained batch 510 batch loss 1.34998763 epoch total loss 1.37018788\n",
      "Trained batch 511 batch loss 1.38295519 epoch total loss 1.37021291\n",
      "Trained batch 512 batch loss 1.4796313 epoch total loss 1.37042654\n",
      "Trained batch 513 batch loss 1.38211322 epoch total loss 1.37044942\n",
      "Trained batch 514 batch loss 1.28732753 epoch total loss 1.37028766\n",
      "Trained batch 515 batch loss 1.37738907 epoch total loss 1.37030149\n",
      "Trained batch 516 batch loss 1.36228609 epoch total loss 1.37028599\n",
      "Trained batch 517 batch loss 1.31230974 epoch total loss 1.37017381\n",
      "Trained batch 518 batch loss 1.25784016 epoch total loss 1.36995697\n",
      "Trained batch 519 batch loss 1.19267869 epoch total loss 1.36961544\n",
      "Trained batch 520 batch loss 1.36872423 epoch total loss 1.36961365\n",
      "Trained batch 521 batch loss 1.30231214 epoch total loss 1.36948442\n",
      "Trained batch 522 batch loss 1.32110202 epoch total loss 1.3693918\n",
      "Trained batch 523 batch loss 1.48342407 epoch total loss 1.36960971\n",
      "Trained batch 524 batch loss 1.29238081 epoch total loss 1.36946237\n",
      "Trained batch 525 batch loss 1.30611455 epoch total loss 1.36934161\n",
      "Trained batch 526 batch loss 1.25571382 epoch total loss 1.3691256\n",
      "Trained batch 527 batch loss 1.26544237 epoch total loss 1.36892891\n",
      "Trained batch 528 batch loss 1.34951794 epoch total loss 1.36889219\n",
      "Trained batch 529 batch loss 1.45116615 epoch total loss 1.36904776\n",
      "Trained batch 530 batch loss 1.38387227 epoch total loss 1.36907566\n",
      "Trained batch 531 batch loss 1.3740027 epoch total loss 1.36908495\n",
      "Trained batch 532 batch loss 1.32882822 epoch total loss 1.36900938\n",
      "Trained batch 533 batch loss 1.25279045 epoch total loss 1.36879134\n",
      "Trained batch 534 batch loss 1.26887822 epoch total loss 1.36860418\n",
      "Trained batch 535 batch loss 1.36687 epoch total loss 1.36860096\n",
      "Trained batch 536 batch loss 1.49137247 epoch total loss 1.36883008\n",
      "Trained batch 537 batch loss 1.43400836 epoch total loss 1.36895144\n",
      "Trained batch 538 batch loss 1.46888804 epoch total loss 1.36913717\n",
      "Trained batch 539 batch loss 1.30942667 epoch total loss 1.36902642\n",
      "Trained batch 540 batch loss 1.31392384 epoch total loss 1.36892438\n",
      "Trained batch 541 batch loss 1.23767149 epoch total loss 1.36868179\n",
      "Trained batch 542 batch loss 1.39730632 epoch total loss 1.3687346\n",
      "Trained batch 543 batch loss 1.37698698 epoch total loss 1.36874986\n",
      "Trained batch 544 batch loss 1.37831473 epoch total loss 1.36876738\n",
      "Trained batch 545 batch loss 1.26610303 epoch total loss 1.36857903\n",
      "Trained batch 546 batch loss 1.23881328 epoch total loss 1.36834133\n",
      "Trained batch 547 batch loss 1.23258448 epoch total loss 1.36809325\n",
      "Trained batch 548 batch loss 1.33933663 epoch total loss 1.3680408\n",
      "Trained batch 549 batch loss 1.2651633 epoch total loss 1.36785328\n",
      "Trained batch 550 batch loss 1.37209654 epoch total loss 1.36786103\n",
      "Trained batch 551 batch loss 1.32537699 epoch total loss 1.3677839\n",
      "Trained batch 552 batch loss 1.34801495 epoch total loss 1.36774814\n",
      "Trained batch 553 batch loss 1.30220687 epoch total loss 1.36762953\n",
      "Trained batch 554 batch loss 1.42485785 epoch total loss 1.36773288\n",
      "Trained batch 555 batch loss 1.39704132 epoch total loss 1.36778569\n",
      "Trained batch 556 batch loss 1.36597157 epoch total loss 1.36778235\n",
      "Trained batch 557 batch loss 1.385373 epoch total loss 1.36781394\n",
      "Trained batch 558 batch loss 1.30751109 epoch total loss 1.36770582\n",
      "Trained batch 559 batch loss 1.23615122 epoch total loss 1.3674705\n",
      "Trained batch 560 batch loss 1.20907855 epoch total loss 1.36718774\n",
      "Trained batch 561 batch loss 1.38390744 epoch total loss 1.36721754\n",
      "Trained batch 562 batch loss 1.36249 epoch total loss 1.36720908\n",
      "Trained batch 563 batch loss 1.26616764 epoch total loss 1.36702967\n",
      "Trained batch 564 batch loss 1.38466573 epoch total loss 1.3670609\n",
      "Trained batch 565 batch loss 1.30298555 epoch total loss 1.36694741\n",
      "Trained batch 566 batch loss 1.32306886 epoch total loss 1.36686993\n",
      "Trained batch 567 batch loss 1.23206449 epoch total loss 1.3666321\n",
      "Trained batch 568 batch loss 1.26230812 epoch total loss 1.36644852\n",
      "Trained batch 569 batch loss 1.33977962 epoch total loss 1.36640167\n",
      "Trained batch 570 batch loss 1.39002895 epoch total loss 1.36644304\n",
      "Trained batch 571 batch loss 1.45790792 epoch total loss 1.36660326\n",
      "Trained batch 572 batch loss 1.31894302 epoch total loss 1.36651993\n",
      "Trained batch 573 batch loss 1.319327 epoch total loss 1.36643755\n",
      "Trained batch 574 batch loss 1.3346839 epoch total loss 1.36638224\n",
      "Trained batch 575 batch loss 1.33788288 epoch total loss 1.36633265\n",
      "Trained batch 576 batch loss 1.31534207 epoch total loss 1.3662442\n",
      "Trained batch 577 batch loss 1.28312099 epoch total loss 1.36610019\n",
      "Trained batch 578 batch loss 1.30057311 epoch total loss 1.36598682\n",
      "Trained batch 579 batch loss 1.34899211 epoch total loss 1.3659575\n",
      "Trained batch 580 batch loss 1.41901684 epoch total loss 1.36604893\n",
      "Trained batch 581 batch loss 1.33042061 epoch total loss 1.36598766\n",
      "Trained batch 582 batch loss 1.34946418 epoch total loss 1.36595941\n",
      "Trained batch 583 batch loss 1.36087155 epoch total loss 1.3659507\n",
      "Trained batch 584 batch loss 1.41828477 epoch total loss 1.36604023\n",
      "Trained batch 585 batch loss 1.34872806 epoch total loss 1.36601067\n",
      "Trained batch 586 batch loss 1.28127837 epoch total loss 1.36586607\n",
      "Trained batch 587 batch loss 1.27881348 epoch total loss 1.36571777\n",
      "Trained batch 588 batch loss 1.20499468 epoch total loss 1.36544442\n",
      "Trained batch 589 batch loss 1.18665087 epoch total loss 1.36514091\n",
      "Trained batch 590 batch loss 1.22022438 epoch total loss 1.36489522\n",
      "Trained batch 591 batch loss 1.19843292 epoch total loss 1.36461365\n",
      "Trained batch 592 batch loss 1.2895298 epoch total loss 1.36448681\n",
      "Trained batch 593 batch loss 1.34054422 epoch total loss 1.3644464\n",
      "Trained batch 594 batch loss 1.36096692 epoch total loss 1.36444056\n",
      "Trained batch 595 batch loss 1.3419826 epoch total loss 1.36440277\n",
      "Trained batch 596 batch loss 1.31936526 epoch total loss 1.36432719\n",
      "Trained batch 597 batch loss 1.31437349 epoch total loss 1.36424351\n",
      "Trained batch 598 batch loss 1.21929395 epoch total loss 1.36400115\n",
      "Trained batch 599 batch loss 1.38066292 epoch total loss 1.36402893\n",
      "Trained batch 600 batch loss 1.25604856 epoch total loss 1.36384892\n",
      "Trained batch 601 batch loss 1.13629293 epoch total loss 1.36347032\n",
      "Trained batch 602 batch loss 1.06427705 epoch total loss 1.36297333\n",
      "Trained batch 603 batch loss 1.20853305 epoch total loss 1.36271727\n",
      "Trained batch 604 batch loss 1.38307023 epoch total loss 1.36275089\n",
      "Trained batch 605 batch loss 1.47937036 epoch total loss 1.36294365\n",
      "Trained batch 606 batch loss 1.58264482 epoch total loss 1.36330628\n",
      "Trained batch 607 batch loss 1.4554882 epoch total loss 1.36345816\n",
      "Trained batch 608 batch loss 1.29498243 epoch total loss 1.3633455\n",
      "Trained batch 609 batch loss 1.35781646 epoch total loss 1.36333632\n",
      "Trained batch 610 batch loss 1.36582124 epoch total loss 1.3633405\n",
      "Trained batch 611 batch loss 1.27256465 epoch total loss 1.36319196\n",
      "Trained batch 612 batch loss 1.28843951 epoch total loss 1.36306977\n",
      "Trained batch 613 batch loss 1.3503139 epoch total loss 1.36304903\n",
      "Trained batch 614 batch loss 1.30235612 epoch total loss 1.36295021\n",
      "Trained batch 615 batch loss 1.25737238 epoch total loss 1.36277854\n",
      "Trained batch 616 batch loss 1.18074107 epoch total loss 1.36248302\n",
      "Trained batch 617 batch loss 1.07421958 epoch total loss 1.36201584\n",
      "Trained batch 618 batch loss 1.19471622 epoch total loss 1.36174512\n",
      "Trained batch 619 batch loss 1.44977641 epoch total loss 1.36188734\n",
      "Trained batch 620 batch loss 1.30221343 epoch total loss 1.36179101\n",
      "Trained batch 621 batch loss 1.39889288 epoch total loss 1.36185074\n",
      "Trained batch 622 batch loss 1.28467548 epoch total loss 1.36172664\n",
      "Trained batch 623 batch loss 1.36749077 epoch total loss 1.36173582\n",
      "Trained batch 624 batch loss 1.28047085 epoch total loss 1.36160564\n",
      "Trained batch 625 batch loss 1.23711157 epoch total loss 1.36140645\n",
      "Trained batch 626 batch loss 1.25014472 epoch total loss 1.3612287\n",
      "Trained batch 627 batch loss 1.28351784 epoch total loss 1.36110473\n",
      "Trained batch 628 batch loss 1.34968722 epoch total loss 1.36108649\n",
      "Trained batch 629 batch loss 1.26206696 epoch total loss 1.36092913\n",
      "Trained batch 630 batch loss 1.23024392 epoch total loss 1.36072171\n",
      "Trained batch 631 batch loss 1.23484468 epoch total loss 1.36052215\n",
      "Trained batch 632 batch loss 1.31578135 epoch total loss 1.36045146\n",
      "Trained batch 633 batch loss 1.36617637 epoch total loss 1.3604604\n",
      "Trained batch 634 batch loss 1.29692006 epoch total loss 1.36036026\n",
      "Trained batch 635 batch loss 1.38524866 epoch total loss 1.36039948\n",
      "Trained batch 636 batch loss 1.41461015 epoch total loss 1.36048472\n",
      "Trained batch 637 batch loss 1.43171668 epoch total loss 1.36059642\n",
      "Trained batch 638 batch loss 1.31482446 epoch total loss 1.36052477\n",
      "Trained batch 639 batch loss 1.41924727 epoch total loss 1.36061656\n",
      "Trained batch 640 batch loss 1.45612264 epoch total loss 1.36076581\n",
      "Trained batch 641 batch loss 1.29943728 epoch total loss 1.36067021\n",
      "Trained batch 642 batch loss 1.39882612 epoch total loss 1.36072958\n",
      "Trained batch 643 batch loss 1.47906566 epoch total loss 1.36091363\n",
      "Trained batch 644 batch loss 1.41742671 epoch total loss 1.36100137\n",
      "Trained batch 645 batch loss 1.32464862 epoch total loss 1.36094499\n",
      "Trained batch 646 batch loss 1.44558072 epoch total loss 1.361076\n",
      "Trained batch 647 batch loss 1.48911572 epoch total loss 1.36127388\n",
      "Trained batch 648 batch loss 1.48425543 epoch total loss 1.36146367\n",
      "Trained batch 649 batch loss 1.4497683 epoch total loss 1.36159968\n",
      "Trained batch 650 batch loss 1.23554111 epoch total loss 1.36140573\n",
      "Trained batch 651 batch loss 1.34225798 epoch total loss 1.3613764\n",
      "Trained batch 652 batch loss 1.33414304 epoch total loss 1.36133468\n",
      "Trained batch 653 batch loss 1.46780658 epoch total loss 1.36149776\n",
      "Trained batch 654 batch loss 1.29828107 epoch total loss 1.36140108\n",
      "Trained batch 655 batch loss 1.43275273 epoch total loss 1.36151\n",
      "Trained batch 656 batch loss 1.48977625 epoch total loss 1.36170554\n",
      "Trained batch 657 batch loss 1.59815192 epoch total loss 1.36206543\n",
      "Trained batch 658 batch loss 1.35820735 epoch total loss 1.36205947\n",
      "Trained batch 659 batch loss 1.30782092 epoch total loss 1.36197722\n",
      "Trained batch 660 batch loss 1.33307266 epoch total loss 1.36193335\n",
      "Trained batch 661 batch loss 1.29914653 epoch total loss 1.36183834\n",
      "Trained batch 662 batch loss 1.19177127 epoch total loss 1.36158144\n",
      "Trained batch 663 batch loss 1.28097701 epoch total loss 1.36146\n",
      "Trained batch 664 batch loss 1.30790389 epoch total loss 1.36137927\n",
      "Trained batch 665 batch loss 1.25347829 epoch total loss 1.36121702\n",
      "Trained batch 666 batch loss 1.28198206 epoch total loss 1.36109805\n",
      "Trained batch 667 batch loss 1.18824458 epoch total loss 1.36083889\n",
      "Trained batch 668 batch loss 1.1641072 epoch total loss 1.36054444\n",
      "Trained batch 669 batch loss 1.3664943 epoch total loss 1.36055338\n",
      "Trained batch 670 batch loss 1.29318726 epoch total loss 1.36045289\n",
      "Trained batch 671 batch loss 1.14961171 epoch total loss 1.36013865\n",
      "Trained batch 672 batch loss 1.2627244 epoch total loss 1.35999358\n",
      "Trained batch 673 batch loss 1.4056437 epoch total loss 1.36006141\n",
      "Trained batch 674 batch loss 1.38209319 epoch total loss 1.36009407\n",
      "Trained batch 675 batch loss 1.28880215 epoch total loss 1.35998857\n",
      "Trained batch 676 batch loss 1.50122786 epoch total loss 1.36019742\n",
      "Trained batch 677 batch loss 1.40857577 epoch total loss 1.36026883\n",
      "Trained batch 678 batch loss 1.47248685 epoch total loss 1.36043441\n",
      "Trained batch 679 batch loss 1.37723494 epoch total loss 1.36045921\n",
      "Trained batch 680 batch loss 1.40668869 epoch total loss 1.36052716\n",
      "Trained batch 681 batch loss 1.39674342 epoch total loss 1.36058033\n",
      "Trained batch 682 batch loss 1.40090704 epoch total loss 1.36063933\n",
      "Trained batch 683 batch loss 1.36506236 epoch total loss 1.36064589\n",
      "Trained batch 684 batch loss 1.32535815 epoch total loss 1.36059427\n",
      "Trained batch 685 batch loss 1.41604805 epoch total loss 1.36067522\n",
      "Trained batch 686 batch loss 1.36807489 epoch total loss 1.36068606\n",
      "Trained batch 687 batch loss 1.29713082 epoch total loss 1.36059356\n",
      "Trained batch 688 batch loss 1.27939975 epoch total loss 1.36047554\n",
      "Trained batch 689 batch loss 1.21294379 epoch total loss 1.36026144\n",
      "Trained batch 690 batch loss 1.24184525 epoch total loss 1.36008978\n",
      "Trained batch 691 batch loss 1.3049705 epoch total loss 1.36001\n",
      "Trained batch 692 batch loss 1.22568798 epoch total loss 1.35981596\n",
      "Trained batch 693 batch loss 1.21013713 epoch total loss 1.35960007\n",
      "Trained batch 694 batch loss 1.32042527 epoch total loss 1.35954356\n",
      "Trained batch 695 batch loss 1.25435305 epoch total loss 1.35939217\n",
      "Trained batch 696 batch loss 1.4365027 epoch total loss 1.35950303\n",
      "Trained batch 697 batch loss 1.42090774 epoch total loss 1.35959113\n",
      "Trained batch 698 batch loss 1.43042982 epoch total loss 1.35969257\n",
      "Trained batch 699 batch loss 1.32919621 epoch total loss 1.35964906\n",
      "Trained batch 700 batch loss 1.29720259 epoch total loss 1.35955977\n",
      "Trained batch 701 batch loss 1.39266551 epoch total loss 1.35960698\n",
      "Trained batch 702 batch loss 1.3765192 epoch total loss 1.35963106\n",
      "Trained batch 703 batch loss 1.38252878 epoch total loss 1.35966361\n",
      "Trained batch 704 batch loss 1.4381777 epoch total loss 1.35977507\n",
      "Trained batch 705 batch loss 1.36374247 epoch total loss 1.35978079\n",
      "Trained batch 706 batch loss 1.36781681 epoch total loss 1.35979211\n",
      "Trained batch 707 batch loss 1.4180311 epoch total loss 1.35987449\n",
      "Trained batch 708 batch loss 1.35198963 epoch total loss 1.3598634\n",
      "Trained batch 709 batch loss 1.36782575 epoch total loss 1.35987461\n",
      "Trained batch 710 batch loss 1.32583785 epoch total loss 1.35982668\n",
      "Trained batch 711 batch loss 1.36692786 epoch total loss 1.3598367\n",
      "Trained batch 712 batch loss 1.3910836 epoch total loss 1.35988057\n",
      "Trained batch 713 batch loss 1.30422461 epoch total loss 1.35980248\n",
      "Trained batch 714 batch loss 1.33013892 epoch total loss 1.359761\n",
      "Trained batch 715 batch loss 1.21570361 epoch total loss 1.35955942\n",
      "Trained batch 716 batch loss 1.23499584 epoch total loss 1.35938549\n",
      "Trained batch 717 batch loss 1.33173573 epoch total loss 1.35934687\n",
      "Trained batch 718 batch loss 1.30062485 epoch total loss 1.35926509\n",
      "Trained batch 719 batch loss 1.29557061 epoch total loss 1.35917652\n",
      "Trained batch 720 batch loss 1.42863786 epoch total loss 1.35927308\n",
      "Trained batch 721 batch loss 1.35081792 epoch total loss 1.35926127\n",
      "Trained batch 722 batch loss 1.33042288 epoch total loss 1.35922134\n",
      "Trained batch 723 batch loss 1.32480812 epoch total loss 1.35917377\n",
      "Trained batch 724 batch loss 1.29426348 epoch total loss 1.35908413\n",
      "Trained batch 725 batch loss 1.25827146 epoch total loss 1.35894513\n",
      "Trained batch 726 batch loss 1.27757847 epoch total loss 1.35883307\n",
      "Trained batch 727 batch loss 1.23518801 epoch total loss 1.35866296\n",
      "Trained batch 728 batch loss 1.3500309 epoch total loss 1.35865116\n",
      "Trained batch 729 batch loss 1.52010453 epoch total loss 1.35887253\n",
      "Trained batch 730 batch loss 1.58031893 epoch total loss 1.35917592\n",
      "Trained batch 731 batch loss 1.4294498 epoch total loss 1.359272\n",
      "Trained batch 732 batch loss 1.48066747 epoch total loss 1.35943782\n",
      "Trained batch 733 batch loss 1.38721323 epoch total loss 1.35947573\n",
      "Trained batch 734 batch loss 1.2304064 epoch total loss 1.3592999\n",
      "Trained batch 735 batch loss 1.09661543 epoch total loss 1.35894251\n",
      "Trained batch 736 batch loss 1.06119514 epoch total loss 1.35853803\n",
      "Trained batch 737 batch loss 1.1141789 epoch total loss 1.35820651\n",
      "Trained batch 738 batch loss 1.22979712 epoch total loss 1.35803246\n",
      "Trained batch 739 batch loss 1.50724518 epoch total loss 1.35823441\n",
      "Trained batch 740 batch loss 1.25763392 epoch total loss 1.35809851\n",
      "Trained batch 741 batch loss 1.13025463 epoch total loss 1.35779095\n",
      "Trained batch 742 batch loss 1.17197978 epoch total loss 1.35754061\n",
      "Trained batch 743 batch loss 1.28455901 epoch total loss 1.35744238\n",
      "Trained batch 744 batch loss 1.21192145 epoch total loss 1.35724676\n",
      "Trained batch 745 batch loss 1.27733898 epoch total loss 1.35713947\n",
      "Trained batch 746 batch loss 1.2311281 epoch total loss 1.35697055\n",
      "Trained batch 747 batch loss 1.33349681 epoch total loss 1.3569392\n",
      "Trained batch 748 batch loss 1.32401443 epoch total loss 1.35689521\n",
      "Trained batch 749 batch loss 1.33994174 epoch total loss 1.35687256\n",
      "Trained batch 750 batch loss 1.42377281 epoch total loss 1.35696173\n",
      "Trained batch 751 batch loss 1.22796595 epoch total loss 1.35679\n",
      "Trained batch 752 batch loss 1.2483995 epoch total loss 1.35664582\n",
      "Trained batch 753 batch loss 1.29978728 epoch total loss 1.35657036\n",
      "Trained batch 754 batch loss 1.35672188 epoch total loss 1.3565706\n",
      "Trained batch 755 batch loss 1.47057581 epoch total loss 1.35672164\n",
      "Trained batch 756 batch loss 1.38081431 epoch total loss 1.35675359\n",
      "Trained batch 757 batch loss 1.29482591 epoch total loss 1.35667169\n",
      "Trained batch 758 batch loss 1.28610182 epoch total loss 1.35657871\n",
      "Trained batch 759 batch loss 1.281515 epoch total loss 1.35647976\n",
      "Trained batch 760 batch loss 1.34037149 epoch total loss 1.35645843\n",
      "Trained batch 761 batch loss 1.38423347 epoch total loss 1.35649502\n",
      "Trained batch 762 batch loss 1.33183932 epoch total loss 1.3564626\n",
      "Trained batch 763 batch loss 1.29532886 epoch total loss 1.35638249\n",
      "Trained batch 764 batch loss 1.18284988 epoch total loss 1.35615528\n",
      "Trained batch 765 batch loss 1.26612043 epoch total loss 1.35603762\n",
      "Trained batch 766 batch loss 1.32310176 epoch total loss 1.35599458\n",
      "Trained batch 767 batch loss 1.34860229 epoch total loss 1.35598505\n",
      "Trained batch 768 batch loss 1.48839188 epoch total loss 1.35615742\n",
      "Trained batch 769 batch loss 1.51675367 epoch total loss 1.35636628\n",
      "Trained batch 770 batch loss 1.48578262 epoch total loss 1.35653436\n",
      "Trained batch 771 batch loss 1.38118649 epoch total loss 1.35656643\n",
      "Trained batch 772 batch loss 1.34972763 epoch total loss 1.35655761\n",
      "Trained batch 773 batch loss 1.17667758 epoch total loss 1.35632479\n",
      "Trained batch 774 batch loss 1.28972852 epoch total loss 1.35623872\n",
      "Trained batch 775 batch loss 1.35233474 epoch total loss 1.3562336\n",
      "Trained batch 776 batch loss 1.30110526 epoch total loss 1.35616267\n",
      "Trained batch 777 batch loss 1.30950952 epoch total loss 1.35610271\n",
      "Trained batch 778 batch loss 1.37743223 epoch total loss 1.35613012\n",
      "Trained batch 779 batch loss 1.31870914 epoch total loss 1.35608208\n",
      "Trained batch 780 batch loss 1.31029773 epoch total loss 1.35602343\n",
      "Trained batch 781 batch loss 1.26148522 epoch total loss 1.35590231\n",
      "Trained batch 782 batch loss 1.38566458 epoch total loss 1.35594034\n",
      "Trained batch 783 batch loss 1.42014122 epoch total loss 1.35602236\n",
      "Trained batch 784 batch loss 1.37961149 epoch total loss 1.35605252\n",
      "Trained batch 785 batch loss 1.36687922 epoch total loss 1.35606623\n",
      "Trained batch 786 batch loss 1.35827398 epoch total loss 1.35606897\n",
      "Trained batch 787 batch loss 1.41384375 epoch total loss 1.3561424\n",
      "Trained batch 788 batch loss 1.30846536 epoch total loss 1.35608184\n",
      "Trained batch 789 batch loss 1.38623953 epoch total loss 1.35612011\n",
      "Trained batch 790 batch loss 1.41201389 epoch total loss 1.3561908\n",
      "Trained batch 791 batch loss 1.40823317 epoch total loss 1.3562566\n",
      "Trained batch 792 batch loss 1.24909794 epoch total loss 1.3561213\n",
      "Trained batch 793 batch loss 1.33838129 epoch total loss 1.35609901\n",
      "Trained batch 794 batch loss 1.36273742 epoch total loss 1.35610735\n",
      "Trained batch 795 batch loss 1.42814147 epoch total loss 1.35619795\n",
      "Trained batch 796 batch loss 1.34487534 epoch total loss 1.35618365\n",
      "Trained batch 797 batch loss 1.32607329 epoch total loss 1.35614586\n",
      "Trained batch 798 batch loss 1.30596268 epoch total loss 1.35608292\n",
      "Trained batch 799 batch loss 1.34509504 epoch total loss 1.35606921\n",
      "Trained batch 800 batch loss 1.40941238 epoch total loss 1.35613585\n",
      "Trained batch 801 batch loss 1.36740053 epoch total loss 1.35614991\n",
      "Trained batch 802 batch loss 1.25148582 epoch total loss 1.35601938\n",
      "Trained batch 803 batch loss 1.36291397 epoch total loss 1.35602808\n",
      "Trained batch 804 batch loss 1.24281049 epoch total loss 1.35588717\n",
      "Trained batch 805 batch loss 1.46415687 epoch total loss 1.35602164\n",
      "Trained batch 806 batch loss 1.2777257 epoch total loss 1.35592449\n",
      "Trained batch 807 batch loss 1.450032 epoch total loss 1.35604119\n",
      "Trained batch 808 batch loss 1.38938844 epoch total loss 1.35608244\n",
      "Trained batch 809 batch loss 1.17725897 epoch total loss 1.35586143\n",
      "Trained batch 810 batch loss 1.34204042 epoch total loss 1.35584426\n",
      "Trained batch 811 batch loss 1.2371695 epoch total loss 1.35569799\n",
      "Trained batch 812 batch loss 1.18847704 epoch total loss 1.355492\n",
      "Trained batch 813 batch loss 1.20338213 epoch total loss 1.35530496\n",
      "Trained batch 814 batch loss 1.28079009 epoch total loss 1.3552134\n",
      "Trained batch 815 batch loss 1.31721783 epoch total loss 1.35516679\n",
      "Trained batch 816 batch loss 1.20465779 epoch total loss 1.35498238\n",
      "Trained batch 817 batch loss 1.17419231 epoch total loss 1.35476112\n",
      "Trained batch 818 batch loss 1.2886858 epoch total loss 1.35468042\n",
      "Trained batch 819 batch loss 1.25750566 epoch total loss 1.35456169\n",
      "Trained batch 820 batch loss 1.28261685 epoch total loss 1.35447383\n",
      "Trained batch 821 batch loss 1.38668537 epoch total loss 1.35451317\n",
      "Trained batch 822 batch loss 1.36901581 epoch total loss 1.35453081\n",
      "Trained batch 823 batch loss 1.35294604 epoch total loss 1.35452878\n",
      "Trained batch 824 batch loss 1.37141132 epoch total loss 1.35454941\n",
      "Trained batch 825 batch loss 1.36936355 epoch total loss 1.35456741\n",
      "Trained batch 826 batch loss 1.27091849 epoch total loss 1.35446608\n",
      "Trained batch 827 batch loss 1.28455448 epoch total loss 1.35438144\n",
      "Trained batch 828 batch loss 1.14305401 epoch total loss 1.35412621\n",
      "Trained batch 829 batch loss 1.13853073 epoch total loss 1.35386622\n",
      "Trained batch 830 batch loss 1.38383579 epoch total loss 1.35390222\n",
      "Trained batch 831 batch loss 1.55310225 epoch total loss 1.35414195\n",
      "Trained batch 832 batch loss 1.38709235 epoch total loss 1.35418153\n",
      "Trained batch 833 batch loss 1.33022833 epoch total loss 1.3541528\n",
      "Trained batch 834 batch loss 1.30874908 epoch total loss 1.35409832\n",
      "Trained batch 835 batch loss 1.28073692 epoch total loss 1.35401046\n",
      "Trained batch 836 batch loss 1.29161167 epoch total loss 1.35393584\n",
      "Trained batch 837 batch loss 1.27737844 epoch total loss 1.3538444\n",
      "Trained batch 838 batch loss 1.37187123 epoch total loss 1.35386586\n",
      "Trained batch 839 batch loss 1.34452176 epoch total loss 1.35385466\n",
      "Trained batch 840 batch loss 1.36012971 epoch total loss 1.35386205\n",
      "Trained batch 841 batch loss 1.36426723 epoch total loss 1.35387444\n",
      "Trained batch 842 batch loss 1.33083022 epoch total loss 1.35384703\n",
      "Trained batch 843 batch loss 1.25626349 epoch total loss 1.35373127\n",
      "Trained batch 844 batch loss 1.29178905 epoch total loss 1.35365784\n",
      "Trained batch 845 batch loss 1.30928826 epoch total loss 1.35360539\n",
      "Trained batch 846 batch loss 1.3665781 epoch total loss 1.35362065\n",
      "Trained batch 847 batch loss 1.28154135 epoch total loss 1.35353553\n",
      "Trained batch 848 batch loss 1.36873555 epoch total loss 1.35355353\n",
      "Trained batch 849 batch loss 1.5296 epoch total loss 1.35376072\n",
      "Trained batch 850 batch loss 1.26111233 epoch total loss 1.35365176\n",
      "Trained batch 851 batch loss 1.23316991 epoch total loss 1.35351014\n",
      "Trained batch 852 batch loss 1.23133898 epoch total loss 1.35336673\n",
      "Trained batch 853 batch loss 1.21587455 epoch total loss 1.35320556\n",
      "Trained batch 854 batch loss 1.31313241 epoch total loss 1.35315859\n",
      "Trained batch 855 batch loss 1.18858433 epoch total loss 1.35296607\n",
      "Trained batch 856 batch loss 1.19351244 epoch total loss 1.35277975\n",
      "Trained batch 857 batch loss 1.32214928 epoch total loss 1.35274398\n",
      "Trained batch 858 batch loss 1.29719663 epoch total loss 1.35267937\n",
      "Trained batch 859 batch loss 1.22346938 epoch total loss 1.35252893\n",
      "Trained batch 860 batch loss 1.16992462 epoch total loss 1.35231662\n",
      "Trained batch 861 batch loss 1.31248474 epoch total loss 1.35227036\n",
      "Trained batch 862 batch loss 1.28930902 epoch total loss 1.35219741\n",
      "Trained batch 863 batch loss 1.40510333 epoch total loss 1.35225868\n",
      "Trained batch 864 batch loss 1.43067551 epoch total loss 1.3523494\n",
      "Trained batch 865 batch loss 1.41270936 epoch total loss 1.35241926\n",
      "Trained batch 866 batch loss 1.36436498 epoch total loss 1.35243309\n",
      "Trained batch 867 batch loss 1.33990121 epoch total loss 1.35241854\n",
      "Trained batch 868 batch loss 1.27118278 epoch total loss 1.35232496\n",
      "Trained batch 869 batch loss 1.1689297 epoch total loss 1.35211396\n",
      "Trained batch 870 batch loss 1.25352502 epoch total loss 1.35200071\n",
      "Trained batch 871 batch loss 1.20976305 epoch total loss 1.35183728\n",
      "Trained batch 872 batch loss 1.28189862 epoch total loss 1.35175705\n",
      "Trained batch 873 batch loss 1.30822325 epoch total loss 1.35170722\n",
      "Trained batch 874 batch loss 1.3326273 epoch total loss 1.3516854\n",
      "Trained batch 875 batch loss 1.29989362 epoch total loss 1.35162628\n",
      "Trained batch 876 batch loss 1.31716681 epoch total loss 1.35158694\n",
      "Trained batch 877 batch loss 1.47252667 epoch total loss 1.35172474\n",
      "Trained batch 878 batch loss 1.37154603 epoch total loss 1.35174739\n",
      "Trained batch 879 batch loss 1.34096313 epoch total loss 1.35173512\n",
      "Trained batch 880 batch loss 1.21957397 epoch total loss 1.35158503\n",
      "Trained batch 881 batch loss 1.32094145 epoch total loss 1.35155022\n",
      "Trained batch 882 batch loss 1.2469362 epoch total loss 1.35143161\n",
      "Trained batch 883 batch loss 1.39511561 epoch total loss 1.35148108\n",
      "Trained batch 884 batch loss 1.33806336 epoch total loss 1.35146582\n",
      "Trained batch 885 batch loss 1.45164216 epoch total loss 1.35157907\n",
      "Trained batch 886 batch loss 1.40814304 epoch total loss 1.35164297\n",
      "Trained batch 887 batch loss 1.22636843 epoch total loss 1.3515017\n",
      "Trained batch 888 batch loss 1.29010403 epoch total loss 1.35143256\n",
      "Trained batch 889 batch loss 1.34882689 epoch total loss 1.3514297\n",
      "Trained batch 890 batch loss 1.36351228 epoch total loss 1.35144329\n",
      "Trained batch 891 batch loss 1.41245532 epoch total loss 1.35151184\n",
      "Trained batch 892 batch loss 1.42093277 epoch total loss 1.35158956\n",
      "Trained batch 893 batch loss 1.39617586 epoch total loss 1.35163951\n",
      "Trained batch 894 batch loss 1.36152673 epoch total loss 1.3516506\n",
      "Trained batch 895 batch loss 1.41062891 epoch total loss 1.35171652\n",
      "Trained batch 896 batch loss 1.38564932 epoch total loss 1.35175431\n",
      "Trained batch 897 batch loss 1.28861046 epoch total loss 1.35168386\n",
      "Trained batch 898 batch loss 1.3394506 epoch total loss 1.35167027\n",
      "Trained batch 899 batch loss 1.49238873 epoch total loss 1.35182691\n",
      "Trained batch 900 batch loss 1.50100899 epoch total loss 1.35199261\n",
      "Trained batch 901 batch loss 1.35670233 epoch total loss 1.35199785\n",
      "Trained batch 902 batch loss 1.2658664 epoch total loss 1.35190237\n",
      "Trained batch 903 batch loss 1.39564574 epoch total loss 1.35195076\n",
      "Trained batch 904 batch loss 1.38067341 epoch total loss 1.35198247\n",
      "Trained batch 905 batch loss 1.37365472 epoch total loss 1.35200644\n",
      "Trained batch 906 batch loss 1.396819 epoch total loss 1.35205591\n",
      "Trained batch 907 batch loss 1.47365701 epoch total loss 1.3521899\n",
      "Trained batch 908 batch loss 1.41224039 epoch total loss 1.35225606\n",
      "Trained batch 909 batch loss 1.28414917 epoch total loss 1.3521812\n",
      "Trained batch 910 batch loss 1.20221603 epoch total loss 1.35201645\n",
      "Trained batch 911 batch loss 1.18998754 epoch total loss 1.35183859\n",
      "Trained batch 912 batch loss 1.23503733 epoch total loss 1.35171044\n",
      "Trained batch 913 batch loss 1.31463099 epoch total loss 1.35166979\n",
      "Trained batch 914 batch loss 1.3512156 epoch total loss 1.35166919\n",
      "Trained batch 915 batch loss 1.33950877 epoch total loss 1.35165584\n",
      "Trained batch 916 batch loss 1.48599219 epoch total loss 1.35180247\n",
      "Trained batch 917 batch loss 1.46032321 epoch total loss 1.35192084\n",
      "Trained batch 918 batch loss 1.3538568 epoch total loss 1.35192299\n",
      "Trained batch 919 batch loss 1.35328913 epoch total loss 1.35192442\n",
      "Trained batch 920 batch loss 1.3113308 epoch total loss 1.35188031\n",
      "Trained batch 921 batch loss 1.3112936 epoch total loss 1.3518362\n",
      "Trained batch 922 batch loss 1.42259228 epoch total loss 1.35191298\n",
      "Trained batch 923 batch loss 1.45954454 epoch total loss 1.35202956\n",
      "Trained batch 924 batch loss 1.53619397 epoch total loss 1.352229\n",
      "Trained batch 925 batch loss 1.51387811 epoch total loss 1.35240376\n",
      "Trained batch 926 batch loss 1.41701484 epoch total loss 1.3524735\n",
      "Trained batch 927 batch loss 1.565153 epoch total loss 1.35270298\n",
      "Trained batch 928 batch loss 1.38599443 epoch total loss 1.35273886\n",
      "Trained batch 929 batch loss 1.41768217 epoch total loss 1.35280883\n",
      "Trained batch 930 batch loss 1.48440707 epoch total loss 1.35295033\n",
      "Trained batch 931 batch loss 1.34540164 epoch total loss 1.35294223\n",
      "Trained batch 932 batch loss 1.2630204 epoch total loss 1.35284579\n",
      "Trained batch 933 batch loss 1.24487579 epoch total loss 1.35273\n",
      "Trained batch 934 batch loss 1.24061584 epoch total loss 1.35261\n",
      "Trained batch 935 batch loss 1.24884701 epoch total loss 1.35249913\n",
      "Trained batch 936 batch loss 1.21918762 epoch total loss 1.35235679\n",
      "Trained batch 937 batch loss 1.28764164 epoch total loss 1.35228765\n",
      "Trained batch 938 batch loss 1.13517666 epoch total loss 1.35205615\n",
      "Trained batch 939 batch loss 1.27261412 epoch total loss 1.35197151\n",
      "Trained batch 940 batch loss 1.22490215 epoch total loss 1.3518362\n",
      "Trained batch 941 batch loss 1.25811303 epoch total loss 1.35173655\n",
      "Trained batch 942 batch loss 1.26031375 epoch total loss 1.35163951\n",
      "Trained batch 943 batch loss 1.19345558 epoch total loss 1.35147178\n",
      "Trained batch 944 batch loss 1.24675941 epoch total loss 1.3513608\n",
      "Trained batch 945 batch loss 1.23670197 epoch total loss 1.35123944\n",
      "Trained batch 946 batch loss 1.42267692 epoch total loss 1.35131502\n",
      "Trained batch 947 batch loss 1.42650092 epoch total loss 1.35139441\n",
      "Trained batch 948 batch loss 1.35963058 epoch total loss 1.35140312\n",
      "Trained batch 949 batch loss 1.39814067 epoch total loss 1.35145235\n",
      "Trained batch 950 batch loss 1.46731901 epoch total loss 1.3515743\n",
      "Trained batch 951 batch loss 1.37526941 epoch total loss 1.35159922\n",
      "Trained batch 952 batch loss 1.40299666 epoch total loss 1.3516531\n",
      "Trained batch 953 batch loss 1.32186174 epoch total loss 1.35162199\n",
      "Trained batch 954 batch loss 1.23577821 epoch total loss 1.35150039\n",
      "Trained batch 955 batch loss 1.36520803 epoch total loss 1.35151482\n",
      "Trained batch 956 batch loss 1.3747499 epoch total loss 1.35153913\n",
      "Trained batch 957 batch loss 1.26001191 epoch total loss 1.35144353\n",
      "Trained batch 958 batch loss 1.36382365 epoch total loss 1.3514564\n",
      "Trained batch 959 batch loss 1.43633723 epoch total loss 1.35154486\n",
      "Trained batch 960 batch loss 1.35317266 epoch total loss 1.35154653\n",
      "Trained batch 961 batch loss 1.3402735 epoch total loss 1.35153484\n",
      "Trained batch 962 batch loss 1.32094467 epoch total loss 1.35150301\n",
      "Trained batch 963 batch loss 1.25059414 epoch total loss 1.35139823\n",
      "Trained batch 964 batch loss 1.23807585 epoch total loss 1.35128057\n",
      "Trained batch 965 batch loss 1.3804003 epoch total loss 1.35131073\n",
      "Trained batch 966 batch loss 1.46781707 epoch total loss 1.35143137\n",
      "Trained batch 967 batch loss 1.32660985 epoch total loss 1.35140574\n",
      "Trained batch 968 batch loss 1.33152723 epoch total loss 1.35138524\n",
      "Trained batch 969 batch loss 1.40080452 epoch total loss 1.35143614\n",
      "Trained batch 970 batch loss 1.36056018 epoch total loss 1.35144556\n",
      "Trained batch 971 batch loss 1.35145795 epoch total loss 1.35144556\n",
      "Trained batch 972 batch loss 1.15756392 epoch total loss 1.35124612\n",
      "Trained batch 973 batch loss 1.32394576 epoch total loss 1.3512181\n",
      "Trained batch 974 batch loss 1.33953714 epoch total loss 1.35120606\n",
      "Trained batch 975 batch loss 1.33810508 epoch total loss 1.35119271\n",
      "Trained batch 976 batch loss 1.35350609 epoch total loss 1.3511951\n",
      "Trained batch 977 batch loss 1.27753711 epoch total loss 1.35111976\n",
      "Trained batch 978 batch loss 1.24482369 epoch total loss 1.35101104\n",
      "Trained batch 979 batch loss 1.32473993 epoch total loss 1.35098422\n",
      "Trained batch 980 batch loss 1.40603828 epoch total loss 1.35104036\n",
      "Trained batch 981 batch loss 1.51149893 epoch total loss 1.35120392\n",
      "Trained batch 982 batch loss 1.46514702 epoch total loss 1.35131991\n",
      "Trained batch 983 batch loss 1.37663007 epoch total loss 1.35134554\n",
      "Trained batch 984 batch loss 1.34747601 epoch total loss 1.35134172\n",
      "Trained batch 985 batch loss 1.288589 epoch total loss 1.35127795\n",
      "Trained batch 986 batch loss 1.3897301 epoch total loss 1.35131705\n",
      "Trained batch 987 batch loss 1.24950552 epoch total loss 1.35121381\n",
      "Trained batch 988 batch loss 1.38700068 epoch total loss 1.35125\n",
      "Trained batch 989 batch loss 1.42603958 epoch total loss 1.35132563\n",
      "Trained batch 990 batch loss 1.4634645 epoch total loss 1.351439\n",
      "Trained batch 991 batch loss 1.40685141 epoch total loss 1.35149491\n",
      "Trained batch 992 batch loss 1.50781345 epoch total loss 1.3516525\n",
      "Trained batch 993 batch loss 1.48449779 epoch total loss 1.35178626\n",
      "Trained batch 994 batch loss 1.50161481 epoch total loss 1.35193694\n",
      "Trained batch 995 batch loss 1.41062903 epoch total loss 1.35199594\n",
      "Trained batch 996 batch loss 1.40619814 epoch total loss 1.35205042\n",
      "Trained batch 997 batch loss 1.44446552 epoch total loss 1.35214305\n",
      "Trained batch 998 batch loss 1.44178915 epoch total loss 1.35223293\n",
      "Trained batch 999 batch loss 1.45078993 epoch total loss 1.35233164\n",
      "Trained batch 1000 batch loss 1.30653417 epoch total loss 1.35228574\n",
      "Trained batch 1001 batch loss 1.12833548 epoch total loss 1.35206199\n",
      "Trained batch 1002 batch loss 1.27391171 epoch total loss 1.35198402\n",
      "Trained batch 1003 batch loss 1.3269242 epoch total loss 1.35195899\n",
      "Trained batch 1004 batch loss 1.45690894 epoch total loss 1.35206354\n",
      "Trained batch 1005 batch loss 1.3140974 epoch total loss 1.35202575\n",
      "Trained batch 1006 batch loss 1.31486416 epoch total loss 1.35198879\n",
      "Trained batch 1007 batch loss 1.37488067 epoch total loss 1.35201156\n",
      "Trained batch 1008 batch loss 1.35755122 epoch total loss 1.35201705\n",
      "Trained batch 1009 batch loss 1.33961535 epoch total loss 1.35200465\n",
      "Trained batch 1010 batch loss 1.25029206 epoch total loss 1.35190392\n",
      "Trained batch 1011 batch loss 1.30303454 epoch total loss 1.35185552\n",
      "Trained batch 1012 batch loss 1.35216177 epoch total loss 1.35185587\n",
      "Trained batch 1013 batch loss 1.2743628 epoch total loss 1.35177946\n",
      "Trained batch 1014 batch loss 1.31215811 epoch total loss 1.35174036\n",
      "Trained batch 1015 batch loss 1.32848239 epoch total loss 1.35171735\n",
      "Trained batch 1016 batch loss 1.3300066 epoch total loss 1.35169601\n",
      "Trained batch 1017 batch loss 1.40552783 epoch total loss 1.35174894\n",
      "Trained batch 1018 batch loss 1.34529746 epoch total loss 1.35174263\n",
      "Trained batch 1019 batch loss 1.29981363 epoch total loss 1.3516916\n",
      "Trained batch 1020 batch loss 1.38625896 epoch total loss 1.35172546\n",
      "Trained batch 1021 batch loss 1.20990729 epoch total loss 1.3515867\n",
      "Trained batch 1022 batch loss 1.22532344 epoch total loss 1.35146308\n",
      "Trained batch 1023 batch loss 1.18982685 epoch total loss 1.35130513\n",
      "Trained batch 1024 batch loss 1.12917471 epoch total loss 1.35108817\n",
      "Trained batch 1025 batch loss 1.25711536 epoch total loss 1.35099649\n",
      "Trained batch 1026 batch loss 1.27532279 epoch total loss 1.35092258\n",
      "Trained batch 1027 batch loss 1.41042638 epoch total loss 1.35098052\n",
      "Trained batch 1028 batch loss 1.52195275 epoch total loss 1.35114694\n",
      "Trained batch 1029 batch loss 1.31618631 epoch total loss 1.35111284\n",
      "Trained batch 1030 batch loss 1.26026773 epoch total loss 1.35102463\n",
      "Trained batch 1031 batch loss 1.40965474 epoch total loss 1.35108161\n",
      "Trained batch 1032 batch loss 1.25294471 epoch total loss 1.35098648\n",
      "Trained batch 1033 batch loss 1.36465538 epoch total loss 1.35099959\n",
      "Trained batch 1034 batch loss 1.40664446 epoch total loss 1.35105348\n",
      "Trained batch 1035 batch loss 1.37965012 epoch total loss 1.35108101\n",
      "Trained batch 1036 batch loss 1.49344468 epoch total loss 1.35121846\n",
      "Trained batch 1037 batch loss 1.386958 epoch total loss 1.35125291\n",
      "Trained batch 1038 batch loss 1.36821938 epoch total loss 1.35126925\n",
      "Trained batch 1039 batch loss 1.24591184 epoch total loss 1.3511678\n",
      "Trained batch 1040 batch loss 1.25186992 epoch total loss 1.35107231\n",
      "Trained batch 1041 batch loss 1.22774315 epoch total loss 1.35095394\n",
      "Trained batch 1042 batch loss 1.30305934 epoch total loss 1.35090804\n",
      "Trained batch 1043 batch loss 1.31697905 epoch total loss 1.3508755\n",
      "Trained batch 1044 batch loss 1.36552 epoch total loss 1.35088944\n",
      "Trained batch 1045 batch loss 1.30559766 epoch total loss 1.35084605\n",
      "Trained batch 1046 batch loss 1.59144199 epoch total loss 1.35107613\n",
      "Trained batch 1047 batch loss 1.58746266 epoch total loss 1.35130179\n",
      "Trained batch 1048 batch loss 1.36778545 epoch total loss 1.35131752\n",
      "Trained batch 1049 batch loss 1.24213409 epoch total loss 1.35121346\n",
      "Trained batch 1050 batch loss 1.16292143 epoch total loss 1.35103428\n",
      "Trained batch 1051 batch loss 1.10272217 epoch total loss 1.35079801\n",
      "Trained batch 1052 batch loss 1.18899381 epoch total loss 1.35064423\n",
      "Trained batch 1053 batch loss 1.26294422 epoch total loss 1.3505609\n",
      "Trained batch 1054 batch loss 1.05138302 epoch total loss 1.35027707\n",
      "Trained batch 1055 batch loss 1.04102492 epoch total loss 1.34998393\n",
      "Trained batch 1056 batch loss 1.09715056 epoch total loss 1.34974456\n",
      "Trained batch 1057 batch loss 1.13461924 epoch total loss 1.34954095\n",
      "Trained batch 1058 batch loss 1.21285582 epoch total loss 1.34941185\n",
      "Trained batch 1059 batch loss 1.35821044 epoch total loss 1.34942007\n",
      "Trained batch 1060 batch loss 1.37637472 epoch total loss 1.34944546\n",
      "Trained batch 1061 batch loss 1.35239112 epoch total loss 1.34944832\n",
      "Trained batch 1062 batch loss 1.37068701 epoch total loss 1.34946835\n",
      "Trained batch 1063 batch loss 1.15929627 epoch total loss 1.34928942\n",
      "Trained batch 1064 batch loss 1.16944408 epoch total loss 1.34912038\n",
      "Trained batch 1065 batch loss 1.45739686 epoch total loss 1.34922206\n",
      "Trained batch 1066 batch loss 1.35181332 epoch total loss 1.34922445\n",
      "Trained batch 1067 batch loss 1.3247726 epoch total loss 1.34920168\n",
      "Trained batch 1068 batch loss 1.35983884 epoch total loss 1.34921157\n",
      "Trained batch 1069 batch loss 1.43019962 epoch total loss 1.34928739\n",
      "Trained batch 1070 batch loss 1.33364499 epoch total loss 1.34927273\n",
      "Trained batch 1071 batch loss 1.18511462 epoch total loss 1.34911942\n",
      "Trained batch 1072 batch loss 1.24527848 epoch total loss 1.34902251\n",
      "Trained batch 1073 batch loss 1.29926586 epoch total loss 1.34897614\n",
      "Trained batch 1074 batch loss 1.32125485 epoch total loss 1.34895039\n",
      "Trained batch 1075 batch loss 1.35241 epoch total loss 1.3489536\n",
      "Trained batch 1076 batch loss 1.48821473 epoch total loss 1.34908295\n",
      "Trained batch 1077 batch loss 1.35401058 epoch total loss 1.3490876\n",
      "Trained batch 1078 batch loss 1.39643121 epoch total loss 1.34913146\n",
      "Trained batch 1079 batch loss 1.45544124 epoch total loss 1.34923\n",
      "Trained batch 1080 batch loss 1.44882989 epoch total loss 1.34932232\n",
      "Trained batch 1081 batch loss 1.46811938 epoch total loss 1.34943223\n",
      "Trained batch 1082 batch loss 1.400967 epoch total loss 1.34947991\n",
      "Trained batch 1083 batch loss 1.47378981 epoch total loss 1.34959459\n",
      "Trained batch 1084 batch loss 1.43518364 epoch total loss 1.34967351\n",
      "Trained batch 1085 batch loss 1.33912396 epoch total loss 1.34966385\n",
      "Trained batch 1086 batch loss 1.30143559 epoch total loss 1.34961939\n",
      "Trained batch 1087 batch loss 1.31952286 epoch total loss 1.34959173\n",
      "Trained batch 1088 batch loss 1.37751019 epoch total loss 1.34961748\n",
      "Trained batch 1089 batch loss 1.33132339 epoch total loss 1.34960067\n",
      "Trained batch 1090 batch loss 1.25234389 epoch total loss 1.34951138\n",
      "Trained batch 1091 batch loss 1.27192843 epoch total loss 1.34944034\n",
      "Trained batch 1092 batch loss 1.23715758 epoch total loss 1.34933746\n",
      "Trained batch 1093 batch loss 1.15313089 epoch total loss 1.34915793\n",
      "Trained batch 1094 batch loss 1.32080579 epoch total loss 1.34913206\n",
      "Trained batch 1095 batch loss 1.39748025 epoch total loss 1.34917617\n",
      "Trained batch 1096 batch loss 1.34683025 epoch total loss 1.34917402\n",
      "Trained batch 1097 batch loss 1.28787184 epoch total loss 1.34911811\n",
      "Trained batch 1098 batch loss 1.25693953 epoch total loss 1.34903419\n",
      "Trained batch 1099 batch loss 1.37442017 epoch total loss 1.3490572\n",
      "Trained batch 1100 batch loss 1.41396499 epoch total loss 1.34911621\n",
      "Trained batch 1101 batch loss 1.31908786 epoch total loss 1.34908891\n",
      "Trained batch 1102 batch loss 1.23229694 epoch total loss 1.34898293\n",
      "Trained batch 1103 batch loss 1.19713187 epoch total loss 1.34884524\n",
      "Trained batch 1104 batch loss 1.05129957 epoch total loss 1.34857571\n",
      "Trained batch 1105 batch loss 1.14088058 epoch total loss 1.34838784\n",
      "Trained batch 1106 batch loss 1.11708546 epoch total loss 1.34817863\n",
      "Trained batch 1107 batch loss 1.23026443 epoch total loss 1.34807205\n",
      "Trained batch 1108 batch loss 1.30022418 epoch total loss 1.3480289\n",
      "Trained batch 1109 batch loss 1.32325459 epoch total loss 1.34800649\n",
      "Trained batch 1110 batch loss 1.2619319 epoch total loss 1.347929\n",
      "Trained batch 1111 batch loss 1.37086797 epoch total loss 1.34794962\n",
      "Trained batch 1112 batch loss 1.24546063 epoch total loss 1.34785748\n",
      "Trained batch 1113 batch loss 1.24603581 epoch total loss 1.34776604\n",
      "Trained batch 1114 batch loss 1.36566663 epoch total loss 1.34778214\n",
      "Trained batch 1115 batch loss 1.41336453 epoch total loss 1.34784091\n",
      "Trained batch 1116 batch loss 1.38773084 epoch total loss 1.34787667\n",
      "Trained batch 1117 batch loss 1.34748936 epoch total loss 1.34787631\n",
      "Trained batch 1118 batch loss 1.18999052 epoch total loss 1.34773505\n",
      "Trained batch 1119 batch loss 1.27484548 epoch total loss 1.34767\n",
      "Trained batch 1120 batch loss 1.29822981 epoch total loss 1.34762585\n",
      "Trained batch 1121 batch loss 1.34935272 epoch total loss 1.3476274\n",
      "Trained batch 1122 batch loss 1.42190552 epoch total loss 1.34769356\n",
      "Trained batch 1123 batch loss 1.46490538 epoch total loss 1.34779799\n",
      "Trained batch 1124 batch loss 1.39827132 epoch total loss 1.34784293\n",
      "Trained batch 1125 batch loss 1.3957721 epoch total loss 1.34788549\n",
      "Trained batch 1126 batch loss 1.36828172 epoch total loss 1.34790361\n",
      "Trained batch 1127 batch loss 1.31583786 epoch total loss 1.34787512\n",
      "Trained batch 1128 batch loss 1.30032516 epoch total loss 1.34783292\n",
      "Trained batch 1129 batch loss 1.36783993 epoch total loss 1.34785068\n",
      "Trained batch 1130 batch loss 1.24481499 epoch total loss 1.34775949\n",
      "Trained batch 1131 batch loss 1.28455544 epoch total loss 1.34770358\n",
      "Trained batch 1132 batch loss 1.35209751 epoch total loss 1.34770751\n",
      "Trained batch 1133 batch loss 1.34919834 epoch total loss 1.34770882\n",
      "Trained batch 1134 batch loss 1.24967396 epoch total loss 1.34762239\n",
      "Trained batch 1135 batch loss 1.26439929 epoch total loss 1.34754908\n",
      "Trained batch 1136 batch loss 1.41385305 epoch total loss 1.34760737\n",
      "Trained batch 1137 batch loss 1.37912691 epoch total loss 1.34763515\n",
      "Trained batch 1138 batch loss 1.43371022 epoch total loss 1.34771073\n",
      "Trained batch 1139 batch loss 1.33116245 epoch total loss 1.34769619\n",
      "Trained batch 1140 batch loss 1.38815987 epoch total loss 1.34773171\n",
      "Trained batch 1141 batch loss 1.33868766 epoch total loss 1.34772384\n",
      "Trained batch 1142 batch loss 1.2334559 epoch total loss 1.34762371\n",
      "Trained batch 1143 batch loss 1.27474546 epoch total loss 1.34756\n",
      "Trained batch 1144 batch loss 1.33773041 epoch total loss 1.34755147\n",
      "Trained batch 1145 batch loss 1.32260251 epoch total loss 1.34752965\n",
      "Trained batch 1146 batch loss 1.33874321 epoch total loss 1.34752202\n",
      "Trained batch 1147 batch loss 1.27683926 epoch total loss 1.34746039\n",
      "Trained batch 1148 batch loss 1.29100621 epoch total loss 1.34741127\n",
      "Trained batch 1149 batch loss 1.34031403 epoch total loss 1.34740508\n",
      "Trained batch 1150 batch loss 1.43572092 epoch total loss 1.34748185\n",
      "Trained batch 1151 batch loss 1.44309115 epoch total loss 1.34756494\n",
      "Trained batch 1152 batch loss 1.34518695 epoch total loss 1.34756291\n",
      "Trained batch 1153 batch loss 1.381001 epoch total loss 1.34759188\n",
      "Trained batch 1154 batch loss 1.37294817 epoch total loss 1.34761381\n",
      "Trained batch 1155 batch loss 1.41207528 epoch total loss 1.34766972\n",
      "Trained batch 1156 batch loss 1.32730114 epoch total loss 1.34765208\n",
      "Trained batch 1157 batch loss 1.23101437 epoch total loss 1.34755111\n",
      "Trained batch 1158 batch loss 1.17698455 epoch total loss 1.34740388\n",
      "Trained batch 1159 batch loss 1.14861429 epoch total loss 1.34723234\n",
      "Trained batch 1160 batch loss 1.14006865 epoch total loss 1.34705365\n",
      "Trained batch 1161 batch loss 1.20770907 epoch total loss 1.34693372\n",
      "Trained batch 1162 batch loss 1.23228717 epoch total loss 1.34683502\n",
      "Trained batch 1163 batch loss 1.03532839 epoch total loss 1.34656715\n",
      "Trained batch 1164 batch loss 1.01707494 epoch total loss 1.34628415\n",
      "Trained batch 1165 batch loss 0.973296463 epoch total loss 1.34596395\n",
      "Trained batch 1166 batch loss 1.16856647 epoch total loss 1.34581184\n",
      "Trained batch 1167 batch loss 1.35988212 epoch total loss 1.34582388\n",
      "Trained batch 1168 batch loss 1.29939926 epoch total loss 1.34578407\n",
      "Trained batch 1169 batch loss 1.26815748 epoch total loss 1.34571779\n",
      "Trained batch 1170 batch loss 1.25981498 epoch total loss 1.34564424\n",
      "Trained batch 1171 batch loss 1.30261827 epoch total loss 1.34560752\n",
      "Trained batch 1172 batch loss 1.37962627 epoch total loss 1.34563661\n",
      "Trained batch 1173 batch loss 1.23114848 epoch total loss 1.34553897\n",
      "Trained batch 1174 batch loss 1.18481421 epoch total loss 1.34540212\n",
      "Trained batch 1175 batch loss 1.31317329 epoch total loss 1.3453747\n",
      "Trained batch 1176 batch loss 1.33141923 epoch total loss 1.3453629\n",
      "Trained batch 1177 batch loss 1.37866545 epoch total loss 1.34539115\n",
      "Trained batch 1178 batch loss 1.39674568 epoch total loss 1.34543478\n",
      "Trained batch 1179 batch loss 1.29503334 epoch total loss 1.34539199\n",
      "Trained batch 1180 batch loss 1.35920215 epoch total loss 1.34540379\n",
      "Trained batch 1181 batch loss 1.42695332 epoch total loss 1.34547281\n",
      "Trained batch 1182 batch loss 1.32027602 epoch total loss 1.34545159\n",
      "Trained batch 1183 batch loss 1.29012609 epoch total loss 1.34540486\n",
      "Trained batch 1184 batch loss 1.29060769 epoch total loss 1.34535861\n",
      "Trained batch 1185 batch loss 1.35019326 epoch total loss 1.34536266\n",
      "Trained batch 1186 batch loss 1.21894681 epoch total loss 1.34525609\n",
      "Trained batch 1187 batch loss 1.26371968 epoch total loss 1.34518743\n",
      "Trained batch 1188 batch loss 1.32538843 epoch total loss 1.34517074\n",
      "Trained batch 1189 batch loss 1.2245568 epoch total loss 1.34506941\n",
      "Trained batch 1190 batch loss 1.26537728 epoch total loss 1.34500241\n",
      "Trained batch 1191 batch loss 1.21587396 epoch total loss 1.34489393\n",
      "Trained batch 1192 batch loss 1.1847769 epoch total loss 1.34475958\n",
      "Trained batch 1193 batch loss 1.33365607 epoch total loss 1.34475029\n",
      "Trained batch 1194 batch loss 1.28790677 epoch total loss 1.34470272\n",
      "Trained batch 1195 batch loss 1.35420644 epoch total loss 1.34471071\n",
      "Trained batch 1196 batch loss 1.360762 epoch total loss 1.34472406\n",
      "Trained batch 1197 batch loss 1.27259576 epoch total loss 1.34466386\n",
      "Trained batch 1198 batch loss 1.35046625 epoch total loss 1.34466863\n",
      "Trained batch 1199 batch loss 1.34807193 epoch total loss 1.34467149\n",
      "Trained batch 1200 batch loss 1.31868744 epoch total loss 1.34464991\n",
      "Trained batch 1201 batch loss 1.35957682 epoch total loss 1.34466231\n",
      "Trained batch 1202 batch loss 1.34073222 epoch total loss 1.34465897\n",
      "Trained batch 1203 batch loss 1.2383076 epoch total loss 1.34457064\n",
      "Trained batch 1204 batch loss 1.33740914 epoch total loss 1.34456468\n",
      "Trained batch 1205 batch loss 1.38040197 epoch total loss 1.34459436\n",
      "Trained batch 1206 batch loss 1.35006869 epoch total loss 1.34459889\n",
      "Trained batch 1207 batch loss 1.38298893 epoch total loss 1.34463072\n",
      "Trained batch 1208 batch loss 1.24474967 epoch total loss 1.34454799\n",
      "Trained batch 1209 batch loss 1.15986264 epoch total loss 1.34439528\n",
      "Trained batch 1210 batch loss 1.1294986 epoch total loss 1.34421766\n",
      "Trained batch 1211 batch loss 1.29273224 epoch total loss 1.34417522\n",
      "Trained batch 1212 batch loss 1.30172157 epoch total loss 1.34414017\n",
      "Trained batch 1213 batch loss 1.35784769 epoch total loss 1.34415138\n",
      "Trained batch 1214 batch loss 1.40834785 epoch total loss 1.34420431\n",
      "Trained batch 1215 batch loss 1.37868905 epoch total loss 1.34423268\n",
      "Trained batch 1216 batch loss 1.34401023 epoch total loss 1.34423244\n",
      "Trained batch 1217 batch loss 1.41330183 epoch total loss 1.34428918\n",
      "Trained batch 1218 batch loss 1.46151304 epoch total loss 1.3443855\n",
      "Trained batch 1219 batch loss 1.68522882 epoch total loss 1.34466505\n",
      "Trained batch 1220 batch loss 1.47230816 epoch total loss 1.34476972\n",
      "Trained batch 1221 batch loss 1.42164266 epoch total loss 1.34483266\n",
      "Trained batch 1222 batch loss 1.41837883 epoch total loss 1.34489274\n",
      "Trained batch 1223 batch loss 1.29149544 epoch total loss 1.34484911\n",
      "Trained batch 1224 batch loss 1.24520981 epoch total loss 1.34476781\n",
      "Trained batch 1225 batch loss 1.26102448 epoch total loss 1.34469938\n",
      "Trained batch 1226 batch loss 1.40879059 epoch total loss 1.3447516\n",
      "Trained batch 1227 batch loss 1.36255729 epoch total loss 1.34476614\n",
      "Trained batch 1228 batch loss 1.3344084 epoch total loss 1.34475768\n",
      "Trained batch 1229 batch loss 1.21945536 epoch total loss 1.34465575\n",
      "Trained batch 1230 batch loss 1.24179423 epoch total loss 1.34457219\n",
      "Trained batch 1231 batch loss 1.15948617 epoch total loss 1.34442186\n",
      "Trained batch 1232 batch loss 1.27732301 epoch total loss 1.34436738\n",
      "Trained batch 1233 batch loss 1.2750349 epoch total loss 1.34431112\n",
      "Trained batch 1234 batch loss 1.20855963 epoch total loss 1.34420121\n",
      "Trained batch 1235 batch loss 1.20555508 epoch total loss 1.34408891\n",
      "Trained batch 1236 batch loss 1.40073669 epoch total loss 1.34413481\n",
      "Trained batch 1237 batch loss 1.28357589 epoch total loss 1.34408581\n",
      "Trained batch 1238 batch loss 1.40374112 epoch total loss 1.34413397\n",
      "Trained batch 1239 batch loss 1.39535487 epoch total loss 1.34417534\n",
      "Trained batch 1240 batch loss 1.29935575 epoch total loss 1.3441391\n",
      "Trained batch 1241 batch loss 1.34404624 epoch total loss 1.34413898\n",
      "Trained batch 1242 batch loss 1.40523815 epoch total loss 1.34418821\n",
      "Trained batch 1243 batch loss 1.37543273 epoch total loss 1.34421349\n",
      "Trained batch 1244 batch loss 1.36115861 epoch total loss 1.34422708\n",
      "Trained batch 1245 batch loss 1.44703054 epoch total loss 1.34430969\n",
      "Trained batch 1246 batch loss 1.45723808 epoch total loss 1.34440029\n",
      "Trained batch 1247 batch loss 1.37951314 epoch total loss 1.34442854\n",
      "Trained batch 1248 batch loss 1.31879795 epoch total loss 1.34440804\n",
      "Trained batch 1249 batch loss 1.24729908 epoch total loss 1.34433019\n",
      "Trained batch 1250 batch loss 1.31410873 epoch total loss 1.34430611\n",
      "Trained batch 1251 batch loss 1.27432656 epoch total loss 1.34425008\n",
      "Trained batch 1252 batch loss 1.32223904 epoch total loss 1.34423256\n",
      "Trained batch 1253 batch loss 1.35524523 epoch total loss 1.34424126\n",
      "Trained batch 1254 batch loss 1.28642857 epoch total loss 1.34419513\n",
      "Trained batch 1255 batch loss 1.23593187 epoch total loss 1.34410894\n",
      "Trained batch 1256 batch loss 1.27599609 epoch total loss 1.3440547\n",
      "Trained batch 1257 batch loss 1.15097094 epoch total loss 1.34390116\n",
      "Trained batch 1258 batch loss 1.29286599 epoch total loss 1.34386051\n",
      "Trained batch 1259 batch loss 1.4132576 epoch total loss 1.34391558\n",
      "Trained batch 1260 batch loss 1.34056056 epoch total loss 1.34391296\n",
      "Trained batch 1261 batch loss 1.38341522 epoch total loss 1.34394431\n",
      "Trained batch 1262 batch loss 1.38404179 epoch total loss 1.34397602\n",
      "Trained batch 1263 batch loss 1.33517408 epoch total loss 1.34396911\n",
      "Trained batch 1264 batch loss 1.35804892 epoch total loss 1.34398019\n",
      "Trained batch 1265 batch loss 1.35284579 epoch total loss 1.34398735\n",
      "Trained batch 1266 batch loss 1.30871975 epoch total loss 1.34395945\n",
      "Trained batch 1267 batch loss 1.26725054 epoch total loss 1.34389889\n",
      "Trained batch 1268 batch loss 1.29141736 epoch total loss 1.34385741\n",
      "Trained batch 1269 batch loss 1.23023975 epoch total loss 1.34376788\n",
      "Trained batch 1270 batch loss 1.32290363 epoch total loss 1.34375143\n",
      "Trained batch 1271 batch loss 1.24853933 epoch total loss 1.34367657\n",
      "Trained batch 1272 batch loss 1.26092076 epoch total loss 1.34361148\n",
      "Trained batch 1273 batch loss 1.17028856 epoch total loss 1.34347522\n",
      "Trained batch 1274 batch loss 1.18802011 epoch total loss 1.34335327\n",
      "Trained batch 1275 batch loss 1.25774205 epoch total loss 1.34328604\n",
      "Trained batch 1276 batch loss 1.31345248 epoch total loss 1.34326267\n",
      "Trained batch 1277 batch loss 1.30671358 epoch total loss 1.34323406\n",
      "Trained batch 1278 batch loss 1.35121298 epoch total loss 1.34324038\n",
      "Trained batch 1279 batch loss 1.38977313 epoch total loss 1.34327674\n",
      "Trained batch 1280 batch loss 1.34976292 epoch total loss 1.34328175\n",
      "Trained batch 1281 batch loss 1.345119 epoch total loss 1.34328318\n",
      "Trained batch 1282 batch loss 1.31788564 epoch total loss 1.34326339\n",
      "Trained batch 1283 batch loss 1.22843242 epoch total loss 1.34317386\n",
      "Trained batch 1284 batch loss 1.29137468 epoch total loss 1.34313345\n",
      "Trained batch 1285 batch loss 1.13073397 epoch total loss 1.34296823\n",
      "Trained batch 1286 batch loss 1.24577498 epoch total loss 1.34289253\n",
      "Trained batch 1287 batch loss 1.23425865 epoch total loss 1.34280813\n",
      "Trained batch 1288 batch loss 1.18437 epoch total loss 1.3426851\n",
      "Trained batch 1289 batch loss 1.22040963 epoch total loss 1.34259033\n",
      "Trained batch 1290 batch loss 1.19666874 epoch total loss 1.3424772\n",
      "Trained batch 1291 batch loss 1.19809616 epoch total loss 1.34236538\n",
      "Trained batch 1292 batch loss 1.23317337 epoch total loss 1.34228086\n",
      "Trained batch 1293 batch loss 1.4752593 epoch total loss 1.34238362\n",
      "Trained batch 1294 batch loss 1.21528482 epoch total loss 1.34228539\n",
      "Trained batch 1295 batch loss 1.17973495 epoch total loss 1.34215987\n",
      "Trained batch 1296 batch loss 1.3012898 epoch total loss 1.34212828\n",
      "Trained batch 1297 batch loss 1.38652563 epoch total loss 1.34216249\n",
      "Trained batch 1298 batch loss 1.26039338 epoch total loss 1.34209955\n",
      "Trained batch 1299 batch loss 1.22889757 epoch total loss 1.34201241\n",
      "Trained batch 1300 batch loss 1.3648262 epoch total loss 1.34202993\n",
      "Trained batch 1301 batch loss 1.43365991 epoch total loss 1.34210038\n",
      "Trained batch 1302 batch loss 1.55964255 epoch total loss 1.34226751\n",
      "Trained batch 1303 batch loss 1.44366074 epoch total loss 1.34234536\n",
      "Trained batch 1304 batch loss 1.37961602 epoch total loss 1.34237385\n",
      "Trained batch 1305 batch loss 1.47412765 epoch total loss 1.34247482\n",
      "Trained batch 1306 batch loss 1.39910138 epoch total loss 1.34251821\n",
      "Trained batch 1307 batch loss 1.43679881 epoch total loss 1.34259033\n",
      "Trained batch 1308 batch loss 1.23685431 epoch total loss 1.34250939\n",
      "Trained batch 1309 batch loss 1.22901464 epoch total loss 1.34242272\n",
      "Trained batch 1310 batch loss 1.26509476 epoch total loss 1.34236372\n",
      "Trained batch 1311 batch loss 1.42799628 epoch total loss 1.34242904\n",
      "Trained batch 1312 batch loss 1.28384471 epoch total loss 1.34238434\n",
      "Trained batch 1313 batch loss 1.33728528 epoch total loss 1.3423804\n",
      "Trained batch 1314 batch loss 1.36747205 epoch total loss 1.34239948\n",
      "Trained batch 1315 batch loss 1.35468912 epoch total loss 1.3424089\n",
      "Trained batch 1316 batch loss 1.46298909 epoch total loss 1.34250057\n",
      "Trained batch 1317 batch loss 1.29519641 epoch total loss 1.34246457\n",
      "Trained batch 1318 batch loss 1.16391325 epoch total loss 1.34232914\n",
      "Trained batch 1319 batch loss 1.23899722 epoch total loss 1.34225082\n",
      "Trained batch 1320 batch loss 1.32847929 epoch total loss 1.34224045\n",
      "Trained batch 1321 batch loss 1.37443829 epoch total loss 1.34226477\n",
      "Trained batch 1322 batch loss 1.2630986 epoch total loss 1.34220481\n",
      "Trained batch 1323 batch loss 1.26702964 epoch total loss 1.34214807\n",
      "Trained batch 1324 batch loss 1.3974874 epoch total loss 1.34218979\n",
      "Trained batch 1325 batch loss 1.22492313 epoch total loss 1.34210134\n",
      "Trained batch 1326 batch loss 1.31459665 epoch total loss 1.34208059\n",
      "Trained batch 1327 batch loss 1.29854834 epoch total loss 1.34204781\n",
      "Trained batch 1328 batch loss 1.20072186 epoch total loss 1.34194136\n",
      "Trained batch 1329 batch loss 1.26968265 epoch total loss 1.341887\n",
      "Trained batch 1330 batch loss 1.22352576 epoch total loss 1.34179795\n",
      "Trained batch 1331 batch loss 1.29194582 epoch total loss 1.34176052\n",
      "Trained batch 1332 batch loss 1.31820762 epoch total loss 1.34174287\n",
      "Trained batch 1333 batch loss 1.25121546 epoch total loss 1.34167504\n",
      "Trained batch 1334 batch loss 1.35103571 epoch total loss 1.34168208\n",
      "Trained batch 1335 batch loss 1.28573024 epoch total loss 1.34164011\n",
      "Trained batch 1336 batch loss 1.22802258 epoch total loss 1.34155512\n",
      "Trained batch 1337 batch loss 1.26184285 epoch total loss 1.34149551\n",
      "Trained batch 1338 batch loss 1.3511281 epoch total loss 1.34150267\n",
      "Trained batch 1339 batch loss 1.25629008 epoch total loss 1.34143901\n",
      "Trained batch 1340 batch loss 1.28730142 epoch total loss 1.34139872\n",
      "Trained batch 1341 batch loss 1.3717525 epoch total loss 1.34142125\n",
      "Trained batch 1342 batch loss 1.16651106 epoch total loss 1.34129095\n",
      "Trained batch 1343 batch loss 1.32332206 epoch total loss 1.3412776\n",
      "Trained batch 1344 batch loss 1.27158678 epoch total loss 1.34122574\n",
      "Trained batch 1345 batch loss 1.2919482 epoch total loss 1.34118915\n",
      "Trained batch 1346 batch loss 1.21227241 epoch total loss 1.34109342\n",
      "Trained batch 1347 batch loss 1.18351007 epoch total loss 1.34097636\n",
      "Trained batch 1348 batch loss 1.26644456 epoch total loss 1.34092104\n",
      "Trained batch 1349 batch loss 1.28660679 epoch total loss 1.34088087\n",
      "Trained batch 1350 batch loss 1.28225029 epoch total loss 1.34083736\n",
      "Trained batch 1351 batch loss 1.2853713 epoch total loss 1.34079635\n",
      "Trained batch 1352 batch loss 1.30355883 epoch total loss 1.34076881\n",
      "Trained batch 1353 batch loss 1.18586814 epoch total loss 1.34065437\n",
      "Trained batch 1354 batch loss 1.14886212 epoch total loss 1.34051275\n",
      "Trained batch 1355 batch loss 1.23077297 epoch total loss 1.34043169\n",
      "Trained batch 1356 batch loss 1.23430872 epoch total loss 1.34035337\n",
      "Trained batch 1357 batch loss 1.24883246 epoch total loss 1.3402859\n",
      "Trained batch 1358 batch loss 1.3495177 epoch total loss 1.34029269\n",
      "Trained batch 1359 batch loss 1.39960325 epoch total loss 1.34033632\n",
      "Trained batch 1360 batch loss 1.26637769 epoch total loss 1.34028196\n",
      "Trained batch 1361 batch loss 1.26127505 epoch total loss 1.34022391\n",
      "Trained batch 1362 batch loss 1.22428632 epoch total loss 1.34013867\n",
      "Trained batch 1363 batch loss 1.26761758 epoch total loss 1.34008551\n",
      "Trained batch 1364 batch loss 1.29823041 epoch total loss 1.34005475\n",
      "Trained batch 1365 batch loss 1.29429102 epoch total loss 1.34002125\n",
      "Trained batch 1366 batch loss 1.39306509 epoch total loss 1.34006011\n",
      "Trained batch 1367 batch loss 1.38314426 epoch total loss 1.34009159\n",
      "Trained batch 1368 batch loss 1.28243697 epoch total loss 1.34004951\n",
      "Trained batch 1369 batch loss 1.38213038 epoch total loss 1.34008026\n",
      "Trained batch 1370 batch loss 1.29228115 epoch total loss 1.34004533\n",
      "Trained batch 1371 batch loss 1.16546273 epoch total loss 1.3399179\n",
      "Trained batch 1372 batch loss 1.36246789 epoch total loss 1.33993435\n",
      "Trained batch 1373 batch loss 1.14972067 epoch total loss 1.33979583\n",
      "Trained batch 1374 batch loss 1.37744462 epoch total loss 1.33982325\n",
      "Trained batch 1375 batch loss 1.32955694 epoch total loss 1.33981574\n",
      "Trained batch 1376 batch loss 1.28297842 epoch total loss 1.33977449\n",
      "Trained batch 1377 batch loss 1.36075246 epoch total loss 1.33978963\n",
      "Trained batch 1378 batch loss 1.24341094 epoch total loss 1.33971977\n",
      "Trained batch 1379 batch loss 1.2347095 epoch total loss 1.3396436\n",
      "Trained batch 1380 batch loss 1.21860516 epoch total loss 1.33955586\n",
      "Trained batch 1381 batch loss 1.29067528 epoch total loss 1.33952045\n",
      "Trained batch 1382 batch loss 1.27881289 epoch total loss 1.33947659\n",
      "Trained batch 1383 batch loss 1.21792376 epoch total loss 1.33938861\n",
      "Trained batch 1384 batch loss 1.34150577 epoch total loss 1.33939028\n",
      "Trained batch 1385 batch loss 1.36095572 epoch total loss 1.33940578\n",
      "Trained batch 1386 batch loss 1.2949661 epoch total loss 1.33937371\n",
      "Trained batch 1387 batch loss 1.22684598 epoch total loss 1.33929253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 03:33:52.492205: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "2025-03-19 03:33:52.492255: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "\t [[RemoteCall]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1388 batch loss 1.13880289 epoch total loss 1.33914804\n",
      "Epoch 2 train loss 1.3391480445861816\n",
      "Validated batch 1 batch loss 1.27874565\n",
      "Validated batch 2 batch loss 1.17354274\n",
      "Validated batch 3 batch loss 1.30219412\n",
      "Validated batch 4 batch loss 1.21014631\n",
      "Validated batch 5 batch loss 1.2933538\n",
      "Validated batch 6 batch loss 1.30647361\n",
      "Validated batch 7 batch loss 1.31139958\n",
      "Validated batch 8 batch loss 1.43423915\n",
      "Validated batch 9 batch loss 1.42075038\n",
      "Validated batch 10 batch loss 1.32824719\n",
      "Validated batch 11 batch loss 1.33824182\n",
      "Validated batch 12 batch loss 1.40835166\n",
      "Validated batch 13 batch loss 1.40200114\n",
      "Validated batch 14 batch loss 1.38072622\n",
      "Validated batch 15 batch loss 1.38516414\n",
      "Validated batch 16 batch loss 1.38563073\n",
      "Validated batch 17 batch loss 1.33839655\n",
      "Validated batch 18 batch loss 1.20144522\n",
      "Validated batch 19 batch loss 1.29891264\n",
      "Validated batch 20 batch loss 1.38113618\n",
      "Validated batch 21 batch loss 1.32936323\n",
      "Validated batch 22 batch loss 1.34309375\n",
      "Validated batch 23 batch loss 1.31658971\n",
      "Validated batch 24 batch loss 1.26043117\n",
      "Validated batch 25 batch loss 1.21360981\n",
      "Validated batch 26 batch loss 1.24657714\n",
      "Validated batch 27 batch loss 1.26593304\n",
      "Validated batch 28 batch loss 1.29978728\n",
      "Validated batch 29 batch loss 1.29576921\n",
      "Validated batch 30 batch loss 1.34864902\n",
      "Validated batch 31 batch loss 1.26051235\n",
      "Validated batch 32 batch loss 1.2861743\n",
      "Validated batch 33 batch loss 1.3276695\n",
      "Validated batch 34 batch loss 1.34099877\n",
      "Validated batch 35 batch loss 1.31322861\n",
      "Validated batch 36 batch loss 1.24993789\n",
      "Validated batch 37 batch loss 1.27547097\n",
      "Validated batch 38 batch loss 1.34792709\n",
      "Validated batch 39 batch loss 1.30223012\n",
      "Validated batch 40 batch loss 1.40191269\n",
      "Validated batch 41 batch loss 1.37498188\n",
      "Validated batch 42 batch loss 1.22734249\n",
      "Validated batch 43 batch loss 1.39994729\n",
      "Validated batch 44 batch loss 1.26800287\n",
      "Validated batch 45 batch loss 1.25788283\n",
      "Validated batch 46 batch loss 1.35443354\n",
      "Validated batch 47 batch loss 1.39679456\n",
      "Validated batch 48 batch loss 1.32645726\n",
      "Validated batch 49 batch loss 1.2830615\n",
      "Validated batch 50 batch loss 1.27873015\n",
      "Validated batch 51 batch loss 1.3290441\n",
      "Validated batch 52 batch loss 1.47808599\n",
      "Validated batch 53 batch loss 1.25454557\n",
      "Validated batch 54 batch loss 1.38277817\n",
      "Validated batch 55 batch loss 1.28815687\n",
      "Validated batch 56 batch loss 1.34774327\n",
      "Validated batch 57 batch loss 1.31755638\n",
      "Validated batch 58 batch loss 1.19680929\n",
      "Validated batch 59 batch loss 1.4555912\n",
      "Validated batch 60 batch loss 1.27082014\n",
      "Validated batch 61 batch loss 1.38996816\n",
      "Validated batch 62 batch loss 1.29078317\n",
      "Validated batch 63 batch loss 1.37529016\n",
      "Validated batch 64 batch loss 1.20382297\n",
      "Validated batch 65 batch loss 1.30560279\n",
      "Validated batch 66 batch loss 1.28055096\n",
      "Validated batch 67 batch loss 1.27775061\n",
      "Validated batch 68 batch loss 1.35570133\n",
      "Validated batch 69 batch loss 1.33505642\n",
      "Validated batch 70 batch loss 1.24592292\n",
      "Validated batch 71 batch loss 1.37386084\n",
      "Validated batch 72 batch loss 1.34860098\n",
      "Validated batch 73 batch loss 1.25582361\n",
      "Validated batch 74 batch loss 1.3455987\n",
      "Validated batch 75 batch loss 1.46758\n",
      "Validated batch 76 batch loss 1.17176604\n",
      "Validated batch 77 batch loss 1.29314089\n",
      "Validated batch 78 batch loss 1.28435683\n",
      "Validated batch 79 batch loss 1.33877277\n",
      "Validated batch 80 batch loss 1.3340714\n",
      "Validated batch 81 batch loss 1.22132683\n",
      "Validated batch 82 batch loss 1.11142743\n",
      "Validated batch 83 batch loss 1.27347469\n",
      "Validated batch 84 batch loss 1.24781859\n",
      "Validated batch 85 batch loss 1.22588229\n",
      "Validated batch 86 batch loss 1.29010057\n",
      "Validated batch 87 batch loss 1.25991344\n",
      "Validated batch 88 batch loss 1.3231082\n",
      "Validated batch 89 batch loss 1.38452935\n",
      "Validated batch 90 batch loss 1.34041715\n",
      "Validated batch 91 batch loss 1.29639387\n",
      "Validated batch 92 batch loss 1.20820677\n",
      "Validated batch 93 batch loss 1.26970673\n",
      "Validated batch 94 batch loss 1.33652782\n",
      "Validated batch 95 batch loss 1.27321875\n",
      "Validated batch 96 batch loss 1.14228678\n",
      "Validated batch 97 batch loss 1.20862031\n",
      "Validated batch 98 batch loss 1.41049242\n",
      "Validated batch 99 batch loss 1.2231437\n",
      "Validated batch 100 batch loss 1.21140575\n",
      "Validated batch 101 batch loss 1.22797215\n",
      "Validated batch 102 batch loss 1.28736031\n",
      "Validated batch 103 batch loss 1.23332703\n",
      "Validated batch 104 batch loss 1.26982117\n",
      "Validated batch 105 batch loss 1.29510498\n",
      "Validated batch 106 batch loss 1.25114202\n",
      "Validated batch 107 batch loss 1.36091137\n",
      "Validated batch 108 batch loss 1.44865942\n",
      "Validated batch 109 batch loss 1.22138262\n",
      "Validated batch 110 batch loss 1.41630101\n",
      "Validated batch 111 batch loss 1.14525306\n",
      "Validated batch 112 batch loss 1.26006651\n",
      "Validated batch 113 batch loss 1.25749111\n",
      "Validated batch 114 batch loss 1.26800978\n",
      "Validated batch 115 batch loss 1.45488501\n",
      "Validated batch 116 batch loss 1.44489419\n",
      "Validated batch 117 batch loss 1.30288339\n",
      "Validated batch 118 batch loss 1.28602815\n",
      "Validated batch 119 batch loss 1.31869912\n",
      "Validated batch 120 batch loss 1.27665591\n",
      "Validated batch 121 batch loss 1.3318069\n",
      "Validated batch 122 batch loss 1.32483947\n",
      "Validated batch 123 batch loss 1.27262735\n",
      "Validated batch 124 batch loss 1.23458505\n",
      "Validated batch 125 batch loss 1.30409169\n",
      "Validated batch 126 batch loss 1.33574283\n",
      "Validated batch 127 batch loss 1.38173354\n",
      "Validated batch 128 batch loss 1.32862\n",
      "Validated batch 129 batch loss 1.23763442\n",
      "Validated batch 130 batch loss 1.27083313\n",
      "Validated batch 131 batch loss 1.32087755\n",
      "Validated batch 132 batch loss 1.31046224\n",
      "Validated batch 133 batch loss 1.35825491\n",
      "Validated batch 134 batch loss 1.38876653\n",
      "Validated batch 135 batch loss 1.53374124\n",
      "Validated batch 136 batch loss 1.43463719\n",
      "Validated batch 137 batch loss 1.34203553\n",
      "Validated batch 138 batch loss 1.2254324\n",
      "Validated batch 139 batch loss 1.26041245\n",
      "Validated batch 140 batch loss 1.35982704\n",
      "Validated batch 141 batch loss 1.23377192\n",
      "Validated batch 142 batch loss 1.25667655\n",
      "Validated batch 143 batch loss 1.30512917\n",
      "Validated batch 144 batch loss 1.31150961\n",
      "Validated batch 145 batch loss 1.28735828\n",
      "Validated batch 146 batch loss 1.30476141\n",
      "Validated batch 147 batch loss 1.23061359\n",
      "Validated batch 148 batch loss 1.4334538\n",
      "Validated batch 149 batch loss 1.25928771\n",
      "Validated batch 150 batch loss 1.19603634\n",
      "Validated batch 151 batch loss 1.27299094\n",
      "Validated batch 152 batch loss 1.44938374\n",
      "Validated batch 153 batch loss 1.35078204\n",
      "Validated batch 154 batch loss 1.47054768\n",
      "Validated batch 155 batch loss 1.25542939\n",
      "Validated batch 156 batch loss 1.46088517\n",
      "Validated batch 157 batch loss 1.32483089\n",
      "Validated batch 158 batch loss 1.39862764\n",
      "Validated batch 159 batch loss 1.37633359\n",
      "Validated batch 160 batch loss 1.09213233\n",
      "Validated batch 161 batch loss 1.27943313\n",
      "Validated batch 162 batch loss 1.3620615\n",
      "Validated batch 163 batch loss 1.33394623\n",
      "Validated batch 164 batch loss 1.31460822\n",
      "Validated batch 165 batch loss 1.23164105\n",
      "Validated batch 166 batch loss 1.34752035\n",
      "Validated batch 167 batch loss 1.32546186\n",
      "Validated batch 168 batch loss 1.38823676\n",
      "Validated batch 169 batch loss 1.41300285\n",
      "Validated batch 170 batch loss 1.38794529\n",
      "Validated batch 171 batch loss 1.2771368\n",
      "Validated batch 172 batch loss 1.32310021\n",
      "Validated batch 173 batch loss 1.36036325\n",
      "Validated batch 174 batch loss 1.27555895\n",
      "Validated batch 175 batch loss 1.39627683\n",
      "Validated batch 176 batch loss 1.43992352\n",
      "Validated batch 177 batch loss 1.33766627\n",
      "Validated batch 178 batch loss 1.3776207\n",
      "Validated batch 179 batch loss 1.30799305\n",
      "Validated batch 180 batch loss 1.24709988\n",
      "Validated batch 181 batch loss 1.30791664\n",
      "Validated batch 182 batch loss 1.20633233\n",
      "Validated batch 183 batch loss 1.42428589\n",
      "Validated batch 184 batch loss 1.29898572\n",
      "Validated batch 185 batch loss 1.29658794\n",
      "Epoch 2 val loss 1.3121122121810913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 03:34:08.453132: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "2025-03-19 03:34:08.453173: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "\t [[RemoteCall]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model /home/minho/Desktop/aiffel/pose_estimation/models/model-epoch-2-loss-1.3121.weights.h5 saved.\n",
      "Start epoch 3 with learning rate 0.001\n",
      "Start distributed training...\n",
      "Trained batch 1 batch loss 1.20434117 epoch total loss 1.20434117\n",
      "Trained batch 2 batch loss 1.26488757 epoch total loss 1.23461437\n",
      "Trained batch 3 batch loss 1.20529819 epoch total loss 1.22484231\n",
      "Trained batch 4 batch loss 1.35389638 epoch total loss 1.25710583\n",
      "Trained batch 5 batch loss 1.46730888 epoch total loss 1.29914641\n",
      "Trained batch 6 batch loss 1.38583207 epoch total loss 1.31359398\n",
      "Trained batch 7 batch loss 1.28937888 epoch total loss 1.31013477\n",
      "Trained batch 8 batch loss 1.35152352 epoch total loss 1.31530833\n",
      "Trained batch 9 batch loss 1.22613692 epoch total loss 1.30540037\n",
      "Trained batch 10 batch loss 1.2715888 epoch total loss 1.30201936\n",
      "Trained batch 11 batch loss 1.34293616 epoch total loss 1.30573905\n",
      "Trained batch 12 batch loss 1.43868327 epoch total loss 1.31681776\n",
      "Trained batch 13 batch loss 1.50502479 epoch total loss 1.33129513\n",
      "Trained batch 14 batch loss 1.49171448 epoch total loss 1.34275365\n",
      "Trained batch 15 batch loss 1.4026196 epoch total loss 1.34674478\n",
      "Trained batch 16 batch loss 1.37434387 epoch total loss 1.34846973\n",
      "Trained batch 17 batch loss 1.39623618 epoch total loss 1.3512795\n",
      "Trained batch 18 batch loss 1.43257093 epoch total loss 1.35579574\n",
      "Trained batch 19 batch loss 1.40037799 epoch total loss 1.35814214\n",
      "Trained batch 20 batch loss 1.2973299 epoch total loss 1.35510159\n",
      "Trained batch 21 batch loss 1.22566438 epoch total loss 1.34893787\n",
      "Trained batch 22 batch loss 1.1824317 epoch total loss 1.34136951\n",
      "Trained batch 23 batch loss 1.29306722 epoch total loss 1.3392694\n",
      "Trained batch 24 batch loss 1.26885557 epoch total loss 1.33633554\n",
      "Trained batch 25 batch loss 1.34257627 epoch total loss 1.33658504\n",
      "Trained batch 26 batch loss 1.22978425 epoch total loss 1.33247745\n",
      "Trained batch 27 batch loss 1.3103646 epoch total loss 1.33165836\n",
      "Trained batch 28 batch loss 1.2851547 epoch total loss 1.32999766\n",
      "Trained batch 29 batch loss 1.32587469 epoch total loss 1.32985544\n",
      "Trained batch 30 batch loss 1.22943223 epoch total loss 1.32650793\n",
      "Trained batch 31 batch loss 1.23464787 epoch total loss 1.32354474\n",
      "Trained batch 32 batch loss 1.29914021 epoch total loss 1.32278216\n",
      "Trained batch 33 batch loss 1.32762873 epoch total loss 1.32292902\n",
      "Trained batch 34 batch loss 1.23831809 epoch total loss 1.32044053\n",
      "Trained batch 35 batch loss 1.39737618 epoch total loss 1.32263875\n",
      "Trained batch 36 batch loss 1.28802681 epoch total loss 1.32167733\n",
      "Trained batch 37 batch loss 1.26652884 epoch total loss 1.32018685\n",
      "Trained batch 38 batch loss 1.38540697 epoch total loss 1.32190311\n",
      "Trained batch 39 batch loss 1.38483036 epoch total loss 1.32351661\n",
      "Trained batch 40 batch loss 1.30865288 epoch total loss 1.32314515\n",
      "Trained batch 41 batch loss 1.22132802 epoch total loss 1.32066178\n",
      "Trained batch 42 batch loss 1.26532376 epoch total loss 1.31934416\n",
      "Trained batch 43 batch loss 1.3336252 epoch total loss 1.31967628\n",
      "Trained batch 44 batch loss 1.30334377 epoch total loss 1.31930518\n",
      "Trained batch 45 batch loss 1.24002934 epoch total loss 1.31754351\n",
      "Trained batch 46 batch loss 1.24262381 epoch total loss 1.31591475\n",
      "Trained batch 47 batch loss 1.31727338 epoch total loss 1.3159436\n",
      "Trained batch 48 batch loss 1.30582237 epoch total loss 1.31573284\n",
      "Trained batch 49 batch loss 1.10816908 epoch total loss 1.31149685\n",
      "Trained batch 50 batch loss 1.24771 epoch total loss 1.31022108\n",
      "Trained batch 51 batch loss 1.347893 epoch total loss 1.31095982\n",
      "Trained batch 52 batch loss 1.2934401 epoch total loss 1.31062293\n",
      "Trained batch 53 batch loss 1.32410872 epoch total loss 1.31087744\n",
      "Trained batch 54 batch loss 1.13120949 epoch total loss 1.30755019\n",
      "Trained batch 55 batch loss 1.21185541 epoch total loss 1.30581021\n",
      "Trained batch 56 batch loss 1.15176165 epoch total loss 1.30305946\n",
      "Trained batch 57 batch loss 1.12200403 epoch total loss 1.29988301\n",
      "Trained batch 58 batch loss 1.12537134 epoch total loss 1.29687417\n",
      "Trained batch 59 batch loss 1.15290797 epoch total loss 1.29443407\n",
      "Trained batch 60 batch loss 1.20989656 epoch total loss 1.29302526\n",
      "Trained batch 61 batch loss 1.20567381 epoch total loss 1.29159319\n",
      "Trained batch 62 batch loss 1.36579978 epoch total loss 1.29279\n",
      "Trained batch 63 batch loss 1.30692196 epoch total loss 1.29301441\n",
      "Trained batch 64 batch loss 1.34547925 epoch total loss 1.29383421\n",
      "Trained batch 65 batch loss 1.23970819 epoch total loss 1.29300153\n",
      "Trained batch 66 batch loss 1.10771894 epoch total loss 1.29019415\n",
      "Trained batch 67 batch loss 1.06918752 epoch total loss 1.28689563\n",
      "Trained batch 68 batch loss 1.28120577 epoch total loss 1.28681195\n",
      "Trained batch 69 batch loss 1.19867051 epoch total loss 1.2855345\n",
      "Trained batch 70 batch loss 1.22333562 epoch total loss 1.28464592\n",
      "Trained batch 71 batch loss 1.18762302 epoch total loss 1.28327942\n",
      "Trained batch 72 batch loss 1.14703071 epoch total loss 1.28138709\n",
      "Trained batch 73 batch loss 1.25671887 epoch total loss 1.28104925\n",
      "Trained batch 74 batch loss 1.27858639 epoch total loss 1.28101599\n",
      "Trained batch 75 batch loss 1.21599221 epoch total loss 1.28014898\n",
      "Trained batch 76 batch loss 1.40067768 epoch total loss 1.28173494\n",
      "Trained batch 77 batch loss 1.13860726 epoch total loss 1.27987623\n",
      "Trained batch 78 batch loss 1.15966535 epoch total loss 1.27833509\n",
      "Trained batch 79 batch loss 1.27717221 epoch total loss 1.27832043\n",
      "Trained batch 80 batch loss 1.2908709 epoch total loss 1.27847731\n",
      "Trained batch 81 batch loss 1.32674158 epoch total loss 1.27907312\n",
      "Trained batch 82 batch loss 1.54132819 epoch total loss 1.28227139\n",
      "Trained batch 83 batch loss 1.45298529 epoch total loss 1.28432822\n",
      "Trained batch 84 batch loss 1.21530426 epoch total loss 1.28350651\n",
      "Trained batch 85 batch loss 1.39010644 epoch total loss 1.28476059\n",
      "Trained batch 86 batch loss 1.243155 epoch total loss 1.28427684\n",
      "Trained batch 87 batch loss 1.34274971 epoch total loss 1.28494895\n",
      "Trained batch 88 batch loss 1.24799228 epoch total loss 1.28452897\n",
      "Trained batch 89 batch loss 1.32298088 epoch total loss 1.2849611\n",
      "Trained batch 90 batch loss 1.23128653 epoch total loss 1.2843647\n",
      "Trained batch 91 batch loss 1.30195534 epoch total loss 1.28455794\n",
      "Trained batch 92 batch loss 1.19010985 epoch total loss 1.28353131\n",
      "Trained batch 93 batch loss 1.43311214 epoch total loss 1.2851398\n",
      "Trained batch 94 batch loss 1.3677268 epoch total loss 1.28601837\n",
      "Trained batch 95 batch loss 1.2296977 epoch total loss 1.28542554\n",
      "Trained batch 96 batch loss 1.24231 epoch total loss 1.28497636\n",
      "Trained batch 97 batch loss 1.25622988 epoch total loss 1.28468013\n",
      "Trained batch 98 batch loss 1.33390749 epoch total loss 1.28518236\n",
      "Trained batch 99 batch loss 1.40272641 epoch total loss 1.28636968\n",
      "Trained batch 100 batch loss 1.22634649 epoch total loss 1.28576946\n",
      "Trained batch 101 batch loss 1.23440361 epoch total loss 1.28526092\n",
      "Trained batch 102 batch loss 1.28680158 epoch total loss 1.28527606\n",
      "Trained batch 103 batch loss 1.33909774 epoch total loss 1.28579855\n",
      "Trained batch 104 batch loss 1.38936126 epoch total loss 1.28679442\n",
      "Trained batch 105 batch loss 1.31374502 epoch total loss 1.28705108\n",
      "Trained batch 106 batch loss 1.18697345 epoch total loss 1.28610694\n",
      "Trained batch 107 batch loss 1.43736124 epoch total loss 1.28752053\n",
      "Trained batch 108 batch loss 1.27497768 epoch total loss 1.28740442\n",
      "Trained batch 109 batch loss 1.39646137 epoch total loss 1.28840482\n",
      "Trained batch 110 batch loss 1.26971602 epoch total loss 1.28823495\n",
      "Trained batch 111 batch loss 1.34988415 epoch total loss 1.28879035\n",
      "Trained batch 112 batch loss 1.27165067 epoch total loss 1.28863728\n",
      "Trained batch 113 batch loss 1.22243893 epoch total loss 1.28805149\n",
      "Trained batch 114 batch loss 1.37600124 epoch total loss 1.28882301\n",
      "Trained batch 115 batch loss 1.26708364 epoch total loss 1.28863406\n",
      "Trained batch 116 batch loss 1.23431325 epoch total loss 1.28816581\n",
      "Trained batch 117 batch loss 1.07877207 epoch total loss 1.286376\n",
      "Trained batch 118 batch loss 1.10264015 epoch total loss 1.28481901\n",
      "Trained batch 119 batch loss 1.20842218 epoch total loss 1.28417695\n",
      "Trained batch 120 batch loss 1.16376889 epoch total loss 1.28317368\n",
      "Trained batch 121 batch loss 1.08174646 epoch total loss 1.28150892\n",
      "Trained batch 122 batch loss 1.20647705 epoch total loss 1.28089392\n",
      "Trained batch 123 batch loss 1.23756707 epoch total loss 1.28054166\n",
      "Trained batch 124 batch loss 1.31865323 epoch total loss 1.28084898\n",
      "Trained batch 125 batch loss 1.17308617 epoch total loss 1.27998686\n",
      "Trained batch 126 batch loss 1.18357575 epoch total loss 1.27922165\n",
      "Trained batch 127 batch loss 1.23485637 epoch total loss 1.27887237\n",
      "Trained batch 128 batch loss 1.32105708 epoch total loss 1.27920198\n",
      "Trained batch 129 batch loss 1.38034689 epoch total loss 1.27998602\n",
      "Trained batch 130 batch loss 1.30545568 epoch total loss 1.28018188\n",
      "Trained batch 131 batch loss 1.30033803 epoch total loss 1.28033578\n",
      "Trained batch 132 batch loss 1.30856478 epoch total loss 1.28054965\n",
      "Trained batch 133 batch loss 1.28623748 epoch total loss 1.28059232\n",
      "Trained batch 134 batch loss 1.08989632 epoch total loss 1.2791692\n",
      "Trained batch 135 batch loss 1.16769564 epoch total loss 1.27834344\n",
      "Trained batch 136 batch loss 1.12929618 epoch total loss 1.27724755\n",
      "Trained batch 137 batch loss 1.27013493 epoch total loss 1.27719569\n",
      "Trained batch 138 batch loss 1.32286048 epoch total loss 1.27752662\n",
      "Trained batch 139 batch loss 1.50578713 epoch total loss 1.27916873\n",
      "Trained batch 140 batch loss 1.46308732 epoch total loss 1.28048253\n",
      "Trained batch 141 batch loss 1.41532969 epoch total loss 1.28143883\n",
      "Trained batch 142 batch loss 1.35279632 epoch total loss 1.28194141\n",
      "Trained batch 143 batch loss 1.35095561 epoch total loss 1.28242397\n",
      "Trained batch 144 batch loss 1.20464242 epoch total loss 1.28188372\n",
      "Trained batch 145 batch loss 1.10944271 epoch total loss 1.28069448\n",
      "Trained batch 146 batch loss 1.31723046 epoch total loss 1.28094471\n",
      "Trained batch 147 batch loss 1.31353414 epoch total loss 1.28116643\n",
      "Trained batch 148 batch loss 1.27022386 epoch total loss 1.28109241\n",
      "Trained batch 149 batch loss 1.34705234 epoch total loss 1.28153515\n",
      "Trained batch 150 batch loss 1.26679146 epoch total loss 1.2814368\n",
      "Trained batch 151 batch loss 1.28593063 epoch total loss 1.2814666\n",
      "Trained batch 152 batch loss 1.17435384 epoch total loss 1.28076184\n",
      "Trained batch 153 batch loss 1.15544319 epoch total loss 1.27994275\n",
      "Trained batch 154 batch loss 1.19397283 epoch total loss 1.27938449\n",
      "Trained batch 155 batch loss 1.2567302 epoch total loss 1.27923834\n",
      "Trained batch 156 batch loss 1.4014554 epoch total loss 1.28002179\n",
      "Trained batch 157 batch loss 1.37285709 epoch total loss 1.28061306\n",
      "Trained batch 158 batch loss 1.35299289 epoch total loss 1.28107119\n",
      "Trained batch 159 batch loss 1.41101253 epoch total loss 1.28188848\n",
      "Trained batch 160 batch loss 1.37615621 epoch total loss 1.28247762\n",
      "Trained batch 161 batch loss 1.28936708 epoch total loss 1.28252041\n",
      "Trained batch 162 batch loss 1.4059509 epoch total loss 1.28328228\n",
      "Trained batch 163 batch loss 1.33305395 epoch total loss 1.28358769\n",
      "Trained batch 164 batch loss 1.16813421 epoch total loss 1.28288376\n",
      "Trained batch 165 batch loss 1.26318812 epoch total loss 1.28276432\n",
      "Trained batch 166 batch loss 1.21851563 epoch total loss 1.28237736\n",
      "Trained batch 167 batch loss 1.12698507 epoch total loss 1.28144681\n",
      "Trained batch 168 batch loss 1.19250381 epoch total loss 1.28091741\n",
      "Trained batch 169 batch loss 1.24155617 epoch total loss 1.28068447\n",
      "Trained batch 170 batch loss 1.32590199 epoch total loss 1.28095043\n",
      "Trained batch 171 batch loss 1.39256406 epoch total loss 1.28160322\n",
      "Trained batch 172 batch loss 1.29202008 epoch total loss 1.28166378\n",
      "Trained batch 173 batch loss 1.27018607 epoch total loss 1.28159738\n",
      "Trained batch 174 batch loss 1.26334047 epoch total loss 1.28149247\n",
      "Trained batch 175 batch loss 1.22633457 epoch total loss 1.28117728\n",
      "Trained batch 176 batch loss 1.24856544 epoch total loss 1.28099203\n",
      "Trained batch 177 batch loss 1.17913699 epoch total loss 1.28041649\n",
      "Trained batch 178 batch loss 1.19325161 epoch total loss 1.2799269\n",
      "Trained batch 179 batch loss 1.17071366 epoch total loss 1.27931678\n",
      "Trained batch 180 batch loss 1.08436918 epoch total loss 1.27823365\n",
      "Trained batch 181 batch loss 1.13160455 epoch total loss 1.27742362\n",
      "Trained batch 182 batch loss 1.16413403 epoch total loss 1.27680111\n",
      "Trained batch 183 batch loss 1.17887378 epoch total loss 1.2762661\n",
      "Trained batch 184 batch loss 1.18782461 epoch total loss 1.27578533\n",
      "Trained batch 185 batch loss 1.14214063 epoch total loss 1.27506292\n",
      "Trained batch 186 batch loss 1.09557426 epoch total loss 1.27409792\n",
      "Trained batch 187 batch loss 1.46263611 epoch total loss 1.27510619\n",
      "Trained batch 188 batch loss 1.36314452 epoch total loss 1.27557445\n",
      "Trained batch 189 batch loss 1.37825739 epoch total loss 1.27611768\n",
      "Trained batch 190 batch loss 1.27444816 epoch total loss 1.27610886\n",
      "Trained batch 191 batch loss 1.54577851 epoch total loss 1.27752078\n",
      "Trained batch 192 batch loss 1.29168952 epoch total loss 1.27759457\n",
      "Trained batch 193 batch loss 1.27165985 epoch total loss 1.27756381\n",
      "Trained batch 194 batch loss 1.19374037 epoch total loss 1.2771318\n",
      "Trained batch 195 batch loss 1.200508 epoch total loss 1.27673876\n",
      "Trained batch 196 batch loss 1.18026221 epoch total loss 1.27624655\n",
      "Trained batch 197 batch loss 1.24225616 epoch total loss 1.27607405\n",
      "Trained batch 198 batch loss 1.1099602 epoch total loss 1.27523506\n",
      "Trained batch 199 batch loss 1.23406482 epoch total loss 1.27502823\n",
      "Trained batch 200 batch loss 1.30600333 epoch total loss 1.27518308\n",
      "Trained batch 201 batch loss 1.30916977 epoch total loss 1.27535224\n",
      "Trained batch 202 batch loss 1.25878811 epoch total loss 1.27527022\n",
      "Trained batch 203 batch loss 1.32069504 epoch total loss 1.2754941\n",
      "Trained batch 204 batch loss 1.38526475 epoch total loss 1.27603209\n",
      "Trained batch 205 batch loss 1.25805902 epoch total loss 1.27594435\n",
      "Trained batch 206 batch loss 1.33246517 epoch total loss 1.27621877\n",
      "Trained batch 207 batch loss 1.27587986 epoch total loss 1.2762171\n",
      "Trained batch 208 batch loss 1.23642707 epoch total loss 1.27602577\n",
      "Trained batch 209 batch loss 1.1413455 epoch total loss 1.27538145\n",
      "Trained batch 210 batch loss 1.28528559 epoch total loss 1.27542853\n",
      "Trained batch 211 batch loss 1.31343317 epoch total loss 1.27560878\n",
      "Trained batch 212 batch loss 1.29359972 epoch total loss 1.27569366\n",
      "Trained batch 213 batch loss 1.43230879 epoch total loss 1.27642894\n",
      "Trained batch 214 batch loss 1.41315556 epoch total loss 1.27706778\n",
      "Trained batch 215 batch loss 1.14954388 epoch total loss 1.2764746\n",
      "Trained batch 216 batch loss 1.28279543 epoch total loss 1.27650392\n",
      "Trained batch 217 batch loss 1.2928896 epoch total loss 1.27657938\n",
      "Trained batch 218 batch loss 1.20941329 epoch total loss 1.27627134\n",
      "Trained batch 219 batch loss 1.30439639 epoch total loss 1.27639961\n",
      "Trained batch 220 batch loss 1.21716142 epoch total loss 1.27613044\n",
      "Trained batch 221 batch loss 1.26056981 epoch total loss 1.27606\n",
      "Trained batch 222 batch loss 1.15449989 epoch total loss 1.27551246\n",
      "Trained batch 223 batch loss 1.25942731 epoch total loss 1.27544034\n",
      "Trained batch 224 batch loss 1.19850683 epoch total loss 1.27509689\n",
      "Trained batch 225 batch loss 1.16078532 epoch total loss 1.27458894\n",
      "Trained batch 226 batch loss 1.15403843 epoch total loss 1.2740556\n",
      "Trained batch 227 batch loss 1.02867448 epoch total loss 1.27297461\n",
      "Trained batch 228 batch loss 1.09395325 epoch total loss 1.2721895\n",
      "Trained batch 229 batch loss 1.31144428 epoch total loss 1.2723608\n",
      "Trained batch 230 batch loss 1.37442601 epoch total loss 1.27280462\n",
      "Trained batch 231 batch loss 1.23761046 epoch total loss 1.27265227\n",
      "Trained batch 232 batch loss 1.32825971 epoch total loss 1.27289188\n",
      "Trained batch 233 batch loss 1.29806757 epoch total loss 1.27299988\n",
      "Trained batch 234 batch loss 1.13982737 epoch total loss 1.27243078\n",
      "Trained batch 235 batch loss 1.18811774 epoch total loss 1.27207196\n",
      "Trained batch 236 batch loss 1.33035588 epoch total loss 1.27231896\n",
      "Trained batch 237 batch loss 1.27926898 epoch total loss 1.27234828\n",
      "Trained batch 238 batch loss 1.20944309 epoch total loss 1.272084\n",
      "Trained batch 239 batch loss 1.35503221 epoch total loss 1.27243102\n",
      "Trained batch 240 batch loss 1.24068379 epoch total loss 1.27229881\n",
      "Trained batch 241 batch loss 1.30091858 epoch total loss 1.27241766\n",
      "Trained batch 242 batch loss 1.40889168 epoch total loss 1.27298164\n",
      "Trained batch 243 batch loss 1.21971869 epoch total loss 1.27276242\n",
      "Trained batch 244 batch loss 1.16019464 epoch total loss 1.27230108\n",
      "Trained batch 245 batch loss 1.25226 epoch total loss 1.2722193\n",
      "Trained batch 246 batch loss 1.14499283 epoch total loss 1.27170205\n",
      "Trained batch 247 batch loss 1.26336336 epoch total loss 1.27166831\n",
      "Trained batch 248 batch loss 1.15334642 epoch total loss 1.27119124\n",
      "Trained batch 249 batch loss 1.08468413 epoch total loss 1.27044225\n",
      "Trained batch 250 batch loss 1.22002149 epoch total loss 1.27024055\n",
      "Trained batch 251 batch loss 1.19476783 epoch total loss 1.2699399\n",
      "Trained batch 252 batch loss 1.17514718 epoch total loss 1.26956367\n",
      "Trained batch 253 batch loss 1.19575357 epoch total loss 1.26927197\n",
      "Trained batch 254 batch loss 1.22351587 epoch total loss 1.26909173\n",
      "Trained batch 255 batch loss 1.29510355 epoch total loss 1.26919377\n",
      "Trained batch 256 batch loss 1.18324924 epoch total loss 1.26885808\n",
      "Trained batch 257 batch loss 1.39492905 epoch total loss 1.26934862\n",
      "Trained batch 258 batch loss 1.36078715 epoch total loss 1.26970303\n",
      "Trained batch 259 batch loss 1.35610366 epoch total loss 1.27003658\n",
      "Trained batch 260 batch loss 1.28977585 epoch total loss 1.27011251\n",
      "Trained batch 261 batch loss 1.28958464 epoch total loss 1.27018714\n",
      "Trained batch 262 batch loss 1.24893653 epoch total loss 1.27010596\n",
      "Trained batch 263 batch loss 1.33971894 epoch total loss 1.2703706\n",
      "Trained batch 264 batch loss 1.26062226 epoch total loss 1.27033377\n",
      "Trained batch 265 batch loss 1.26419759 epoch total loss 1.27031052\n",
      "Trained batch 266 batch loss 1.32625067 epoch total loss 1.27052093\n",
      "Trained batch 267 batch loss 1.29155838 epoch total loss 1.27059972\n",
      "Trained batch 268 batch loss 1.25931883 epoch total loss 1.27055752\n",
      "Trained batch 269 batch loss 1.20376754 epoch total loss 1.27030933\n",
      "Trained batch 270 batch loss 1.18448138 epoch total loss 1.2699914\n",
      "Trained batch 271 batch loss 1.29472387 epoch total loss 1.27008271\n",
      "Trained batch 272 batch loss 1.32203603 epoch total loss 1.27027369\n",
      "Trained batch 273 batch loss 1.32565522 epoch total loss 1.27047646\n",
      "Trained batch 274 batch loss 1.2165581 epoch total loss 1.27027977\n",
      "Trained batch 275 batch loss 1.25122845 epoch total loss 1.27021039\n",
      "Trained batch 276 batch loss 1.29051518 epoch total loss 1.27028406\n",
      "Trained batch 277 batch loss 1.23591304 epoch total loss 1.27015984\n",
      "Trained batch 278 batch loss 1.159343 epoch total loss 1.2697612\n",
      "Trained batch 279 batch loss 1.22953081 epoch total loss 1.26961696\n",
      "Trained batch 280 batch loss 1.37718058 epoch total loss 1.27000105\n",
      "Trained batch 281 batch loss 1.31717396 epoch total loss 1.27016902\n",
      "Trained batch 282 batch loss 1.29828846 epoch total loss 1.27026868\n",
      "Trained batch 283 batch loss 1.28639245 epoch total loss 1.27032566\n",
      "Trained batch 284 batch loss 1.18388784 epoch total loss 1.27002132\n",
      "Trained batch 285 batch loss 1.17861 epoch total loss 1.26970065\n",
      "Trained batch 286 batch loss 1.05934012 epoch total loss 1.26896513\n",
      "Trained batch 287 batch loss 1.11517882 epoch total loss 1.26842916\n",
      "Trained batch 288 batch loss 1.35205615 epoch total loss 1.26871955\n",
      "Trained batch 289 batch loss 1.40811586 epoch total loss 1.26920187\n",
      "Trained batch 290 batch loss 1.29108858 epoch total loss 1.26927733\n",
      "Trained batch 291 batch loss 1.20141888 epoch total loss 1.26904416\n",
      "Trained batch 292 batch loss 1.40901458 epoch total loss 1.2695235\n",
      "Trained batch 293 batch loss 1.24506533 epoch total loss 1.26944\n",
      "Trained batch 294 batch loss 1.1579957 epoch total loss 1.26906097\n",
      "Trained batch 295 batch loss 1.26722169 epoch total loss 1.26905465\n",
      "Trained batch 296 batch loss 1.25899255 epoch total loss 1.26902068\n",
      "Trained batch 297 batch loss 1.21813846 epoch total loss 1.26884937\n",
      "Trained batch 298 batch loss 1.389727 epoch total loss 1.26925504\n",
      "Trained batch 299 batch loss 1.28767705 epoch total loss 1.26931667\n",
      "Trained batch 300 batch loss 1.26295841 epoch total loss 1.26929557\n",
      "Trained batch 301 batch loss 1.20115042 epoch total loss 1.26906908\n",
      "Trained batch 302 batch loss 1.16712928 epoch total loss 1.26873147\n",
      "Trained batch 303 batch loss 1.19318807 epoch total loss 1.26848221\n",
      "Trained batch 304 batch loss 1.29119205 epoch total loss 1.26855695\n",
      "Trained batch 305 batch loss 1.20032263 epoch total loss 1.2683332\n",
      "Trained batch 306 batch loss 1.26785564 epoch total loss 1.26833165\n",
      "Trained batch 307 batch loss 1.25352478 epoch total loss 1.26828337\n",
      "Trained batch 308 batch loss 1.26305389 epoch total loss 1.26826632\n",
      "Trained batch 309 batch loss 1.30281949 epoch total loss 1.26837826\n",
      "Trained batch 310 batch loss 1.35917163 epoch total loss 1.26867104\n",
      "Trained batch 311 batch loss 1.47951102 epoch total loss 1.26934898\n",
      "Trained batch 312 batch loss 1.44301462 epoch total loss 1.26990569\n",
      "Trained batch 313 batch loss 1.34644794 epoch total loss 1.27015018\n",
      "Trained batch 314 batch loss 1.36803889 epoch total loss 1.27046192\n",
      "Trained batch 315 batch loss 1.26687789 epoch total loss 1.27045059\n",
      "Trained batch 316 batch loss 1.3634901 epoch total loss 1.27074504\n",
      "Trained batch 317 batch loss 1.31555414 epoch total loss 1.2708863\n",
      "Trained batch 318 batch loss 1.32575095 epoch total loss 1.2710588\n",
      "Trained batch 319 batch loss 1.2712456 epoch total loss 1.27105939\n",
      "Trained batch 320 batch loss 1.32284558 epoch total loss 1.27122128\n",
      "Trained batch 321 batch loss 1.34767973 epoch total loss 1.27145946\n",
      "Trained batch 322 batch loss 1.223575 epoch total loss 1.27131081\n",
      "Trained batch 323 batch loss 1.21985161 epoch total loss 1.27115142\n",
      "Trained batch 324 batch loss 1.24235785 epoch total loss 1.27106261\n",
      "Trained batch 325 batch loss 1.25741208 epoch total loss 1.27102065\n",
      "Trained batch 326 batch loss 1.2423296 epoch total loss 1.27093267\n",
      "Trained batch 327 batch loss 1.37789392 epoch total loss 1.27125978\n",
      "Trained batch 328 batch loss 1.25522566 epoch total loss 1.27121079\n",
      "Trained batch 329 batch loss 1.28746259 epoch total loss 1.27126026\n",
      "Trained batch 330 batch loss 1.32140493 epoch total loss 1.27141225\n",
      "Trained batch 331 batch loss 1.20562458 epoch total loss 1.27121353\n",
      "Trained batch 332 batch loss 1.24609482 epoch total loss 1.27113783\n",
      "Trained batch 333 batch loss 1.28159964 epoch total loss 1.27116919\n",
      "Trained batch 334 batch loss 1.18339431 epoch total loss 1.27090633\n",
      "Trained batch 335 batch loss 1.2357074 epoch total loss 1.27080131\n",
      "Trained batch 336 batch loss 1.35288155 epoch total loss 1.27104557\n",
      "Trained batch 337 batch loss 1.2538166 epoch total loss 1.27099442\n",
      "Trained batch 338 batch loss 1.25871468 epoch total loss 1.27095819\n",
      "Trained batch 339 batch loss 1.15108597 epoch total loss 1.27060461\n",
      "Trained batch 340 batch loss 1.30263 epoch total loss 1.27069879\n",
      "Trained batch 341 batch loss 1.31880605 epoch total loss 1.27083993\n",
      "Trained batch 342 batch loss 1.21704412 epoch total loss 1.27068257\n",
      "Trained batch 343 batch loss 1.27086568 epoch total loss 1.27068317\n",
      "Trained batch 344 batch loss 1.24904311 epoch total loss 1.27062035\n",
      "Trained batch 345 batch loss 1.11946714 epoch total loss 1.27018225\n",
      "Trained batch 346 batch loss 1.31913793 epoch total loss 1.27032375\n",
      "Trained batch 347 batch loss 1.27552938 epoch total loss 1.27033877\n",
      "Trained batch 348 batch loss 1.23490882 epoch total loss 1.27023697\n",
      "Trained batch 349 batch loss 1.18072927 epoch total loss 1.26998043\n",
      "Trained batch 350 batch loss 1.20649254 epoch total loss 1.26979899\n",
      "Trained batch 351 batch loss 1.23608446 epoch total loss 1.26970291\n",
      "Trained batch 352 batch loss 1.38599443 epoch total loss 1.27003336\n",
      "Trained batch 353 batch loss 1.21986699 epoch total loss 1.26989126\n",
      "Trained batch 354 batch loss 1.28579319 epoch total loss 1.2699362\n",
      "Trained batch 355 batch loss 1.3860054 epoch total loss 1.2702632\n",
      "Trained batch 356 batch loss 1.23208237 epoch total loss 1.27015591\n",
      "Trained batch 357 batch loss 1.38930857 epoch total loss 1.27048969\n",
      "Trained batch 358 batch loss 1.27912235 epoch total loss 1.27051377\n",
      "Trained batch 359 batch loss 1.24150157 epoch total loss 1.27043295\n",
      "Trained batch 360 batch loss 1.3328315 epoch total loss 1.27060628\n",
      "Trained batch 361 batch loss 1.34736574 epoch total loss 1.27081895\n",
      "Trained batch 362 batch loss 1.39307261 epoch total loss 1.27115655\n",
      "Trained batch 363 batch loss 1.23739207 epoch total loss 1.27106357\n",
      "Trained batch 364 batch loss 1.28112197 epoch total loss 1.27109122\n",
      "Trained batch 365 batch loss 1.4653616 epoch total loss 1.27162349\n",
      "Trained batch 366 batch loss 1.42860544 epoch total loss 1.27205241\n",
      "Trained batch 367 batch loss 1.33455443 epoch total loss 1.27222276\n",
      "Trained batch 368 batch loss 1.18858695 epoch total loss 1.27199554\n",
      "Trained batch 369 batch loss 1.32655025 epoch total loss 1.27214336\n",
      "Trained batch 370 batch loss 1.20499122 epoch total loss 1.27196181\n",
      "Trained batch 371 batch loss 1.2762208 epoch total loss 1.27197337\n",
      "Trained batch 372 batch loss 1.32702243 epoch total loss 1.27212131\n",
      "Trained batch 373 batch loss 1.2150135 epoch total loss 1.27196825\n",
      "Trained batch 374 batch loss 1.17712879 epoch total loss 1.27171469\n",
      "Trained batch 375 batch loss 1.28052044 epoch total loss 1.27173817\n",
      "Trained batch 376 batch loss 1.19395971 epoch total loss 1.27153134\n",
      "Trained batch 377 batch loss 1.26954913 epoch total loss 1.2715261\n",
      "Trained batch 378 batch loss 1.42316163 epoch total loss 1.27192724\n",
      "Trained batch 379 batch loss 1.37117422 epoch total loss 1.27218914\n",
      "Trained batch 380 batch loss 1.39418268 epoch total loss 1.27251017\n",
      "Trained batch 381 batch loss 1.32780194 epoch total loss 1.27265525\n",
      "Trained batch 382 batch loss 1.27545547 epoch total loss 1.27266252\n",
      "Trained batch 383 batch loss 1.29913354 epoch total loss 1.27273166\n",
      "Trained batch 384 batch loss 1.42929459 epoch total loss 1.27313936\n",
      "Trained batch 385 batch loss 1.25053382 epoch total loss 1.27308059\n",
      "Trained batch 386 batch loss 1.30406976 epoch total loss 1.27316093\n",
      "Trained batch 387 batch loss 1.23702526 epoch total loss 1.27306759\n",
      "Trained batch 388 batch loss 1.22332788 epoch total loss 1.27293944\n",
      "Trained batch 389 batch loss 1.09431374 epoch total loss 1.27248013\n",
      "Trained batch 390 batch loss 1.20142174 epoch total loss 1.27229798\n",
      "Trained batch 391 batch loss 1.14532495 epoch total loss 1.27197325\n",
      "Trained batch 392 batch loss 1.15263867 epoch total loss 1.27166879\n",
      "Trained batch 393 batch loss 1.22931981 epoch total loss 1.27156103\n",
      "Trained batch 394 batch loss 1.28327811 epoch total loss 1.27159071\n",
      "Trained batch 395 batch loss 1.15664935 epoch total loss 1.27129972\n",
      "Trained batch 396 batch loss 1.29863906 epoch total loss 1.27136874\n",
      "Trained batch 397 batch loss 1.38370514 epoch total loss 1.27165174\n",
      "Trained batch 398 batch loss 1.41823864 epoch total loss 1.2720201\n",
      "Trained batch 399 batch loss 1.36455727 epoch total loss 1.27225196\n",
      "Trained batch 400 batch loss 1.44123399 epoch total loss 1.27267444\n",
      "Trained batch 401 batch loss 1.30600655 epoch total loss 1.27275753\n",
      "Trained batch 402 batch loss 1.25278389 epoch total loss 1.27270782\n",
      "Trained batch 403 batch loss 1.23547792 epoch total loss 1.27261543\n",
      "Trained batch 404 batch loss 1.32484674 epoch total loss 1.27274466\n",
      "Trained batch 405 batch loss 1.3381331 epoch total loss 1.27290606\n",
      "Trained batch 406 batch loss 1.17576814 epoch total loss 1.27266693\n",
      "Trained batch 407 batch loss 1.15717649 epoch total loss 1.27238309\n",
      "Trained batch 408 batch loss 1.21736169 epoch total loss 1.27224827\n",
      "Trained batch 409 batch loss 1.1671617 epoch total loss 1.27199125\n",
      "Trained batch 410 batch loss 1.18335819 epoch total loss 1.27177513\n",
      "Trained batch 411 batch loss 1.24009514 epoch total loss 1.27169812\n",
      "Trained batch 412 batch loss 1.23337626 epoch total loss 1.27160513\n",
      "Trained batch 413 batch loss 1.30567288 epoch total loss 1.27168763\n",
      "Trained batch 414 batch loss 1.36432862 epoch total loss 1.27191138\n",
      "Trained batch 415 batch loss 1.35546613 epoch total loss 1.27211273\n",
      "Trained batch 416 batch loss 1.26991439 epoch total loss 1.27210736\n",
      "Trained batch 417 batch loss 1.36394179 epoch total loss 1.27232754\n",
      "Trained batch 418 batch loss 1.2826122 epoch total loss 1.2723521\n",
      "Trained batch 419 batch loss 1.24432063 epoch total loss 1.27228522\n",
      "Trained batch 420 batch loss 1.3141768 epoch total loss 1.27238488\n",
      "Trained batch 421 batch loss 1.41855121 epoch total loss 1.27273214\n",
      "Trained batch 422 batch loss 1.37169099 epoch total loss 1.27296674\n",
      "Trained batch 423 batch loss 1.18374622 epoch total loss 1.27275574\n",
      "Trained batch 424 batch loss 1.30761194 epoch total loss 1.272838\n",
      "Trained batch 425 batch loss 1.29841614 epoch total loss 1.27289808\n",
      "Trained batch 426 batch loss 1.34579873 epoch total loss 1.27306926\n",
      "Trained batch 427 batch loss 1.23732781 epoch total loss 1.27298558\n",
      "Trained batch 428 batch loss 1.3201859 epoch total loss 1.27309585\n",
      "Trained batch 429 batch loss 1.31438708 epoch total loss 1.27319205\n",
      "Trained batch 430 batch loss 1.31078196 epoch total loss 1.27327955\n",
      "Trained batch 431 batch loss 1.25745249 epoch total loss 1.27324283\n",
      "Trained batch 432 batch loss 1.41022933 epoch total loss 1.27355981\n",
      "Trained batch 433 batch loss 1.1782521 epoch total loss 1.27333963\n",
      "Trained batch 434 batch loss 1.23641014 epoch total loss 1.27325451\n",
      "Trained batch 435 batch loss 1.34657037 epoch total loss 1.27342308\n",
      "Trained batch 436 batch loss 1.29798508 epoch total loss 1.27347934\n",
      "Trained batch 437 batch loss 1.36592877 epoch total loss 1.27369082\n",
      "Trained batch 438 batch loss 1.25035238 epoch total loss 1.27363765\n",
      "Trained batch 439 batch loss 1.18645144 epoch total loss 1.27343905\n",
      "Trained batch 440 batch loss 1.05486357 epoch total loss 1.2729423\n",
      "Trained batch 441 batch loss 1.22875738 epoch total loss 1.27284205\n",
      "Trained batch 442 batch loss 1.29294157 epoch total loss 1.27288759\n",
      "Trained batch 443 batch loss 1.38675177 epoch total loss 1.27314472\n",
      "Trained batch 444 batch loss 1.30651629 epoch total loss 1.27322\n",
      "Trained batch 445 batch loss 1.41734433 epoch total loss 1.27354383\n",
      "Trained batch 446 batch loss 1.31677985 epoch total loss 1.27364075\n",
      "Trained batch 447 batch loss 1.33846283 epoch total loss 1.27378571\n",
      "Trained batch 448 batch loss 1.40085649 epoch total loss 1.27406943\n",
      "Trained batch 449 batch loss 1.18529499 epoch total loss 1.27387166\n",
      "Trained batch 450 batch loss 1.31175387 epoch total loss 1.27395594\n",
      "Trained batch 451 batch loss 1.35379529 epoch total loss 1.27413297\n",
      "Trained batch 452 batch loss 1.36516166 epoch total loss 1.27433443\n",
      "Trained batch 453 batch loss 1.46981454 epoch total loss 1.27476585\n",
      "Trained batch 454 batch loss 1.29064536 epoch total loss 1.2748009\n",
      "Trained batch 455 batch loss 1.24007463 epoch total loss 1.27472448\n",
      "Trained batch 456 batch loss 1.25989246 epoch total loss 1.27469194\n",
      "Trained batch 457 batch loss 1.2873131 epoch total loss 1.27471948\n",
      "Trained batch 458 batch loss 1.28135562 epoch total loss 1.27473402\n",
      "Trained batch 459 batch loss 1.34102345 epoch total loss 1.27487838\n",
      "Trained batch 460 batch loss 1.37635553 epoch total loss 1.27509904\n",
      "Trained batch 461 batch loss 1.44611573 epoch total loss 1.2754699\n",
      "Trained batch 462 batch loss 1.5803833 epoch total loss 1.27613\n",
      "Trained batch 463 batch loss 1.33389759 epoch total loss 1.27625477\n",
      "Trained batch 464 batch loss 1.14084315 epoch total loss 1.27596295\n",
      "Trained batch 465 batch loss 1.24920964 epoch total loss 1.27590537\n",
      "Trained batch 466 batch loss 1.21029568 epoch total loss 1.27576458\n",
      "Trained batch 467 batch loss 1.18096936 epoch total loss 1.27556157\n",
      "Trained batch 468 batch loss 1.15972924 epoch total loss 1.27531409\n",
      "Trained batch 469 batch loss 1.17467546 epoch total loss 1.27509952\n",
      "Trained batch 470 batch loss 1.17192948 epoch total loss 1.27488\n",
      "Trained batch 471 batch loss 1.10245621 epoch total loss 1.27451396\n",
      "Trained batch 472 batch loss 1.14255059 epoch total loss 1.27423441\n",
      "Trained batch 473 batch loss 1.13159764 epoch total loss 1.27393293\n",
      "Trained batch 474 batch loss 1.1865977 epoch total loss 1.27374864\n",
      "Trained batch 475 batch loss 1.21673894 epoch total loss 1.27362859\n",
      "Trained batch 476 batch loss 1.07433653 epoch total loss 1.27320993\n",
      "Trained batch 477 batch loss 1.14231968 epoch total loss 1.27293551\n",
      "Trained batch 478 batch loss 1.36172509 epoch total loss 1.27312136\n",
      "Trained batch 479 batch loss 1.54462051 epoch total loss 1.2736882\n",
      "Trained batch 480 batch loss 1.45391023 epoch total loss 1.27406359\n",
      "Trained batch 481 batch loss 1.28828168 epoch total loss 1.27409315\n",
      "Trained batch 482 batch loss 1.28529203 epoch total loss 1.2741164\n",
      "Trained batch 483 batch loss 1.25670266 epoch total loss 1.27408028\n",
      "Trained batch 484 batch loss 1.27327394 epoch total loss 1.27407861\n",
      "Trained batch 485 batch loss 1.2701385 epoch total loss 1.2740705\n",
      "Trained batch 486 batch loss 1.28209567 epoch total loss 1.27408707\n",
      "Trained batch 487 batch loss 1.34437478 epoch total loss 1.27423131\n",
      "Trained batch 488 batch loss 1.29352939 epoch total loss 1.27427089\n",
      "Trained batch 489 batch loss 1.27652621 epoch total loss 1.27427554\n",
      "Trained batch 490 batch loss 1.37032914 epoch total loss 1.27447152\n",
      "Trained batch 491 batch loss 1.13879597 epoch total loss 1.27419519\n",
      "Trained batch 492 batch loss 1.22445166 epoch total loss 1.27409399\n",
      "Trained batch 493 batch loss 1.22586274 epoch total loss 1.27399623\n",
      "Trained batch 494 batch loss 1.22394443 epoch total loss 1.27389491\n",
      "Trained batch 495 batch loss 1.15954661 epoch total loss 1.27366388\n",
      "Trained batch 496 batch loss 1.15186632 epoch total loss 1.27341831\n",
      "Trained batch 497 batch loss 1.16076016 epoch total loss 1.27319169\n",
      "Trained batch 498 batch loss 1.24510348 epoch total loss 1.2731353\n",
      "Trained batch 499 batch loss 1.32042813 epoch total loss 1.27323008\n",
      "Trained batch 500 batch loss 1.23710084 epoch total loss 1.27315784\n",
      "Trained batch 501 batch loss 1.44176435 epoch total loss 1.27349436\n",
      "Trained batch 502 batch loss 1.47746491 epoch total loss 1.27390075\n",
      "Trained batch 503 batch loss 1.45780706 epoch total loss 1.27426636\n",
      "Trained batch 504 batch loss 1.33916092 epoch total loss 1.27439523\n",
      "Trained batch 505 batch loss 1.30403697 epoch total loss 1.27445388\n",
      "Trained batch 506 batch loss 1.28910124 epoch total loss 1.27448285\n",
      "Trained batch 507 batch loss 1.09841394 epoch total loss 1.27413547\n",
      "Trained batch 508 batch loss 1.38364649 epoch total loss 1.27435112\n",
      "Trained batch 509 batch loss 1.23697877 epoch total loss 1.27427769\n",
      "Trained batch 510 batch loss 1.14819741 epoch total loss 1.27403045\n",
      "Trained batch 511 batch loss 1.14158845 epoch total loss 1.27377129\n",
      "Trained batch 512 batch loss 1.07783318 epoch total loss 1.27338862\n",
      "Trained batch 513 batch loss 1.09278512 epoch total loss 1.2730366\n",
      "Trained batch 514 batch loss 1.19463408 epoch total loss 1.27288401\n",
      "Trained batch 515 batch loss 1.39456499 epoch total loss 1.2731204\n",
      "Trained batch 516 batch loss 1.65879822 epoch total loss 1.27386785\n",
      "Trained batch 517 batch loss 1.49358797 epoch total loss 1.27429283\n",
      "Trained batch 518 batch loss 1.44286966 epoch total loss 1.27461827\n",
      "Trained batch 519 batch loss 1.43281102 epoch total loss 1.27492309\n",
      "Trained batch 520 batch loss 1.44024813 epoch total loss 1.27524102\n",
      "Trained batch 521 batch loss 1.3554883 epoch total loss 1.27539492\n",
      "Trained batch 522 batch loss 1.2153 epoch total loss 1.27527976\n",
      "Trained batch 523 batch loss 1.23718381 epoch total loss 1.27520692\n",
      "Trained batch 524 batch loss 1.27832747 epoch total loss 1.27521288\n",
      "Trained batch 525 batch loss 1.17294097 epoch total loss 1.27501798\n",
      "Trained batch 526 batch loss 1.15166044 epoch total loss 1.27478349\n",
      "Trained batch 527 batch loss 1.35830581 epoch total loss 1.27494192\n",
      "Trained batch 528 batch loss 1.1081177 epoch total loss 1.2746259\n",
      "Trained batch 529 batch loss 1.26351976 epoch total loss 1.27460504\n",
      "Trained batch 530 batch loss 1.20478225 epoch total loss 1.27447331\n",
      "Trained batch 531 batch loss 1.09678316 epoch total loss 1.27413869\n",
      "Trained batch 532 batch loss 1.19018769 epoch total loss 1.27398086\n",
      "Trained batch 533 batch loss 1.10291815 epoch total loss 1.27365983\n",
      "Trained batch 534 batch loss 1.20952654 epoch total loss 1.27353978\n",
      "Trained batch 535 batch loss 1.28486824 epoch total loss 1.27356088\n",
      "Trained batch 536 batch loss 1.25840569 epoch total loss 1.27353275\n",
      "Trained batch 537 batch loss 1.16236866 epoch total loss 1.27332568\n",
      "Trained batch 538 batch loss 1.37449324 epoch total loss 1.27351379\n",
      "Trained batch 539 batch loss 1.34466577 epoch total loss 1.27364576\n",
      "Trained batch 540 batch loss 1.37358522 epoch total loss 1.27383089\n",
      "Trained batch 541 batch loss 1.36465836 epoch total loss 1.27399874\n",
      "Trained batch 542 batch loss 1.27517307 epoch total loss 1.27400088\n",
      "Trained batch 543 batch loss 1.3326087 epoch total loss 1.27410877\n",
      "Trained batch 544 batch loss 1.30720842 epoch total loss 1.27416956\n",
      "Trained batch 545 batch loss 1.23783076 epoch total loss 1.27410293\n",
      "Trained batch 546 batch loss 1.26967633 epoch total loss 1.27409482\n",
      "Trained batch 547 batch loss 1.37017715 epoch total loss 1.27427042\n",
      "Trained batch 548 batch loss 1.34845531 epoch total loss 1.27440584\n",
      "Trained batch 549 batch loss 1.22434568 epoch total loss 1.27431464\n",
      "Trained batch 550 batch loss 1.17340374 epoch total loss 1.27413118\n",
      "Trained batch 551 batch loss 1.12063456 epoch total loss 1.27385259\n",
      "Trained batch 552 batch loss 1.12266552 epoch total loss 1.27357864\n",
      "Trained batch 553 batch loss 1.20944285 epoch total loss 1.27346277\n",
      "Trained batch 554 batch loss 1.16838574 epoch total loss 1.27327311\n",
      "Trained batch 555 batch loss 1.15943396 epoch total loss 1.27306795\n",
      "Trained batch 556 batch loss 1.21969783 epoch total loss 1.27297211\n",
      "Trained batch 557 batch loss 1.15363789 epoch total loss 1.27275777\n",
      "Trained batch 558 batch loss 1.22764349 epoch total loss 1.27267694\n",
      "Trained batch 559 batch loss 1.16195178 epoch total loss 1.27247882\n",
      "Trained batch 560 batch loss 1.23590159 epoch total loss 1.27241349\n",
      "Trained batch 561 batch loss 1.31364202 epoch total loss 1.27248704\n",
      "Trained batch 562 batch loss 1.37568533 epoch total loss 1.27267063\n",
      "Trained batch 563 batch loss 1.24287772 epoch total loss 1.2726177\n",
      "Trained batch 564 batch loss 1.33004141 epoch total loss 1.2727195\n",
      "Trained batch 565 batch loss 1.25961685 epoch total loss 1.27269638\n",
      "Trained batch 566 batch loss 1.34474874 epoch total loss 1.27282357\n",
      "Trained batch 567 batch loss 1.32346034 epoch total loss 1.27291298\n",
      "Trained batch 568 batch loss 1.28209496 epoch total loss 1.27292907\n",
      "Trained batch 569 batch loss 1.30136287 epoch total loss 1.27297914\n",
      "Trained batch 570 batch loss 1.26393175 epoch total loss 1.27296329\n",
      "Trained batch 571 batch loss 1.36958838 epoch total loss 1.27313244\n",
      "Trained batch 572 batch loss 1.19949722 epoch total loss 1.2730037\n",
      "Trained batch 573 batch loss 1.29998708 epoch total loss 1.27305079\n",
      "Trained batch 574 batch loss 1.19287241 epoch total loss 1.27291119\n",
      "Trained batch 575 batch loss 1.27929747 epoch total loss 1.27292228\n",
      "Trained batch 576 batch loss 1.37830555 epoch total loss 1.27310514\n",
      "Trained batch 577 batch loss 1.28946877 epoch total loss 1.27313364\n",
      "Trained batch 578 batch loss 1.27471268 epoch total loss 1.27313638\n",
      "Trained batch 579 batch loss 1.23843372 epoch total loss 1.2730763\n",
      "Trained batch 580 batch loss 1.13043404 epoch total loss 1.27283037\n",
      "Trained batch 581 batch loss 1.15973508 epoch total loss 1.2726357\n",
      "Trained batch 582 batch loss 1.16635072 epoch total loss 1.27245307\n",
      "Trained batch 583 batch loss 1.17184842 epoch total loss 1.27228057\n",
      "Trained batch 584 batch loss 1.31297231 epoch total loss 1.27235031\n",
      "Trained batch 585 batch loss 1.37516069 epoch total loss 1.27252603\n",
      "Trained batch 586 batch loss 1.28716624 epoch total loss 1.27255106\n",
      "Trained batch 587 batch loss 1.24485266 epoch total loss 1.27250385\n",
      "Trained batch 588 batch loss 1.35618401 epoch total loss 1.27264619\n",
      "Trained batch 589 batch loss 1.32595813 epoch total loss 1.27273667\n",
      "Trained batch 590 batch loss 1.37735343 epoch total loss 1.27291405\n",
      "Trained batch 591 batch loss 1.4265672 epoch total loss 1.27317405\n",
      "Trained batch 592 batch loss 1.39919889 epoch total loss 1.27338684\n",
      "Trained batch 593 batch loss 1.24121761 epoch total loss 1.2733326\n",
      "Trained batch 594 batch loss 1.27429986 epoch total loss 1.27333426\n",
      "Trained batch 595 batch loss 1.29719305 epoch total loss 1.27337432\n",
      "Trained batch 596 batch loss 1.25896287 epoch total loss 1.27335012\n",
      "Trained batch 597 batch loss 1.30114222 epoch total loss 1.27339673\n",
      "Trained batch 598 batch loss 1.3245374 epoch total loss 1.2734822\n",
      "Trained batch 599 batch loss 1.42831087 epoch total loss 1.27374065\n",
      "Trained batch 600 batch loss 1.42335844 epoch total loss 1.27398992\n",
      "Trained batch 601 batch loss 1.38481212 epoch total loss 1.27417433\n",
      "Trained batch 602 batch loss 1.44186473 epoch total loss 1.27445304\n",
      "Trained batch 603 batch loss 1.44109154 epoch total loss 1.27472937\n",
      "Trained batch 604 batch loss 1.3766669 epoch total loss 1.27489805\n",
      "Trained batch 605 batch loss 1.33426034 epoch total loss 1.27499628\n",
      "Trained batch 606 batch loss 1.44068563 epoch total loss 1.27526963\n",
      "Trained batch 607 batch loss 1.42981136 epoch total loss 1.27552426\n",
      "Trained batch 608 batch loss 1.44260669 epoch total loss 1.27579916\n",
      "Trained batch 609 batch loss 1.39152706 epoch total loss 1.27598917\n",
      "Trained batch 610 batch loss 1.359308 epoch total loss 1.27612579\n",
      "Trained batch 611 batch loss 1.40242195 epoch total loss 1.27633238\n",
      "Trained batch 612 batch loss 1.37316585 epoch total loss 1.27649069\n",
      "Trained batch 613 batch loss 1.31719732 epoch total loss 1.27655709\n",
      "Trained batch 614 batch loss 1.27148259 epoch total loss 1.27654874\n",
      "Trained batch 615 batch loss 1.22865629 epoch total loss 1.2764709\n",
      "Trained batch 616 batch loss 1.19992423 epoch total loss 1.27634668\n",
      "Trained batch 617 batch loss 1.22540736 epoch total loss 1.27626407\n",
      "Trained batch 618 batch loss 1.36189604 epoch total loss 1.27640259\n",
      "Trained batch 619 batch loss 1.31716251 epoch total loss 1.2764684\n",
      "Trained batch 620 batch loss 1.39378262 epoch total loss 1.2766577\n",
      "Trained batch 621 batch loss 1.32169306 epoch total loss 1.2767303\n",
      "Trained batch 622 batch loss 1.42429566 epoch total loss 1.27696753\n",
      "Trained batch 623 batch loss 1.25458694 epoch total loss 1.27693164\n",
      "Trained batch 624 batch loss 1.16602707 epoch total loss 1.27675378\n",
      "Trained batch 625 batch loss 1.18773723 epoch total loss 1.27661145\n",
      "Trained batch 626 batch loss 1.13033926 epoch total loss 1.27637768\n",
      "Trained batch 627 batch loss 1.3995055 epoch total loss 1.27657402\n",
      "Trained batch 628 batch loss 1.20342791 epoch total loss 1.27645755\n",
      "Trained batch 629 batch loss 1.23792577 epoch total loss 1.27639627\n",
      "Trained batch 630 batch loss 1.19983268 epoch total loss 1.2762748\n",
      "Trained batch 631 batch loss 1.08571863 epoch total loss 1.27597272\n",
      "Trained batch 632 batch loss 1.14590526 epoch total loss 1.27576697\n",
      "Trained batch 633 batch loss 1.17390013 epoch total loss 1.27560604\n",
      "Trained batch 634 batch loss 1.1903429 epoch total loss 1.27547157\n",
      "Trained batch 635 batch loss 1.24042201 epoch total loss 1.27541637\n",
      "Trained batch 636 batch loss 1.29890251 epoch total loss 1.27545333\n",
      "Trained batch 637 batch loss 1.33269131 epoch total loss 1.27554321\n",
      "Trained batch 638 batch loss 1.29842961 epoch total loss 1.27557898\n",
      "Trained batch 639 batch loss 1.24900675 epoch total loss 1.27553749\n",
      "Trained batch 640 batch loss 1.33694565 epoch total loss 1.27563345\n",
      "Trained batch 641 batch loss 1.13768148 epoch total loss 1.27541828\n",
      "Trained batch 642 batch loss 1.12152195 epoch total loss 1.27517855\n",
      "Trained batch 643 batch loss 1.26030886 epoch total loss 1.27515543\n",
      "Trained batch 644 batch loss 1.32729626 epoch total loss 1.27523637\n",
      "Trained batch 645 batch loss 1.35342836 epoch total loss 1.2753576\n",
      "Trained batch 646 batch loss 1.28979063 epoch total loss 1.2753799\n",
      "Trained batch 647 batch loss 1.20134282 epoch total loss 1.27526557\n",
      "Trained batch 648 batch loss 1.25333714 epoch total loss 1.27523172\n",
      "Trained batch 649 batch loss 1.25661492 epoch total loss 1.27520299\n",
      "Trained batch 650 batch loss 1.20081925 epoch total loss 1.27508855\n",
      "Trained batch 651 batch loss 1.23353171 epoch total loss 1.27502465\n",
      "Trained batch 652 batch loss 1.28159022 epoch total loss 1.27503479\n",
      "Trained batch 653 batch loss 1.36293793 epoch total loss 1.27516937\n",
      "Trained batch 654 batch loss 1.23871708 epoch total loss 1.27511358\n",
      "Trained batch 655 batch loss 1.18985689 epoch total loss 1.27498353\n",
      "Trained batch 656 batch loss 1.14721537 epoch total loss 1.27478874\n",
      "Trained batch 657 batch loss 1.26638067 epoch total loss 1.27477586\n",
      "Trained batch 658 batch loss 1.15197492 epoch total loss 1.2745893\n",
      "Trained batch 659 batch loss 1.28202951 epoch total loss 1.27460063\n",
      "Trained batch 660 batch loss 1.25173962 epoch total loss 1.27456605\n",
      "Trained batch 661 batch loss 1.27761078 epoch total loss 1.27457058\n",
      "Trained batch 662 batch loss 1.24231374 epoch total loss 1.27452183\n",
      "Trained batch 663 batch loss 1.17154098 epoch total loss 1.27436662\n",
      "Trained batch 664 batch loss 1.21706927 epoch total loss 1.27428019\n",
      "Trained batch 665 batch loss 1.20417619 epoch total loss 1.27417481\n",
      "Trained batch 666 batch loss 1.17167151 epoch total loss 1.27402091\n",
      "Trained batch 667 batch loss 1.284868 epoch total loss 1.27403712\n",
      "Trained batch 668 batch loss 1.15134716 epoch total loss 1.27385354\n",
      "Trained batch 669 batch loss 1.28553581 epoch total loss 1.27387094\n",
      "Trained batch 670 batch loss 1.25910091 epoch total loss 1.27384889\n",
      "Trained batch 671 batch loss 1.16802979 epoch total loss 1.27369118\n",
      "Trained batch 672 batch loss 1.11980867 epoch total loss 1.27346218\n",
      "Trained batch 673 batch loss 1.1711576 epoch total loss 1.27331018\n",
      "Trained batch 674 batch loss 1.27281046 epoch total loss 1.27330947\n",
      "Trained batch 675 batch loss 1.38767815 epoch total loss 1.27347887\n",
      "Trained batch 676 batch loss 1.53997731 epoch total loss 1.27387309\n",
      "Trained batch 677 batch loss 1.50188017 epoch total loss 1.27421\n",
      "Trained batch 678 batch loss 1.40985954 epoch total loss 1.27441\n",
      "Trained batch 679 batch loss 1.45808792 epoch total loss 1.2746805\n",
      "Trained batch 680 batch loss 1.31874478 epoch total loss 1.27474523\n",
      "Trained batch 681 batch loss 1.25882697 epoch total loss 1.27472198\n",
      "Trained batch 682 batch loss 1.23283231 epoch total loss 1.27466059\n",
      "Trained batch 683 batch loss 1.25664353 epoch total loss 1.27463412\n",
      "Trained batch 684 batch loss 1.29465532 epoch total loss 1.27466345\n",
      "Trained batch 685 batch loss 1.26472068 epoch total loss 1.2746489\n",
      "Trained batch 686 batch loss 1.30457723 epoch total loss 1.27469254\n",
      "Trained batch 687 batch loss 1.2503891 epoch total loss 1.27465713\n",
      "Trained batch 688 batch loss 1.16056871 epoch total loss 1.27449131\n",
      "Trained batch 689 batch loss 1.08929157 epoch total loss 1.27422249\n",
      "Trained batch 690 batch loss 1.14230418 epoch total loss 1.2740314\n",
      "Trained batch 691 batch loss 1.20093548 epoch total loss 1.27392566\n",
      "Trained batch 692 batch loss 1.14612341 epoch total loss 1.27374089\n",
      "Trained batch 693 batch loss 1.16714263 epoch total loss 1.27358711\n",
      "Trained batch 694 batch loss 1.27625859 epoch total loss 1.27359092\n",
      "Trained batch 695 batch loss 1.33228183 epoch total loss 1.27367532\n",
      "Trained batch 696 batch loss 1.431072 epoch total loss 1.27390146\n",
      "Trained batch 697 batch loss 1.33995473 epoch total loss 1.27399623\n",
      "Trained batch 698 batch loss 1.41719 epoch total loss 1.27420139\n",
      "Trained batch 699 batch loss 1.27232301 epoch total loss 1.27419877\n",
      "Trained batch 700 batch loss 1.31862545 epoch total loss 1.27426219\n",
      "Trained batch 701 batch loss 1.24850726 epoch total loss 1.27422547\n",
      "Trained batch 702 batch loss 1.23737705 epoch total loss 1.27417302\n",
      "Trained batch 703 batch loss 1.12143159 epoch total loss 1.2739557\n",
      "Trained batch 704 batch loss 1.16171455 epoch total loss 1.27379632\n",
      "Trained batch 705 batch loss 1.05128872 epoch total loss 1.27348065\n",
      "Trained batch 706 batch loss 1.1076045 epoch total loss 1.27324569\n",
      "Trained batch 707 batch loss 1.09175444 epoch total loss 1.27298903\n",
      "Trained batch 708 batch loss 1.02215624 epoch total loss 1.27263474\n",
      "Trained batch 709 batch loss 0.994126678 epoch total loss 1.27224195\n",
      "Trained batch 710 batch loss 0.982444167 epoch total loss 1.27183378\n",
      "Trained batch 711 batch loss 1.14222479 epoch total loss 1.27165139\n",
      "Trained batch 712 batch loss 1.19540834 epoch total loss 1.27154434\n",
      "Trained batch 713 batch loss 1.27038288 epoch total loss 1.27154279\n",
      "Trained batch 714 batch loss 1.19347918 epoch total loss 1.27143347\n",
      "Trained batch 715 batch loss 1.27905011 epoch total loss 1.27144408\n",
      "Trained batch 716 batch loss 1.36029696 epoch total loss 1.27156818\n",
      "Trained batch 717 batch loss 1.31542516 epoch total loss 1.27162933\n",
      "Trained batch 718 batch loss 1.30566955 epoch total loss 1.27167678\n",
      "Trained batch 719 batch loss 1.24490464 epoch total loss 1.27163959\n",
      "Trained batch 720 batch loss 1.24138284 epoch total loss 1.2715975\n",
      "Trained batch 721 batch loss 1.13987553 epoch total loss 1.27141488\n",
      "Trained batch 722 batch loss 1.08067894 epoch total loss 1.27115071\n",
      "Trained batch 723 batch loss 1.21565855 epoch total loss 1.27107394\n",
      "Trained batch 724 batch loss 1.13743699 epoch total loss 1.2708894\n",
      "Trained batch 725 batch loss 1.19687521 epoch total loss 1.27078736\n",
      "Trained batch 726 batch loss 1.10090113 epoch total loss 1.27055335\n",
      "Trained batch 727 batch loss 1.20080101 epoch total loss 1.27045739\n",
      "Trained batch 728 batch loss 1.05324197 epoch total loss 1.27015901\n",
      "Trained batch 729 batch loss 1.32418609 epoch total loss 1.27023304\n",
      "Trained batch 730 batch loss 1.35671961 epoch total loss 1.27035141\n",
      "Trained batch 731 batch loss 1.26301634 epoch total loss 1.2703414\n",
      "Trained batch 732 batch loss 1.21319985 epoch total loss 1.27026331\n",
      "Trained batch 733 batch loss 1.25344217 epoch total loss 1.27024031\n",
      "Trained batch 734 batch loss 1.23683679 epoch total loss 1.27019477\n",
      "Trained batch 735 batch loss 1.19519436 epoch total loss 1.27009273\n",
      "Trained batch 736 batch loss 1.21353602 epoch total loss 1.27001595\n",
      "Trained batch 737 batch loss 1.39093411 epoch total loss 1.27018\n",
      "Trained batch 738 batch loss 1.29153407 epoch total loss 1.27020895\n",
      "Trained batch 739 batch loss 1.39624763 epoch total loss 1.27037942\n",
      "Trained batch 740 batch loss 1.36490178 epoch total loss 1.27050722\n",
      "Trained batch 741 batch loss 1.33551967 epoch total loss 1.27059495\n",
      "Trained batch 742 batch loss 1.44192028 epoch total loss 1.27082586\n",
      "Trained batch 743 batch loss 1.45712984 epoch total loss 1.27107656\n",
      "Trained batch 744 batch loss 1.33224463 epoch total loss 1.27115881\n",
      "Trained batch 745 batch loss 1.27531624 epoch total loss 1.27116442\n",
      "Trained batch 746 batch loss 1.23693287 epoch total loss 1.27111852\n",
      "Trained batch 747 batch loss 1.44875574 epoch total loss 1.27135623\n",
      "Trained batch 748 batch loss 1.25274825 epoch total loss 1.27133143\n",
      "Trained batch 749 batch loss 1.21078622 epoch total loss 1.27125061\n",
      "Trained batch 750 batch loss 1.29475141 epoch total loss 1.27128196\n",
      "Trained batch 751 batch loss 1.20407414 epoch total loss 1.27119243\n",
      "Trained batch 752 batch loss 1.1509763 epoch total loss 1.27103257\n",
      "Trained batch 753 batch loss 1.16740692 epoch total loss 1.270895\n",
      "Trained batch 754 batch loss 1.05335701 epoch total loss 1.27060652\n",
      "Trained batch 755 batch loss 1.20111823 epoch total loss 1.27051449\n",
      "Trained batch 756 batch loss 1.27541566 epoch total loss 1.27052093\n",
      "Trained batch 757 batch loss 1.40214944 epoch total loss 1.27069473\n",
      "Trained batch 758 batch loss 1.46473646 epoch total loss 1.27095079\n",
      "Trained batch 759 batch loss 1.33609521 epoch total loss 1.27103662\n",
      "Trained batch 760 batch loss 1.29588985 epoch total loss 1.27106929\n",
      "Trained batch 761 batch loss 1.31259513 epoch total loss 1.27112389\n",
      "Trained batch 762 batch loss 1.31859231 epoch total loss 1.27118623\n",
      "Trained batch 763 batch loss 1.30439568 epoch total loss 1.27122974\n",
      "Trained batch 764 batch loss 1.42032325 epoch total loss 1.27142489\n",
      "Trained batch 765 batch loss 1.44815946 epoch total loss 1.27165604\n",
      "Trained batch 766 batch loss 1.2910583 epoch total loss 1.27168131\n",
      "Trained batch 767 batch loss 1.21845663 epoch total loss 1.27161193\n",
      "Trained batch 768 batch loss 1.19223857 epoch total loss 1.27150857\n",
      "Trained batch 769 batch loss 1.06792092 epoch total loss 1.27124393\n",
      "Trained batch 770 batch loss 1.23480582 epoch total loss 1.2711966\n",
      "Trained batch 771 batch loss 1.13732636 epoch total loss 1.27102292\n",
      "Trained batch 772 batch loss 0.993171 epoch total loss 1.27066302\n",
      "Trained batch 773 batch loss 1.00559235 epoch total loss 1.27032018\n",
      "Trained batch 774 batch loss 1.03888166 epoch total loss 1.27002108\n",
      "Trained batch 775 batch loss 1.0273819 epoch total loss 1.26970804\n",
      "Trained batch 776 batch loss 1.22502244 epoch total loss 1.26965046\n",
      "Trained batch 777 batch loss 1.26398349 epoch total loss 1.26964319\n",
      "Trained batch 778 batch loss 1.17851663 epoch total loss 1.26952612\n",
      "Trained batch 779 batch loss 1.34148216 epoch total loss 1.26961851\n",
      "Trained batch 780 batch loss 1.38202798 epoch total loss 1.26976252\n",
      "Trained batch 781 batch loss 1.2805922 epoch total loss 1.26977646\n",
      "Trained batch 782 batch loss 1.35673225 epoch total loss 1.26988769\n",
      "Trained batch 783 batch loss 1.22856665 epoch total loss 1.26983488\n",
      "Trained batch 784 batch loss 1.32297826 epoch total loss 1.26990271\n",
      "Trained batch 785 batch loss 1.37618327 epoch total loss 1.27003801\n",
      "Trained batch 786 batch loss 1.33332992 epoch total loss 1.27011859\n",
      "Trained batch 787 batch loss 1.27812493 epoch total loss 1.27012873\n",
      "Trained batch 788 batch loss 1.28168797 epoch total loss 1.27014339\n",
      "Trained batch 789 batch loss 1.30682683 epoch total loss 1.27018988\n",
      "Trained batch 790 batch loss 1.19049382 epoch total loss 1.27008903\n",
      "Trained batch 791 batch loss 1.19369411 epoch total loss 1.26999235\n",
      "Trained batch 792 batch loss 1.25263846 epoch total loss 1.26997042\n",
      "Trained batch 793 batch loss 1.17146087 epoch total loss 1.2698462\n",
      "Trained batch 794 batch loss 1.24460316 epoch total loss 1.26981449\n",
      "Trained batch 795 batch loss 1.14044046 epoch total loss 1.26965165\n",
      "Trained batch 796 batch loss 1.1418736 epoch total loss 1.2694912\n",
      "Trained batch 797 batch loss 1.19199574 epoch total loss 1.26939392\n",
      "Trained batch 798 batch loss 1.18758047 epoch total loss 1.2692914\n",
      "Trained batch 799 batch loss 1.30677474 epoch total loss 1.26933825\n",
      "Trained batch 800 batch loss 1.26074612 epoch total loss 1.26932752\n",
      "Trained batch 801 batch loss 1.31459975 epoch total loss 1.26938403\n",
      "Trained batch 802 batch loss 1.29560649 epoch total loss 1.26941669\n",
      "Trained batch 803 batch loss 1.36559355 epoch total loss 1.2695365\n",
      "Trained batch 804 batch loss 1.26190448 epoch total loss 1.26952696\n",
      "Trained batch 805 batch loss 1.24107528 epoch total loss 1.26949167\n",
      "Trained batch 806 batch loss 1.28330445 epoch total loss 1.26950884\n",
      "Trained batch 807 batch loss 1.2390765 epoch total loss 1.26947117\n",
      "Trained batch 808 batch loss 1.2523998 epoch total loss 1.26945007\n",
      "Trained batch 809 batch loss 1.21381688 epoch total loss 1.2693814\n",
      "Trained batch 810 batch loss 1.26214802 epoch total loss 1.26937258\n",
      "Trained batch 811 batch loss 1.18424594 epoch total loss 1.26926756\n",
      "Trained batch 812 batch loss 1.22944546 epoch total loss 1.26921856\n",
      "Trained batch 813 batch loss 1.20720935 epoch total loss 1.26914227\n",
      "Trained batch 814 batch loss 1.33429122 epoch total loss 1.26922238\n",
      "Trained batch 815 batch loss 1.32899141 epoch total loss 1.26929569\n",
      "Trained batch 816 batch loss 1.32531643 epoch total loss 1.26936436\n",
      "Trained batch 817 batch loss 1.28298068 epoch total loss 1.26938093\n",
      "Trained batch 818 batch loss 1.24119306 epoch total loss 1.26934648\n",
      "Trained batch 819 batch loss 1.31192815 epoch total loss 1.26939845\n",
      "Trained batch 820 batch loss 1.18083918 epoch total loss 1.26929033\n",
      "Trained batch 821 batch loss 1.23424149 epoch total loss 1.26924765\n",
      "Trained batch 822 batch loss 1.25765431 epoch total loss 1.2692337\n",
      "Trained batch 823 batch loss 1.17110574 epoch total loss 1.26911449\n",
      "Trained batch 824 batch loss 1.25689125 epoch total loss 1.26909959\n",
      "Trained batch 825 batch loss 1.32017207 epoch total loss 1.26916146\n",
      "Trained batch 826 batch loss 1.2624433 epoch total loss 1.26915336\n",
      "Trained batch 827 batch loss 1.11369801 epoch total loss 1.26896536\n",
      "Trained batch 828 batch loss 1.07150888 epoch total loss 1.26872683\n",
      "Trained batch 829 batch loss 1.0807693 epoch total loss 1.26850021\n",
      "Trained batch 830 batch loss 1.1985116 epoch total loss 1.26841581\n",
      "Trained batch 831 batch loss 1.1536622 epoch total loss 1.26827776\n",
      "Trained batch 832 batch loss 1.26962376 epoch total loss 1.26827943\n",
      "Trained batch 833 batch loss 1.23703241 epoch total loss 1.268242\n",
      "Trained batch 834 batch loss 1.41320515 epoch total loss 1.26841581\n",
      "Trained batch 835 batch loss 1.25792575 epoch total loss 1.26840329\n",
      "Trained batch 836 batch loss 1.23497057 epoch total loss 1.26836324\n",
      "Trained batch 837 batch loss 1.3912766 epoch total loss 1.2685101\n",
      "Trained batch 838 batch loss 1.17307711 epoch total loss 1.26839626\n",
      "Trained batch 839 batch loss 1.23163533 epoch total loss 1.26835251\n",
      "Trained batch 840 batch loss 1.19861078 epoch total loss 1.26826942\n",
      "Trained batch 841 batch loss 1.33397782 epoch total loss 1.2683475\n",
      "Trained batch 842 batch loss 1.43804312 epoch total loss 1.26854908\n",
      "Trained batch 843 batch loss 1.33702862 epoch total loss 1.26863027\n",
      "Trained batch 844 batch loss 1.22167993 epoch total loss 1.2685746\n",
      "Trained batch 845 batch loss 1.20051134 epoch total loss 1.26849413\n",
      "Trained batch 846 batch loss 1.10931349 epoch total loss 1.2683059\n",
      "Trained batch 847 batch loss 1.00567603 epoch total loss 1.26799583\n",
      "Trained batch 848 batch loss 0.984423518 epoch total loss 1.26766133\n",
      "Trained batch 849 batch loss 1.20707202 epoch total loss 1.26758993\n",
      "Trained batch 850 batch loss 1.35703683 epoch total loss 1.26769519\n",
      "Trained batch 851 batch loss 1.58807874 epoch total loss 1.26807177\n",
      "Trained batch 852 batch loss 1.40840721 epoch total loss 1.26823652\n",
      "Trained batch 853 batch loss 1.15031135 epoch total loss 1.26809824\n",
      "Trained batch 854 batch loss 1.17955232 epoch total loss 1.26799452\n",
      "Trained batch 855 batch loss 1.34476805 epoch total loss 1.26808429\n",
      "Trained batch 856 batch loss 1.34876192 epoch total loss 1.26817846\n",
      "Trained batch 857 batch loss 1.30619287 epoch total loss 1.26822281\n",
      "Trained batch 858 batch loss 1.22335064 epoch total loss 1.2681706\n",
      "Trained batch 859 batch loss 1.33450532 epoch total loss 1.26824772\n",
      "Trained batch 860 batch loss 1.29514623 epoch total loss 1.26827908\n",
      "Trained batch 861 batch loss 1.26581907 epoch total loss 1.26827621\n",
      "Trained batch 862 batch loss 1.18570435 epoch total loss 1.26818037\n",
      "Trained batch 863 batch loss 1.16719747 epoch total loss 1.26806343\n",
      "Trained batch 864 batch loss 1.2856828 epoch total loss 1.26808381\n",
      "Trained batch 865 batch loss 1.25940681 epoch total loss 1.2680738\n",
      "Trained batch 866 batch loss 1.16607571 epoch total loss 1.2679559\n",
      "Trained batch 867 batch loss 1.16530859 epoch total loss 1.26783752\n",
      "Trained batch 868 batch loss 1.19964266 epoch total loss 1.26775885\n",
      "Trained batch 869 batch loss 1.24135423 epoch total loss 1.26772845\n",
      "Trained batch 870 batch loss 1.18250489 epoch total loss 1.26763046\n",
      "Trained batch 871 batch loss 1.28212857 epoch total loss 1.26764703\n",
      "Trained batch 872 batch loss 1.34678805 epoch total loss 1.26773787\n",
      "Trained batch 873 batch loss 1.32753706 epoch total loss 1.26780629\n",
      "Trained batch 874 batch loss 1.2023716 epoch total loss 1.26773143\n",
      "Trained batch 875 batch loss 1.26708615 epoch total loss 1.26773071\n",
      "Trained batch 876 batch loss 1.09790015 epoch total loss 1.26753688\n",
      "Trained batch 877 batch loss 1.20315087 epoch total loss 1.26746345\n",
      "Trained batch 878 batch loss 1.25424492 epoch total loss 1.26744843\n",
      "Trained batch 879 batch loss 1.17315388 epoch total loss 1.26734102\n",
      "Trained batch 880 batch loss 1.33683395 epoch total loss 1.26741993\n",
      "Trained batch 881 batch loss 1.43919218 epoch total loss 1.26761496\n",
      "Trained batch 882 batch loss 1.31942081 epoch total loss 1.26767373\n",
      "Trained batch 883 batch loss 1.50765908 epoch total loss 1.26794553\n",
      "Trained batch 884 batch loss 1.36203027 epoch total loss 1.2680521\n",
      "Trained batch 885 batch loss 1.24434114 epoch total loss 1.26802528\n",
      "Trained batch 886 batch loss 1.1359 epoch total loss 1.26787615\n",
      "Trained batch 887 batch loss 0.99374038 epoch total loss 1.26756716\n",
      "Trained batch 888 batch loss 1.02700388 epoch total loss 1.2672962\n",
      "Trained batch 889 batch loss 1.12240076 epoch total loss 1.26713324\n",
      "Trained batch 890 batch loss 1.2128675 epoch total loss 1.26707232\n",
      "Trained batch 891 batch loss 1.26854384 epoch total loss 1.26707399\n",
      "Trained batch 892 batch loss 1.23851669 epoch total loss 1.26704192\n",
      "Trained batch 893 batch loss 1.281268 epoch total loss 1.2670579\n",
      "Trained batch 894 batch loss 1.33933532 epoch total loss 1.26713872\n",
      "Trained batch 895 batch loss 1.32415938 epoch total loss 1.2672025\n",
      "Trained batch 896 batch loss 1.31717455 epoch total loss 1.26725829\n",
      "Trained batch 897 batch loss 1.19924569 epoch total loss 1.26718235\n",
      "Trained batch 898 batch loss 1.41794848 epoch total loss 1.26735032\n",
      "Trained batch 899 batch loss 1.36083412 epoch total loss 1.26745427\n",
      "Trained batch 900 batch loss 1.26882684 epoch total loss 1.26745582\n",
      "Trained batch 901 batch loss 1.18591714 epoch total loss 1.26736534\n",
      "Trained batch 902 batch loss 1.16338098 epoch total loss 1.26725\n",
      "Trained batch 903 batch loss 1.30041647 epoch total loss 1.26728666\n",
      "Trained batch 904 batch loss 1.24297225 epoch total loss 1.26725972\n",
      "Trained batch 905 batch loss 1.31225622 epoch total loss 1.26730943\n",
      "Trained batch 906 batch loss 1.23892677 epoch total loss 1.26727808\n",
      "Trained batch 907 batch loss 1.16665053 epoch total loss 1.26716709\n",
      "Trained batch 908 batch loss 1.18413043 epoch total loss 1.26707554\n",
      "Trained batch 909 batch loss 1.18582106 epoch total loss 1.26698613\n",
      "Trained batch 910 batch loss 1.14574575 epoch total loss 1.26685297\n",
      "Trained batch 911 batch loss 1.25344837 epoch total loss 1.26683819\n",
      "Trained batch 912 batch loss 1.28971982 epoch total loss 1.26686323\n",
      "Trained batch 913 batch loss 1.07525134 epoch total loss 1.2666533\n",
      "Trained batch 914 batch loss 1.27636695 epoch total loss 1.26666391\n",
      "Trained batch 915 batch loss 1.24032474 epoch total loss 1.26663518\n",
      "Trained batch 916 batch loss 1.25803208 epoch total loss 1.26662576\n",
      "Trained batch 917 batch loss 1.17980361 epoch total loss 1.26653111\n",
      "Trained batch 918 batch loss 1.21527815 epoch total loss 1.26647532\n",
      "Trained batch 919 batch loss 1.21184635 epoch total loss 1.26641583\n",
      "Trained batch 920 batch loss 1.18186593 epoch total loss 1.26632404\n",
      "Trained batch 921 batch loss 1.34388971 epoch total loss 1.26640821\n",
      "Trained batch 922 batch loss 1.2528522 epoch total loss 1.26639342\n",
      "Trained batch 923 batch loss 1.110291 epoch total loss 1.26622438\n",
      "Trained batch 924 batch loss 1.20021152 epoch total loss 1.26615286\n",
      "Trained batch 925 batch loss 1.2942493 epoch total loss 1.26618326\n",
      "Trained batch 926 batch loss 1.25343394 epoch total loss 1.26616943\n",
      "Trained batch 927 batch loss 1.26518989 epoch total loss 1.26616836\n",
      "Trained batch 928 batch loss 1.32991624 epoch total loss 1.26623702\n",
      "Trained batch 929 batch loss 1.18266785 epoch total loss 1.26614702\n",
      "Trained batch 930 batch loss 1.17278588 epoch total loss 1.26604664\n",
      "Trained batch 931 batch loss 1.2281096 epoch total loss 1.26600587\n",
      "Trained batch 932 batch loss 1.13047135 epoch total loss 1.26586044\n",
      "Trained batch 933 batch loss 1.14278328 epoch total loss 1.26572859\n",
      "Trained batch 934 batch loss 1.26213813 epoch total loss 1.26572478\n",
      "Trained batch 935 batch loss 1.14911127 epoch total loss 1.26560009\n",
      "Trained batch 936 batch loss 1.06761146 epoch total loss 1.26538861\n",
      "Trained batch 937 batch loss 1.09072471 epoch total loss 1.26520216\n",
      "Trained batch 938 batch loss 1.09819555 epoch total loss 1.26502407\n",
      "Trained batch 939 batch loss 1.19142914 epoch total loss 1.26494563\n",
      "Trained batch 940 batch loss 1.35771382 epoch total loss 1.26504421\n",
      "Trained batch 941 batch loss 1.47395921 epoch total loss 1.2652663\n",
      "Trained batch 942 batch loss 1.28636861 epoch total loss 1.26528871\n",
      "Trained batch 943 batch loss 1.11367953 epoch total loss 1.2651279\n",
      "Trained batch 944 batch loss 1.24737847 epoch total loss 1.26510918\n",
      "Trained batch 945 batch loss 1.24205256 epoch total loss 1.26508474\n",
      "Trained batch 946 batch loss 1.27318025 epoch total loss 1.26509333\n",
      "Trained batch 947 batch loss 1.26928687 epoch total loss 1.26509774\n",
      "Trained batch 948 batch loss 1.3357724 epoch total loss 1.26517236\n",
      "Trained batch 949 batch loss 1.38583863 epoch total loss 1.26529956\n",
      "Trained batch 950 batch loss 1.36341918 epoch total loss 1.26540279\n",
      "Trained batch 951 batch loss 1.25557935 epoch total loss 1.26539254\n",
      "Trained batch 952 batch loss 1.22704315 epoch total loss 1.26535225\n",
      "Trained batch 953 batch loss 1.13325322 epoch total loss 1.26521373\n",
      "Trained batch 954 batch loss 1.21223557 epoch total loss 1.26515818\n",
      "Trained batch 955 batch loss 1.17259371 epoch total loss 1.26506126\n",
      "Trained batch 956 batch loss 1.26984727 epoch total loss 1.26506639\n",
      "Trained batch 957 batch loss 1.31451905 epoch total loss 1.26511812\n",
      "Trained batch 958 batch loss 1.26168561 epoch total loss 1.26511455\n",
      "Trained batch 959 batch loss 1.37737477 epoch total loss 1.26523149\n",
      "Trained batch 960 batch loss 1.18969345 epoch total loss 1.26515281\n",
      "Trained batch 961 batch loss 1.12707043 epoch total loss 1.26500916\n",
      "Trained batch 962 batch loss 1.12783563 epoch total loss 1.26486659\n",
      "Trained batch 963 batch loss 1.22881842 epoch total loss 1.26482904\n",
      "Trained batch 964 batch loss 1.20701122 epoch total loss 1.26476908\n",
      "Trained batch 965 batch loss 1.29329419 epoch total loss 1.26479876\n",
      "Trained batch 966 batch loss 1.22351432 epoch total loss 1.26475596\n",
      "Trained batch 967 batch loss 1.15057492 epoch total loss 1.26463795\n",
      "Trained batch 968 batch loss 1.11704445 epoch total loss 1.26448548\n",
      "Trained batch 969 batch loss 1.22870374 epoch total loss 1.26444864\n",
      "Trained batch 970 batch loss 1.37842822 epoch total loss 1.26456618\n",
      "Trained batch 971 batch loss 1.24466646 epoch total loss 1.26454556\n",
      "Trained batch 972 batch loss 1.26589131 epoch total loss 1.26454699\n",
      "Trained batch 973 batch loss 1.07922792 epoch total loss 1.26435649\n",
      "Trained batch 974 batch loss 0.94943 epoch total loss 1.2640332\n",
      "Trained batch 975 batch loss 0.988027215 epoch total loss 1.26375008\n",
      "Trained batch 976 batch loss 1.01009178 epoch total loss 1.26349032\n",
      "Trained batch 977 batch loss 1.1246866 epoch total loss 1.2633481\n",
      "Trained batch 978 batch loss 1.08765554 epoch total loss 1.26316845\n",
      "Trained batch 979 batch loss 1.16118169 epoch total loss 1.26306427\n",
      "Trained batch 980 batch loss 1.18693233 epoch total loss 1.26298654\n",
      "Trained batch 981 batch loss 1.28063548 epoch total loss 1.26300454\n",
      "Trained batch 982 batch loss 1.31979835 epoch total loss 1.26306236\n",
      "Trained batch 983 batch loss 1.26195765 epoch total loss 1.26306129\n",
      "Trained batch 984 batch loss 1.13769877 epoch total loss 1.26293385\n",
      "Trained batch 985 batch loss 1.1361742 epoch total loss 1.26280522\n",
      "Trained batch 986 batch loss 1.14838946 epoch total loss 1.26268923\n",
      "Trained batch 987 batch loss 1.21109653 epoch total loss 1.2626369\n",
      "Trained batch 988 batch loss 1.18786407 epoch total loss 1.2625612\n",
      "Trained batch 989 batch loss 1.39998901 epoch total loss 1.2627002\n",
      "Trained batch 990 batch loss 1.35356688 epoch total loss 1.26279199\n",
      "Trained batch 991 batch loss 1.24305689 epoch total loss 1.26277208\n",
      "Trained batch 992 batch loss 1.15570056 epoch total loss 1.26266408\n",
      "Trained batch 993 batch loss 1.25583291 epoch total loss 1.26265717\n",
      "Trained batch 994 batch loss 1.20422876 epoch total loss 1.2625984\n",
      "Trained batch 995 batch loss 1.26934278 epoch total loss 1.26260519\n",
      "Trained batch 996 batch loss 1.19328785 epoch total loss 1.26253545\n",
      "Trained batch 997 batch loss 1.27783322 epoch total loss 1.26255083\n",
      "Trained batch 998 batch loss 1.28028166 epoch total loss 1.26256859\n",
      "Trained batch 999 batch loss 1.27045226 epoch total loss 1.26257658\n",
      "Trained batch 1000 batch loss 1.37819839 epoch total loss 1.26269209\n",
      "Trained batch 1001 batch loss 1.26510942 epoch total loss 1.2626946\n",
      "Trained batch 1002 batch loss 1.21919274 epoch total loss 1.26265121\n",
      "Trained batch 1003 batch loss 1.18458259 epoch total loss 1.26257336\n",
      "Trained batch 1004 batch loss 1.21812403 epoch total loss 1.26252913\n",
      "Trained batch 1005 batch loss 1.16752934 epoch total loss 1.26243448\n",
      "Trained batch 1006 batch loss 1.32317436 epoch total loss 1.2624948\n",
      "Trained batch 1007 batch loss 1.17699182 epoch total loss 1.26240993\n",
      "Trained batch 1008 batch loss 1.23252904 epoch total loss 1.26238036\n",
      "Trained batch 1009 batch loss 1.2075702 epoch total loss 1.262326\n",
      "Trained batch 1010 batch loss 1.21168113 epoch total loss 1.26227582\n",
      "Trained batch 1011 batch loss 1.20274591 epoch total loss 1.26221693\n",
      "Trained batch 1012 batch loss 1.17747343 epoch total loss 1.26213324\n",
      "Trained batch 1013 batch loss 1.21059048 epoch total loss 1.26208234\n",
      "Trained batch 1014 batch loss 1.21144617 epoch total loss 1.26203239\n",
      "Trained batch 1015 batch loss 1.15636671 epoch total loss 1.2619282\n",
      "Trained batch 1016 batch loss 1.30317473 epoch total loss 1.26196885\n",
      "Trained batch 1017 batch loss 1.17992032 epoch total loss 1.26188827\n",
      "Trained batch 1018 batch loss 1.2113198 epoch total loss 1.26183856\n",
      "Trained batch 1019 batch loss 1.39304531 epoch total loss 1.2619673\n",
      "Trained batch 1020 batch loss 1.50788915 epoch total loss 1.26220846\n",
      "Trained batch 1021 batch loss 1.48628485 epoch total loss 1.26242793\n",
      "Trained batch 1022 batch loss 1.42107129 epoch total loss 1.26258314\n",
      "Trained batch 1023 batch loss 1.16861796 epoch total loss 1.26249123\n",
      "Trained batch 1024 batch loss 1.13567924 epoch total loss 1.26236737\n",
      "Trained batch 1025 batch loss 1.29324007 epoch total loss 1.26239741\n",
      "Trained batch 1026 batch loss 1.22651875 epoch total loss 1.26236248\n",
      "Trained batch 1027 batch loss 1.24865556 epoch total loss 1.26234925\n",
      "Trained batch 1028 batch loss 1.34439862 epoch total loss 1.262429\n",
      "Trained batch 1029 batch loss 1.27520585 epoch total loss 1.26244128\n",
      "Trained batch 1030 batch loss 1.37611938 epoch total loss 1.26255167\n",
      "Trained batch 1031 batch loss 1.44316435 epoch total loss 1.26272678\n",
      "Trained batch 1032 batch loss 1.30724812 epoch total loss 1.26276994\n",
      "Trained batch 1033 batch loss 1.28157187 epoch total loss 1.26278818\n",
      "Trained batch 1034 batch loss 1.15367782 epoch total loss 1.26268268\n",
      "Trained batch 1035 batch loss 1.27237833 epoch total loss 1.26269197\n",
      "Trained batch 1036 batch loss 1.33773875 epoch total loss 1.26276445\n",
      "Trained batch 1037 batch loss 1.16514277 epoch total loss 1.2626704\n",
      "Trained batch 1038 batch loss 1.18508124 epoch total loss 1.26259553\n",
      "Trained batch 1039 batch loss 1.21354926 epoch total loss 1.26254833\n",
      "Trained batch 1040 batch loss 1.15381503 epoch total loss 1.26244378\n",
      "Trained batch 1041 batch loss 1.22840405 epoch total loss 1.26241112\n",
      "Trained batch 1042 batch loss 1.21109569 epoch total loss 1.26236176\n",
      "Trained batch 1043 batch loss 1.15118241 epoch total loss 1.26225519\n",
      "Trained batch 1044 batch loss 1.35072911 epoch total loss 1.26233983\n",
      "Trained batch 1045 batch loss 1.31343675 epoch total loss 1.26238883\n",
      "Trained batch 1046 batch loss 1.32122982 epoch total loss 1.26244509\n",
      "Trained batch 1047 batch loss 1.27204812 epoch total loss 1.26245427\n",
      "Trained batch 1048 batch loss 1.25219846 epoch total loss 1.2624445\n",
      "Trained batch 1049 batch loss 1.27209568 epoch total loss 1.26245368\n",
      "Trained batch 1050 batch loss 1.29561961 epoch total loss 1.26248538\n",
      "Trained batch 1051 batch loss 1.38908255 epoch total loss 1.26260579\n",
      "Trained batch 1052 batch loss 1.35507095 epoch total loss 1.26269364\n",
      "Trained batch 1053 batch loss 1.26234281 epoch total loss 1.26269329\n",
      "Trained batch 1054 batch loss 1.23737073 epoch total loss 1.26266932\n",
      "Trained batch 1055 batch loss 1.2577858 epoch total loss 1.26266479\n",
      "Trained batch 1056 batch loss 1.22171211 epoch total loss 1.26262593\n",
      "Trained batch 1057 batch loss 1.28462029 epoch total loss 1.26264679\n",
      "Trained batch 1058 batch loss 1.27239799 epoch total loss 1.26265597\n",
      "Trained batch 1059 batch loss 1.41747892 epoch total loss 1.26280212\n",
      "Trained batch 1060 batch loss 1.1925633 epoch total loss 1.26273584\n",
      "Trained batch 1061 batch loss 1.28217411 epoch total loss 1.2627542\n",
      "Trained batch 1062 batch loss 1.13546693 epoch total loss 1.2626344\n",
      "Trained batch 1063 batch loss 1.33154786 epoch total loss 1.26269925\n",
      "Trained batch 1064 batch loss 1.33691919 epoch total loss 1.26276898\n",
      "Trained batch 1065 batch loss 1.14175069 epoch total loss 1.26265526\n",
      "Trained batch 1066 batch loss 1.03720951 epoch total loss 1.2624439\n",
      "Trained batch 1067 batch loss 1.06429279 epoch total loss 1.26225817\n",
      "Trained batch 1068 batch loss 1.35684252 epoch total loss 1.26234674\n",
      "Trained batch 1069 batch loss 1.35712683 epoch total loss 1.26243544\n",
      "Trained batch 1070 batch loss 1.29504681 epoch total loss 1.26246583\n",
      "Trained batch 1071 batch loss 1.31340671 epoch total loss 1.2625134\n",
      "Trained batch 1072 batch loss 1.24212301 epoch total loss 1.26249433\n",
      "Trained batch 1073 batch loss 1.23306584 epoch total loss 1.26246691\n",
      "Trained batch 1074 batch loss 1.40790033 epoch total loss 1.26260233\n",
      "Trained batch 1075 batch loss 1.19939613 epoch total loss 1.26254344\n",
      "Trained batch 1076 batch loss 1.19868684 epoch total loss 1.26248419\n",
      "Trained batch 1077 batch loss 1.15702486 epoch total loss 1.2623862\n",
      "Trained batch 1078 batch loss 1.24156117 epoch total loss 1.26236689\n",
      "Trained batch 1079 batch loss 1.26023793 epoch total loss 1.26236498\n",
      "Trained batch 1080 batch loss 1.26836061 epoch total loss 1.26237047\n",
      "Trained batch 1081 batch loss 1.29402137 epoch total loss 1.26239979\n",
      "Trained batch 1082 batch loss 1.27115989 epoch total loss 1.2624079\n",
      "Trained batch 1083 batch loss 1.30966341 epoch total loss 1.26245153\n",
      "Trained batch 1084 batch loss 1.27967501 epoch total loss 1.26246738\n",
      "Trained batch 1085 batch loss 1.36320281 epoch total loss 1.26256025\n",
      "Trained batch 1086 batch loss 1.26852643 epoch total loss 1.26256573\n",
      "Trained batch 1087 batch loss 1.30454731 epoch total loss 1.26260436\n",
      "Trained batch 1088 batch loss 1.22324657 epoch total loss 1.26256824\n",
      "Trained batch 1089 batch loss 1.22047961 epoch total loss 1.26252949\n",
      "Trained batch 1090 batch loss 1.2545687 epoch total loss 1.26252222\n",
      "Trained batch 1091 batch loss 1.2829504 epoch total loss 1.26254094\n",
      "Trained batch 1092 batch loss 1.31366742 epoch total loss 1.26258779\n",
      "Trained batch 1093 batch loss 1.33377016 epoch total loss 1.26265287\n",
      "Trained batch 1094 batch loss 1.30661583 epoch total loss 1.26269305\n",
      "Trained batch 1095 batch loss 1.39701033 epoch total loss 1.26281571\n",
      "Trained batch 1096 batch loss 1.25989735 epoch total loss 1.26281309\n",
      "Trained batch 1097 batch loss 1.33642733 epoch total loss 1.26288009\n",
      "Trained batch 1098 batch loss 1.19308138 epoch total loss 1.26281655\n",
      "Trained batch 1099 batch loss 1.18661773 epoch total loss 1.26274729\n",
      "Trained batch 1100 batch loss 1.22531199 epoch total loss 1.26271331\n",
      "Trained batch 1101 batch loss 1.27657986 epoch total loss 1.26272595\n",
      "Trained batch 1102 batch loss 1.23233247 epoch total loss 1.26269829\n",
      "Trained batch 1103 batch loss 1.30110931 epoch total loss 1.26273322\n",
      "Trained batch 1104 batch loss 1.28428674 epoch total loss 1.26275265\n",
      "Trained batch 1105 batch loss 1.26880527 epoch total loss 1.26275814\n",
      "Trained batch 1106 batch loss 1.40807486 epoch total loss 1.26288962\n",
      "Trained batch 1107 batch loss 1.28742278 epoch total loss 1.2629118\n",
      "Trained batch 1108 batch loss 1.25424492 epoch total loss 1.26290393\n",
      "Trained batch 1109 batch loss 1.31155097 epoch total loss 1.2629478\n",
      "Trained batch 1110 batch loss 1.32337976 epoch total loss 1.26300228\n",
      "Trained batch 1111 batch loss 1.26885033 epoch total loss 1.26300752\n",
      "Trained batch 1112 batch loss 1.29818618 epoch total loss 1.26303911\n",
      "Trained batch 1113 batch loss 1.21323955 epoch total loss 1.26299441\n",
      "Trained batch 1114 batch loss 1.40933406 epoch total loss 1.26312578\n",
      "Trained batch 1115 batch loss 1.22775006 epoch total loss 1.26309407\n",
      "Trained batch 1116 batch loss 1.32533038 epoch total loss 1.26314974\n",
      "Trained batch 1117 batch loss 1.31882167 epoch total loss 1.26319969\n",
      "Trained batch 1118 batch loss 1.27195525 epoch total loss 1.26320755\n",
      "Trained batch 1119 batch loss 1.2899524 epoch total loss 1.2632314\n",
      "Trained batch 1120 batch loss 1.20219111 epoch total loss 1.2631768\n",
      "Trained batch 1121 batch loss 1.13633478 epoch total loss 1.26306367\n",
      "Trained batch 1122 batch loss 1.13202393 epoch total loss 1.26294696\n",
      "Trained batch 1123 batch loss 1.11233008 epoch total loss 1.26281285\n",
      "Trained batch 1124 batch loss 1.14895833 epoch total loss 1.26271152\n",
      "Trained batch 1125 batch loss 1.16013277 epoch total loss 1.26262033\n",
      "Trained batch 1126 batch loss 1.1035074 epoch total loss 1.26247907\n",
      "Trained batch 1127 batch loss 1.09718549 epoch total loss 1.26233232\n",
      "Trained batch 1128 batch loss 1.02942848 epoch total loss 1.26212585\n",
      "Trained batch 1129 batch loss 1.05996585 epoch total loss 1.2619468\n",
      "Trained batch 1130 batch loss 1.30211425 epoch total loss 1.26198232\n",
      "Trained batch 1131 batch loss 1.26796901 epoch total loss 1.26198757\n",
      "Trained batch 1132 batch loss 1.37240243 epoch total loss 1.2620852\n",
      "Trained batch 1133 batch loss 1.28728163 epoch total loss 1.26210737\n",
      "Trained batch 1134 batch loss 1.31019199 epoch total loss 1.26214981\n",
      "Trained batch 1135 batch loss 1.23190188 epoch total loss 1.26212311\n",
      "Trained batch 1136 batch loss 1.11635208 epoch total loss 1.26199484\n",
      "Trained batch 1137 batch loss 1.13297248 epoch total loss 1.26188135\n",
      "Trained batch 1138 batch loss 1.24288332 epoch total loss 1.26186466\n",
      "Trained batch 1139 batch loss 1.17394722 epoch total loss 1.26178741\n",
      "Trained batch 1140 batch loss 1.28964984 epoch total loss 1.26181185\n",
      "Trained batch 1141 batch loss 1.32966208 epoch total loss 1.26187146\n",
      "Trained batch 1142 batch loss 1.32796049 epoch total loss 1.26192927\n",
      "Trained batch 1143 batch loss 1.34205747 epoch total loss 1.26199937\n",
      "Trained batch 1144 batch loss 1.30483675 epoch total loss 1.2620368\n",
      "Trained batch 1145 batch loss 1.39548922 epoch total loss 1.26215339\n",
      "Trained batch 1146 batch loss 1.40738213 epoch total loss 1.26228011\n",
      "Trained batch 1147 batch loss 1.33763909 epoch total loss 1.26234579\n",
      "Trained batch 1148 batch loss 1.37617123 epoch total loss 1.26244497\n",
      "Trained batch 1149 batch loss 1.27539706 epoch total loss 1.2624563\n",
      "Trained batch 1150 batch loss 1.19448614 epoch total loss 1.26239717\n",
      "Trained batch 1151 batch loss 1.19963539 epoch total loss 1.26234257\n",
      "Trained batch 1152 batch loss 1.20780897 epoch total loss 1.26229525\n",
      "Trained batch 1153 batch loss 1.383775 epoch total loss 1.26240051\n",
      "Trained batch 1154 batch loss 1.27474749 epoch total loss 1.26241124\n",
      "Trained batch 1155 batch loss 1.25961375 epoch total loss 1.26240885\n",
      "Trained batch 1156 batch loss 1.23034716 epoch total loss 1.2623812\n",
      "Trained batch 1157 batch loss 1.24243057 epoch total loss 1.26236391\n",
      "Trained batch 1158 batch loss 1.22464013 epoch total loss 1.26233137\n",
      "Trained batch 1159 batch loss 1.24756122 epoch total loss 1.26231861\n",
      "Trained batch 1160 batch loss 1.25166464 epoch total loss 1.26230943\n",
      "Trained batch 1161 batch loss 1.22512114 epoch total loss 1.26227736\n",
      "Trained batch 1162 batch loss 1.14536572 epoch total loss 1.26217675\n",
      "Trained batch 1163 batch loss 1.22961533 epoch total loss 1.26214874\n",
      "Trained batch 1164 batch loss 1.22289717 epoch total loss 1.262115\n",
      "Trained batch 1165 batch loss 1.33391643 epoch total loss 1.26217663\n",
      "Trained batch 1166 batch loss 1.2486726 epoch total loss 1.26216507\n",
      "Trained batch 1167 batch loss 1.06703115 epoch total loss 1.26199782\n",
      "Trained batch 1168 batch loss 1.06223917 epoch total loss 1.26182687\n",
      "Trained batch 1169 batch loss 1.00796211 epoch total loss 1.26160967\n",
      "Trained batch 1170 batch loss 1.03413069 epoch total loss 1.26141524\n",
      "Trained batch 1171 batch loss 1.14392054 epoch total loss 1.26131487\n",
      "Trained batch 1172 batch loss 1.23004913 epoch total loss 1.26128829\n",
      "Trained batch 1173 batch loss 1.33367479 epoch total loss 1.26134992\n",
      "Trained batch 1174 batch loss 1.20522 epoch total loss 1.26130211\n",
      "Trained batch 1175 batch loss 1.2385819 epoch total loss 1.26128268\n",
      "Trained batch 1176 batch loss 1.0821439 epoch total loss 1.26113045\n",
      "Trained batch 1177 batch loss 1.30105257 epoch total loss 1.26116431\n",
      "Trained batch 1178 batch loss 1.20998955 epoch total loss 1.2611208\n",
      "Trained batch 1179 batch loss 1.30997133 epoch total loss 1.26116228\n",
      "Trained batch 1180 batch loss 1.27883053 epoch total loss 1.26117718\n",
      "Trained batch 1181 batch loss 1.35298681 epoch total loss 1.26125491\n",
      "Trained batch 1182 batch loss 1.25396931 epoch total loss 1.26124883\n",
      "Trained batch 1183 batch loss 1.35742497 epoch total loss 1.26133013\n",
      "Trained batch 1184 batch loss 1.27810824 epoch total loss 1.26134431\n",
      "Trained batch 1185 batch loss 1.42381799 epoch total loss 1.2614814\n",
      "Trained batch 1186 batch loss 1.2290386 epoch total loss 1.26145399\n",
      "Trained batch 1187 batch loss 1.26404095 epoch total loss 1.26145625\n",
      "Trained batch 1188 batch loss 1.23325443 epoch total loss 1.26143253\n",
      "Trained batch 1189 batch loss 1.1816324 epoch total loss 1.26136541\n",
      "Trained batch 1190 batch loss 1.08901048 epoch total loss 1.26122057\n",
      "Trained batch 1191 batch loss 1.19510067 epoch total loss 1.26116502\n",
      "Trained batch 1192 batch loss 1.27330101 epoch total loss 1.26117516\n",
      "Trained batch 1193 batch loss 1.19098186 epoch total loss 1.26111639\n",
      "Trained batch 1194 batch loss 1.25530708 epoch total loss 1.2611115\n",
      "Trained batch 1195 batch loss 1.39302111 epoch total loss 1.26122189\n",
      "Trained batch 1196 batch loss 1.36965561 epoch total loss 1.26131248\n",
      "Trained batch 1197 batch loss 1.21986115 epoch total loss 1.26127791\n",
      "Trained batch 1198 batch loss 1.08804274 epoch total loss 1.26113331\n",
      "Trained batch 1199 batch loss 1.20783806 epoch total loss 1.26108885\n",
      "Trained batch 1200 batch loss 1.19601309 epoch total loss 1.26103461\n",
      "Trained batch 1201 batch loss 1.39577413 epoch total loss 1.26114678\n",
      "Trained batch 1202 batch loss 1.23977458 epoch total loss 1.26112902\n",
      "Trained batch 1203 batch loss 1.28763139 epoch total loss 1.26115108\n",
      "Trained batch 1204 batch loss 1.28729677 epoch total loss 1.26117277\n",
      "Trained batch 1205 batch loss 1.19963264 epoch total loss 1.26112163\n",
      "Trained batch 1206 batch loss 1.18683589 epoch total loss 1.26106012\n",
      "Trained batch 1207 batch loss 1.2066952 epoch total loss 1.26101506\n",
      "Trained batch 1208 batch loss 1.33921695 epoch total loss 1.26107979\n",
      "Trained batch 1209 batch loss 1.28259444 epoch total loss 1.26109755\n",
      "Trained batch 1210 batch loss 1.38944459 epoch total loss 1.26120365\n",
      "Trained batch 1211 batch loss 1.10244644 epoch total loss 1.26107252\n",
      "Trained batch 1212 batch loss 1.19433606 epoch total loss 1.26101744\n",
      "Trained batch 1213 batch loss 1.19195712 epoch total loss 1.26096058\n",
      "Trained batch 1214 batch loss 1.25620294 epoch total loss 1.26095665\n",
      "Trained batch 1215 batch loss 1.35342157 epoch total loss 1.2610327\n",
      "Trained batch 1216 batch loss 1.2327894 epoch total loss 1.26100957\n",
      "Trained batch 1217 batch loss 1.25174689 epoch total loss 1.26100183\n",
      "Trained batch 1218 batch loss 1.16463137 epoch total loss 1.26092279\n",
      "Trained batch 1219 batch loss 1.16566432 epoch total loss 1.26084459\n",
      "Trained batch 1220 batch loss 1.07880342 epoch total loss 1.26069546\n",
      "Trained batch 1221 batch loss 1.18145466 epoch total loss 1.26063049\n",
      "Trained batch 1222 batch loss 1.11663675 epoch total loss 1.26051259\n",
      "Trained batch 1223 batch loss 1.13402879 epoch total loss 1.26040924\n",
      "Trained batch 1224 batch loss 1.19918418 epoch total loss 1.26035917\n",
      "Trained batch 1225 batch loss 1.15220618 epoch total loss 1.26027095\n",
      "Trained batch 1226 batch loss 1.11775398 epoch total loss 1.26015472\n",
      "Trained batch 1227 batch loss 1.15777767 epoch total loss 1.2600714\n",
      "Trained batch 1228 batch loss 1.3520534 epoch total loss 1.26014626\n",
      "Trained batch 1229 batch loss 1.29805779 epoch total loss 1.26017714\n",
      "Trained batch 1230 batch loss 1.33502221 epoch total loss 1.26023805\n",
      "Trained batch 1231 batch loss 1.33920193 epoch total loss 1.26030219\n",
      "Trained batch 1232 batch loss 1.30968428 epoch total loss 1.26034224\n",
      "Trained batch 1233 batch loss 1.20044029 epoch total loss 1.26029372\n",
      "Trained batch 1234 batch loss 1.21893585 epoch total loss 1.26026022\n",
      "Trained batch 1235 batch loss 1.24025238 epoch total loss 1.26024401\n",
      "Trained batch 1236 batch loss 1.16526818 epoch total loss 1.26016724\n",
      "Trained batch 1237 batch loss 1.19295335 epoch total loss 1.26011288\n",
      "Trained batch 1238 batch loss 1.20340383 epoch total loss 1.26006711\n",
      "Trained batch 1239 batch loss 1.11346924 epoch total loss 1.25994873\n",
      "Trained batch 1240 batch loss 1.27008784 epoch total loss 1.25995696\n",
      "Trained batch 1241 batch loss 1.24968636 epoch total loss 1.25994873\n",
      "Trained batch 1242 batch loss 1.33086598 epoch total loss 1.26000571\n",
      "Trained batch 1243 batch loss 1.25798094 epoch total loss 1.26000404\n",
      "Trained batch 1244 batch loss 1.21483612 epoch total loss 1.2599678\n",
      "Trained batch 1245 batch loss 1.2962687 epoch total loss 1.25999689\n",
      "Trained batch 1246 batch loss 1.37610006 epoch total loss 1.26009011\n",
      "Trained batch 1247 batch loss 1.24766815 epoch total loss 1.2600801\n",
      "Trained batch 1248 batch loss 1.18485463 epoch total loss 1.2600199\n",
      "Trained batch 1249 batch loss 1.1571033 epoch total loss 1.25993741\n",
      "Trained batch 1250 batch loss 1.25543821 epoch total loss 1.25993383\n",
      "Trained batch 1251 batch loss 1.20560217 epoch total loss 1.25989044\n",
      "Trained batch 1252 batch loss 1.14306939 epoch total loss 1.2597971\n",
      "Trained batch 1253 batch loss 1.22389412 epoch total loss 1.25976849\n",
      "Trained batch 1254 batch loss 1.21063852 epoch total loss 1.25972927\n",
      "Trained batch 1255 batch loss 1.17369246 epoch total loss 1.25966072\n",
      "Trained batch 1256 batch loss 1.11664605 epoch total loss 1.259547\n",
      "Trained batch 1257 batch loss 1.15756798 epoch total loss 1.25946581\n",
      "Trained batch 1258 batch loss 1.20876813 epoch total loss 1.25942552\n",
      "Trained batch 1259 batch loss 1.38053703 epoch total loss 1.25952172\n",
      "Trained batch 1260 batch loss 1.30283701 epoch total loss 1.25955606\n",
      "Trained batch 1261 batch loss 1.07863736 epoch total loss 1.25941253\n",
      "Trained batch 1262 batch loss 1.13677573 epoch total loss 1.25931537\n",
      "Trained batch 1263 batch loss 1.18306625 epoch total loss 1.25925505\n",
      "Trained batch 1264 batch loss 1.02024829 epoch total loss 1.25906599\n",
      "Trained batch 1265 batch loss 1.16956484 epoch total loss 1.25899518\n",
      "Trained batch 1266 batch loss 1.25080037 epoch total loss 1.25898874\n",
      "Trained batch 1267 batch loss 1.22618771 epoch total loss 1.25896287\n",
      "Trained batch 1268 batch loss 1.31613362 epoch total loss 1.25900793\n",
      "Trained batch 1269 batch loss 1.18135977 epoch total loss 1.25894678\n",
      "Trained batch 1270 batch loss 1.2768147 epoch total loss 1.25896096\n",
      "Trained batch 1271 batch loss 1.37182927 epoch total loss 1.25904977\n",
      "Trained batch 1272 batch loss 1.51896405 epoch total loss 1.25925398\n",
      "Trained batch 1273 batch loss 1.46843565 epoch total loss 1.25941825\n",
      "Trained batch 1274 batch loss 1.36806858 epoch total loss 1.2595036\n",
      "Trained batch 1275 batch loss 1.21510184 epoch total loss 1.25946867\n",
      "Trained batch 1276 batch loss 1.26715183 epoch total loss 1.25947475\n",
      "Trained batch 1277 batch loss 1.3973676 epoch total loss 1.25958276\n",
      "Trained batch 1278 batch loss 1.32494867 epoch total loss 1.2596339\n",
      "Trained batch 1279 batch loss 1.40917921 epoch total loss 1.25975084\n",
      "Trained batch 1280 batch loss 1.31540775 epoch total loss 1.25979435\n",
      "Trained batch 1281 batch loss 1.26429033 epoch total loss 1.25979781\n",
      "Trained batch 1282 batch loss 1.28112 epoch total loss 1.2598145\n",
      "Trained batch 1283 batch loss 1.13796294 epoch total loss 1.25971949\n",
      "Trained batch 1284 batch loss 1.1545347 epoch total loss 1.25963759\n",
      "Trained batch 1285 batch loss 1.21982872 epoch total loss 1.2596066\n",
      "Trained batch 1286 batch loss 1.22798026 epoch total loss 1.25958204\n",
      "Trained batch 1287 batch loss 1.27198493 epoch total loss 1.2595917\n",
      "Trained batch 1288 batch loss 1.25701094 epoch total loss 1.25958967\n",
      "Trained batch 1289 batch loss 1.23270643 epoch total loss 1.25956869\n",
      "Trained batch 1290 batch loss 1.17108679 epoch total loss 1.25950015\n",
      "Trained batch 1291 batch loss 1.27489126 epoch total loss 1.25951207\n",
      "Trained batch 1292 batch loss 1.30058885 epoch total loss 1.2595439\n",
      "Trained batch 1293 batch loss 1.33514404 epoch total loss 1.25960231\n",
      "Trained batch 1294 batch loss 1.23616827 epoch total loss 1.25958419\n",
      "Trained batch 1295 batch loss 1.27860045 epoch total loss 1.25959885\n",
      "Trained batch 1296 batch loss 1.17400575 epoch total loss 1.25953281\n",
      "Trained batch 1297 batch loss 1.17426682 epoch total loss 1.25946712\n",
      "Trained batch 1298 batch loss 1.18133855 epoch total loss 1.25940692\n",
      "Trained batch 1299 batch loss 1.21559775 epoch total loss 1.25937319\n",
      "Trained batch 1300 batch loss 1.32778168 epoch total loss 1.25942576\n",
      "Trained batch 1301 batch loss 1.19710958 epoch total loss 1.25937796\n",
      "Trained batch 1302 batch loss 1.15476644 epoch total loss 1.25929761\n",
      "Trained batch 1303 batch loss 1.1050725 epoch total loss 1.25917923\n",
      "Trained batch 1304 batch loss 1.02083969 epoch total loss 1.25899649\n",
      "Trained batch 1305 batch loss 1.07108223 epoch total loss 1.25885248\n",
      "Trained batch 1306 batch loss 1.18718243 epoch total loss 1.25879753\n",
      "Trained batch 1307 batch loss 1.19854808 epoch total loss 1.25875151\n",
      "Trained batch 1308 batch loss 1.17228186 epoch total loss 1.25868535\n",
      "Trained batch 1309 batch loss 1.30618429 epoch total loss 1.25872159\n",
      "Trained batch 1310 batch loss 1.31941032 epoch total loss 1.25876796\n",
      "Trained batch 1311 batch loss 1.31848383 epoch total loss 1.2588135\n",
      "Trained batch 1312 batch loss 1.23593211 epoch total loss 1.2587961\n",
      "Trained batch 1313 batch loss 1.1150893 epoch total loss 1.25868666\n",
      "Trained batch 1314 batch loss 1.21703458 epoch total loss 1.25865495\n",
      "Trained batch 1315 batch loss 1.28613901 epoch total loss 1.25867593\n",
      "Trained batch 1316 batch loss 1.03949285 epoch total loss 1.2585094\n",
      "Trained batch 1317 batch loss 0.976773143 epoch total loss 1.25829554\n",
      "Trained batch 1318 batch loss 1.00448251 epoch total loss 1.25810289\n",
      "Trained batch 1319 batch loss 1.25563812 epoch total loss 1.25810111\n",
      "Trained batch 1320 batch loss 1.27632058 epoch total loss 1.25811493\n",
      "Trained batch 1321 batch loss 1.48812675 epoch total loss 1.2582891\n",
      "Trained batch 1322 batch loss 1.37321472 epoch total loss 1.258376\n",
      "Trained batch 1323 batch loss 1.33311856 epoch total loss 1.25843239\n",
      "Trained batch 1324 batch loss 1.24966884 epoch total loss 1.25842583\n",
      "Trained batch 1325 batch loss 1.41326463 epoch total loss 1.25854266\n",
      "Trained batch 1326 batch loss 1.33558989 epoch total loss 1.25860071\n",
      "Trained batch 1327 batch loss 1.2267369 epoch total loss 1.25857663\n",
      "Trained batch 1328 batch loss 1.28381848 epoch total loss 1.25859571\n",
      "Trained batch 1329 batch loss 1.35223794 epoch total loss 1.25866616\n",
      "Trained batch 1330 batch loss 1.18156564 epoch total loss 1.2586081\n",
      "Trained batch 1331 batch loss 1.24220026 epoch total loss 1.25859582\n",
      "Trained batch 1332 batch loss 1.279737 epoch total loss 1.25861168\n",
      "Trained batch 1333 batch loss 1.34198391 epoch total loss 1.25867426\n",
      "Trained batch 1334 batch loss 1.38415158 epoch total loss 1.25876844\n",
      "Trained batch 1335 batch loss 1.27448905 epoch total loss 1.25878024\n",
      "Trained batch 1336 batch loss 1.27933264 epoch total loss 1.2587955\n",
      "Trained batch 1337 batch loss 1.20818555 epoch total loss 1.25875771\n",
      "Trained batch 1338 batch loss 1.26565313 epoch total loss 1.25876284\n",
      "Trained batch 1339 batch loss 1.29067051 epoch total loss 1.25878656\n",
      "Trained batch 1340 batch loss 1.35314417 epoch total loss 1.25885701\n",
      "Trained batch 1341 batch loss 1.44852805 epoch total loss 1.25899839\n",
      "Trained batch 1342 batch loss 1.29849815 epoch total loss 1.25902784\n",
      "Trained batch 1343 batch loss 1.13324153 epoch total loss 1.25893426\n",
      "Trained batch 1344 batch loss 1.13436508 epoch total loss 1.25884151\n",
      "Trained batch 1345 batch loss 1.14313686 epoch total loss 1.25875556\n",
      "Trained batch 1346 batch loss 1.20766473 epoch total loss 1.25871766\n",
      "Trained batch 1347 batch loss 1.2358731 epoch total loss 1.25870061\n",
      "Trained batch 1348 batch loss 1.22928476 epoch total loss 1.25867879\n",
      "Trained batch 1349 batch loss 1.26636589 epoch total loss 1.25868452\n",
      "Trained batch 1350 batch loss 1.27189732 epoch total loss 1.25869417\n",
      "Trained batch 1351 batch loss 1.15711832 epoch total loss 1.25861907\n",
      "Trained batch 1352 batch loss 1.2437557 epoch total loss 1.25860798\n",
      "Trained batch 1353 batch loss 1.30508339 epoch total loss 1.25864232\n",
      "Trained batch 1354 batch loss 1.22201324 epoch total loss 1.25861537\n",
      "Trained batch 1355 batch loss 1.37411952 epoch total loss 1.25870061\n",
      "Trained batch 1356 batch loss 1.19704545 epoch total loss 1.25865507\n",
      "Trained batch 1357 batch loss 1.15270627 epoch total loss 1.25857699\n",
      "Trained batch 1358 batch loss 1.15269911 epoch total loss 1.25849903\n",
      "Trained batch 1359 batch loss 1.29764569 epoch total loss 1.25852787\n",
      "Trained batch 1360 batch loss 1.311077 epoch total loss 1.2585665\n",
      "Trained batch 1361 batch loss 1.31899333 epoch total loss 1.25861084\n",
      "Trained batch 1362 batch loss 1.46772051 epoch total loss 1.25876439\n",
      "Trained batch 1363 batch loss 1.28990757 epoch total loss 1.25878727\n",
      "Trained batch 1364 batch loss 1.31330776 epoch total loss 1.25882733\n",
      "Trained batch 1365 batch loss 1.28030133 epoch total loss 1.25884295\n",
      "Trained batch 1366 batch loss 1.17176557 epoch total loss 1.25877929\n",
      "Trained batch 1367 batch loss 1.22809398 epoch total loss 1.25875688\n",
      "Trained batch 1368 batch loss 1.24473214 epoch total loss 1.25874662\n",
      "Trained batch 1369 batch loss 1.35767078 epoch total loss 1.25881886\n",
      "Trained batch 1370 batch loss 1.20216942 epoch total loss 1.2587775\n",
      "Trained batch 1371 batch loss 1.20805013 epoch total loss 1.25874043\n",
      "Trained batch 1372 batch loss 1.21876574 epoch total loss 1.25871134\n",
      "Trained batch 1373 batch loss 1.17588437 epoch total loss 1.25865102\n",
      "Trained batch 1374 batch loss 1.33059716 epoch total loss 1.25870335\n",
      "Trained batch 1375 batch loss 1.30791187 epoch total loss 1.25873911\n",
      "Trained batch 1376 batch loss 1.11149919 epoch total loss 1.25863206\n",
      "Trained batch 1377 batch loss 1.02080011 epoch total loss 1.25845933\n",
      "Trained batch 1378 batch loss 1.18625295 epoch total loss 1.25840688\n",
      "Trained batch 1379 batch loss 1.19917774 epoch total loss 1.25836396\n",
      "Trained batch 1380 batch loss 1.1221298 epoch total loss 1.25826526\n",
      "Trained batch 1381 batch loss 1.26831007 epoch total loss 1.25827253\n",
      "Trained batch 1382 batch loss 1.17571712 epoch total loss 1.25821269\n",
      "Trained batch 1383 batch loss 1.24262738 epoch total loss 1.25820148\n",
      "Trained batch 1384 batch loss 1.29525244 epoch total loss 1.2582283\n",
      "Trained batch 1385 batch loss 1.2938447 epoch total loss 1.25825393\n",
      "Trained batch 1386 batch loss 1.35811949 epoch total loss 1.25832605\n",
      "Trained batch 1387 batch loss 1.49266148 epoch total loss 1.25849497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 03:42:24.503040: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "2025-03-19 03:42:24.503092: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "\t [[RemoteCall]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1388 batch loss 1.539271 epoch total loss 1.25869739\n",
      "Epoch 3 train loss 1.2586973905563354\n",
      "Validated batch 1 batch loss 1.2317791\n",
      "Validated batch 2 batch loss 1.24083757\n",
      "Validated batch 3 batch loss 1.18792486\n",
      "Validated batch 4 batch loss 1.29458368\n",
      "Validated batch 5 batch loss 1.24708831\n",
      "Validated batch 6 batch loss 1.29729271\n",
      "Validated batch 7 batch loss 1.31594634\n",
      "Validated batch 8 batch loss 1.29771233\n",
      "Validated batch 9 batch loss 1.2862885\n",
      "Validated batch 10 batch loss 1.2149545\n",
      "Validated batch 11 batch loss 1.29638267\n",
      "Validated batch 12 batch loss 1.21957064\n",
      "Validated batch 13 batch loss 1.25941658\n",
      "Validated batch 14 batch loss 1.35404694\n",
      "Validated batch 15 batch loss 1.31517911\n",
      "Validated batch 16 batch loss 1.24412012\n",
      "Validated batch 17 batch loss 1.38652968\n",
      "Validated batch 18 batch loss 1.10850835\n",
      "Validated batch 19 batch loss 1.31125855\n",
      "Validated batch 20 batch loss 1.03006113\n",
      "Validated batch 21 batch loss 1.23597181\n",
      "Validated batch 22 batch loss 1.30570579\n",
      "Validated batch 23 batch loss 1.16778\n",
      "Validated batch 24 batch loss 1.25223374\n",
      "Validated batch 25 batch loss 1.16599798\n",
      "Validated batch 26 batch loss 1.22296846\n",
      "Validated batch 27 batch loss 1.13177311\n",
      "Validated batch 28 batch loss 1.2178967\n",
      "Validated batch 29 batch loss 1.26234043\n",
      "Validated batch 30 batch loss 1.2523489\n",
      "Validated batch 31 batch loss 1.36287653\n",
      "Validated batch 32 batch loss 1.29645085\n",
      "Validated batch 33 batch loss 1.21927619\n",
      "Validated batch 34 batch loss 1.24343312\n",
      "Validated batch 35 batch loss 1.3418231\n",
      "Validated batch 36 batch loss 1.34107304\n",
      "Validated batch 37 batch loss 1.32351947\n",
      "Validated batch 38 batch loss 1.31993794\n",
      "Validated batch 39 batch loss 1.31384134\n",
      "Validated batch 40 batch loss 1.32833314\n",
      "Validated batch 41 batch loss 1.182652\n",
      "Validated batch 42 batch loss 1.27710533\n",
      "Validated batch 43 batch loss 1.31064832\n",
      "Validated batch 44 batch loss 1.2579155\n",
      "Validated batch 45 batch loss 1.25133157\n",
      "Validated batch 46 batch loss 1.22096741\n",
      "Validated batch 47 batch loss 1.26965952\n",
      "Validated batch 48 batch loss 1.21750987\n",
      "Validated batch 49 batch loss 1.19639122\n",
      "Validated batch 50 batch loss 1.19631505\n",
      "Validated batch 51 batch loss 1.21544075\n",
      "Validated batch 52 batch loss 1.26375461\n",
      "Validated batch 53 batch loss 1.23004222\n",
      "Validated batch 54 batch loss 1.20688224\n",
      "Validated batch 55 batch loss 1.23276544\n",
      "Validated batch 56 batch loss 1.31115794\n",
      "Validated batch 57 batch loss 1.11509383\n",
      "Validated batch 58 batch loss 1.09333158\n",
      "Validated batch 59 batch loss 1.29927456\n",
      "Validated batch 60 batch loss 1.27654922\n",
      "Validated batch 61 batch loss 1.35015941\n",
      "Validated batch 62 batch loss 1.36781633\n",
      "Validated batch 63 batch loss 1.22930229\n",
      "Validated batch 64 batch loss 1.39081144\n",
      "Validated batch 65 batch loss 1.20505881\n",
      "Validated batch 66 batch loss 1.30487478\n",
      "Validated batch 67 batch loss 1.26733851\n",
      "Validated batch 68 batch loss 1.0193013\n",
      "Validated batch 69 batch loss 1.25332284\n",
      "Validated batch 70 batch loss 1.35275292\n",
      "Validated batch 71 batch loss 1.22504616\n",
      "Validated batch 72 batch loss 1.19137716\n",
      "Validated batch 73 batch loss 1.2088443\n",
      "Validated batch 74 batch loss 1.15523362\n",
      "Validated batch 75 batch loss 1.26183856\n",
      "Validated batch 76 batch loss 1.24742019\n",
      "Validated batch 77 batch loss 1.16702688\n",
      "Validated batch 78 batch loss 1.22643423\n",
      "Validated batch 79 batch loss 1.22546136\n",
      "Validated batch 80 batch loss 1.29237127\n",
      "Validated batch 81 batch loss 1.27127075\n",
      "Validated batch 82 batch loss 1.23012722\n",
      "Validated batch 83 batch loss 1.14734\n",
      "Validated batch 84 batch loss 1.16541445\n",
      "Validated batch 85 batch loss 1.28577757\n",
      "Validated batch 86 batch loss 1.20877922\n",
      "Validated batch 87 batch loss 1.28368652\n",
      "Validated batch 88 batch loss 1.27688217\n",
      "Validated batch 89 batch loss 1.49036098\n",
      "Validated batch 90 batch loss 1.30275249\n",
      "Validated batch 91 batch loss 1.26811278\n",
      "Validated batch 92 batch loss 1.163028\n",
      "Validated batch 93 batch loss 1.1679225\n",
      "Validated batch 94 batch loss 1.07960224\n",
      "Validated batch 95 batch loss 1.23610115\n",
      "Validated batch 96 batch loss 1.17580318\n",
      "Validated batch 97 batch loss 1.2029016\n",
      "Validated batch 98 batch loss 1.22462881\n",
      "Validated batch 99 batch loss 1.22076678\n",
      "Validated batch 100 batch loss 1.22716594\n",
      "Validated batch 101 batch loss 1.2640692\n",
      "Validated batch 102 batch loss 1.15292811\n",
      "Validated batch 103 batch loss 1.30012834\n",
      "Validated batch 104 batch loss 1.24358273\n",
      "Validated batch 105 batch loss 1.19764149\n",
      "Validated batch 106 batch loss 1.20639229\n",
      "Validated batch 107 batch loss 1.2876451\n",
      "Validated batch 108 batch loss 1.18359566\n",
      "Validated batch 109 batch loss 1.3810482\n",
      "Validated batch 110 batch loss 1.24715614\n",
      "Validated batch 111 batch loss 1.18116188\n",
      "Validated batch 112 batch loss 1.26680887\n",
      "Validated batch 113 batch loss 1.22746968\n",
      "Validated batch 114 batch loss 1.16299438\n",
      "Validated batch 115 batch loss 1.20056891\n",
      "Validated batch 116 batch loss 1.1841836\n",
      "Validated batch 117 batch loss 1.19041026\n",
      "Validated batch 118 batch loss 1.29805958\n",
      "Validated batch 119 batch loss 1.148386\n",
      "Validated batch 120 batch loss 1.25895369\n",
      "Validated batch 121 batch loss 1.38725674\n",
      "Validated batch 122 batch loss 1.12361121\n",
      "Validated batch 123 batch loss 1.18811917\n",
      "Validated batch 124 batch loss 1.23018754\n",
      "Validated batch 125 batch loss 1.2481544\n",
      "Validated batch 126 batch loss 1.27698898\n",
      "Validated batch 127 batch loss 1.18272507\n",
      "Validated batch 128 batch loss 1.03541815\n",
      "Validated batch 129 batch loss 1.23115838\n",
      "Validated batch 130 batch loss 1.16716647\n",
      "Validated batch 131 batch loss 1.18684602\n",
      "Validated batch 132 batch loss 1.27291596\n",
      "Validated batch 133 batch loss 1.09494114\n",
      "Validated batch 134 batch loss 1.25453818\n",
      "Validated batch 135 batch loss 1.29946351\n",
      "Validated batch 136 batch loss 1.26684785\n",
      "Validated batch 137 batch loss 1.21917331\n",
      "Validated batch 138 batch loss 1.095348\n",
      "Validated batch 139 batch loss 1.18477941\n",
      "Validated batch 140 batch loss 1.2476027\n",
      "Validated batch 141 batch loss 1.20598793\n",
      "Validated batch 142 batch loss 1.12316465\n",
      "Validated batch 143 batch loss 1.17566752\n",
      "Validated batch 144 batch loss 1.31378889\n",
      "Validated batch 145 batch loss 1.11609173\n",
      "Validated batch 146 batch loss 1.13648391\n",
      "Validated batch 147 batch loss 1.19412827\n",
      "Validated batch 148 batch loss 1.24185872\n",
      "Validated batch 149 batch loss 1.11306643\n",
      "Validated batch 150 batch loss 1.24475873\n",
      "Validated batch 151 batch loss 1.12296462\n",
      "Validated batch 152 batch loss 1.19735718\n",
      "Validated batch 153 batch loss 1.30491519\n",
      "Validated batch 154 batch loss 1.31939197\n",
      "Validated batch 155 batch loss 1.1714623\n",
      "Validated batch 156 batch loss 1.33721542\n",
      "Validated batch 157 batch loss 1.07627368\n",
      "Validated batch 158 batch loss 1.08744133\n",
      "Validated batch 159 batch loss 1.20141983\n",
      "Validated batch 160 batch loss 1.20701766\n",
      "Validated batch 161 batch loss 1.33409214\n",
      "Validated batch 162 batch loss 1.26200259\n",
      "Validated batch 163 batch loss 1.17107582\n",
      "Validated batch 164 batch loss 1.23646748\n",
      "Validated batch 165 batch loss 1.17993891\n",
      "Validated batch 166 batch loss 1.25419283\n",
      "Validated batch 167 batch loss 1.38241446\n",
      "Validated batch 168 batch loss 1.15388119\n",
      "Validated batch 169 batch loss 1.23544633\n",
      "Validated batch 170 batch loss 1.17184651\n",
      "Validated batch 171 batch loss 1.2855202\n",
      "Validated batch 172 batch loss 1.23772383\n",
      "Validated batch 173 batch loss 1.22589588\n",
      "Validated batch 174 batch loss 1.22671461\n",
      "Validated batch 175 batch loss 1.30799782\n",
      "Validated batch 176 batch loss 1.23367214\n",
      "Validated batch 177 batch loss 1.25574923\n",
      "Validated batch 178 batch loss 1.28354025\n",
      "Validated batch 179 batch loss 1.23644888\n",
      "Validated batch 180 batch loss 1.18402\n",
      "Validated batch 181 batch loss 1.24191856\n",
      "Validated batch 182 batch loss 1.18233132\n",
      "Validated batch 183 batch loss 1.19448042\n",
      "Validated batch 184 batch loss 1.25493729\n",
      "Validated batch 185 batch loss 1.37509\n",
      "Epoch 3 val loss 1.2346725463867188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 03:42:40.367576: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "2025-03-19 03:42:40.367668: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "\t [[RemoteCall]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model /home/minho/Desktop/aiffel/pose_estimation/models/model-epoch-3-loss-1.2347.weights.h5 saved.\n",
      "Start epoch 4 with learning rate 0.001\n",
      "Start distributed training...\n",
      "Trained batch 1 batch loss 1.24992788 epoch total loss 1.24992788\n",
      "Trained batch 2 batch loss 1.26227164 epoch total loss 1.2560997\n",
      "Trained batch 3 batch loss 1.23461413 epoch total loss 1.24893785\n",
      "Trained batch 4 batch loss 1.28478575 epoch total loss 1.25789976\n",
      "Trained batch 5 batch loss 1.15569258 epoch total loss 1.23745835\n",
      "Trained batch 6 batch loss 1.23535681 epoch total loss 1.23710811\n",
      "Trained batch 7 batch loss 1.15414536 epoch total loss 1.2252562\n",
      "Trained batch 8 batch loss 1.13585508 epoch total loss 1.21408105\n",
      "Trained batch 9 batch loss 1.09155726 epoch total loss 1.20046735\n",
      "Trained batch 10 batch loss 1.12560749 epoch total loss 1.19298136\n",
      "Trained batch 11 batch loss 1.08963442 epoch total loss 1.18358612\n",
      "Trained batch 12 batch loss 1.23080611 epoch total loss 1.1875211\n",
      "Trained batch 13 batch loss 1.11253071 epoch total loss 1.18175268\n",
      "Trained batch 14 batch loss 1.09188032 epoch total loss 1.17533314\n",
      "Trained batch 15 batch loss 1.12997317 epoch total loss 1.17230916\n",
      "Trained batch 16 batch loss 1.16003644 epoch total loss 1.17154205\n",
      "Trained batch 17 batch loss 1.16971552 epoch total loss 1.17143464\n",
      "Trained batch 18 batch loss 1.25952947 epoch total loss 1.17632878\n",
      "Trained batch 19 batch loss 1.28534746 epoch total loss 1.18206656\n",
      "Trained batch 20 batch loss 1.30027163 epoch total loss 1.18797684\n",
      "Trained batch 21 batch loss 1.30536664 epoch total loss 1.1935668\n",
      "Trained batch 22 batch loss 1.41992009 epoch total loss 1.20385563\n",
      "Trained batch 23 batch loss 1.40386117 epoch total loss 1.21255147\n",
      "Trained batch 24 batch loss 1.21722901 epoch total loss 1.2127465\n",
      "Trained batch 25 batch loss 1.05935645 epoch total loss 1.20661092\n",
      "Trained batch 26 batch loss 1.15249932 epoch total loss 1.20452964\n",
      "Trained batch 27 batch loss 1.25725687 epoch total loss 1.20648253\n",
      "Trained batch 28 batch loss 1.2455883 epoch total loss 1.20787919\n",
      "Trained batch 29 batch loss 1.33849597 epoch total loss 1.21238327\n",
      "Trained batch 30 batch loss 1.30535018 epoch total loss 1.21548223\n",
      "Trained batch 31 batch loss 1.17745018 epoch total loss 1.21425533\n",
      "Trained batch 32 batch loss 1.29683161 epoch total loss 1.21683586\n",
      "Trained batch 33 batch loss 1.13651514 epoch total loss 1.21440196\n",
      "Trained batch 34 batch loss 1.28303933 epoch total loss 1.21642065\n",
      "Trained batch 35 batch loss 1.29272151 epoch total loss 1.21860063\n",
      "Trained batch 36 batch loss 1.15036857 epoch total loss 1.21670532\n",
      "Trained batch 37 batch loss 1.19737422 epoch total loss 1.21618283\n",
      "Trained batch 38 batch loss 1.24049747 epoch total loss 1.21682262\n",
      "Trained batch 39 batch loss 1.19561672 epoch total loss 1.21627891\n",
      "Trained batch 40 batch loss 1.22899151 epoch total loss 1.21659684\n",
      "Trained batch 41 batch loss 1.2915988 epoch total loss 1.21842611\n",
      "Trained batch 42 batch loss 1.27356184 epoch total loss 1.21973896\n",
      "Trained batch 43 batch loss 1.200719 epoch total loss 1.21929657\n",
      "Trained batch 44 batch loss 1.22545445 epoch total loss 1.21943653\n",
      "Trained batch 45 batch loss 1.31702936 epoch total loss 1.2216053\n",
      "Trained batch 46 batch loss 1.20999312 epoch total loss 1.22135282\n",
      "Trained batch 47 batch loss 1.02324903 epoch total loss 1.21713781\n",
      "Trained batch 48 batch loss 1.04107046 epoch total loss 1.21346974\n",
      "Trained batch 49 batch loss 1.12742448 epoch total loss 1.21171379\n",
      "Trained batch 50 batch loss 1.10827303 epoch total loss 1.20964491\n",
      "Trained batch 51 batch loss 1.16702735 epoch total loss 1.20880926\n",
      "Trained batch 52 batch loss 1.23327124 epoch total loss 1.20927978\n",
      "Trained batch 53 batch loss 1.13406038 epoch total loss 1.20786059\n",
      "Trained batch 54 batch loss 1.28523624 epoch total loss 1.20929337\n",
      "Trained batch 55 batch loss 1.36816442 epoch total loss 1.21218193\n",
      "Trained batch 56 batch loss 1.35550106 epoch total loss 1.21474111\n",
      "Trained batch 57 batch loss 1.10342276 epoch total loss 1.21278822\n",
      "Trained batch 58 batch loss 1.10811388 epoch total loss 1.21098351\n",
      "Trained batch 59 batch loss 1.12667453 epoch total loss 1.20955455\n",
      "Trained batch 60 batch loss 1.22193646 epoch total loss 1.2097609\n",
      "Trained batch 61 batch loss 1.34789252 epoch total loss 1.2120254\n",
      "Trained batch 62 batch loss 1.31420612 epoch total loss 1.21367347\n",
      "Trained batch 63 batch loss 1.19573116 epoch total loss 1.21338868\n",
      "Trained batch 64 batch loss 1.18283856 epoch total loss 1.21291137\n",
      "Trained batch 65 batch loss 1.10971677 epoch total loss 1.21132374\n",
      "Trained batch 66 batch loss 1.15740418 epoch total loss 1.2105068\n",
      "Trained batch 67 batch loss 1.18412864 epoch total loss 1.21011305\n",
      "Trained batch 68 batch loss 1.30347657 epoch total loss 1.21148598\n",
      "Trained batch 69 batch loss 1.22066844 epoch total loss 1.21161902\n",
      "Trained batch 70 batch loss 1.28488719 epoch total loss 1.2126658\n",
      "Trained batch 71 batch loss 1.25074363 epoch total loss 1.213202\n",
      "Trained batch 72 batch loss 1.35201526 epoch total loss 1.21513\n",
      "Trained batch 73 batch loss 1.29664373 epoch total loss 1.2162466\n",
      "Trained batch 74 batch loss 1.27282834 epoch total loss 1.21701121\n",
      "Trained batch 75 batch loss 1.19475353 epoch total loss 1.2167145\n",
      "Trained batch 76 batch loss 1.28219712 epoch total loss 1.21757603\n",
      "Trained batch 77 batch loss 1.27820313 epoch total loss 1.21836352\n",
      "Trained batch 78 batch loss 1.25097084 epoch total loss 1.21878147\n",
      "Trained batch 79 batch loss 1.24896836 epoch total loss 1.21916366\n",
      "Trained batch 80 batch loss 1.283288 epoch total loss 1.21996522\n",
      "Trained batch 81 batch loss 1.21141672 epoch total loss 1.2198596\n",
      "Trained batch 82 batch loss 1.2535615 epoch total loss 1.22027063\n",
      "Trained batch 83 batch loss 1.20923841 epoch total loss 1.22013772\n",
      "Trained batch 84 batch loss 1.25517869 epoch total loss 1.22055495\n",
      "Trained batch 85 batch loss 1.17843664 epoch total loss 1.22005939\n",
      "Trained batch 86 batch loss 1.16000366 epoch total loss 1.21936107\n",
      "Trained batch 87 batch loss 1.19485104 epoch total loss 1.21907938\n",
      "Trained batch 88 batch loss 1.17827272 epoch total loss 1.21861565\n",
      "Trained batch 89 batch loss 1.19950128 epoch total loss 1.21840096\n",
      "Trained batch 90 batch loss 1.30293751 epoch total loss 1.21934021\n",
      "Trained batch 91 batch loss 1.16267681 epoch total loss 1.21871758\n",
      "Trained batch 92 batch loss 1.08065629 epoch total loss 1.21721685\n",
      "Trained batch 93 batch loss 1.18855703 epoch total loss 1.21690881\n",
      "Trained batch 94 batch loss 1.27275491 epoch total loss 1.21750295\n",
      "Trained batch 95 batch loss 1.20802 epoch total loss 1.21740317\n",
      "Trained batch 96 batch loss 1.22136605 epoch total loss 1.21744442\n",
      "Trained batch 97 batch loss 1.15802884 epoch total loss 1.21683192\n",
      "Trained batch 98 batch loss 1.17252648 epoch total loss 1.21637976\n",
      "Trained batch 99 batch loss 1.21030414 epoch total loss 1.21631837\n",
      "Trained batch 100 batch loss 1.14131749 epoch total loss 1.21556842\n",
      "Trained batch 101 batch loss 1.21465385 epoch total loss 1.21555936\n",
      "Trained batch 102 batch loss 1.24627352 epoch total loss 1.21586049\n",
      "Trained batch 103 batch loss 1.24887085 epoch total loss 1.21618092\n",
      "Trained batch 104 batch loss 1.29630613 epoch total loss 1.21695137\n",
      "Trained batch 105 batch loss 1.20329952 epoch total loss 1.21682131\n",
      "Trained batch 106 batch loss 1.05669212 epoch total loss 1.21531069\n",
      "Trained batch 107 batch loss 1.21556377 epoch total loss 1.21531308\n",
      "Trained batch 108 batch loss 1.20239687 epoch total loss 1.21519339\n",
      "Trained batch 109 batch loss 1.31200612 epoch total loss 1.21608162\n",
      "Trained batch 110 batch loss 1.27572119 epoch total loss 1.2166239\n",
      "Trained batch 111 batch loss 1.26134205 epoch total loss 1.21702671\n",
      "Trained batch 112 batch loss 1.3320899 epoch total loss 1.21805406\n",
      "Trained batch 113 batch loss 1.13762379 epoch total loss 1.21734226\n",
      "Trained batch 114 batch loss 1.0751884 epoch total loss 1.21609533\n",
      "Trained batch 115 batch loss 1.0717392 epoch total loss 1.21484\n",
      "Trained batch 116 batch loss 1.25388324 epoch total loss 1.21517658\n",
      "Trained batch 117 batch loss 1.23302174 epoch total loss 1.21532905\n",
      "Trained batch 118 batch loss 1.25566101 epoch total loss 1.21567082\n",
      "Trained batch 119 batch loss 1.32596099 epoch total loss 1.21659756\n",
      "Trained batch 120 batch loss 1.33308363 epoch total loss 1.21756828\n",
      "Trained batch 121 batch loss 1.24859893 epoch total loss 1.21782482\n",
      "Trained batch 122 batch loss 1.27274168 epoch total loss 1.21827483\n",
      "Trained batch 123 batch loss 1.37015295 epoch total loss 1.2195096\n",
      "Trained batch 124 batch loss 1.22102165 epoch total loss 1.21952176\n",
      "Trained batch 125 batch loss 1.28740501 epoch total loss 1.22006488\n",
      "Trained batch 126 batch loss 1.31563497 epoch total loss 1.22082329\n",
      "Trained batch 127 batch loss 1.35118651 epoch total loss 1.22184968\n",
      "Trained batch 128 batch loss 1.26213098 epoch total loss 1.22216439\n",
      "Trained batch 129 batch loss 1.18805635 epoch total loss 1.2219\n",
      "Trained batch 130 batch loss 1.298563 epoch total loss 1.22248971\n",
      "Trained batch 131 batch loss 1.12726724 epoch total loss 1.2217629\n",
      "Trained batch 132 batch loss 1.31640172 epoch total loss 1.22247982\n",
      "Trained batch 133 batch loss 1.07650936 epoch total loss 1.22138226\n",
      "Trained batch 134 batch loss 1.01834142 epoch total loss 1.21986711\n",
      "Trained batch 135 batch loss 1.04855561 epoch total loss 1.21859813\n",
      "Trained batch 136 batch loss 1.24709189 epoch total loss 1.21880758\n",
      "Trained batch 137 batch loss 1.4042244 epoch total loss 1.22016096\n",
      "Trained batch 138 batch loss 1.35716057 epoch total loss 1.22115374\n",
      "Trained batch 139 batch loss 1.10690582 epoch total loss 1.22033179\n",
      "Trained batch 140 batch loss 1.1695056 epoch total loss 1.2199688\n",
      "Trained batch 141 batch loss 1.12190318 epoch total loss 1.21927321\n",
      "Trained batch 142 batch loss 1.10881484 epoch total loss 1.21849537\n",
      "Trained batch 143 batch loss 1.10572124 epoch total loss 1.2177068\n",
      "Trained batch 144 batch loss 1.19284713 epoch total loss 1.21753407\n",
      "Trained batch 145 batch loss 1.20825577 epoch total loss 1.21747\n",
      "Trained batch 146 batch loss 1.17327023 epoch total loss 1.21716726\n",
      "Trained batch 147 batch loss 1.26258421 epoch total loss 1.21747625\n",
      "Trained batch 148 batch loss 1.24371 epoch total loss 1.21765351\n",
      "Trained batch 149 batch loss 1.20405853 epoch total loss 1.21756232\n",
      "Trained batch 150 batch loss 1.1279788 epoch total loss 1.21696508\n",
      "Trained batch 151 batch loss 1.06806219 epoch total loss 1.21597898\n",
      "Trained batch 152 batch loss 1.37482214 epoch total loss 1.21702397\n",
      "Trained batch 153 batch loss 1.44122589 epoch total loss 1.21848929\n",
      "Trained batch 154 batch loss 1.37604618 epoch total loss 1.21951246\n",
      "Trained batch 155 batch loss 1.26522422 epoch total loss 1.21980739\n",
      "Trained batch 156 batch loss 1.18229198 epoch total loss 1.21956694\n",
      "Trained batch 157 batch loss 1.11541533 epoch total loss 1.21890354\n",
      "Trained batch 158 batch loss 1.12357795 epoch total loss 1.21830022\n",
      "Trained batch 159 batch loss 1.31189215 epoch total loss 1.21888888\n",
      "Trained batch 160 batch loss 1.23264122 epoch total loss 1.21897483\n",
      "Trained batch 161 batch loss 1.27957547 epoch total loss 1.21935117\n",
      "Trained batch 162 batch loss 1.36814976 epoch total loss 1.22026968\n",
      "Trained batch 163 batch loss 1.38321161 epoch total loss 1.22126925\n",
      "Trained batch 164 batch loss 1.28470445 epoch total loss 1.22165608\n",
      "Trained batch 165 batch loss 1.33177257 epoch total loss 1.22232342\n",
      "Trained batch 166 batch loss 1.23645008 epoch total loss 1.22240853\n",
      "Trained batch 167 batch loss 1.16720116 epoch total loss 1.22207797\n",
      "Trained batch 168 batch loss 1.26194465 epoch total loss 1.22231531\n",
      "Trained batch 169 batch loss 1.21493387 epoch total loss 1.22227156\n",
      "Trained batch 170 batch loss 1.20561683 epoch total loss 1.22217357\n",
      "Trained batch 171 batch loss 1.13377118 epoch total loss 1.22165668\n",
      "Trained batch 172 batch loss 1.15973186 epoch total loss 1.22129667\n",
      "Trained batch 173 batch loss 1.11613286 epoch total loss 1.2206887\n",
      "Trained batch 174 batch loss 1.1377908 epoch total loss 1.22021234\n",
      "Trained batch 175 batch loss 0.928294659 epoch total loss 1.21854424\n",
      "Trained batch 176 batch loss 1.19359541 epoch total loss 1.21840239\n",
      "Trained batch 177 batch loss 1.40341723 epoch total loss 1.21944773\n",
      "Trained batch 178 batch loss 1.36220038 epoch total loss 1.22024965\n",
      "Trained batch 179 batch loss 1.24829459 epoch total loss 1.22040629\n",
      "Trained batch 180 batch loss 1.44383454 epoch total loss 1.2216475\n",
      "Trained batch 181 batch loss 1.26063681 epoch total loss 1.22186291\n",
      "Trained batch 182 batch loss 1.25944877 epoch total loss 1.2220695\n",
      "Trained batch 183 batch loss 1.29216838 epoch total loss 1.22245252\n",
      "Trained batch 184 batch loss 1.33862615 epoch total loss 1.22308385\n",
      "Trained batch 185 batch loss 1.2861917 epoch total loss 1.22342503\n",
      "Trained batch 186 batch loss 1.1318177 epoch total loss 1.22293258\n",
      "Trained batch 187 batch loss 1.1926353 epoch total loss 1.22277057\n",
      "Trained batch 188 batch loss 1.11147976 epoch total loss 1.22217858\n",
      "Trained batch 189 batch loss 1.09762406 epoch total loss 1.22151959\n",
      "Trained batch 190 batch loss 1.1879555 epoch total loss 1.22134292\n",
      "Trained batch 191 batch loss 1.26719928 epoch total loss 1.22158301\n",
      "Trained batch 192 batch loss 1.26005113 epoch total loss 1.2217834\n",
      "Trained batch 193 batch loss 1.36202168 epoch total loss 1.22251\n",
      "Trained batch 194 batch loss 1.25168204 epoch total loss 1.2226603\n",
      "Trained batch 195 batch loss 1.24378312 epoch total loss 1.22276866\n",
      "Trained batch 196 batch loss 1.20348406 epoch total loss 1.22267032\n",
      "Trained batch 197 batch loss 1.2374084 epoch total loss 1.22274518\n",
      "Trained batch 198 batch loss 1.29336917 epoch total loss 1.22310185\n",
      "Trained batch 199 batch loss 1.20614815 epoch total loss 1.22301662\n",
      "Trained batch 200 batch loss 1.15583193 epoch total loss 1.22268069\n",
      "Trained batch 201 batch loss 1.16022027 epoch total loss 1.22237\n",
      "Trained batch 202 batch loss 1.12910414 epoch total loss 1.22190821\n",
      "Trained batch 203 batch loss 1.18971944 epoch total loss 1.22174966\n",
      "Trained batch 204 batch loss 1.20297134 epoch total loss 1.22165763\n",
      "Trained batch 205 batch loss 1.11753607 epoch total loss 1.22114968\n",
      "Trained batch 206 batch loss 1.15080476 epoch total loss 1.22080827\n",
      "Trained batch 207 batch loss 1.15122652 epoch total loss 1.2204721\n",
      "Trained batch 208 batch loss 1.18019009 epoch total loss 1.2202785\n",
      "Trained batch 209 batch loss 1.24122286 epoch total loss 1.22037864\n",
      "Trained batch 210 batch loss 1.12554431 epoch total loss 1.21992707\n",
      "Trained batch 211 batch loss 1.18207288 epoch total loss 1.21974766\n",
      "Trained batch 212 batch loss 1.11181331 epoch total loss 1.21923852\n",
      "Trained batch 213 batch loss 1.1576314 epoch total loss 1.21894932\n",
      "Trained batch 214 batch loss 1.19031465 epoch total loss 1.21881545\n",
      "Trained batch 215 batch loss 1.15828061 epoch total loss 1.21853399\n",
      "Trained batch 216 batch loss 1.30710804 epoch total loss 1.21894395\n",
      "Trained batch 217 batch loss 1.30959606 epoch total loss 1.21936178\n",
      "Trained batch 218 batch loss 1.0811789 epoch total loss 1.21872783\n",
      "Trained batch 219 batch loss 1.31155205 epoch total loss 1.21915174\n",
      "Trained batch 220 batch loss 1.30731773 epoch total loss 1.21955252\n",
      "Trained batch 221 batch loss 1.33682191 epoch total loss 1.22008312\n",
      "Trained batch 222 batch loss 1.39720929 epoch total loss 1.22088099\n",
      "Trained batch 223 batch loss 1.4622916 epoch total loss 1.22196352\n",
      "Trained batch 224 batch loss 1.17923331 epoch total loss 1.22177279\n",
      "Trained batch 225 batch loss 1.17703009 epoch total loss 1.22157395\n",
      "Trained batch 226 batch loss 1.27206254 epoch total loss 1.22179735\n",
      "Trained batch 227 batch loss 1.20849192 epoch total loss 1.2217387\n",
      "Trained batch 228 batch loss 1.2548852 epoch total loss 1.22188413\n",
      "Trained batch 229 batch loss 1.21542025 epoch total loss 1.22185588\n",
      "Trained batch 230 batch loss 1.37543106 epoch total loss 1.22252357\n",
      "Trained batch 231 batch loss 1.32929564 epoch total loss 1.22298574\n",
      "Trained batch 232 batch loss 1.42537904 epoch total loss 1.22385812\n",
      "Trained batch 233 batch loss 1.4565084 epoch total loss 1.22485662\n",
      "Trained batch 234 batch loss 1.34314299 epoch total loss 1.22536218\n",
      "Trained batch 235 batch loss 1.37100267 epoch total loss 1.22598183\n",
      "Trained batch 236 batch loss 1.3258884 epoch total loss 1.22640526\n",
      "Trained batch 237 batch loss 1.32289433 epoch total loss 1.22681248\n",
      "Trained batch 238 batch loss 1.31468022 epoch total loss 1.22718155\n",
      "Trained batch 239 batch loss 1.18710637 epoch total loss 1.22701383\n",
      "Trained batch 240 batch loss 1.28933299 epoch total loss 1.22727358\n",
      "Trained batch 241 batch loss 1.29056644 epoch total loss 1.2275362\n",
      "Trained batch 242 batch loss 1.21120572 epoch total loss 1.22746873\n",
      "Trained batch 243 batch loss 1.35297191 epoch total loss 1.22798514\n",
      "Trained batch 244 batch loss 1.47895503 epoch total loss 1.22901368\n",
      "Trained batch 245 batch loss 1.31255889 epoch total loss 1.22935462\n",
      "Trained batch 246 batch loss 1.2979511 epoch total loss 1.22963345\n",
      "Trained batch 247 batch loss 1.23901117 epoch total loss 1.22967148\n",
      "Trained batch 248 batch loss 0.973200083 epoch total loss 1.22863734\n",
      "Trained batch 249 batch loss 1.077757 epoch total loss 1.2280314\n",
      "Trained batch 250 batch loss 1.28996158 epoch total loss 1.22827899\n",
      "Trained batch 251 batch loss 1.01959729 epoch total loss 1.22744763\n",
      "Trained batch 252 batch loss 0.982866228 epoch total loss 1.22647715\n",
      "Trained batch 253 batch loss 0.910360813 epoch total loss 1.22522771\n",
      "Trained batch 254 batch loss 1.07982183 epoch total loss 1.22465527\n",
      "Trained batch 255 batch loss 1.1116755 epoch total loss 1.22421217\n",
      "Trained batch 256 batch loss 1.08926845 epoch total loss 1.22368503\n",
      "Trained batch 257 batch loss 1.22811913 epoch total loss 1.22370231\n",
      "Trained batch 258 batch loss 1.30219698 epoch total loss 1.22400653\n",
      "Trained batch 259 batch loss 1.23870051 epoch total loss 1.22406328\n",
      "Trained batch 260 batch loss 1.34979081 epoch total loss 1.22454679\n",
      "Trained batch 261 batch loss 1.15823984 epoch total loss 1.22429276\n",
      "Trained batch 262 batch loss 1.19722342 epoch total loss 1.22418952\n",
      "Trained batch 263 batch loss 1.08356667 epoch total loss 1.22365475\n",
      "Trained batch 264 batch loss 1.15342712 epoch total loss 1.22338867\n",
      "Trained batch 265 batch loss 1.17775965 epoch total loss 1.22321653\n",
      "Trained batch 266 batch loss 1.15654135 epoch total loss 1.22296596\n",
      "Trained batch 267 batch loss 1.28481972 epoch total loss 1.22319758\n",
      "Trained batch 268 batch loss 1.10494781 epoch total loss 1.22275639\n",
      "Trained batch 269 batch loss 1.06282 epoch total loss 1.22216177\n",
      "Trained batch 270 batch loss 0.983519554 epoch total loss 1.22127783\n",
      "Trained batch 271 batch loss 1.080971 epoch total loss 1.22076011\n",
      "Trained batch 272 batch loss 1.33373511 epoch total loss 1.22117543\n",
      "Trained batch 273 batch loss 1.17477119 epoch total loss 1.22100556\n",
      "Trained batch 274 batch loss 1.16606081 epoch total loss 1.22080493\n",
      "Trained batch 275 batch loss 1.22395897 epoch total loss 1.22081637\n",
      "Trained batch 276 batch loss 1.30271566 epoch total loss 1.22111309\n",
      "Trained batch 277 batch loss 1.05234313 epoch total loss 1.22050381\n",
      "Trained batch 278 batch loss 1.10819757 epoch total loss 1.22009981\n",
      "Trained batch 279 batch loss 1.21822524 epoch total loss 1.22009313\n",
      "Trained batch 280 batch loss 1.28319919 epoch total loss 1.22031844\n",
      "Trained batch 281 batch loss 1.15072453 epoch total loss 1.22007084\n",
      "Trained batch 282 batch loss 1.22301579 epoch total loss 1.22008133\n",
      "Trained batch 283 batch loss 1.25455117 epoch total loss 1.22020304\n",
      "Trained batch 284 batch loss 1.24349833 epoch total loss 1.22028518\n",
      "Trained batch 285 batch loss 1.1363821 epoch total loss 1.21999073\n",
      "Trained batch 286 batch loss 1.09601283 epoch total loss 1.21955717\n",
      "Trained batch 287 batch loss 1.02518046 epoch total loss 1.21887994\n",
      "Trained batch 288 batch loss 1.06459641 epoch total loss 1.21834421\n",
      "Trained batch 289 batch loss 1.21514964 epoch total loss 1.21833324\n",
      "Trained batch 290 batch loss 1.32342446 epoch total loss 1.21869564\n",
      "Trained batch 291 batch loss 1.60156238 epoch total loss 1.22001123\n",
      "Trained batch 292 batch loss 1.40439284 epoch total loss 1.22064269\n",
      "Trained batch 293 batch loss 1.23851013 epoch total loss 1.22070372\n",
      "Trained batch 294 batch loss 1.38382936 epoch total loss 1.22125852\n",
      "Trained batch 295 batch loss 1.21721697 epoch total loss 1.22124493\n",
      "Trained batch 296 batch loss 1.13003421 epoch total loss 1.22093678\n",
      "Trained batch 297 batch loss 1.15584326 epoch total loss 1.22071767\n",
      "Trained batch 298 batch loss 1.30278921 epoch total loss 1.22099304\n",
      "Trained batch 299 batch loss 1.17690301 epoch total loss 1.22084558\n",
      "Trained batch 300 batch loss 1.32538724 epoch total loss 1.22119403\n",
      "Trained batch 301 batch loss 1.25870299 epoch total loss 1.2213186\n",
      "Trained batch 302 batch loss 1.16895187 epoch total loss 1.22114527\n",
      "Trained batch 303 batch loss 1.26250041 epoch total loss 1.22128177\n",
      "Trained batch 304 batch loss 1.31897688 epoch total loss 1.22160304\n",
      "Trained batch 305 batch loss 1.38623953 epoch total loss 1.22214282\n",
      "Trained batch 306 batch loss 1.40598464 epoch total loss 1.22274363\n",
      "Trained batch 307 batch loss 1.20138741 epoch total loss 1.22267401\n",
      "Trained batch 308 batch loss 1.31579745 epoch total loss 1.22297645\n",
      "Trained batch 309 batch loss 1.22061181 epoch total loss 1.2229687\n",
      "Trained batch 310 batch loss 1.22712708 epoch total loss 1.22298205\n",
      "Trained batch 311 batch loss 1.17339623 epoch total loss 1.22282267\n",
      "Trained batch 312 batch loss 1.21969903 epoch total loss 1.22281265\n",
      "Trained batch 313 batch loss 1.31291878 epoch total loss 1.22310054\n",
      "Trained batch 314 batch loss 1.33977115 epoch total loss 1.22347212\n",
      "Trained batch 315 batch loss 1.18316722 epoch total loss 1.22334421\n",
      "Trained batch 316 batch loss 1.1606921 epoch total loss 1.22314596\n",
      "Trained batch 317 batch loss 1.27361155 epoch total loss 1.22330523\n",
      "Trained batch 318 batch loss 1.06426072 epoch total loss 1.22280514\n",
      "Trained batch 319 batch loss 1.20148158 epoch total loss 1.22273827\n",
      "Trained batch 320 batch loss 1.27182579 epoch total loss 1.22289157\n",
      "Trained batch 321 batch loss 1.0958401 epoch total loss 1.22249579\n",
      "Trained batch 322 batch loss 1.15653431 epoch total loss 1.22229087\n",
      "Trained batch 323 batch loss 1.07846034 epoch total loss 1.22184563\n",
      "Trained batch 324 batch loss 1.20757794 epoch total loss 1.22180152\n",
      "Trained batch 325 batch loss 1.23052657 epoch total loss 1.22182846\n",
      "Trained batch 326 batch loss 1.1029191 epoch total loss 1.22146368\n",
      "Trained batch 327 batch loss 1.13502145 epoch total loss 1.22119927\n",
      "Trained batch 328 batch loss 1.21419477 epoch total loss 1.22117794\n",
      "Trained batch 329 batch loss 1.10827756 epoch total loss 1.22083473\n",
      "Trained batch 330 batch loss 1.21461928 epoch total loss 1.2208159\n",
      "Trained batch 331 batch loss 1.17439604 epoch total loss 1.22067571\n",
      "Trained batch 332 batch loss 1.22219193 epoch total loss 1.22068036\n",
      "Trained batch 333 batch loss 1.21739578 epoch total loss 1.22067046\n",
      "Trained batch 334 batch loss 1.33705866 epoch total loss 1.22101903\n",
      "Trained batch 335 batch loss 1.23337328 epoch total loss 1.22105587\n",
      "Trained batch 336 batch loss 1.27762079 epoch total loss 1.22122419\n",
      "Trained batch 337 batch loss 1.25887012 epoch total loss 1.22133589\n",
      "Trained batch 338 batch loss 1.19817841 epoch total loss 1.22126746\n",
      "Trained batch 339 batch loss 1.10171986 epoch total loss 1.22091472\n",
      "Trained batch 340 batch loss 1.2529887 epoch total loss 1.22100914\n",
      "Trained batch 341 batch loss 1.17562675 epoch total loss 1.22087598\n",
      "Trained batch 342 batch loss 1.36254442 epoch total loss 1.22129023\n",
      "Trained batch 343 batch loss 1.32202744 epoch total loss 1.22158396\n",
      "Trained batch 344 batch loss 1.37011755 epoch total loss 1.22201574\n",
      "Trained batch 345 batch loss 1.11668479 epoch total loss 1.22171044\n",
      "Trained batch 346 batch loss 1.20607567 epoch total loss 1.22166526\n",
      "Trained batch 347 batch loss 1.24879122 epoch total loss 1.22174346\n",
      "Trained batch 348 batch loss 1.21256959 epoch total loss 1.221717\n",
      "Trained batch 349 batch loss 1.26297855 epoch total loss 1.22183526\n",
      "Trained batch 350 batch loss 1.24003506 epoch total loss 1.22188723\n",
      "Trained batch 351 batch loss 1.13820267 epoch total loss 1.22164881\n",
      "Trained batch 352 batch loss 1.3438971 epoch total loss 1.22199619\n",
      "Trained batch 353 batch loss 1.18967259 epoch total loss 1.22190452\n",
      "Trained batch 354 batch loss 1.11415362 epoch total loss 1.22160017\n",
      "Trained batch 355 batch loss 1.18093157 epoch total loss 1.22148561\n",
      "Trained batch 356 batch loss 1.34622216 epoch total loss 1.22183609\n",
      "Trained batch 357 batch loss 1.25385129 epoch total loss 1.22192574\n",
      "Trained batch 358 batch loss 1.30813015 epoch total loss 1.22216654\n",
      "Trained batch 359 batch loss 1.22720575 epoch total loss 1.2221806\n",
      "Trained batch 360 batch loss 1.30488884 epoch total loss 1.22241032\n",
      "Trained batch 361 batch loss 1.29661489 epoch total loss 1.22261584\n",
      "Trained batch 362 batch loss 1.2987386 epoch total loss 1.22282612\n",
      "Trained batch 363 batch loss 1.24555624 epoch total loss 1.22288871\n",
      "Trained batch 364 batch loss 1.28042603 epoch total loss 1.22304678\n",
      "Trained batch 365 batch loss 1.16018164 epoch total loss 1.22287452\n",
      "Trained batch 366 batch loss 1.16022444 epoch total loss 1.22270334\n",
      "Trained batch 367 batch loss 1.10921311 epoch total loss 1.22239411\n",
      "Trained batch 368 batch loss 1.05698919 epoch total loss 1.22194469\n",
      "Trained batch 369 batch loss 1.19950306 epoch total loss 1.22188377\n",
      "Trained batch 370 batch loss 1.23340976 epoch total loss 1.22191489\n",
      "Trained batch 371 batch loss 1.43189692 epoch total loss 1.22248089\n",
      "Trained batch 372 batch loss 1.33724284 epoch total loss 1.22278941\n",
      "Trained batch 373 batch loss 1.3947289 epoch total loss 1.22325027\n",
      "Trained batch 374 batch loss 1.33930206 epoch total loss 1.22356057\n",
      "Trained batch 375 batch loss 1.29534519 epoch total loss 1.22375202\n",
      "Trained batch 376 batch loss 1.21404958 epoch total loss 1.22372627\n",
      "Trained batch 377 batch loss 1.24220872 epoch total loss 1.22377527\n",
      "Trained batch 378 batch loss 1.21593201 epoch total loss 1.22375453\n",
      "Trained batch 379 batch loss 1.20503068 epoch total loss 1.22370517\n",
      "Trained batch 380 batch loss 1.21377921 epoch total loss 1.22367895\n",
      "Trained batch 381 batch loss 1.20393062 epoch total loss 1.22362709\n",
      "Trained batch 382 batch loss 1.24741769 epoch total loss 1.22368932\n",
      "Trained batch 383 batch loss 1.21313155 epoch total loss 1.22366178\n",
      "Trained batch 384 batch loss 1.12220025 epoch total loss 1.22339761\n",
      "Trained batch 385 batch loss 0.969486594 epoch total loss 1.22273803\n",
      "Trained batch 386 batch loss 1.04737055 epoch total loss 1.22228372\n",
      "Trained batch 387 batch loss 1.28401041 epoch total loss 1.22244322\n",
      "Trained batch 388 batch loss 1.25929594 epoch total loss 1.22253823\n",
      "Trained batch 389 batch loss 1.43594885 epoch total loss 1.22308683\n",
      "Trained batch 390 batch loss 1.22439313 epoch total loss 1.22309017\n",
      "Trained batch 391 batch loss 1.06236351 epoch total loss 1.22267914\n",
      "Trained batch 392 batch loss 1.01063716 epoch total loss 1.22213829\n",
      "Trained batch 393 batch loss 1.20219851 epoch total loss 1.2220875\n",
      "Trained batch 394 batch loss 1.10110092 epoch total loss 1.22178042\n",
      "Trained batch 395 batch loss 1.18623364 epoch total loss 1.22169054\n",
      "Trained batch 396 batch loss 1.20271087 epoch total loss 1.22164249\n",
      "Trained batch 397 batch loss 1.10648656 epoch total loss 1.22135246\n",
      "Trained batch 398 batch loss 1.11362052 epoch total loss 1.22108173\n",
      "Trained batch 399 batch loss 1.22371888 epoch total loss 1.22108841\n",
      "Trained batch 400 batch loss 1.05377841 epoch total loss 1.2206701\n",
      "Trained batch 401 batch loss 1.25390851 epoch total loss 1.22075295\n",
      "Trained batch 402 batch loss 1.08419716 epoch total loss 1.22041333\n",
      "Trained batch 403 batch loss 1.18685961 epoch total loss 1.22033\n",
      "Trained batch 404 batch loss 1.20383763 epoch total loss 1.22028923\n",
      "Trained batch 405 batch loss 1.11070919 epoch total loss 1.22001863\n",
      "Trained batch 406 batch loss 1.18918979 epoch total loss 1.21994269\n",
      "Trained batch 407 batch loss 1.42204106 epoch total loss 1.2204392\n",
      "Trained batch 408 batch loss 1.47708035 epoch total loss 1.22106826\n",
      "Trained batch 409 batch loss 1.27386308 epoch total loss 1.22119725\n",
      "Trained batch 410 batch loss 1.15379024 epoch total loss 1.22103286\n",
      "Trained batch 411 batch loss 1.11379766 epoch total loss 1.22077191\n",
      "Trained batch 412 batch loss 1.14137793 epoch total loss 1.22057927\n",
      "Trained batch 413 batch loss 1.2029922 epoch total loss 1.22053671\n",
      "Trained batch 414 batch loss 1.18508339 epoch total loss 1.22045112\n",
      "Trained batch 415 batch loss 1.24763894 epoch total loss 1.22051668\n",
      "Trained batch 416 batch loss 1.2279402 epoch total loss 1.22053444\n",
      "Trained batch 417 batch loss 1.20349026 epoch total loss 1.22049356\n",
      "Trained batch 418 batch loss 1.16483188 epoch total loss 1.2203604\n",
      "Trained batch 419 batch loss 1.25235319 epoch total loss 1.22043681\n",
      "Trained batch 420 batch loss 1.28347957 epoch total loss 1.2205869\n",
      "Trained batch 421 batch loss 1.17702723 epoch total loss 1.2204833\n",
      "Trained batch 422 batch loss 1.2454201 epoch total loss 1.22054243\n",
      "Trained batch 423 batch loss 1.19420815 epoch total loss 1.2204802\n",
      "Trained batch 424 batch loss 1.17558193 epoch total loss 1.22037435\n",
      "Trained batch 425 batch loss 1.1256578 epoch total loss 1.22015154\n",
      "Trained batch 426 batch loss 1.25767684 epoch total loss 1.22023964\n",
      "Trained batch 427 batch loss 1.22020817 epoch total loss 1.22023952\n",
      "Trained batch 428 batch loss 1.23457694 epoch total loss 1.22027302\n",
      "Trained batch 429 batch loss 1.33930039 epoch total loss 1.22055042\n",
      "Trained batch 430 batch loss 1.38279629 epoch total loss 1.22092783\n",
      "Trained batch 431 batch loss 1.26820302 epoch total loss 1.22103751\n",
      "Trained batch 432 batch loss 1.30077529 epoch total loss 1.22122204\n",
      "Trained batch 433 batch loss 1.16164672 epoch total loss 1.22108448\n",
      "Trained batch 434 batch loss 1.14860928 epoch total loss 1.22091746\n",
      "Trained batch 435 batch loss 1.30009532 epoch total loss 1.2210995\n",
      "Trained batch 436 batch loss 1.12017035 epoch total loss 1.22086799\n",
      "Trained batch 437 batch loss 1.2321986 epoch total loss 1.22089386\n",
      "Trained batch 438 batch loss 1.24006569 epoch total loss 1.22093761\n",
      "Trained batch 439 batch loss 1.31572175 epoch total loss 1.22115362\n",
      "Trained batch 440 batch loss 1.20353937 epoch total loss 1.22111356\n",
      "Trained batch 441 batch loss 1.1203922 epoch total loss 1.22088528\n",
      "Trained batch 442 batch loss 1.1733743 epoch total loss 1.22077787\n",
      "Trained batch 443 batch loss 1.05239153 epoch total loss 1.22039771\n",
      "Trained batch 444 batch loss 1.0651083 epoch total loss 1.22004795\n",
      "Trained batch 445 batch loss 1.0873003 epoch total loss 1.21974957\n",
      "Trained batch 446 batch loss 1.11230206 epoch total loss 1.21950865\n",
      "Trained batch 447 batch loss 1.22414482 epoch total loss 1.21951902\n",
      "Trained batch 448 batch loss 1.12900937 epoch total loss 1.21931708\n",
      "Trained batch 449 batch loss 1.13680506 epoch total loss 1.21913326\n",
      "Trained batch 450 batch loss 1.04911304 epoch total loss 1.21875548\n",
      "Trained batch 451 batch loss 1.12322927 epoch total loss 1.21854365\n",
      "Trained batch 452 batch loss 1.26670456 epoch total loss 1.21865022\n",
      "Trained batch 453 batch loss 1.31265581 epoch total loss 1.21885777\n",
      "Trained batch 454 batch loss 1.20837677 epoch total loss 1.21883464\n",
      "Trained batch 455 batch loss 1.25344753 epoch total loss 1.21891069\n",
      "Trained batch 456 batch loss 1.24606872 epoch total loss 1.2189703\n",
      "Trained batch 457 batch loss 1.32015431 epoch total loss 1.21919167\n",
      "Trained batch 458 batch loss 1.32605469 epoch total loss 1.21942496\n",
      "Trained batch 459 batch loss 1.24119711 epoch total loss 1.21947241\n",
      "Trained batch 460 batch loss 1.07734346 epoch total loss 1.21916342\n",
      "Trained batch 461 batch loss 1.15575194 epoch total loss 1.21902597\n",
      "Trained batch 462 batch loss 1.19334495 epoch total loss 1.21897042\n",
      "Trained batch 463 batch loss 1.20432854 epoch total loss 1.21893883\n",
      "Trained batch 464 batch loss 1.17094862 epoch total loss 1.21883535\n",
      "Trained batch 465 batch loss 1.277668 epoch total loss 1.21896183\n",
      "Trained batch 466 batch loss 1.17458236 epoch total loss 1.21886659\n",
      "Trained batch 467 batch loss 1.14930832 epoch total loss 1.21871758\n",
      "Trained batch 468 batch loss 1.18475103 epoch total loss 1.21864498\n",
      "Trained batch 469 batch loss 1.18280077 epoch total loss 1.21856856\n",
      "Trained batch 470 batch loss 1.09679127 epoch total loss 1.21830952\n",
      "Trained batch 471 batch loss 1.18948388 epoch total loss 1.21824837\n",
      "Trained batch 472 batch loss 1.17676592 epoch total loss 1.21816051\n",
      "Trained batch 473 batch loss 1.20276237 epoch total loss 1.21812785\n",
      "Trained batch 474 batch loss 1.18163204 epoch total loss 1.21805096\n",
      "Trained batch 475 batch loss 1.15944493 epoch total loss 1.21792746\n",
      "Trained batch 476 batch loss 1.10748148 epoch total loss 1.21769547\n",
      "Trained batch 477 batch loss 1.11416793 epoch total loss 1.21747851\n",
      "Trained batch 478 batch loss 1.13383484 epoch total loss 1.21730351\n",
      "Trained batch 479 batch loss 1.34342146 epoch total loss 1.21756685\n",
      "Trained batch 480 batch loss 1.3855269 epoch total loss 1.21791673\n",
      "Trained batch 481 batch loss 1.386958 epoch total loss 1.21826816\n",
      "Trained batch 482 batch loss 1.26900291 epoch total loss 1.21837342\n",
      "Trained batch 483 batch loss 1.25630295 epoch total loss 1.21845186\n",
      "Trained batch 484 batch loss 1.19623446 epoch total loss 1.21840596\n",
      "Trained batch 485 batch loss 1.19982314 epoch total loss 1.2183677\n",
      "Trained batch 486 batch loss 1.20564008 epoch total loss 1.21834147\n",
      "Trained batch 487 batch loss 1.26601315 epoch total loss 1.21843934\n",
      "Trained batch 488 batch loss 1.09068584 epoch total loss 1.21817756\n",
      "Trained batch 489 batch loss 1.13594639 epoch total loss 1.21800935\n",
      "Trained batch 490 batch loss 0.989367247 epoch total loss 1.21754277\n",
      "Trained batch 491 batch loss 0.992719114 epoch total loss 1.21708488\n",
      "Trained batch 492 batch loss 1.07087708 epoch total loss 1.2167877\n",
      "Trained batch 493 batch loss 1.29956782 epoch total loss 1.21695554\n",
      "Trained batch 494 batch loss 1.47485685 epoch total loss 1.21747768\n",
      "Trained batch 495 batch loss 1.50408268 epoch total loss 1.21805668\n",
      "Trained batch 496 batch loss 1.34883034 epoch total loss 1.21832025\n",
      "Trained batch 497 batch loss 1.34052682 epoch total loss 1.21856618\n",
      "Trained batch 498 batch loss 1.49845958 epoch total loss 1.21912825\n",
      "Trained batch 499 batch loss 1.27051699 epoch total loss 1.21923113\n",
      "Trained batch 500 batch loss 1.28820896 epoch total loss 1.21936917\n",
      "Trained batch 501 batch loss 1.29468775 epoch total loss 1.2195195\n",
      "Trained batch 502 batch loss 1.18794608 epoch total loss 1.21945655\n",
      "Trained batch 503 batch loss 1.16219342 epoch total loss 1.21934259\n",
      "Trained batch 504 batch loss 1.18147 epoch total loss 1.21926749\n",
      "Trained batch 505 batch loss 1.27723956 epoch total loss 1.21938229\n",
      "Trained batch 506 batch loss 1.14325154 epoch total loss 1.21923172\n",
      "Trained batch 507 batch loss 1.28696537 epoch total loss 1.21936536\n",
      "Trained batch 508 batch loss 1.27808905 epoch total loss 1.21948099\n",
      "Trained batch 509 batch loss 1.29679906 epoch total loss 1.21963286\n",
      "Trained batch 510 batch loss 1.22402048 epoch total loss 1.21964145\n",
      "Trained batch 511 batch loss 1.29408169 epoch total loss 1.21978712\n",
      "Trained batch 512 batch loss 1.37959111 epoch total loss 1.22009921\n",
      "Trained batch 513 batch loss 1.30845082 epoch total loss 1.22027147\n",
      "Trained batch 514 batch loss 1.20272088 epoch total loss 1.22023726\n",
      "Trained batch 515 batch loss 1.14535344 epoch total loss 1.22009182\n",
      "Trained batch 516 batch loss 1.15345609 epoch total loss 1.21996272\n",
      "Trained batch 517 batch loss 1.13309336 epoch total loss 1.21979463\n",
      "Trained batch 518 batch loss 1.05912638 epoch total loss 1.21948457\n",
      "Trained batch 519 batch loss 1.15717411 epoch total loss 1.21936452\n",
      "Trained batch 520 batch loss 1.16104329 epoch total loss 1.21925235\n",
      "Trained batch 521 batch loss 1.25703537 epoch total loss 1.21932483\n",
      "Trained batch 522 batch loss 1.16264355 epoch total loss 1.21921635\n",
      "Trained batch 523 batch loss 1.07989299 epoch total loss 1.21894991\n",
      "Trained batch 524 batch loss 1.27070677 epoch total loss 1.21904862\n",
      "Trained batch 525 batch loss 1.24649012 epoch total loss 1.21910083\n",
      "Trained batch 526 batch loss 1.10258484 epoch total loss 1.21887934\n",
      "Trained batch 527 batch loss 1.13328803 epoch total loss 1.21871698\n",
      "Trained batch 528 batch loss 1.1748383 epoch total loss 1.21863389\n",
      "Trained batch 529 batch loss 1.17513657 epoch total loss 1.21855164\n",
      "Trained batch 530 batch loss 1.25551701 epoch total loss 1.21862137\n",
      "Trained batch 531 batch loss 1.29216909 epoch total loss 1.21875989\n",
      "Trained batch 532 batch loss 1.09698009 epoch total loss 1.21853101\n",
      "Trained batch 533 batch loss 1.08691692 epoch total loss 1.21828401\n",
      "Trained batch 534 batch loss 1.12196887 epoch total loss 1.21810365\n",
      "Trained batch 535 batch loss 1.18243754 epoch total loss 1.21803701\n",
      "Trained batch 536 batch loss 1.2287451 epoch total loss 1.21805704\n",
      "Trained batch 537 batch loss 1.18034887 epoch total loss 1.21798682\n",
      "Trained batch 538 batch loss 1.13739944 epoch total loss 1.21783698\n",
      "Trained batch 539 batch loss 1.15036583 epoch total loss 1.21771181\n",
      "Trained batch 540 batch loss 1.04471171 epoch total loss 1.21739149\n",
      "Trained batch 541 batch loss 1.10197151 epoch total loss 1.21717823\n",
      "Trained batch 542 batch loss 1.13189197 epoch total loss 1.21702087\n",
      "Trained batch 543 batch loss 1.19270635 epoch total loss 1.21697605\n",
      "Trained batch 544 batch loss 1.22379208 epoch total loss 1.21698856\n",
      "Trained batch 545 batch loss 1.21795845 epoch total loss 1.21699035\n",
      "Trained batch 546 batch loss 1.23124683 epoch total loss 1.21701658\n",
      "Trained batch 547 batch loss 1.07598519 epoch total loss 1.21675873\n",
      "Trained batch 548 batch loss 1.05633986 epoch total loss 1.21646595\n",
      "Trained batch 549 batch loss 1.09421229 epoch total loss 1.21624339\n",
      "Trained batch 550 batch loss 1.09088445 epoch total loss 1.21601546\n",
      "Trained batch 551 batch loss 1.10304356 epoch total loss 1.2158103\n",
      "Trained batch 552 batch loss 1.01244068 epoch total loss 1.21544194\n",
      "Trained batch 553 batch loss 1.13161635 epoch total loss 1.21529031\n",
      "Trained batch 554 batch loss 1.09466386 epoch total loss 1.21507263\n",
      "Trained batch 555 batch loss 1.12393451 epoch total loss 1.21490836\n",
      "Trained batch 556 batch loss 1.3304714 epoch total loss 1.21511626\n",
      "Trained batch 557 batch loss 1.24974513 epoch total loss 1.21517837\n",
      "Trained batch 558 batch loss 1.18884373 epoch total loss 1.21513116\n",
      "Trained batch 559 batch loss 1.19689178 epoch total loss 1.21509862\n",
      "Trained batch 560 batch loss 1.21947646 epoch total loss 1.21510637\n",
      "Trained batch 561 batch loss 1.06961823 epoch total loss 1.21484709\n",
      "Trained batch 562 batch loss 1.17643666 epoch total loss 1.21477878\n",
      "Trained batch 563 batch loss 1.39728653 epoch total loss 1.21510291\n",
      "Trained batch 564 batch loss 1.26016521 epoch total loss 1.2151829\n",
      "Trained batch 565 batch loss 1.24919975 epoch total loss 1.2152431\n",
      "Trained batch 566 batch loss 1.15492749 epoch total loss 1.21513653\n",
      "Trained batch 567 batch loss 1.26363492 epoch total loss 1.215222\n",
      "Trained batch 568 batch loss 1.19071031 epoch total loss 1.21517885\n",
      "Trained batch 569 batch loss 1.30952466 epoch total loss 1.21534467\n",
      "Trained batch 570 batch loss 1.24777806 epoch total loss 1.21540165\n",
      "Trained batch 571 batch loss 1.20261073 epoch total loss 1.21537924\n",
      "Trained batch 572 batch loss 1.12985349 epoch total loss 1.21522975\n",
      "Trained batch 573 batch loss 1.11879838 epoch total loss 1.21506143\n",
      "Trained batch 574 batch loss 1.21299815 epoch total loss 1.21505785\n",
      "Trained batch 575 batch loss 1.28163099 epoch total loss 1.2151736\n",
      "Trained batch 576 batch loss 1.3239181 epoch total loss 1.21536243\n",
      "Trained batch 577 batch loss 1.30879092 epoch total loss 1.21552432\n",
      "Trained batch 578 batch loss 1.17403054 epoch total loss 1.21545255\n",
      "Trained batch 579 batch loss 1.23399806 epoch total loss 1.21548462\n",
      "Trained batch 580 batch loss 1.17883801 epoch total loss 1.21542132\n",
      "Trained batch 581 batch loss 1.12495661 epoch total loss 1.21526563\n",
      "Trained batch 582 batch loss 1.16036379 epoch total loss 1.21517122\n",
      "Trained batch 583 batch loss 1.24739373 epoch total loss 1.21522653\n",
      "Trained batch 584 batch loss 1.32688 epoch total loss 1.21541774\n",
      "Trained batch 585 batch loss 1.21467304 epoch total loss 1.21541643\n",
      "Trained batch 586 batch loss 1.2026515 epoch total loss 1.21539462\n",
      "Trained batch 587 batch loss 1.19493377 epoch total loss 1.21535981\n",
      "Trained batch 588 batch loss 1.03164673 epoch total loss 1.21504736\n",
      "Trained batch 589 batch loss 1.12614107 epoch total loss 1.21489644\n",
      "Trained batch 590 batch loss 1.09626245 epoch total loss 1.21469545\n",
      "Trained batch 591 batch loss 1.01785135 epoch total loss 1.21436226\n",
      "Trained batch 592 batch loss 1.07226086 epoch total loss 1.2141223\n",
      "Trained batch 593 batch loss 1.12292957 epoch total loss 1.21396852\n",
      "Trained batch 594 batch loss 1.12564778 epoch total loss 1.21381986\n",
      "Trained batch 595 batch loss 0.968650579 epoch total loss 1.21340775\n",
      "Trained batch 596 batch loss 1.09668195 epoch total loss 1.21321189\n",
      "Trained batch 597 batch loss 1.17088687 epoch total loss 1.21314096\n",
      "Trained batch 598 batch loss 1.06965089 epoch total loss 1.212901\n",
      "Trained batch 599 batch loss 1.34881139 epoch total loss 1.21312797\n",
      "Trained batch 600 batch loss 1.48816025 epoch total loss 1.21358633\n",
      "Trained batch 601 batch loss 1.15394652 epoch total loss 1.21348703\n",
      "Trained batch 602 batch loss 1.29411519 epoch total loss 1.21362102\n",
      "Trained batch 603 batch loss 1.31865311 epoch total loss 1.21379519\n",
      "Trained batch 604 batch loss 1.31605113 epoch total loss 1.21396446\n",
      "Trained batch 605 batch loss 1.24899626 epoch total loss 1.2140224\n",
      "Trained batch 606 batch loss 1.13107967 epoch total loss 1.21388566\n",
      "Trained batch 607 batch loss 1.10820973 epoch total loss 1.2137115\n",
      "Trained batch 608 batch loss 1.28509307 epoch total loss 1.21382892\n",
      "Trained batch 609 batch loss 1.2801466 epoch total loss 1.21393788\n",
      "Trained batch 610 batch loss 1.23131621 epoch total loss 1.21396637\n",
      "Trained batch 611 batch loss 1.14688659 epoch total loss 1.21385658\n",
      "Trained batch 612 batch loss 1.17762256 epoch total loss 1.21379733\n",
      "Trained batch 613 batch loss 1.11453903 epoch total loss 1.21363544\n",
      "Trained batch 614 batch loss 1.3102262 epoch total loss 1.2137928\n",
      "Trained batch 615 batch loss 1.50582647 epoch total loss 1.21426761\n",
      "Trained batch 616 batch loss 1.28674579 epoch total loss 1.21438527\n",
      "Trained batch 617 batch loss 1.10058236 epoch total loss 1.21420085\n",
      "Trained batch 618 batch loss 1.03071809 epoch total loss 1.2139039\n",
      "Trained batch 619 batch loss 1.07363975 epoch total loss 1.21367741\n",
      "Trained batch 620 batch loss 1.05983162 epoch total loss 1.21342921\n",
      "Trained batch 621 batch loss 1.00142384 epoch total loss 1.2130878\n",
      "Trained batch 622 batch loss 1.0445962 epoch total loss 1.21281695\n",
      "Trained batch 623 batch loss 1.0900321 epoch total loss 1.21261978\n",
      "Trained batch 624 batch loss 1.16847563 epoch total loss 1.21254909\n",
      "Trained batch 625 batch loss 1.17341411 epoch total loss 1.21248639\n",
      "Trained batch 626 batch loss 1.24880183 epoch total loss 1.21254444\n",
      "Trained batch 627 batch loss 1.25277805 epoch total loss 1.21260858\n",
      "Trained batch 628 batch loss 1.25124764 epoch total loss 1.21267009\n",
      "Trained batch 629 batch loss 1.25727391 epoch total loss 1.21274102\n",
      "Trained batch 630 batch loss 1.3383857 epoch total loss 1.21294045\n",
      "Trained batch 631 batch loss 1.21259046 epoch total loss 1.21293986\n",
      "Trained batch 632 batch loss 1.30820942 epoch total loss 1.21309066\n",
      "Trained batch 633 batch loss 1.24731588 epoch total loss 1.21314466\n",
      "Trained batch 634 batch loss 1.14337504 epoch total loss 1.21303463\n",
      "Trained batch 635 batch loss 1.18649244 epoch total loss 1.21299279\n",
      "Trained batch 636 batch loss 1.36923289 epoch total loss 1.21323848\n",
      "Trained batch 637 batch loss 1.21984339 epoch total loss 1.21324885\n",
      "Trained batch 638 batch loss 1.31134653 epoch total loss 1.21340263\n",
      "Trained batch 639 batch loss 1.35779345 epoch total loss 1.21362853\n",
      "Trained batch 640 batch loss 1.38197064 epoch total loss 1.21389163\n",
      "Trained batch 641 batch loss 1.30680907 epoch total loss 1.21403658\n",
      "Trained batch 642 batch loss 1.22154832 epoch total loss 1.21404827\n",
      "Trained batch 643 batch loss 1.12221491 epoch total loss 1.21390545\n",
      "Trained batch 644 batch loss 1.22582388 epoch total loss 1.21392393\n",
      "Trained batch 645 batch loss 1.24836636 epoch total loss 1.21397734\n",
      "Trained batch 646 batch loss 1.20871794 epoch total loss 1.21396923\n",
      "Trained batch 647 batch loss 1.2425338 epoch total loss 1.21401346\n",
      "Trained batch 648 batch loss 1.24585223 epoch total loss 1.21406257\n",
      "Trained batch 649 batch loss 1.20546067 epoch total loss 1.21404922\n",
      "Trained batch 650 batch loss 1.28159308 epoch total loss 1.21415317\n",
      "Trained batch 651 batch loss 1.14958787 epoch total loss 1.21405399\n",
      "Trained batch 652 batch loss 1.19675481 epoch total loss 1.21402752\n",
      "Trained batch 653 batch loss 1.29715443 epoch total loss 1.21415484\n",
      "Trained batch 654 batch loss 1.20185065 epoch total loss 1.214136\n",
      "Trained batch 655 batch loss 1.19594991 epoch total loss 1.21410823\n",
      "Trained batch 656 batch loss 1.10131955 epoch total loss 1.21393633\n",
      "Trained batch 657 batch loss 1.14138043 epoch total loss 1.21382582\n",
      "Trained batch 658 batch loss 1.18949568 epoch total loss 1.21378887\n",
      "Trained batch 659 batch loss 1.27161181 epoch total loss 1.21387661\n",
      "Trained batch 660 batch loss 1.27025425 epoch total loss 1.21396208\n",
      "Trained batch 661 batch loss 1.20448256 epoch total loss 1.21394765\n",
      "Trained batch 662 batch loss 1.21433306 epoch total loss 1.21394837\n",
      "Trained batch 663 batch loss 1.22611606 epoch total loss 1.21396673\n",
      "Trained batch 664 batch loss 1.13633299 epoch total loss 1.21384978\n",
      "Trained batch 665 batch loss 1.18824697 epoch total loss 1.21381128\n",
      "Trained batch 666 batch loss 1.21765757 epoch total loss 1.213817\n",
      "Trained batch 667 batch loss 1.34080827 epoch total loss 1.2140075\n",
      "Trained batch 668 batch loss 1.32278132 epoch total loss 1.21417022\n",
      "Trained batch 669 batch loss 1.10300016 epoch total loss 1.21400416\n",
      "Trained batch 670 batch loss 1.16547453 epoch total loss 1.21393168\n",
      "Trained batch 671 batch loss 1.07456446 epoch total loss 1.21372402\n",
      "Trained batch 672 batch loss 1.1679846 epoch total loss 1.21365595\n",
      "Trained batch 673 batch loss 1.13781428 epoch total loss 1.2135433\n",
      "Trained batch 674 batch loss 1.13911855 epoch total loss 1.21343279\n",
      "Trained batch 675 batch loss 1.24764943 epoch total loss 1.21348345\n",
      "Trained batch 676 batch loss 1.23810494 epoch total loss 1.21351981\n",
      "Trained batch 677 batch loss 1.2500838 epoch total loss 1.21357381\n",
      "Trained batch 678 batch loss 1.18574536 epoch total loss 1.21353281\n",
      "Trained batch 679 batch loss 1.17730284 epoch total loss 1.2134794\n",
      "Trained batch 680 batch loss 1.16679859 epoch total loss 1.21341074\n",
      "Trained batch 681 batch loss 1.27989411 epoch total loss 1.21350837\n",
      "Trained batch 682 batch loss 1.2049973 epoch total loss 1.21349597\n",
      "Trained batch 683 batch loss 1.14964342 epoch total loss 1.21340251\n",
      "Trained batch 684 batch loss 1.0832715 epoch total loss 1.21321225\n",
      "Trained batch 685 batch loss 1.11548436 epoch total loss 1.21306956\n",
      "Trained batch 686 batch loss 1.18557572 epoch total loss 1.21302938\n",
      "Trained batch 687 batch loss 1.35065615 epoch total loss 1.21322978\n",
      "Trained batch 688 batch loss 1.2125355 epoch total loss 1.2132287\n",
      "Trained batch 689 batch loss 1.18552256 epoch total loss 1.21318853\n",
      "Trained batch 690 batch loss 1.09670711 epoch total loss 1.21301973\n",
      "Trained batch 691 batch loss 0.991791368 epoch total loss 1.21269953\n",
      "Trained batch 692 batch loss 0.890626788 epoch total loss 1.21223414\n",
      "Trained batch 693 batch loss 1.02077508 epoch total loss 1.21195781\n",
      "Trained batch 694 batch loss 1.08190799 epoch total loss 1.21177042\n",
      "Trained batch 695 batch loss 1.19036222 epoch total loss 1.21173966\n",
      "Trained batch 696 batch loss 1.16581595 epoch total loss 1.21167374\n",
      "Trained batch 697 batch loss 1.04711699 epoch total loss 1.21143758\n",
      "Trained batch 698 batch loss 1.18807697 epoch total loss 1.21140409\n",
      "Trained batch 699 batch loss 1.29681826 epoch total loss 1.21152627\n",
      "Trained batch 700 batch loss 1.30410862 epoch total loss 1.2116586\n",
      "Trained batch 701 batch loss 1.24261081 epoch total loss 1.2117027\n",
      "Trained batch 702 batch loss 1.23549449 epoch total loss 1.21173656\n",
      "Trained batch 703 batch loss 1.25983191 epoch total loss 1.21180499\n",
      "Trained batch 704 batch loss 1.27671099 epoch total loss 1.21189725\n",
      "Trained batch 705 batch loss 1.25270009 epoch total loss 1.21195507\n",
      "Trained batch 706 batch loss 1.13583696 epoch total loss 1.21184731\n",
      "Trained batch 707 batch loss 1.14619637 epoch total loss 1.21175444\n",
      "Trained batch 708 batch loss 1.15382242 epoch total loss 1.21167254\n",
      "Trained batch 709 batch loss 1.18382239 epoch total loss 1.21163332\n",
      "Trained batch 710 batch loss 1.24134076 epoch total loss 1.21167517\n",
      "Trained batch 711 batch loss 1.10139322 epoch total loss 1.21152008\n",
      "Trained batch 712 batch loss 1.13588631 epoch total loss 1.21141374\n",
      "Trained batch 713 batch loss 1.07847011 epoch total loss 1.2112273\n",
      "Trained batch 714 batch loss 1.1165657 epoch total loss 1.21109474\n",
      "Trained batch 715 batch loss 1.16254735 epoch total loss 1.21102691\n",
      "Trained batch 716 batch loss 1.19416118 epoch total loss 1.2110033\n",
      "Trained batch 717 batch loss 1.15112174 epoch total loss 1.21091974\n",
      "Trained batch 718 batch loss 1.18144572 epoch total loss 1.21087873\n",
      "Trained batch 719 batch loss 1.21877396 epoch total loss 1.2108897\n",
      "Trained batch 720 batch loss 1.33745956 epoch total loss 1.21106553\n",
      "Trained batch 721 batch loss 1.33786583 epoch total loss 1.21124136\n",
      "Trained batch 722 batch loss 1.17936265 epoch total loss 1.21119726\n",
      "Trained batch 723 batch loss 1.03528833 epoch total loss 1.21095395\n",
      "Trained batch 724 batch loss 1.05807865 epoch total loss 1.21074283\n",
      "Trained batch 725 batch loss 1.19064248 epoch total loss 1.21071506\n",
      "Trained batch 726 batch loss 1.20159292 epoch total loss 1.21070254\n",
      "Trained batch 727 batch loss 1.19852722 epoch total loss 1.21068585\n",
      "Trained batch 728 batch loss 1.1320771 epoch total loss 1.21057785\n",
      "Trained batch 729 batch loss 1.08201361 epoch total loss 1.21040154\n",
      "Trained batch 730 batch loss 1.12894082 epoch total loss 1.21029\n",
      "Trained batch 731 batch loss 1.34765601 epoch total loss 1.21047783\n",
      "Trained batch 732 batch loss 1.27660668 epoch total loss 1.21056819\n",
      "Trained batch 733 batch loss 1.26955152 epoch total loss 1.21064866\n",
      "Trained batch 734 batch loss 1.0661602 epoch total loss 1.21045184\n",
      "Trained batch 735 batch loss 0.928013086 epoch total loss 1.21006751\n",
      "Trained batch 736 batch loss 0.896180391 epoch total loss 1.2096411\n",
      "Trained batch 737 batch loss 0.951822102 epoch total loss 1.20929134\n",
      "Trained batch 738 batch loss 0.955983281 epoch total loss 1.20894802\n",
      "Trained batch 739 batch loss 1.06585014 epoch total loss 1.20875442\n",
      "Trained batch 740 batch loss 1.10295236 epoch total loss 1.20861149\n",
      "Trained batch 741 batch loss 1.11383677 epoch total loss 1.20848358\n",
      "Trained batch 742 batch loss 1.27642119 epoch total loss 1.20857513\n",
      "Trained batch 743 batch loss 1.23289013 epoch total loss 1.20860791\n",
      "Trained batch 744 batch loss 1.32082772 epoch total loss 1.20875871\n",
      "Trained batch 745 batch loss 1.27622986 epoch total loss 1.20884931\n",
      "Trained batch 746 batch loss 1.26298559 epoch total loss 1.20892191\n",
      "Trained batch 747 batch loss 1.25589454 epoch total loss 1.20898473\n",
      "Trained batch 748 batch loss 1.30423915 epoch total loss 1.20911217\n",
      "Trained batch 749 batch loss 1.05733633 epoch total loss 1.20890951\n",
      "Trained batch 750 batch loss 1.2121408 epoch total loss 1.2089138\n",
      "Trained batch 751 batch loss 1.24729252 epoch total loss 1.20896494\n",
      "Trained batch 752 batch loss 1.32335401 epoch total loss 1.20911705\n",
      "Trained batch 753 batch loss 1.25836158 epoch total loss 1.2091825\n",
      "Trained batch 754 batch loss 1.29455519 epoch total loss 1.20929575\n",
      "Trained batch 755 batch loss 1.3141861 epoch total loss 1.20943463\n",
      "Trained batch 756 batch loss 1.40935636 epoch total loss 1.20969915\n",
      "Trained batch 757 batch loss 1.29652286 epoch total loss 1.20981383\n",
      "Trained batch 758 batch loss 1.15407205 epoch total loss 1.20974028\n",
      "Trained batch 759 batch loss 1.21816552 epoch total loss 1.20975125\n",
      "Trained batch 760 batch loss 1.22500765 epoch total loss 1.20977139\n",
      "Trained batch 761 batch loss 1.2391026 epoch total loss 1.2098099\n",
      "Trained batch 762 batch loss 1.24573612 epoch total loss 1.20985711\n",
      "Trained batch 763 batch loss 1.15183556 epoch total loss 1.20978105\n",
      "Trained batch 764 batch loss 1.24231565 epoch total loss 1.20982361\n",
      "Trained batch 765 batch loss 1.25700569 epoch total loss 1.20988536\n",
      "Trained batch 766 batch loss 1.35245371 epoch total loss 1.21007144\n",
      "Trained batch 767 batch loss 1.18387949 epoch total loss 1.21003735\n",
      "Trained batch 768 batch loss 1.27040374 epoch total loss 1.21011591\n",
      "Trained batch 769 batch loss 1.21207309 epoch total loss 1.21011853\n",
      "Trained batch 770 batch loss 1.18657279 epoch total loss 1.2100879\n",
      "Trained batch 771 batch loss 1.27373469 epoch total loss 1.21017051\n",
      "Trained batch 772 batch loss 1.17007506 epoch total loss 1.21011853\n",
      "Trained batch 773 batch loss 1.19477701 epoch total loss 1.21009874\n",
      "Trained batch 774 batch loss 1.28286695 epoch total loss 1.21019268\n",
      "Trained batch 775 batch loss 1.29394591 epoch total loss 1.2103008\n",
      "Trained batch 776 batch loss 1.26170623 epoch total loss 1.21036708\n",
      "Trained batch 777 batch loss 1.34005737 epoch total loss 1.21053386\n",
      "Trained batch 778 batch loss 1.32169843 epoch total loss 1.21067679\n",
      "Trained batch 779 batch loss 1.29886901 epoch total loss 1.21079\n",
      "Trained batch 780 batch loss 1.25927711 epoch total loss 1.21085227\n",
      "Trained batch 781 batch loss 1.06593931 epoch total loss 1.21066666\n",
      "Trained batch 782 batch loss 1.264588 epoch total loss 1.21073556\n",
      "Trained batch 783 batch loss 1.22210717 epoch total loss 1.2107501\n",
      "Trained batch 784 batch loss 1.40226209 epoch total loss 1.21099436\n",
      "Trained batch 785 batch loss 1.49310875 epoch total loss 1.21135378\n",
      "Trained batch 786 batch loss 1.34658 epoch total loss 1.2115258\n",
      "Trained batch 787 batch loss 1.26666808 epoch total loss 1.21159589\n",
      "Trained batch 788 batch loss 1.35558152 epoch total loss 1.21177864\n",
      "Trained batch 789 batch loss 1.15517366 epoch total loss 1.21170688\n",
      "Trained batch 790 batch loss 1.11319542 epoch total loss 1.21158218\n",
      "Trained batch 791 batch loss 1.21471357 epoch total loss 1.21158612\n",
      "Trained batch 792 batch loss 1.29963124 epoch total loss 1.21169722\n",
      "Trained batch 793 batch loss 1.26927805 epoch total loss 1.21176994\n",
      "Trained batch 794 batch loss 1.21879816 epoch total loss 1.21177876\n",
      "Trained batch 795 batch loss 1.15838063 epoch total loss 1.21171165\n",
      "Trained batch 796 batch loss 1.074579 epoch total loss 1.21153939\n",
      "Trained batch 797 batch loss 1.13902688 epoch total loss 1.21144843\n",
      "Trained batch 798 batch loss 1.02571559 epoch total loss 1.21121562\n",
      "Trained batch 799 batch loss 1.09639525 epoch total loss 1.21107185\n",
      "Trained batch 800 batch loss 1.13931465 epoch total loss 1.2109822\n",
      "Trained batch 801 batch loss 1.1402508 epoch total loss 1.21089387\n",
      "Trained batch 802 batch loss 1.17105269 epoch total loss 1.21084428\n",
      "Trained batch 803 batch loss 1.18119371 epoch total loss 1.21080732\n",
      "Trained batch 804 batch loss 1.19768012 epoch total loss 1.21079111\n",
      "Trained batch 805 batch loss 1.26912427 epoch total loss 1.21086347\n",
      "Trained batch 806 batch loss 1.23900843 epoch total loss 1.2108984\n",
      "Trained batch 807 batch loss 1.34869409 epoch total loss 1.21106923\n",
      "Trained batch 808 batch loss 1.42654419 epoch total loss 1.2113359\n",
      "Trained batch 809 batch loss 1.33549273 epoch total loss 1.21148944\n",
      "Trained batch 810 batch loss 1.22860694 epoch total loss 1.21151042\n",
      "Trained batch 811 batch loss 1.20723391 epoch total loss 1.21150517\n",
      "Trained batch 812 batch loss 1.16601872 epoch total loss 1.21144915\n",
      "Trained batch 813 batch loss 1.30630445 epoch total loss 1.21156585\n",
      "Trained batch 814 batch loss 1.27334511 epoch total loss 1.21164167\n",
      "Trained batch 815 batch loss 1.40409946 epoch total loss 1.21187782\n",
      "Trained batch 816 batch loss 1.51056254 epoch total loss 1.2122438\n",
      "Trained batch 817 batch loss 1.24753177 epoch total loss 1.21228707\n",
      "Trained batch 818 batch loss 1.32137048 epoch total loss 1.21242034\n",
      "Trained batch 819 batch loss 1.26643813 epoch total loss 1.21248639\n",
      "Trained batch 820 batch loss 1.24115574 epoch total loss 1.21252131\n",
      "Trained batch 821 batch loss 1.30158901 epoch total loss 1.2126298\n",
      "Trained batch 822 batch loss 1.31805944 epoch total loss 1.21275806\n",
      "Trained batch 823 batch loss 1.14494276 epoch total loss 1.21267557\n",
      "Trained batch 824 batch loss 1.09848118 epoch total loss 1.21253705\n",
      "Trained batch 825 batch loss 1.20937824 epoch total loss 1.21253324\n",
      "Trained batch 826 batch loss 1.25390899 epoch total loss 1.2125833\n",
      "Trained batch 827 batch loss 1.15433657 epoch total loss 1.21251285\n",
      "Trained batch 828 batch loss 1.28151512 epoch total loss 1.21259618\n",
      "Trained batch 829 batch loss 1.35273206 epoch total loss 1.21276522\n",
      "Trained batch 830 batch loss 1.2186904 epoch total loss 1.21277237\n",
      "Trained batch 831 batch loss 1.28601742 epoch total loss 1.21286047\n",
      "Trained batch 832 batch loss 1.29658031 epoch total loss 1.21296108\n",
      "Trained batch 833 batch loss 1.37128067 epoch total loss 1.21315122\n",
      "Trained batch 834 batch loss 1.3824625 epoch total loss 1.21335411\n",
      "Trained batch 835 batch loss 1.09409702 epoch total loss 1.21321142\n",
      "Trained batch 836 batch loss 1.32787323 epoch total loss 1.21334851\n",
      "Trained batch 837 batch loss 1.23851299 epoch total loss 1.21337867\n",
      "Trained batch 838 batch loss 1.26049495 epoch total loss 1.21343482\n",
      "Trained batch 839 batch loss 1.34915066 epoch total loss 1.21359658\n",
      "Trained batch 840 batch loss 1.24410009 epoch total loss 1.21363282\n",
      "Trained batch 841 batch loss 1.23541439 epoch total loss 1.21365881\n",
      "Trained batch 842 batch loss 1.26976657 epoch total loss 1.21372545\n",
      "Trained batch 843 batch loss 1.2209034 epoch total loss 1.21373391\n",
      "Trained batch 844 batch loss 1.26287472 epoch total loss 1.21379209\n",
      "Trained batch 845 batch loss 1.24481964 epoch total loss 1.21382892\n",
      "Trained batch 846 batch loss 1.19298756 epoch total loss 1.21380424\n",
      "Trained batch 847 batch loss 1.26880741 epoch total loss 1.21386921\n",
      "Trained batch 848 batch loss 1.22267461 epoch total loss 1.21387959\n",
      "Trained batch 849 batch loss 1.31710744 epoch total loss 1.21400118\n",
      "Trained batch 850 batch loss 1.26803303 epoch total loss 1.21406484\n",
      "Trained batch 851 batch loss 1.35879183 epoch total loss 1.21423483\n",
      "Trained batch 852 batch loss 1.29163146 epoch total loss 1.21432567\n",
      "Trained batch 853 batch loss 1.25228333 epoch total loss 1.21437025\n",
      "Trained batch 854 batch loss 1.22442472 epoch total loss 1.21438193\n",
      "Trained batch 855 batch loss 1.17538702 epoch total loss 1.2143364\n",
      "Trained batch 856 batch loss 1.15626442 epoch total loss 1.21426845\n",
      "Trained batch 857 batch loss 1.16337538 epoch total loss 1.21420908\n",
      "Trained batch 858 batch loss 1.20314956 epoch total loss 1.21419609\n",
      "Trained batch 859 batch loss 1.14561939 epoch total loss 1.21411633\n",
      "Trained batch 860 batch loss 1.1022383 epoch total loss 1.21398628\n",
      "Trained batch 861 batch loss 1.02569175 epoch total loss 1.21376753\n",
      "Trained batch 862 batch loss 1.1311059 epoch total loss 1.21367157\n",
      "Trained batch 863 batch loss 1.23545969 epoch total loss 1.21369684\n",
      "Trained batch 864 batch loss 1.22624552 epoch total loss 1.21371138\n",
      "Trained batch 865 batch loss 1.21478105 epoch total loss 1.21371257\n",
      "Trained batch 866 batch loss 1.19171834 epoch total loss 1.21368718\n",
      "Trained batch 867 batch loss 1.25921655 epoch total loss 1.21373975\n",
      "Trained batch 868 batch loss 1.1363337 epoch total loss 1.21365058\n",
      "Trained batch 869 batch loss 1.32471812 epoch total loss 1.21377838\n",
      "Trained batch 870 batch loss 1.2250526 epoch total loss 1.21379137\n",
      "Trained batch 871 batch loss 1.38331294 epoch total loss 1.21398604\n",
      "Trained batch 872 batch loss 1.29288459 epoch total loss 1.21407652\n",
      "Trained batch 873 batch loss 1.31550217 epoch total loss 1.21419275\n",
      "Trained batch 874 batch loss 1.32559299 epoch total loss 1.21432018\n",
      "Trained batch 875 batch loss 1.22157335 epoch total loss 1.21432841\n",
      "Trained batch 876 batch loss 1.25293136 epoch total loss 1.21437252\n",
      "Trained batch 877 batch loss 1.19673312 epoch total loss 1.21435237\n",
      "Trained batch 878 batch loss 1.18279028 epoch total loss 1.21431637\n",
      "Trained batch 879 batch loss 1.26875973 epoch total loss 1.21437836\n",
      "Trained batch 880 batch loss 1.40327644 epoch total loss 1.21459305\n",
      "Trained batch 881 batch loss 1.20178139 epoch total loss 1.21457851\n",
      "Trained batch 882 batch loss 1.25330758 epoch total loss 1.2146225\n",
      "Trained batch 883 batch loss 1.10634017 epoch total loss 1.21449983\n",
      "Trained batch 884 batch loss 1.06257927 epoch total loss 1.21432793\n",
      "Trained batch 885 batch loss 1.17571604 epoch total loss 1.2142843\n",
      "Trained batch 886 batch loss 1.27849901 epoch total loss 1.21435666\n",
      "Trained batch 887 batch loss 1.15588546 epoch total loss 1.21429074\n",
      "Trained batch 888 batch loss 1.27629924 epoch total loss 1.21436059\n",
      "Trained batch 889 batch loss 1.28294706 epoch total loss 1.21443772\n",
      "Trained batch 890 batch loss 1.18931758 epoch total loss 1.21440947\n",
      "Trained batch 891 batch loss 1.16312778 epoch total loss 1.21435189\n",
      "Trained batch 892 batch loss 1.05623758 epoch total loss 1.21417475\n",
      "Trained batch 893 batch loss 1.00352335 epoch total loss 1.21393883\n",
      "Trained batch 894 batch loss 1.18549037 epoch total loss 1.213907\n",
      "Trained batch 895 batch loss 1.2035439 epoch total loss 1.21389544\n",
      "Trained batch 896 batch loss 1.21188307 epoch total loss 1.21389318\n",
      "Trained batch 897 batch loss 1.17278337 epoch total loss 1.21384728\n",
      "Trained batch 898 batch loss 1.32552779 epoch total loss 1.21397173\n",
      "Trained batch 899 batch loss 1.25624251 epoch total loss 1.2140187\n",
      "Trained batch 900 batch loss 1.16196334 epoch total loss 1.21396089\n",
      "Trained batch 901 batch loss 1.26823449 epoch total loss 1.21402109\n",
      "Trained batch 902 batch loss 1.12211466 epoch total loss 1.21391916\n",
      "Trained batch 903 batch loss 1.2691834 epoch total loss 1.21398032\n",
      "Trained batch 904 batch loss 1.01037598 epoch total loss 1.21375513\n",
      "Trained batch 905 batch loss 0.955094099 epoch total loss 1.21346927\n",
      "Trained batch 906 batch loss 0.938526511 epoch total loss 1.21316576\n",
      "Trained batch 907 batch loss 1.0852356 epoch total loss 1.21302462\n",
      "Trained batch 908 batch loss 1.32158184 epoch total loss 1.21314418\n",
      "Trained batch 909 batch loss 1.46636569 epoch total loss 1.21342266\n",
      "Trained batch 910 batch loss 1.35540915 epoch total loss 1.21357882\n",
      "Trained batch 911 batch loss 1.32136869 epoch total loss 1.2136972\n",
      "Trained batch 912 batch loss 1.25068498 epoch total loss 1.21373773\n",
      "Trained batch 913 batch loss 1.17800701 epoch total loss 1.21369863\n",
      "Trained batch 914 batch loss 1.29679799 epoch total loss 1.21378946\n",
      "Trained batch 915 batch loss 1.21479714 epoch total loss 1.21379066\n",
      "Trained batch 916 batch loss 1.26110578 epoch total loss 1.21384227\n",
      "Trained batch 917 batch loss 1.2213378 epoch total loss 1.21385038\n",
      "Trained batch 918 batch loss 1.2498039 epoch total loss 1.21388948\n",
      "Trained batch 919 batch loss 1.28257751 epoch total loss 1.21396434\n",
      "Trained batch 920 batch loss 1.1196239 epoch total loss 1.2138617\n",
      "Trained batch 921 batch loss 1.0735395 epoch total loss 1.21370935\n",
      "Trained batch 922 batch loss 1.14215183 epoch total loss 1.21363175\n",
      "Trained batch 923 batch loss 1.13999271 epoch total loss 1.213552\n",
      "Trained batch 924 batch loss 1.1223532 epoch total loss 1.21345329\n",
      "Trained batch 925 batch loss 1.16101122 epoch total loss 1.21339655\n",
      "Trained batch 926 batch loss 1.22492564 epoch total loss 1.21340907\n",
      "Trained batch 927 batch loss 1.25161541 epoch total loss 1.21345031\n",
      "Trained batch 928 batch loss 1.22705698 epoch total loss 1.21346498\n",
      "Trained batch 929 batch loss 1.26333642 epoch total loss 1.21351862\n",
      "Trained batch 930 batch loss 1.21436405 epoch total loss 1.21351945\n",
      "Trained batch 931 batch loss 1.40732539 epoch total loss 1.21372771\n",
      "Trained batch 932 batch loss 1.49733758 epoch total loss 1.21403193\n",
      "Trained batch 933 batch loss 1.25651515 epoch total loss 1.21407747\n",
      "Trained batch 934 batch loss 1.13222766 epoch total loss 1.21398973\n",
      "Trained batch 935 batch loss 1.23362505 epoch total loss 1.21401083\n",
      "Trained batch 936 batch loss 1.30597401 epoch total loss 1.21410906\n",
      "Trained batch 937 batch loss 1.38969886 epoch total loss 1.21429646\n",
      "Trained batch 938 batch loss 1.38519 epoch total loss 1.21447861\n",
      "Trained batch 939 batch loss 1.49934769 epoch total loss 1.214782\n",
      "Trained batch 940 batch loss 1.30714905 epoch total loss 1.21488023\n",
      "Trained batch 941 batch loss 1.15460861 epoch total loss 1.21481621\n",
      "Trained batch 942 batch loss 1.12902737 epoch total loss 1.21472514\n",
      "Trained batch 943 batch loss 1.232131 epoch total loss 1.21474373\n",
      "Trained batch 944 batch loss 1.16125751 epoch total loss 1.21468699\n",
      "Trained batch 945 batch loss 1.08797848 epoch total loss 1.214553\n",
      "Trained batch 946 batch loss 1.12502599 epoch total loss 1.21445835\n",
      "Trained batch 947 batch loss 1.2052958 epoch total loss 1.21444869\n",
      "Trained batch 948 batch loss 0.982932627 epoch total loss 1.21420443\n",
      "Trained batch 949 batch loss 1.02128744 epoch total loss 1.21400106\n",
      "Trained batch 950 batch loss 1.06714094 epoch total loss 1.21384645\n",
      "Trained batch 951 batch loss 1.15599227 epoch total loss 1.21378565\n",
      "Trained batch 952 batch loss 1.14977121 epoch total loss 1.21371841\n",
      "Trained batch 953 batch loss 0.963846684 epoch total loss 1.21345627\n",
      "Trained batch 954 batch loss 1.10522759 epoch total loss 1.21334279\n",
      "Trained batch 955 batch loss 1.1562475 epoch total loss 1.21328306\n",
      "Trained batch 956 batch loss 1.16369939 epoch total loss 1.21323121\n",
      "Trained batch 957 batch loss 1.20866561 epoch total loss 1.21322632\n",
      "Trained batch 958 batch loss 1.10495257 epoch total loss 1.21311331\n",
      "Trained batch 959 batch loss 1.15000105 epoch total loss 1.21304762\n",
      "Trained batch 960 batch loss 1.26453567 epoch total loss 1.21310115\n",
      "Trained batch 961 batch loss 1.2647481 epoch total loss 1.21315491\n",
      "Trained batch 962 batch loss 1.29893732 epoch total loss 1.2132442\n",
      "Trained batch 963 batch loss 1.18847966 epoch total loss 1.21321845\n",
      "Trained batch 964 batch loss 1.19592512 epoch total loss 1.21320045\n",
      "Trained batch 965 batch loss 1.13014042 epoch total loss 1.21311438\n",
      "Trained batch 966 batch loss 1.22909701 epoch total loss 1.21313095\n",
      "Trained batch 967 batch loss 1.29706442 epoch total loss 1.21321785\n",
      "Trained batch 968 batch loss 1.24085295 epoch total loss 1.21324635\n",
      "Trained batch 969 batch loss 1.15813971 epoch total loss 1.21318948\n",
      "Trained batch 970 batch loss 1.20073533 epoch total loss 1.21317649\n",
      "Trained batch 971 batch loss 1.15174 epoch total loss 1.21311331\n",
      "Trained batch 972 batch loss 1.11642671 epoch total loss 1.21301377\n",
      "Trained batch 973 batch loss 1.22087932 epoch total loss 1.21302187\n",
      "Trained batch 974 batch loss 1.21540928 epoch total loss 1.21302438\n",
      "Trained batch 975 batch loss 1.0756824 epoch total loss 1.21288347\n",
      "Trained batch 976 batch loss 1.19296885 epoch total loss 1.21286309\n",
      "Trained batch 977 batch loss 1.17942011 epoch total loss 1.21282887\n",
      "Trained batch 978 batch loss 1.34692252 epoch total loss 1.21296597\n",
      "Trained batch 979 batch loss 1.20712519 epoch total loss 1.21296012\n",
      "Trained batch 980 batch loss 1.35668707 epoch total loss 1.21310675\n",
      "Trained batch 981 batch loss 1.33719134 epoch total loss 1.21323323\n",
      "Trained batch 982 batch loss 1.24765229 epoch total loss 1.21326828\n",
      "Trained batch 983 batch loss 1.17287409 epoch total loss 1.21322715\n",
      "Trained batch 984 batch loss 1.19789374 epoch total loss 1.21321154\n",
      "Trained batch 985 batch loss 1.22965777 epoch total loss 1.21322823\n",
      "Trained batch 986 batch loss 1.35195589 epoch total loss 1.21336889\n",
      "Trained batch 987 batch loss 1.31310797 epoch total loss 1.21347\n",
      "Trained batch 988 batch loss 1.21286726 epoch total loss 1.21346939\n",
      "Trained batch 989 batch loss 1.22602487 epoch total loss 1.21348214\n",
      "Trained batch 990 batch loss 1.23892522 epoch total loss 1.21350777\n",
      "Trained batch 991 batch loss 1.26425099 epoch total loss 1.21355903\n",
      "Trained batch 992 batch loss 1.34659982 epoch total loss 1.21369302\n",
      "Trained batch 993 batch loss 1.19275379 epoch total loss 1.21367192\n",
      "Trained batch 994 batch loss 1.336128 epoch total loss 1.21379519\n",
      "Trained batch 995 batch loss 1.29227376 epoch total loss 1.2138741\n",
      "Trained batch 996 batch loss 1.1408422 epoch total loss 1.21380079\n",
      "Trained batch 997 batch loss 1.10500884 epoch total loss 1.21369159\n",
      "Trained batch 998 batch loss 1.21118891 epoch total loss 1.21368909\n",
      "Trained batch 999 batch loss 1.22173452 epoch total loss 1.21369708\n",
      "Trained batch 1000 batch loss 1.09364235 epoch total loss 1.21357703\n",
      "Trained batch 1001 batch loss 1.1464622 epoch total loss 1.21351\n",
      "Trained batch 1002 batch loss 1.11878455 epoch total loss 1.2134155\n",
      "Trained batch 1003 batch loss 1.01982343 epoch total loss 1.21322238\n",
      "Trained batch 1004 batch loss 1.12772906 epoch total loss 1.21313715\n",
      "Trained batch 1005 batch loss 1.12395406 epoch total loss 1.21304846\n",
      "Trained batch 1006 batch loss 1.23645258 epoch total loss 1.2130717\n",
      "Trained batch 1007 batch loss 1.24223113 epoch total loss 1.21310055\n",
      "Trained batch 1008 batch loss 1.12547636 epoch total loss 1.21301365\n",
      "Trained batch 1009 batch loss 1.24443018 epoch total loss 1.21304476\n",
      "Trained batch 1010 batch loss 1.20596731 epoch total loss 1.21303773\n",
      "Trained batch 1011 batch loss 1.3554163 epoch total loss 1.21317863\n",
      "Trained batch 1012 batch loss 1.40946579 epoch total loss 1.21337247\n",
      "Trained batch 1013 batch loss 1.41706276 epoch total loss 1.21357369\n",
      "Trained batch 1014 batch loss 1.29731703 epoch total loss 1.21365631\n",
      "Trained batch 1015 batch loss 1.10196137 epoch total loss 1.21354616\n",
      "Trained batch 1016 batch loss 1.24933171 epoch total loss 1.21358144\n",
      "Trained batch 1017 batch loss 1.37565649 epoch total loss 1.21374083\n",
      "Trained batch 1018 batch loss 1.42889869 epoch total loss 1.21395218\n",
      "Trained batch 1019 batch loss 1.27283156 epoch total loss 1.21401\n",
      "Trained batch 1020 batch loss 1.16705298 epoch total loss 1.21396387\n",
      "Trained batch 1021 batch loss 1.16213274 epoch total loss 1.21391308\n",
      "Trained batch 1022 batch loss 1.15641451 epoch total loss 1.21385682\n",
      "Trained batch 1023 batch loss 1.12828875 epoch total loss 1.21377313\n",
      "Trained batch 1024 batch loss 1.15039515 epoch total loss 1.21371126\n",
      "Trained batch 1025 batch loss 1.18397427 epoch total loss 1.21368229\n",
      "Trained batch 1026 batch loss 1.27567148 epoch total loss 1.21374261\n",
      "Trained batch 1027 batch loss 1.27407789 epoch total loss 1.21380138\n",
      "Trained batch 1028 batch loss 1.29730487 epoch total loss 1.21388257\n",
      "Trained batch 1029 batch loss 1.23388457 epoch total loss 1.21390212\n",
      "Trained batch 1030 batch loss 1.21251905 epoch total loss 1.21390069\n",
      "Trained batch 1031 batch loss 1.11111283 epoch total loss 1.21380103\n",
      "Trained batch 1032 batch loss 1.13645744 epoch total loss 1.21372604\n",
      "Trained batch 1033 batch loss 1.13207221 epoch total loss 1.21364701\n",
      "Trained batch 1034 batch loss 1.22670925 epoch total loss 1.21365964\n",
      "Trained batch 1035 batch loss 1.30668187 epoch total loss 1.21374953\n",
      "Trained batch 1036 batch loss 1.19000268 epoch total loss 1.21372664\n",
      "Trained batch 1037 batch loss 1.22954941 epoch total loss 1.21374178\n",
      "Trained batch 1038 batch loss 1.17655921 epoch total loss 1.2137059\n",
      "Trained batch 1039 batch loss 1.27730894 epoch total loss 1.21376717\n",
      "Trained batch 1040 batch loss 1.16241217 epoch total loss 1.21371782\n",
      "Trained batch 1041 batch loss 1.14103556 epoch total loss 1.21364796\n",
      "Trained batch 1042 batch loss 1.22562373 epoch total loss 1.21365941\n",
      "Trained batch 1043 batch loss 1.10103583 epoch total loss 1.2135514\n",
      "Trained batch 1044 batch loss 1.16901779 epoch total loss 1.21350884\n",
      "Trained batch 1045 batch loss 1.30317116 epoch total loss 1.21359468\n",
      "Trained batch 1046 batch loss 1.23044097 epoch total loss 1.21361077\n",
      "Trained batch 1047 batch loss 1.17049 epoch total loss 1.21356964\n",
      "Trained batch 1048 batch loss 1.14371204 epoch total loss 1.213503\n",
      "Trained batch 1049 batch loss 1.13081741 epoch total loss 1.21342421\n",
      "Trained batch 1050 batch loss 1.09270036 epoch total loss 1.21330917\n",
      "Trained batch 1051 batch loss 1.17826092 epoch total loss 1.21327579\n",
      "Trained batch 1052 batch loss 1.20284474 epoch total loss 1.2132659\n",
      "Trained batch 1053 batch loss 1.25641382 epoch total loss 1.2133069\n",
      "Trained batch 1054 batch loss 1.21094227 epoch total loss 1.21330464\n",
      "Trained batch 1055 batch loss 1.25638115 epoch total loss 1.21334541\n",
      "Trained batch 1056 batch loss 1.27887571 epoch total loss 1.21340752\n",
      "Trained batch 1057 batch loss 1.3308363 epoch total loss 1.21351862\n",
      "Trained batch 1058 batch loss 1.15372145 epoch total loss 1.21346211\n",
      "Trained batch 1059 batch loss 1.08056974 epoch total loss 1.21333659\n",
      "Trained batch 1060 batch loss 1.21301436 epoch total loss 1.21333635\n",
      "Trained batch 1061 batch loss 1.10323143 epoch total loss 1.21323252\n",
      "Trained batch 1062 batch loss 1.12617731 epoch total loss 1.21315062\n",
      "Trained batch 1063 batch loss 1.1944114 epoch total loss 1.2131331\n",
      "Trained batch 1064 batch loss 1.25299501 epoch total loss 1.21317053\n",
      "Trained batch 1065 batch loss 1.13565075 epoch total loss 1.21309769\n",
      "Trained batch 1066 batch loss 1.24511623 epoch total loss 1.21312773\n",
      "Trained batch 1067 batch loss 1.16456735 epoch total loss 1.21308219\n",
      "Trained batch 1068 batch loss 1.29816484 epoch total loss 1.21316195\n",
      "Trained batch 1069 batch loss 1.54357183 epoch total loss 1.21347106\n",
      "Trained batch 1070 batch loss 1.28290236 epoch total loss 1.21353602\n",
      "Trained batch 1071 batch loss 1.34826779 epoch total loss 1.21366179\n",
      "Trained batch 1072 batch loss 1.2029202 epoch total loss 1.21365178\n",
      "Trained batch 1073 batch loss 1.3521651 epoch total loss 1.21378088\n",
      "Trained batch 1074 batch loss 1.25518346 epoch total loss 1.21381938\n",
      "Trained batch 1075 batch loss 1.22376764 epoch total loss 1.21382856\n",
      "Trained batch 1076 batch loss 1.26008523 epoch total loss 1.2138716\n",
      "Trained batch 1077 batch loss 1.1573199 epoch total loss 1.21381915\n",
      "Trained batch 1078 batch loss 1.20182323 epoch total loss 1.21380794\n",
      "Trained batch 1079 batch loss 1.18429959 epoch total loss 1.21378064\n",
      "Trained batch 1080 batch loss 1.24044549 epoch total loss 1.21380532\n",
      "Trained batch 1081 batch loss 1.22901547 epoch total loss 1.21381938\n",
      "Trained batch 1082 batch loss 1.16626132 epoch total loss 1.21377552\n",
      "Trained batch 1083 batch loss 1.14227009 epoch total loss 1.21370935\n",
      "Trained batch 1084 batch loss 1.03904426 epoch total loss 1.2135483\n",
      "Trained batch 1085 batch loss 0.996379137 epoch total loss 1.21334803\n",
      "Trained batch 1086 batch loss 1.11664104 epoch total loss 1.2132591\n",
      "Trained batch 1087 batch loss 1.20720887 epoch total loss 1.2132535\n",
      "Trained batch 1088 batch loss 1.31156445 epoch total loss 1.21334374\n",
      "Trained batch 1089 batch loss 1.27506483 epoch total loss 1.21340048\n",
      "Trained batch 1090 batch loss 1.28508449 epoch total loss 1.21346617\n",
      "Trained batch 1091 batch loss 1.17003131 epoch total loss 1.21342635\n",
      "Trained batch 1092 batch loss 1.14467585 epoch total loss 1.21336341\n",
      "Trained batch 1093 batch loss 1.08749413 epoch total loss 1.21324825\n",
      "Trained batch 1094 batch loss 1.15133524 epoch total loss 1.21319163\n",
      "Trained batch 1095 batch loss 1.19462681 epoch total loss 1.2131747\n",
      "Trained batch 1096 batch loss 1.14058161 epoch total loss 1.21310854\n",
      "Trained batch 1097 batch loss 1.31948411 epoch total loss 1.21320546\n",
      "Trained batch 1098 batch loss 1.3600657 epoch total loss 1.21333921\n",
      "Trained batch 1099 batch loss 1.23246121 epoch total loss 1.21335661\n",
      "Trained batch 1100 batch loss 1.24745512 epoch total loss 1.21338761\n",
      "Trained batch 1101 batch loss 1.23508978 epoch total loss 1.21340728\n",
      "Trained batch 1102 batch loss 1.41604805 epoch total loss 1.2135911\n",
      "Trained batch 1103 batch loss 1.31422305 epoch total loss 1.21368241\n",
      "Trained batch 1104 batch loss 1.33745813 epoch total loss 1.21379447\n",
      "Trained batch 1105 batch loss 1.32431173 epoch total loss 1.21389449\n",
      "Trained batch 1106 batch loss 1.25512671 epoch total loss 1.2139318\n",
      "Trained batch 1107 batch loss 1.25051975 epoch total loss 1.21396482\n",
      "Trained batch 1108 batch loss 1.21813655 epoch total loss 1.21396852\n",
      "Trained batch 1109 batch loss 1.20564938 epoch total loss 1.21396112\n",
      "Trained batch 1110 batch loss 1.23197412 epoch total loss 1.21397722\n",
      "Trained batch 1111 batch loss 1.30277014 epoch total loss 1.21405721\n",
      "Trained batch 1112 batch loss 1.12945151 epoch total loss 1.21398103\n",
      "Trained batch 1113 batch loss 1.23723936 epoch total loss 1.21400189\n",
      "Trained batch 1114 batch loss 1.08644128 epoch total loss 1.21388733\n",
      "Trained batch 1115 batch loss 1.14365399 epoch total loss 1.21382439\n",
      "Trained batch 1116 batch loss 1.15797305 epoch total loss 1.21377432\n",
      "Trained batch 1117 batch loss 1.17710876 epoch total loss 1.21374154\n",
      "Trained batch 1118 batch loss 1.07678306 epoch total loss 1.21361899\n",
      "Trained batch 1119 batch loss 1.32498944 epoch total loss 1.21371853\n",
      "Trained batch 1120 batch loss 1.44058967 epoch total loss 1.21392107\n",
      "Trained batch 1121 batch loss 1.45738792 epoch total loss 1.21413827\n",
      "Trained batch 1122 batch loss 1.29636312 epoch total loss 1.21421158\n",
      "Trained batch 1123 batch loss 1.31564081 epoch total loss 1.21430182\n",
      "Trained batch 1124 batch loss 1.10698128 epoch total loss 1.21420634\n",
      "Trained batch 1125 batch loss 1.05224502 epoch total loss 1.21406233\n",
      "Trained batch 1126 batch loss 0.9651829 epoch total loss 1.21384144\n",
      "Trained batch 1127 batch loss 0.961100459 epoch total loss 1.21361709\n",
      "Trained batch 1128 batch loss 1.25814891 epoch total loss 1.21365654\n",
      "Trained batch 1129 batch loss 1.22346354 epoch total loss 1.21366537\n",
      "Trained batch 1130 batch loss 1.20201337 epoch total loss 1.21365499\n",
      "Trained batch 1131 batch loss 1.21077061 epoch total loss 1.21365249\n",
      "Trained batch 1132 batch loss 1.21106887 epoch total loss 1.21365023\n",
      "Trained batch 1133 batch loss 1.18759751 epoch total loss 1.21362722\n",
      "Trained batch 1134 batch loss 1.21546733 epoch total loss 1.21362889\n",
      "Trained batch 1135 batch loss 1.24553585 epoch total loss 1.2136569\n",
      "Trained batch 1136 batch loss 1.30459988 epoch total loss 1.21373689\n",
      "Trained batch 1137 batch loss 1.28184164 epoch total loss 1.21379685\n",
      "Trained batch 1138 batch loss 1.17875075 epoch total loss 1.21376598\n",
      "Trained batch 1139 batch loss 1.21351528 epoch total loss 1.21376586\n",
      "Trained batch 1140 batch loss 1.28056669 epoch total loss 1.21382439\n",
      "Trained batch 1141 batch loss 1.11300123 epoch total loss 1.21373606\n",
      "Trained batch 1142 batch loss 1.24580801 epoch total loss 1.21376419\n",
      "Trained batch 1143 batch loss 1.0568825 epoch total loss 1.21362686\n",
      "Trained batch 1144 batch loss 0.999500394 epoch total loss 1.2134397\n",
      "Trained batch 1145 batch loss 1.11527872 epoch total loss 1.21335399\n",
      "Trained batch 1146 batch loss 1.10474491 epoch total loss 1.21325922\n",
      "Trained batch 1147 batch loss 1.07027578 epoch total loss 1.21313453\n",
      "Trained batch 1148 batch loss 1.07508755 epoch total loss 1.21301425\n",
      "Trained batch 1149 batch loss 1.088943 epoch total loss 1.21290636\n",
      "Trained batch 1150 batch loss 1.16097426 epoch total loss 1.21286118\n",
      "Trained batch 1151 batch loss 1.18486357 epoch total loss 1.21283686\n",
      "Trained batch 1152 batch loss 1.15278649 epoch total loss 1.21278477\n",
      "Trained batch 1153 batch loss 1.00475943 epoch total loss 1.2126044\n",
      "Trained batch 1154 batch loss 1.09357572 epoch total loss 1.21250129\n",
      "Trained batch 1155 batch loss 1.07563829 epoch total loss 1.21238279\n",
      "Trained batch 1156 batch loss 1.16689503 epoch total loss 1.21234345\n",
      "Trained batch 1157 batch loss 0.970127702 epoch total loss 1.212134\n",
      "Trained batch 1158 batch loss 1.08437598 epoch total loss 1.21202374\n",
      "Trained batch 1159 batch loss 1.12786365 epoch total loss 1.21195102\n",
      "Trained batch 1160 batch loss 1.13133073 epoch total loss 1.21188152\n",
      "Trained batch 1161 batch loss 1.08163798 epoch total loss 1.21176934\n",
      "Trained batch 1162 batch loss 1.08259273 epoch total loss 1.21165824\n",
      "Trained batch 1163 batch loss 1.22452521 epoch total loss 1.21166933\n",
      "Trained batch 1164 batch loss 1.26868129 epoch total loss 1.21171832\n",
      "Trained batch 1165 batch loss 1.20839977 epoch total loss 1.21171534\n",
      "Trained batch 1166 batch loss 1.31312215 epoch total loss 1.21180236\n",
      "Trained batch 1167 batch loss 1.21351838 epoch total loss 1.21180379\n",
      "Trained batch 1168 batch loss 1.26081204 epoch total loss 1.21184587\n",
      "Trained batch 1169 batch loss 1.10035717 epoch total loss 1.21175039\n",
      "Trained batch 1170 batch loss 1.17570055 epoch total loss 1.21171963\n",
      "Trained batch 1171 batch loss 1.11350739 epoch total loss 1.21163571\n",
      "Trained batch 1172 batch loss 1.40875149 epoch total loss 1.21180391\n",
      "Trained batch 1173 batch loss 1.25961196 epoch total loss 1.21184468\n",
      "Trained batch 1174 batch loss 1.24611747 epoch total loss 1.21187377\n",
      "Trained batch 1175 batch loss 1.29348707 epoch total loss 1.21194327\n",
      "Trained batch 1176 batch loss 1.15085554 epoch total loss 1.21189129\n",
      "Trained batch 1177 batch loss 1.21208847 epoch total loss 1.21189141\n",
      "Trained batch 1178 batch loss 1.19911408 epoch total loss 1.21188056\n",
      "Trained batch 1179 batch loss 1.07064986 epoch total loss 1.21176088\n",
      "Trained batch 1180 batch loss 1.0148983 epoch total loss 1.21159399\n",
      "Trained batch 1181 batch loss 1.08713102 epoch total loss 1.2114886\n",
      "Trained batch 1182 batch loss 1.19289172 epoch total loss 1.21147287\n",
      "Trained batch 1183 batch loss 1.05586 epoch total loss 1.21134138\n",
      "Trained batch 1184 batch loss 1.05537498 epoch total loss 1.21120965\n",
      "Trained batch 1185 batch loss 1.11463034 epoch total loss 1.21112823\n",
      "Trained batch 1186 batch loss 1.21108353 epoch total loss 1.21112812\n",
      "Trained batch 1187 batch loss 1.17263484 epoch total loss 1.21109569\n",
      "Trained batch 1188 batch loss 1.11540675 epoch total loss 1.21101511\n",
      "Trained batch 1189 batch loss 1.1545428 epoch total loss 1.21096754\n",
      "Trained batch 1190 batch loss 1.19385505 epoch total loss 1.21095324\n",
      "Trained batch 1191 batch loss 1.30583751 epoch total loss 1.21103287\n",
      "Trained batch 1192 batch loss 1.22566366 epoch total loss 1.21104515\n",
      "Trained batch 1193 batch loss 1.25175 epoch total loss 1.21107924\n",
      "Trained batch 1194 batch loss 1.19106328 epoch total loss 1.21106243\n",
      "Trained batch 1195 batch loss 1.19189763 epoch total loss 1.21104634\n",
      "Trained batch 1196 batch loss 0.958741128 epoch total loss 1.21083546\n",
      "Trained batch 1197 batch loss 0.9799245 epoch total loss 1.21064258\n",
      "Trained batch 1198 batch loss 1.00469685 epoch total loss 1.21047056\n",
      "Trained batch 1199 batch loss 1.16249275 epoch total loss 1.21043062\n",
      "Trained batch 1200 batch loss 1.34014034 epoch total loss 1.21053863\n",
      "Trained batch 1201 batch loss 1.38958621 epoch total loss 1.21068764\n",
      "Trained batch 1202 batch loss 1.15598857 epoch total loss 1.21064222\n",
      "Trained batch 1203 batch loss 1.22579074 epoch total loss 1.21065485\n",
      "Trained batch 1204 batch loss 1.02567959 epoch total loss 1.21050107\n",
      "Trained batch 1205 batch loss 1.16374862 epoch total loss 1.21046233\n",
      "Trained batch 1206 batch loss 1.11952698 epoch total loss 1.21038687\n",
      "Trained batch 1207 batch loss 1.28504276 epoch total loss 1.21044874\n",
      "Trained batch 1208 batch loss 1.26778364 epoch total loss 1.21049619\n",
      "Trained batch 1209 batch loss 1.3076545 epoch total loss 1.21057653\n",
      "Trained batch 1210 batch loss 1.33411312 epoch total loss 1.21067858\n",
      "Trained batch 1211 batch loss 1.22806811 epoch total loss 1.210693\n",
      "Trained batch 1212 batch loss 1.26259303 epoch total loss 1.2107358\n",
      "Trained batch 1213 batch loss 1.10366058 epoch total loss 1.21064746\n",
      "Trained batch 1214 batch loss 1.12153256 epoch total loss 1.21057415\n",
      "Trained batch 1215 batch loss 1.04161 epoch total loss 1.21043503\n",
      "Trained batch 1216 batch loss 1.09829974 epoch total loss 1.21034276\n",
      "Trained batch 1217 batch loss 1.1011163 epoch total loss 1.210253\n",
      "Trained batch 1218 batch loss 1.18200374 epoch total loss 1.21022987\n",
      "Trained batch 1219 batch loss 1.24807847 epoch total loss 1.21026087\n",
      "Trained batch 1220 batch loss 1.20753968 epoch total loss 1.2102586\n",
      "Trained batch 1221 batch loss 1.25560653 epoch total loss 1.2102958\n",
      "Trained batch 1222 batch loss 1.33427548 epoch total loss 1.21039712\n",
      "Trained batch 1223 batch loss 1.38253379 epoch total loss 1.21053791\n",
      "Trained batch 1224 batch loss 1.38615322 epoch total loss 1.21068144\n",
      "Trained batch 1225 batch loss 1.32046747 epoch total loss 1.21077096\n",
      "Trained batch 1226 batch loss 1.16464174 epoch total loss 1.21073341\n",
      "Trained batch 1227 batch loss 1.1557734 epoch total loss 1.21068859\n",
      "Trained batch 1228 batch loss 1.35402 epoch total loss 1.2108053\n",
      "Trained batch 1229 batch loss 1.1946435 epoch total loss 1.21079218\n",
      "Trained batch 1230 batch loss 1.20118594 epoch total loss 1.21078432\n",
      "Trained batch 1231 batch loss 1.13795 epoch total loss 1.21072519\n",
      "Trained batch 1232 batch loss 1.16774595 epoch total loss 1.21069026\n",
      "Trained batch 1233 batch loss 1.17247093 epoch total loss 1.21065927\n",
      "Trained batch 1234 batch loss 1.04844809 epoch total loss 1.2105279\n",
      "Trained batch 1235 batch loss 1.16778064 epoch total loss 1.21049321\n",
      "Trained batch 1236 batch loss 1.21798456 epoch total loss 1.21049929\n",
      "Trained batch 1237 batch loss 1.06644595 epoch total loss 1.21038282\n",
      "Trained batch 1238 batch loss 1.11744368 epoch total loss 1.21030772\n",
      "Trained batch 1239 batch loss 1.20385337 epoch total loss 1.21030247\n",
      "Trained batch 1240 batch loss 1.21434927 epoch total loss 1.21030581\n",
      "Trained batch 1241 batch loss 1.11506271 epoch total loss 1.21022904\n",
      "Trained batch 1242 batch loss 1.12263596 epoch total loss 1.21015859\n",
      "Trained batch 1243 batch loss 1.12524319 epoch total loss 1.21009028\n",
      "Trained batch 1244 batch loss 1.17076325 epoch total loss 1.21005869\n",
      "Trained batch 1245 batch loss 1.1646024 epoch total loss 1.21002209\n",
      "Trained batch 1246 batch loss 1.17146838 epoch total loss 1.20999122\n",
      "Trained batch 1247 batch loss 1.19605732 epoch total loss 1.20998\n",
      "Trained batch 1248 batch loss 1.14987373 epoch total loss 1.20993185\n",
      "Trained batch 1249 batch loss 1.33644629 epoch total loss 1.21003318\n",
      "Trained batch 1250 batch loss 1.11028898 epoch total loss 1.20995331\n",
      "Trained batch 1251 batch loss 1.17648482 epoch total loss 1.20992661\n",
      "Trained batch 1252 batch loss 1.26029563 epoch total loss 1.20996678\n",
      "Trained batch 1253 batch loss 1.294011 epoch total loss 1.21003389\n",
      "Trained batch 1254 batch loss 1.20184159 epoch total loss 1.21002734\n",
      "Trained batch 1255 batch loss 1.22661805 epoch total loss 1.21004045\n",
      "Trained batch 1256 batch loss 1.09758842 epoch total loss 1.20995092\n",
      "Trained batch 1257 batch loss 1.21337533 epoch total loss 1.20995367\n",
      "Trained batch 1258 batch loss 1.14902496 epoch total loss 1.20990527\n",
      "Trained batch 1259 batch loss 1.20452893 epoch total loss 1.20990098\n",
      "Trained batch 1260 batch loss 1.17226315 epoch total loss 1.20987117\n",
      "Trained batch 1261 batch loss 1.1950109 epoch total loss 1.20985937\n",
      "Trained batch 1262 batch loss 1.17040098 epoch total loss 1.20982814\n",
      "Trained batch 1263 batch loss 1.10114169 epoch total loss 1.20974207\n",
      "Trained batch 1264 batch loss 1.21997643 epoch total loss 1.20975018\n",
      "Trained batch 1265 batch loss 1.0718013 epoch total loss 1.2096411\n",
      "Trained batch 1266 batch loss 1.16793263 epoch total loss 1.2096082\n",
      "Trained batch 1267 batch loss 1.10452926 epoch total loss 1.20952523\n",
      "Trained batch 1268 batch loss 1.08941495 epoch total loss 1.20943046\n",
      "Trained batch 1269 batch loss 1.04510951 epoch total loss 1.20930099\n",
      "Trained batch 1270 batch loss 1.09719956 epoch total loss 1.20921278\n",
      "Trained batch 1271 batch loss 1.1018213 epoch total loss 1.20912826\n",
      "Trained batch 1272 batch loss 1.2009182 epoch total loss 1.20912182\n",
      "Trained batch 1273 batch loss 1.2210449 epoch total loss 1.20913124\n",
      "Trained batch 1274 batch loss 1.19754076 epoch total loss 1.20912206\n",
      "Trained batch 1275 batch loss 1.23831081 epoch total loss 1.20914495\n",
      "Trained batch 1276 batch loss 1.30523014 epoch total loss 1.20922017\n",
      "Trained batch 1277 batch loss 1.29683089 epoch total loss 1.20928884\n",
      "Trained batch 1278 batch loss 1.24941015 epoch total loss 1.20932019\n",
      "Trained batch 1279 batch loss 1.33382773 epoch total loss 1.20941758\n",
      "Trained batch 1280 batch loss 1.50890219 epoch total loss 1.20965159\n",
      "Trained batch 1281 batch loss 1.21818089 epoch total loss 1.20965815\n",
      "Trained batch 1282 batch loss 1.24963534 epoch total loss 1.20968938\n",
      "Trained batch 1283 batch loss 1.35159445 epoch total loss 1.2098\n",
      "Trained batch 1284 batch loss 1.29519451 epoch total loss 1.2098664\n",
      "Trained batch 1285 batch loss 1.32864332 epoch total loss 1.20995879\n",
      "Trained batch 1286 batch loss 1.19565511 epoch total loss 1.20994771\n",
      "Trained batch 1287 batch loss 1.15946209 epoch total loss 1.20990849\n",
      "Trained batch 1288 batch loss 1.19561362 epoch total loss 1.2098974\n",
      "Trained batch 1289 batch loss 1.14003074 epoch total loss 1.20984316\n",
      "Trained batch 1290 batch loss 1.28505778 epoch total loss 1.20990145\n",
      "Trained batch 1291 batch loss 1.21273232 epoch total loss 1.2099036\n",
      "Trained batch 1292 batch loss 1.31405675 epoch total loss 1.2099843\n",
      "Trained batch 1293 batch loss 1.27046347 epoch total loss 1.21003103\n",
      "Trained batch 1294 batch loss 1.28660464 epoch total loss 1.21009028\n",
      "Trained batch 1295 batch loss 1.21382809 epoch total loss 1.21009314\n",
      "Trained batch 1296 batch loss 1.16429782 epoch total loss 1.21005785\n",
      "Trained batch 1297 batch loss 1.17441893 epoch total loss 1.21003044\n",
      "Trained batch 1298 batch loss 1.14182639 epoch total loss 1.20997787\n",
      "Trained batch 1299 batch loss 1.18546391 epoch total loss 1.20995891\n",
      "Trained batch 1300 batch loss 1.21422446 epoch total loss 1.20996225\n",
      "Trained batch 1301 batch loss 1.19540751 epoch total loss 1.20995104\n",
      "Trained batch 1302 batch loss 1.27560568 epoch total loss 1.21000159\n",
      "Trained batch 1303 batch loss 1.33602798 epoch total loss 1.21009827\n",
      "Trained batch 1304 batch loss 1.26717043 epoch total loss 1.21014214\n",
      "Trained batch 1305 batch loss 1.27845109 epoch total loss 1.21019447\n",
      "Trained batch 1306 batch loss 1.30408168 epoch total loss 1.21026635\n",
      "Trained batch 1307 batch loss 1.23349035 epoch total loss 1.21028411\n",
      "Trained batch 1308 batch loss 1.19401 epoch total loss 1.2102716\n",
      "Trained batch 1309 batch loss 1.1291635 epoch total loss 1.21020961\n",
      "Trained batch 1310 batch loss 1.08646 epoch total loss 1.21011519\n",
      "Trained batch 1311 batch loss 1.00820541 epoch total loss 1.20996118\n",
      "Trained batch 1312 batch loss 1.05415976 epoch total loss 1.20984244\n",
      "Trained batch 1313 batch loss 1.10235405 epoch total loss 1.20976055\n",
      "Trained batch 1314 batch loss 1.08897448 epoch total loss 1.20966864\n",
      "Trained batch 1315 batch loss 0.893605351 epoch total loss 1.20942819\n",
      "Trained batch 1316 batch loss 0.926045656 epoch total loss 1.20921278\n",
      "Trained batch 1317 batch loss 0.869648457 epoch total loss 1.20895505\n",
      "Trained batch 1318 batch loss 1.04983664 epoch total loss 1.20883429\n",
      "Trained batch 1319 batch loss 1.12862325 epoch total loss 1.20877349\n",
      "Trained batch 1320 batch loss 1.11294007 epoch total loss 1.2087009\n",
      "Trained batch 1321 batch loss 1.10783279 epoch total loss 1.20862448\n",
      "Trained batch 1322 batch loss 1.24214602 epoch total loss 1.20864987\n",
      "Trained batch 1323 batch loss 1.09304166 epoch total loss 1.20856249\n",
      "Trained batch 1324 batch loss 1.06502903 epoch total loss 1.20845401\n",
      "Trained batch 1325 batch loss 1.17926776 epoch total loss 1.20843208\n",
      "Trained batch 1326 batch loss 1.03593063 epoch total loss 1.2083019\n",
      "Trained batch 1327 batch loss 1.19325233 epoch total loss 1.20829058\n",
      "Trained batch 1328 batch loss 1.21416783 epoch total loss 1.20829499\n",
      "Trained batch 1329 batch loss 1.29539537 epoch total loss 1.20836055\n",
      "Trained batch 1330 batch loss 1.19676363 epoch total loss 1.20835185\n",
      "Trained batch 1331 batch loss 1.06627548 epoch total loss 1.20824504\n",
      "Trained batch 1332 batch loss 1.13619971 epoch total loss 1.20819104\n",
      "Trained batch 1333 batch loss 1.16477346 epoch total loss 1.20815849\n",
      "Trained batch 1334 batch loss 1.05838323 epoch total loss 1.2080462\n",
      "Trained batch 1335 batch loss 1.1713928 epoch total loss 1.20801866\n",
      "Trained batch 1336 batch loss 1.32722569 epoch total loss 1.20810795\n",
      "Trained batch 1337 batch loss 1.27172065 epoch total loss 1.20815551\n",
      "Trained batch 1338 batch loss 1.19060171 epoch total loss 1.2081424\n",
      "Trained batch 1339 batch loss 1.18836832 epoch total loss 1.20812762\n",
      "Trained batch 1340 batch loss 1.15351081 epoch total loss 1.20808685\n",
      "Trained batch 1341 batch loss 1.18252861 epoch total loss 1.20806777\n",
      "Trained batch 1342 batch loss 1.22780037 epoch total loss 1.20808244\n",
      "Trained batch 1343 batch loss 1.19797635 epoch total loss 1.20807493\n",
      "Trained batch 1344 batch loss 1.25854671 epoch total loss 1.20811248\n",
      "Trained batch 1345 batch loss 1.06316292 epoch total loss 1.20800471\n",
      "Trained batch 1346 batch loss 0.95181787 epoch total loss 1.20781434\n",
      "Trained batch 1347 batch loss 1.01590455 epoch total loss 1.20767188\n",
      "Trained batch 1348 batch loss 1.20427585 epoch total loss 1.20766926\n",
      "Trained batch 1349 batch loss 1.28369141 epoch total loss 1.20772564\n",
      "Trained batch 1350 batch loss 1.59895587 epoch total loss 1.20801544\n",
      "Trained batch 1351 batch loss 1.18776679 epoch total loss 1.20800054\n",
      "Trained batch 1352 batch loss 1.12630951 epoch total loss 1.2079401\n",
      "Trained batch 1353 batch loss 1.21830809 epoch total loss 1.20794773\n",
      "Trained batch 1354 batch loss 1.19917774 epoch total loss 1.20794129\n",
      "Trained batch 1355 batch loss 1.31708491 epoch total loss 1.20802188\n",
      "Trained batch 1356 batch loss 1.25310552 epoch total loss 1.20805502\n",
      "Trained batch 1357 batch loss 1.17555833 epoch total loss 1.20803106\n",
      "Trained batch 1358 batch loss 1.22080445 epoch total loss 1.20804048\n",
      "Trained batch 1359 batch loss 1.24301922 epoch total loss 1.20806634\n",
      "Trained batch 1360 batch loss 1.17970991 epoch total loss 1.20804536\n",
      "Trained batch 1361 batch loss 1.11501873 epoch total loss 1.20797706\n",
      "Trained batch 1362 batch loss 1.09631264 epoch total loss 1.20789504\n",
      "Trained batch 1363 batch loss 1.18153286 epoch total loss 1.20787573\n",
      "Trained batch 1364 batch loss 1.22974575 epoch total loss 1.2078917\n",
      "Trained batch 1365 batch loss 1.17307723 epoch total loss 1.20786619\n",
      "Trained batch 1366 batch loss 1.00334764 epoch total loss 1.20771646\n",
      "Trained batch 1367 batch loss 1.13254452 epoch total loss 1.20766151\n",
      "Trained batch 1368 batch loss 1.23455691 epoch total loss 1.20768118\n",
      "Trained batch 1369 batch loss 1.26675177 epoch total loss 1.20772421\n",
      "Trained batch 1370 batch loss 1.2219907 epoch total loss 1.2077347\n",
      "Trained batch 1371 batch loss 1.35731339 epoch total loss 1.20784378\n",
      "Trained batch 1372 batch loss 1.38573265 epoch total loss 1.20797348\n",
      "Trained batch 1373 batch loss 1.11942315 epoch total loss 1.20790899\n",
      "Trained batch 1374 batch loss 1.31579936 epoch total loss 1.20798743\n",
      "Trained batch 1375 batch loss 1.33774304 epoch total loss 1.20808184\n",
      "Trained batch 1376 batch loss 1.23577404 epoch total loss 1.20810199\n",
      "Trained batch 1377 batch loss 1.33052516 epoch total loss 1.20819092\n",
      "Trained batch 1378 batch loss 1.12708354 epoch total loss 1.20813203\n",
      "Trained batch 1379 batch loss 1.14593494 epoch total loss 1.20808685\n",
      "Trained batch 1380 batch loss 1.17665303 epoch total loss 1.20806408\n",
      "Trained batch 1381 batch loss 1.23862314 epoch total loss 1.20808625\n",
      "Trained batch 1382 batch loss 1.24307168 epoch total loss 1.20811152\n",
      "Trained batch 1383 batch loss 1.29138482 epoch total loss 1.20817173\n",
      "Trained batch 1384 batch loss 1.15723872 epoch total loss 1.20813489\n",
      "Trained batch 1385 batch loss 1.15473926 epoch total loss 1.20809639\n",
      "Trained batch 1386 batch loss 1.17535222 epoch total loss 1.20807278\n",
      "Trained batch 1387 batch loss 1.13217115 epoch total loss 1.20801806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 03:50:56.131504: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "2025-03-19 03:50:56.131549: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "\t [[RemoteCall]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1388 batch loss 1.20643353 epoch total loss 1.20801687\n",
      "Epoch 4 train loss 1.2080168724060059\n",
      "Validated batch 1 batch loss 1.21494615\n",
      "Validated batch 2 batch loss 1.08691025\n",
      "Validated batch 3 batch loss 1.2213037\n",
      "Validated batch 4 batch loss 1.11891139\n",
      "Validated batch 5 batch loss 1.20273376\n",
      "Validated batch 6 batch loss 1.26386905\n",
      "Validated batch 7 batch loss 1.22052789\n",
      "Validated batch 8 batch loss 1.33565187\n",
      "Validated batch 9 batch loss 1.31222939\n",
      "Validated batch 10 batch loss 1.2015239\n",
      "Validated batch 11 batch loss 1.24798703\n",
      "Validated batch 12 batch loss 1.30617225\n",
      "Validated batch 13 batch loss 1.34235287\n",
      "Validated batch 14 batch loss 1.32834387\n",
      "Validated batch 15 batch loss 1.29018378\n",
      "Validated batch 16 batch loss 1.32842958\n",
      "Validated batch 17 batch loss 1.29714084\n",
      "Validated batch 18 batch loss 1.14368749\n",
      "Validated batch 19 batch loss 1.24551773\n",
      "Validated batch 20 batch loss 1.32716906\n",
      "Validated batch 21 batch loss 1.18773794\n",
      "Validated batch 22 batch loss 1.21344376\n",
      "Validated batch 23 batch loss 1.20455587\n",
      "Validated batch 24 batch loss 1.31000662\n",
      "Validated batch 25 batch loss 1.26084149\n",
      "Validated batch 26 batch loss 1.13619542\n",
      "Validated batch 27 batch loss 1.150962\n",
      "Validated batch 28 batch loss 1.09043694\n",
      "Validated batch 29 batch loss 1.21063614\n",
      "Validated batch 30 batch loss 1.26657093\n",
      "Validated batch 31 batch loss 1.08927989\n",
      "Validated batch 32 batch loss 1.25361681\n",
      "Validated batch 33 batch loss 1.19977617\n",
      "Validated batch 34 batch loss 1.25802767\n",
      "Validated batch 35 batch loss 1.20243216\n",
      "Validated batch 36 batch loss 1.26253939\n",
      "Validated batch 37 batch loss 1.16899955\n",
      "Validated batch 38 batch loss 1.16720784\n",
      "Validated batch 39 batch loss 1.22499335\n",
      "Validated batch 40 batch loss 1.19718993\n",
      "Validated batch 41 batch loss 1.24598551\n",
      "Validated batch 42 batch loss 1.28413248\n",
      "Validated batch 43 batch loss 1.45732641\n",
      "Validated batch 44 batch loss 1.26870072\n",
      "Validated batch 45 batch loss 1.23482\n",
      "Validated batch 46 batch loss 1.10452592\n",
      "Validated batch 47 batch loss 1.12103534\n",
      "Validated batch 48 batch loss 1.10770488\n",
      "Validated batch 49 batch loss 1.17500448\n",
      "Validated batch 50 batch loss 1.17071092\n",
      "Validated batch 51 batch loss 1.21872425\n",
      "Validated batch 52 batch loss 1.21814847\n",
      "Validated batch 53 batch loss 1.22945309\n",
      "Validated batch 54 batch loss 1.17614436\n",
      "Validated batch 55 batch loss 1.26220536\n",
      "Validated batch 56 batch loss 1.1642431\n",
      "Validated batch 57 batch loss 1.20977163\n",
      "Validated batch 58 batch loss 1.26235151\n",
      "Validated batch 59 batch loss 1.15750575\n",
      "Validated batch 60 batch loss 1.18601453\n",
      "Validated batch 61 batch loss 1.22826183\n",
      "Validated batch 62 batch loss 1.18503714\n",
      "Validated batch 63 batch loss 1.34455693\n",
      "Validated batch 64 batch loss 1.25425339\n",
      "Validated batch 65 batch loss 1.08475924\n",
      "Validated batch 66 batch loss 1.29916143\n",
      "Validated batch 67 batch loss 1.2192533\n",
      "Validated batch 68 batch loss 1.13691151\n",
      "Validated batch 69 batch loss 1.21540117\n",
      "Validated batch 70 batch loss 1.09451473\n",
      "Validated batch 71 batch loss 1.22208738\n",
      "Validated batch 72 batch loss 1.28159463\n",
      "Validated batch 73 batch loss 1.15531385\n",
      "Validated batch 74 batch loss 1.31323183\n",
      "Validated batch 75 batch loss 1.36873698\n",
      "Validated batch 76 batch loss 1.06880474\n",
      "Validated batch 77 batch loss 1.19629693\n",
      "Validated batch 78 batch loss 1.2075901\n",
      "Validated batch 79 batch loss 1.26304305\n",
      "Validated batch 80 batch loss 1.24044526\n",
      "Validated batch 81 batch loss 1.09826219\n",
      "Validated batch 82 batch loss 1.03097165\n",
      "Validated batch 83 batch loss 1.18871355\n",
      "Validated batch 84 batch loss 1.13408947\n",
      "Validated batch 85 batch loss 1.14650679\n",
      "Validated batch 86 batch loss 1.24459803\n",
      "Validated batch 87 batch loss 1.12470281\n",
      "Validated batch 88 batch loss 1.22252536\n",
      "Validated batch 89 batch loss 1.27598739\n",
      "Validated batch 90 batch loss 1.27465773\n",
      "Validated batch 91 batch loss 1.19846761\n",
      "Validated batch 92 batch loss 1.09402168\n",
      "Validated batch 93 batch loss 1.14871645\n",
      "Validated batch 94 batch loss 1.2053225\n",
      "Validated batch 95 batch loss 1.19785619\n",
      "Validated batch 96 batch loss 1.07821488\n",
      "Validated batch 97 batch loss 1.14625335\n",
      "Validated batch 98 batch loss 1.31416583\n",
      "Validated batch 99 batch loss 1.13514566\n",
      "Validated batch 100 batch loss 1.08719122\n",
      "Validated batch 101 batch loss 1.1449914\n",
      "Validated batch 102 batch loss 1.15732467\n",
      "Validated batch 103 batch loss 1.10867584\n",
      "Validated batch 104 batch loss 1.1983707\n",
      "Validated batch 105 batch loss 1.18606091\n",
      "Validated batch 106 batch loss 1.125211\n",
      "Validated batch 107 batch loss 1.26085651\n",
      "Validated batch 108 batch loss 1.30518389\n",
      "Validated batch 109 batch loss 1.10827959\n",
      "Validated batch 110 batch loss 1.33294189\n",
      "Validated batch 111 batch loss 1.04289424\n",
      "Validated batch 112 batch loss 1.12739801\n",
      "Validated batch 113 batch loss 1.1504662\n",
      "Validated batch 114 batch loss 1.18527186\n",
      "Validated batch 115 batch loss 1.38451791\n",
      "Validated batch 116 batch loss 1.19472289\n",
      "Validated batch 117 batch loss 1.21063662\n",
      "Validated batch 118 batch loss 1.15935564\n",
      "Validated batch 119 batch loss 1.20174408\n",
      "Validated batch 120 batch loss 1.2529881\n",
      "Validated batch 121 batch loss 1.3411305\n",
      "Validated batch 122 batch loss 1.15567136\n",
      "Validated batch 123 batch loss 1.19813275\n",
      "Validated batch 124 batch loss 1.09582591\n",
      "Validated batch 125 batch loss 1.21951985\n",
      "Validated batch 126 batch loss 1.19283724\n",
      "Validated batch 127 batch loss 1.14206052\n",
      "Validated batch 128 batch loss 1.27381909\n",
      "Validated batch 129 batch loss 1.25333273\n",
      "Validated batch 130 batch loss 1.2904768\n",
      "Validated batch 131 batch loss 1.19420052\n",
      "Validated batch 132 batch loss 1.24955332\n",
      "Validated batch 133 batch loss 1.14744639\n",
      "Validated batch 134 batch loss 1.16420233\n",
      "Validated batch 135 batch loss 1.20905209\n",
      "Validated batch 136 batch loss 1.1614697\n",
      "Validated batch 137 batch loss 1.21701729\n",
      "Validated batch 138 batch loss 1.19917011\n",
      "Validated batch 139 batch loss 1.30661249\n",
      "Validated batch 140 batch loss 1.19073486\n",
      "Validated batch 141 batch loss 1.13947415\n",
      "Validated batch 142 batch loss 1.17911\n",
      "Validated batch 143 batch loss 1.21719253\n",
      "Validated batch 144 batch loss 1.22405457\n",
      "Validated batch 145 batch loss 1.21406531\n",
      "Validated batch 146 batch loss 1.2113961\n",
      "Validated batch 147 batch loss 1.13141346\n",
      "Validated batch 148 batch loss 1.33848524\n",
      "Validated batch 149 batch loss 1.13665724\n",
      "Validated batch 150 batch loss 1.09283423\n",
      "Validated batch 151 batch loss 1.17552114\n",
      "Validated batch 152 batch loss 1.30834711\n",
      "Validated batch 153 batch loss 1.27746236\n",
      "Validated batch 154 batch loss 1.34149218\n",
      "Validated batch 155 batch loss 1.18297505\n",
      "Validated batch 156 batch loss 1.37622249\n",
      "Validated batch 157 batch loss 1.21633911\n",
      "Validated batch 158 batch loss 1.3058238\n",
      "Validated batch 159 batch loss 1.26630402\n",
      "Validated batch 160 batch loss 1.01568937\n",
      "Validated batch 161 batch loss 1.1371845\n",
      "Validated batch 162 batch loss 1.21392536\n",
      "Validated batch 163 batch loss 1.23486006\n",
      "Validated batch 164 batch loss 1.23279762\n",
      "Validated batch 165 batch loss 1.0788362\n",
      "Validated batch 166 batch loss 1.23493612\n",
      "Validated batch 167 batch loss 1.25018895\n",
      "Validated batch 168 batch loss 1.26790929\n",
      "Validated batch 169 batch loss 1.33950281\n",
      "Validated batch 170 batch loss 1.24749279\n",
      "Validated batch 171 batch loss 1.19445181\n",
      "Validated batch 172 batch loss 1.21795261\n",
      "Validated batch 173 batch loss 1.24604821\n",
      "Validated batch 174 batch loss 1.18293\n",
      "Validated batch 175 batch loss 1.29096806\n",
      "Validated batch 176 batch loss 1.3868258\n",
      "Validated batch 177 batch loss 1.21617103\n",
      "Validated batch 178 batch loss 1.28511\n",
      "Validated batch 179 batch loss 1.20126283\n",
      "Validated batch 180 batch loss 1.15277195\n",
      "Validated batch 181 batch loss 1.18955886\n",
      "Validated batch 182 batch loss 1.09675384\n",
      "Validated batch 183 batch loss 1.29248917\n",
      "Validated batch 184 batch loss 1.1859591\n",
      "Validated batch 185 batch loss 1.23538935\n",
      "Epoch 4 val loss 1.2112135887145996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 03:51:12.060921: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "2025-03-19 03:51:12.060955: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "\t [[RemoteCall]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model /home/minho/Desktop/aiffel/pose_estimation/models/model-epoch-4-loss-1.2112.weights.h5 saved.\n",
      "Start epoch 5 with learning rate 0.001\n",
      "Start distributed training...\n",
      "Trained batch 1 batch loss 1.19421244 epoch total loss 1.19421244\n",
      "Trained batch 2 batch loss 1.1660527 epoch total loss 1.18013263\n",
      "Trained batch 3 batch loss 1.262743 epoch total loss 1.20766938\n",
      "Trained batch 4 batch loss 1.26333475 epoch total loss 1.22158575\n",
      "Trained batch 5 batch loss 1.37635136 epoch total loss 1.25253892\n",
      "Trained batch 6 batch loss 1.52117825 epoch total loss 1.29731214\n",
      "Trained batch 7 batch loss 1.15198195 epoch total loss 1.27655065\n",
      "Trained batch 8 batch loss 1.19141817 epoch total loss 1.26590919\n",
      "Trained batch 9 batch loss 1.21915638 epoch total loss 1.26071441\n",
      "Trained batch 10 batch loss 1.16360021 epoch total loss 1.25100303\n",
      "Trained batch 11 batch loss 1.21179938 epoch total loss 1.24743903\n",
      "Trained batch 12 batch loss 1.27987552 epoch total loss 1.2501421\n",
      "Trained batch 13 batch loss 1.35363531 epoch total loss 1.25810313\n",
      "Trained batch 14 batch loss 1.42084837 epoch total loss 1.26972783\n",
      "Trained batch 15 batch loss 1.26557422 epoch total loss 1.2694509\n",
      "Trained batch 16 batch loss 1.32556653 epoch total loss 1.27295816\n",
      "Trained batch 17 batch loss 1.49064898 epoch total loss 1.2857635\n",
      "Trained batch 18 batch loss 1.35160327 epoch total loss 1.2894212\n",
      "Trained batch 19 batch loss 1.30134857 epoch total loss 1.29004896\n",
      "Trained batch 20 batch loss 1.23705387 epoch total loss 1.28739905\n",
      "Trained batch 21 batch loss 1.29732168 epoch total loss 1.2878716\n",
      "Trained batch 22 batch loss 1.19066572 epoch total loss 1.28345311\n",
      "Trained batch 23 batch loss 1.22543216 epoch total loss 1.28093052\n",
      "Trained batch 24 batch loss 1.03902316 epoch total loss 1.27085102\n",
      "Trained batch 25 batch loss 1.04895771 epoch total loss 1.26197529\n",
      "Trained batch 26 batch loss 1.0753938 epoch total loss 1.25479901\n",
      "Trained batch 27 batch loss 1.01808429 epoch total loss 1.24603188\n",
      "Trained batch 28 batch loss 1.21724737 epoch total loss 1.24500382\n",
      "Trained batch 29 batch loss 1.18068 epoch total loss 1.24278569\n",
      "Trained batch 30 batch loss 1.26157868 epoch total loss 1.24341214\n",
      "Trained batch 31 batch loss 1.25568187 epoch total loss 1.24380791\n",
      "Trained batch 32 batch loss 1.15659785 epoch total loss 1.24108255\n",
      "Trained batch 33 batch loss 1.0278039 epoch total loss 1.23461962\n",
      "Trained batch 34 batch loss 1.15272069 epoch total loss 1.23221087\n",
      "Trained batch 35 batch loss 1.23372102 epoch total loss 1.23225403\n",
      "Trained batch 36 batch loss 1.14358664 epoch total loss 1.22979105\n",
      "Trained batch 37 batch loss 0.977829516 epoch total loss 1.22298121\n",
      "Trained batch 38 batch loss 0.850362659 epoch total loss 1.21317542\n",
      "Trained batch 39 batch loss 1.09499526 epoch total loss 1.21014512\n",
      "Trained batch 40 batch loss 1.15906262 epoch total loss 1.20886803\n",
      "Trained batch 41 batch loss 1.3277576 epoch total loss 1.21176779\n",
      "Trained batch 42 batch loss 1.3532511 epoch total loss 1.21513653\n",
      "Trained batch 43 batch loss 1.27603316 epoch total loss 1.21655262\n",
      "Trained batch 44 batch loss 1.24539757 epoch total loss 1.21720815\n",
      "Trained batch 45 batch loss 1.11426282 epoch total loss 1.21492052\n",
      "Trained batch 46 batch loss 1.30747819 epoch total loss 1.21693265\n",
      "Trained batch 47 batch loss 1.07132411 epoch total loss 1.21383452\n",
      "Trained batch 48 batch loss 1.23541784 epoch total loss 1.21428418\n",
      "Trained batch 49 batch loss 1.16559088 epoch total loss 1.21329045\n",
      "Trained batch 50 batch loss 1.15836835 epoch total loss 1.21219206\n",
      "Trained batch 51 batch loss 1.09046769 epoch total loss 1.20980525\n",
      "Trained batch 52 batch loss 1.179214 epoch total loss 1.20921695\n",
      "Trained batch 53 batch loss 1.39110804 epoch total loss 1.21264899\n",
      "Trained batch 54 batch loss 1.32747817 epoch total loss 1.21477532\n",
      "Trained batch 55 batch loss 1.28152323 epoch total loss 1.21598899\n",
      "Trained batch 56 batch loss 1.29415131 epoch total loss 1.2173847\n",
      "Trained batch 57 batch loss 1.20141327 epoch total loss 1.21710455\n",
      "Trained batch 58 batch loss 1.18664098 epoch total loss 1.21657932\n",
      "Trained batch 59 batch loss 1.23170447 epoch total loss 1.21683562\n",
      "Trained batch 60 batch loss 1.24273992 epoch total loss 1.21726739\n",
      "Trained batch 61 batch loss 1.14589834 epoch total loss 1.21609735\n",
      "Trained batch 62 batch loss 1.1236943 epoch total loss 1.214607\n",
      "Trained batch 63 batch loss 1.08238244 epoch total loss 1.2125082\n",
      "Trained batch 64 batch loss 1.08399594 epoch total loss 1.21050024\n",
      "Trained batch 65 batch loss 1.12878406 epoch total loss 1.20924306\n",
      "Trained batch 66 batch loss 1.14977133 epoch total loss 1.20834196\n",
      "Trained batch 67 batch loss 1.24272275 epoch total loss 1.20885515\n",
      "Trained batch 68 batch loss 1.20113719 epoch total loss 1.20874155\n",
      "Trained batch 69 batch loss 1.17157125 epoch total loss 1.20820284\n",
      "Trained batch 70 batch loss 1.16079473 epoch total loss 1.20752561\n",
      "Trained batch 71 batch loss 1.11216557 epoch total loss 1.2061826\n",
      "Trained batch 72 batch loss 1.08279932 epoch total loss 1.20446897\n",
      "Trained batch 73 batch loss 1.13448346 epoch total loss 1.20351028\n",
      "Trained batch 74 batch loss 1.12965786 epoch total loss 1.20251226\n",
      "Trained batch 75 batch loss 1.12568784 epoch total loss 1.2014879\n",
      "Trained batch 76 batch loss 1.1457969 epoch total loss 1.20075512\n",
      "Trained batch 77 batch loss 1.13078225 epoch total loss 1.19984639\n",
      "Trained batch 78 batch loss 1.08171701 epoch total loss 1.19833195\n",
      "Trained batch 79 batch loss 1.06429911 epoch total loss 1.19663537\n",
      "Trained batch 80 batch loss 1.17015874 epoch total loss 1.19630444\n",
      "Trained batch 81 batch loss 1.16902828 epoch total loss 1.19596767\n",
      "Trained batch 82 batch loss 1.08022761 epoch total loss 1.19455624\n",
      "Trained batch 83 batch loss 1.18029904 epoch total loss 1.19438446\n",
      "Trained batch 84 batch loss 1.00701272 epoch total loss 1.19215381\n",
      "Trained batch 85 batch loss 1.18912363 epoch total loss 1.19211817\n",
      "Trained batch 86 batch loss 1.2647382 epoch total loss 1.19296265\n",
      "Trained batch 87 batch loss 1.24762869 epoch total loss 1.193591\n",
      "Trained batch 88 batch loss 1.22656977 epoch total loss 1.19396579\n",
      "Trained batch 89 batch loss 1.17277431 epoch total loss 1.19372761\n",
      "Trained batch 90 batch loss 1.20319235 epoch total loss 1.19383287\n",
      "Trained batch 91 batch loss 1.14373124 epoch total loss 1.19328225\n",
      "Trained batch 92 batch loss 1.21635365 epoch total loss 1.19353306\n",
      "Trained batch 93 batch loss 1.23771834 epoch total loss 1.19400811\n",
      "Trained batch 94 batch loss 1.30529296 epoch total loss 1.19519198\n",
      "Trained batch 95 batch loss 1.10146713 epoch total loss 1.1942054\n",
      "Trained batch 96 batch loss 1.19078982 epoch total loss 1.19416976\n",
      "Trained batch 97 batch loss 1.25365341 epoch total loss 1.19478297\n",
      "Trained batch 98 batch loss 1.10001874 epoch total loss 1.19381607\n",
      "Trained batch 99 batch loss 1.2425704 epoch total loss 1.19430852\n",
      "Trained batch 100 batch loss 1.1190058 epoch total loss 1.19355547\n",
      "Trained batch 101 batch loss 1.20506346 epoch total loss 1.19366944\n",
      "Trained batch 102 batch loss 1.21072805 epoch total loss 1.19383669\n",
      "Trained batch 103 batch loss 1.28688669 epoch total loss 1.19474\n",
      "Trained batch 104 batch loss 1.34676242 epoch total loss 1.1962018\n",
      "Trained batch 105 batch loss 1.22761583 epoch total loss 1.19650102\n",
      "Trained batch 106 batch loss 1.25488901 epoch total loss 1.19705188\n",
      "Trained batch 107 batch loss 1.18476713 epoch total loss 1.19693708\n",
      "Trained batch 108 batch loss 1.18250382 epoch total loss 1.19680345\n",
      "Trained batch 109 batch loss 1.24860406 epoch total loss 1.19727874\n",
      "Trained batch 110 batch loss 1.16203582 epoch total loss 1.19695842\n",
      "Trained batch 111 batch loss 1.28421926 epoch total loss 1.19774461\n",
      "Trained batch 112 batch loss 1.15206456 epoch total loss 1.19733679\n",
      "Trained batch 113 batch loss 1.04873562 epoch total loss 1.19602168\n",
      "Trained batch 114 batch loss 1.24316609 epoch total loss 1.19643521\n",
      "Trained batch 115 batch loss 1.19160414 epoch total loss 1.19639325\n",
      "Trained batch 116 batch loss 1.14029 epoch total loss 1.19590962\n",
      "Trained batch 117 batch loss 1.29667401 epoch total loss 1.19677079\n",
      "Trained batch 118 batch loss 1.0223887 epoch total loss 1.19529295\n",
      "Trained batch 119 batch loss 0.903595805 epoch total loss 1.19284177\n",
      "Trained batch 120 batch loss 1.14182925 epoch total loss 1.19241667\n",
      "Trained batch 121 batch loss 1.18377912 epoch total loss 1.19234526\n",
      "Trained batch 122 batch loss 1.08664119 epoch total loss 1.19147885\n",
      "Trained batch 123 batch loss 1.0600996 epoch total loss 1.19041073\n",
      "Trained batch 124 batch loss 1.16351891 epoch total loss 1.19019377\n",
      "Trained batch 125 batch loss 1.15860748 epoch total loss 1.18994117\n",
      "Trained batch 126 batch loss 1.31497216 epoch total loss 1.19093347\n",
      "Trained batch 127 batch loss 1.17218328 epoch total loss 1.19078577\n",
      "Trained batch 128 batch loss 1.21599543 epoch total loss 1.1909827\n",
      "Trained batch 129 batch loss 1.33901274 epoch total loss 1.19213033\n",
      "Trained batch 130 batch loss 1.38484204 epoch total loss 1.19361269\n",
      "Trained batch 131 batch loss 1.27096081 epoch total loss 1.19420314\n",
      "Trained batch 132 batch loss 1.09551346 epoch total loss 1.19345558\n",
      "Trained batch 133 batch loss 1.22460079 epoch total loss 1.1936897\n",
      "Trained batch 134 batch loss 1.33831298 epoch total loss 1.19476902\n",
      "Trained batch 135 batch loss 1.12234187 epoch total loss 1.19423246\n",
      "Trained batch 136 batch loss 1.08152747 epoch total loss 1.19340384\n",
      "Trained batch 137 batch loss 1.20796394 epoch total loss 1.19351\n",
      "Trained batch 138 batch loss 1.30857873 epoch total loss 1.19434392\n",
      "Trained batch 139 batch loss 1.18534076 epoch total loss 1.19427907\n",
      "Trained batch 140 batch loss 1.07423007 epoch total loss 1.1934216\n",
      "Trained batch 141 batch loss 0.996393681 epoch total loss 1.19202423\n",
      "Trained batch 142 batch loss 1.10275149 epoch total loss 1.19139564\n",
      "Trained batch 143 batch loss 1.02572203 epoch total loss 1.19023705\n",
      "Trained batch 144 batch loss 1.00056839 epoch total loss 1.1889199\n",
      "Trained batch 145 batch loss 1.10349524 epoch total loss 1.18833077\n",
      "Trained batch 146 batch loss 1.02731967 epoch total loss 1.18722796\n",
      "Trained batch 147 batch loss 1.07939398 epoch total loss 1.18649435\n",
      "Trained batch 148 batch loss 1.18939543 epoch total loss 1.1865139\n",
      "Trained batch 149 batch loss 1.17310095 epoch total loss 1.1864239\n",
      "Trained batch 150 batch loss 1.17364872 epoch total loss 1.18633866\n",
      "Trained batch 151 batch loss 1.23888016 epoch total loss 1.18668664\n",
      "Trained batch 152 batch loss 1.19716537 epoch total loss 1.18675554\n",
      "Trained batch 153 batch loss 1.05597782 epoch total loss 1.18590081\n",
      "Trained batch 154 batch loss 1.11190891 epoch total loss 1.18542039\n",
      "Trained batch 155 batch loss 1.22834277 epoch total loss 1.18569732\n",
      "Trained batch 156 batch loss 1.2223866 epoch total loss 1.1859324\n",
      "Trained batch 157 batch loss 1.23234701 epoch total loss 1.18622804\n",
      "Trained batch 158 batch loss 1.10978031 epoch total loss 1.18574429\n",
      "Trained batch 159 batch loss 1.110816 epoch total loss 1.18527293\n",
      "Trained batch 160 batch loss 1.16870725 epoch total loss 1.18516946\n",
      "Trained batch 161 batch loss 1.18224359 epoch total loss 1.18515134\n",
      "Trained batch 162 batch loss 1.07853377 epoch total loss 1.18449318\n",
      "Trained batch 163 batch loss 1.11399007 epoch total loss 1.18406057\n",
      "Trained batch 164 batch loss 1.04352582 epoch total loss 1.1832037\n",
      "Trained batch 165 batch loss 1.19183791 epoch total loss 1.18325603\n",
      "Trained batch 166 batch loss 1.27783942 epoch total loss 1.18382573\n",
      "Trained batch 167 batch loss 1.16705465 epoch total loss 1.18372536\n",
      "Trained batch 168 batch loss 1.1335299 epoch total loss 1.1834265\n",
      "Trained batch 169 batch loss 1.04691672 epoch total loss 1.18261886\n",
      "Trained batch 170 batch loss 1.01744771 epoch total loss 1.18164718\n",
      "Trained batch 171 batch loss 0.899214923 epoch total loss 1.17999554\n",
      "Trained batch 172 batch loss 0.992431819 epoch total loss 1.17890501\n",
      "Trained batch 173 batch loss 1.03640819 epoch total loss 1.17808139\n",
      "Trained batch 174 batch loss 1.14274549 epoch total loss 1.17787826\n",
      "Trained batch 175 batch loss 1.09527206 epoch total loss 1.17740631\n",
      "Trained batch 176 batch loss 1.19269598 epoch total loss 1.17749321\n",
      "Trained batch 177 batch loss 1.20084691 epoch total loss 1.17762518\n",
      "Trained batch 178 batch loss 1.13710713 epoch total loss 1.17739749\n",
      "Trained batch 179 batch loss 1.23149419 epoch total loss 1.17769969\n",
      "Trained batch 180 batch loss 1.14028049 epoch total loss 1.17749178\n",
      "Trained batch 181 batch loss 1.23511171 epoch total loss 1.17781007\n",
      "Trained batch 182 batch loss 1.14522839 epoch total loss 1.17763114\n",
      "Trained batch 183 batch loss 1.26930857 epoch total loss 1.17813206\n",
      "Trained batch 184 batch loss 1.32090807 epoch total loss 1.17890799\n",
      "Trained batch 185 batch loss 1.01183522 epoch total loss 1.17800486\n",
      "Trained batch 186 batch loss 1.20071244 epoch total loss 1.17812705\n",
      "Trained batch 187 batch loss 1.10339 epoch total loss 1.17772734\n",
      "Trained batch 188 batch loss 1.21476877 epoch total loss 1.17792439\n",
      "Trained batch 189 batch loss 1.21899712 epoch total loss 1.17814171\n",
      "Trained batch 190 batch loss 1.27674305 epoch total loss 1.17866063\n",
      "Trained batch 191 batch loss 1.25642252 epoch total loss 1.17906785\n",
      "Trained batch 192 batch loss 1.32739067 epoch total loss 1.17984033\n",
      "Trained batch 193 batch loss 1.31015348 epoch total loss 1.18051553\n",
      "Trained batch 194 batch loss 1.18400753 epoch total loss 1.18053353\n",
      "Trained batch 195 batch loss 1.07823682 epoch total loss 1.18000889\n",
      "Trained batch 196 batch loss 1.10975 epoch total loss 1.17965043\n",
      "Trained batch 197 batch loss 1.18564939 epoch total loss 1.17968094\n",
      "Trained batch 198 batch loss 1.14751887 epoch total loss 1.17951846\n",
      "Trained batch 199 batch loss 1.04798555 epoch total loss 1.17885756\n",
      "Trained batch 200 batch loss 1.08153725 epoch total loss 1.17837095\n",
      "Trained batch 201 batch loss 0.995791078 epoch total loss 1.17746258\n",
      "Trained batch 202 batch loss 1.08315802 epoch total loss 1.17699575\n",
      "Trained batch 203 batch loss 1.33797753 epoch total loss 1.17778885\n",
      "Trained batch 204 batch loss 1.31180203 epoch total loss 1.1784457\n",
      "Trained batch 205 batch loss 1.49188697 epoch total loss 1.17997468\n",
      "Trained batch 206 batch loss 1.17847037 epoch total loss 1.17996728\n",
      "Trained batch 207 batch loss 1.26219261 epoch total loss 1.18036461\n",
      "Trained batch 208 batch loss 1.30841088 epoch total loss 1.18098021\n",
      "Trained batch 209 batch loss 1.06133544 epoch total loss 1.18040776\n",
      "Trained batch 210 batch loss 1.07511151 epoch total loss 1.17990637\n",
      "Trained batch 211 batch loss 1.13281965 epoch total loss 1.17968321\n",
      "Trained batch 212 batch loss 1.13847935 epoch total loss 1.17948878\n",
      "Trained batch 213 batch loss 1.19281793 epoch total loss 1.17955136\n",
      "Trained batch 214 batch loss 1.30667424 epoch total loss 1.18014538\n",
      "Trained batch 215 batch loss 1.17706203 epoch total loss 1.18013108\n",
      "Trained batch 216 batch loss 1.24393499 epoch total loss 1.18042648\n",
      "Trained batch 217 batch loss 1.18449974 epoch total loss 1.18044519\n",
      "Trained batch 218 batch loss 1.12374461 epoch total loss 1.1801852\n",
      "Trained batch 219 batch loss 1.11964965 epoch total loss 1.17990875\n",
      "Trained batch 220 batch loss 1.14554131 epoch total loss 1.17975259\n",
      "Trained batch 221 batch loss 1.12397504 epoch total loss 1.1795001\n",
      "Trained batch 222 batch loss 1.14915168 epoch total loss 1.17936337\n",
      "Trained batch 223 batch loss 1.01043153 epoch total loss 1.17860579\n",
      "Trained batch 224 batch loss 1.01072729 epoch total loss 1.17785645\n",
      "Trained batch 225 batch loss 1.13925648 epoch total loss 1.1776849\n",
      "Trained batch 226 batch loss 1.09742975 epoch total loss 1.17732978\n",
      "Trained batch 227 batch loss 1.17504644 epoch total loss 1.17731977\n",
      "Trained batch 228 batch loss 1.03227258 epoch total loss 1.17668366\n",
      "Trained batch 229 batch loss 1.02728307 epoch total loss 1.17603123\n",
      "Trained batch 230 batch loss 1.05790675 epoch total loss 1.17551756\n",
      "Trained batch 231 batch loss 1.11771178 epoch total loss 1.17526734\n",
      "Trained batch 232 batch loss 1.2017045 epoch total loss 1.17538118\n",
      "Trained batch 233 batch loss 1.16027629 epoch total loss 1.17531645\n",
      "Trained batch 234 batch loss 1.17127621 epoch total loss 1.17529905\n",
      "Trained batch 235 batch loss 1.27913678 epoch total loss 1.17574096\n",
      "Trained batch 236 batch loss 1.19794798 epoch total loss 1.17583501\n",
      "Trained batch 237 batch loss 1.2804718 epoch total loss 1.17627645\n",
      "Trained batch 238 batch loss 1.19487298 epoch total loss 1.17635465\n",
      "Trained batch 239 batch loss 1.25822079 epoch total loss 1.17669713\n",
      "Trained batch 240 batch loss 1.23489666 epoch total loss 1.17693961\n",
      "Trained batch 241 batch loss 1.17432511 epoch total loss 1.17692876\n",
      "Trained batch 242 batch loss 1.19693124 epoch total loss 1.17701137\n",
      "Trained batch 243 batch loss 1.24817932 epoch total loss 1.17730427\n",
      "Trained batch 244 batch loss 1.20845187 epoch total loss 1.17743194\n",
      "Trained batch 245 batch loss 1.24495697 epoch total loss 1.17770755\n",
      "Trained batch 246 batch loss 1.2151686 epoch total loss 1.1778599\n",
      "Trained batch 247 batch loss 1.17290878 epoch total loss 1.17783988\n",
      "Trained batch 248 batch loss 1.21727657 epoch total loss 1.1779989\n",
      "Trained batch 249 batch loss 1.21454716 epoch total loss 1.17814565\n",
      "Trained batch 250 batch loss 1.26822829 epoch total loss 1.17850602\n",
      "Trained batch 251 batch loss 1.15167117 epoch total loss 1.17839909\n",
      "Trained batch 252 batch loss 1.13904917 epoch total loss 1.17824292\n",
      "Trained batch 253 batch loss 1.12085867 epoch total loss 1.17801607\n",
      "Trained batch 254 batch loss 1.00511789 epoch total loss 1.17733538\n",
      "Trained batch 255 batch loss 0.946403921 epoch total loss 1.17642975\n",
      "Trained batch 256 batch loss 1.01425779 epoch total loss 1.17579627\n",
      "Trained batch 257 batch loss 1.09670293 epoch total loss 1.17548859\n",
      "Trained batch 258 batch loss 1.16013253 epoch total loss 1.17542899\n",
      "Trained batch 259 batch loss 1.02922869 epoch total loss 1.17486453\n",
      "Trained batch 260 batch loss 1.04642451 epoch total loss 1.17437053\n",
      "Trained batch 261 batch loss 1.12738371 epoch total loss 1.17419052\n",
      "Trained batch 262 batch loss 1.21085179 epoch total loss 1.17433035\n",
      "Trained batch 263 batch loss 1.15313017 epoch total loss 1.17424977\n",
      "Trained batch 264 batch loss 1.0009104 epoch total loss 1.17359328\n",
      "Trained batch 265 batch loss 1.00250304 epoch total loss 1.17294765\n",
      "Trained batch 266 batch loss 1.2950865 epoch total loss 1.17340672\n",
      "Trained batch 267 batch loss 1.27568769 epoch total loss 1.17378986\n",
      "Trained batch 268 batch loss 1.12245989 epoch total loss 1.17359829\n",
      "Trained batch 269 batch loss 1.1968236 epoch total loss 1.17368472\n",
      "Trained batch 270 batch loss 1.16877091 epoch total loss 1.17366648\n",
      "Trained batch 271 batch loss 1.1513598 epoch total loss 1.17358422\n",
      "Trained batch 272 batch loss 1.05897117 epoch total loss 1.17316282\n",
      "Trained batch 273 batch loss 0.982887685 epoch total loss 1.1724658\n",
      "Trained batch 274 batch loss 1.08348536 epoch total loss 1.17214108\n",
      "Trained batch 275 batch loss 1.17097306 epoch total loss 1.17213678\n",
      "Trained batch 276 batch loss 1.26855767 epoch total loss 1.17248607\n",
      "Trained batch 277 batch loss 1.2020824 epoch total loss 1.172593\n",
      "Trained batch 278 batch loss 1.23920584 epoch total loss 1.17283261\n",
      "Trained batch 279 batch loss 1.03877711 epoch total loss 1.17235208\n",
      "Trained batch 280 batch loss 1.11395478 epoch total loss 1.17214358\n",
      "Trained batch 281 batch loss 1.1892035 epoch total loss 1.17220426\n",
      "Trained batch 282 batch loss 1.15947366 epoch total loss 1.17215919\n",
      "Trained batch 283 batch loss 1.23782039 epoch total loss 1.17239118\n",
      "Trained batch 284 batch loss 1.17266726 epoch total loss 1.17239213\n",
      "Trained batch 285 batch loss 1.22369695 epoch total loss 1.17257214\n",
      "Trained batch 286 batch loss 1.20112407 epoch total loss 1.17267203\n",
      "Trained batch 287 batch loss 1.15362346 epoch total loss 1.17260563\n",
      "Trained batch 288 batch loss 1.1748426 epoch total loss 1.17261338\n",
      "Trained batch 289 batch loss 1.18404603 epoch total loss 1.17265296\n",
      "Trained batch 290 batch loss 1.25642776 epoch total loss 1.1729418\n",
      "Trained batch 291 batch loss 1.23755944 epoch total loss 1.17316389\n",
      "Trained batch 292 batch loss 1.3420949 epoch total loss 1.17374241\n",
      "Trained batch 293 batch loss 1.31760836 epoch total loss 1.17423344\n",
      "Trained batch 294 batch loss 1.20801282 epoch total loss 1.17434824\n",
      "Trained batch 295 batch loss 1.22247529 epoch total loss 1.17451143\n",
      "Trained batch 296 batch loss 1.15901828 epoch total loss 1.1744591\n",
      "Trained batch 297 batch loss 1.13021672 epoch total loss 1.17431009\n",
      "Trained batch 298 batch loss 1.1973418 epoch total loss 1.17438734\n",
      "Trained batch 299 batch loss 1.24384749 epoch total loss 1.17461967\n",
      "Trained batch 300 batch loss 1.19746029 epoch total loss 1.17469573\n",
      "Trained batch 301 batch loss 1.21366787 epoch total loss 1.17482519\n",
      "Trained batch 302 batch loss 1.21327591 epoch total loss 1.17495251\n",
      "Trained batch 303 batch loss 1.11102021 epoch total loss 1.17474151\n",
      "Trained batch 304 batch loss 1.22442412 epoch total loss 1.17490494\n",
      "Trained batch 305 batch loss 1.19996548 epoch total loss 1.17498708\n",
      "Trained batch 306 batch loss 1.25429237 epoch total loss 1.17524624\n",
      "Trained batch 307 batch loss 1.18433285 epoch total loss 1.17527592\n",
      "Trained batch 308 batch loss 1.34809804 epoch total loss 1.17583692\n",
      "Trained batch 309 batch loss 1.31212151 epoch total loss 1.176278\n",
      "Trained batch 310 batch loss 1.52781057 epoch total loss 1.17741191\n",
      "Trained batch 311 batch loss 1.30577457 epoch total loss 1.17782474\n",
      "Trained batch 312 batch loss 1.12428379 epoch total loss 1.17765319\n",
      "Trained batch 313 batch loss 1.10649204 epoch total loss 1.17742586\n",
      "Trained batch 314 batch loss 1.1986469 epoch total loss 1.17749345\n",
      "Trained batch 315 batch loss 1.09582138 epoch total loss 1.17723417\n",
      "Trained batch 316 batch loss 1.07339811 epoch total loss 1.17690551\n",
      "Trained batch 317 batch loss 1.08456421 epoch total loss 1.17661428\n",
      "Trained batch 318 batch loss 1.07354188 epoch total loss 1.17629015\n",
      "Trained batch 319 batch loss 1.03148353 epoch total loss 1.17583621\n",
      "Trained batch 320 batch loss 0.919811785 epoch total loss 1.17503619\n",
      "Trained batch 321 batch loss 1.07632852 epoch total loss 1.17472863\n",
      "Trained batch 322 batch loss 1.11733854 epoch total loss 1.17455041\n",
      "Trained batch 323 batch loss 1.03993177 epoch total loss 1.17413354\n",
      "Trained batch 324 batch loss 1.07212126 epoch total loss 1.17381871\n",
      "Trained batch 325 batch loss 1.03559291 epoch total loss 1.17339337\n",
      "Trained batch 326 batch loss 1.19481707 epoch total loss 1.17345905\n",
      "Trained batch 327 batch loss 1.21683538 epoch total loss 1.17359173\n",
      "Trained batch 328 batch loss 1.0982362 epoch total loss 1.17336202\n",
      "Trained batch 329 batch loss 0.941672206 epoch total loss 1.17265785\n",
      "Trained batch 330 batch loss 1.12926435 epoch total loss 1.17252636\n",
      "Trained batch 331 batch loss 1.05578148 epoch total loss 1.17217362\n",
      "Trained batch 332 batch loss 1.04374492 epoch total loss 1.17178679\n",
      "Trained batch 333 batch loss 1.10118032 epoch total loss 1.17157471\n",
      "Trained batch 334 batch loss 1.20734167 epoch total loss 1.17168176\n",
      "Trained batch 335 batch loss 1.12246132 epoch total loss 1.1715349\n",
      "Trained batch 336 batch loss 1.21582961 epoch total loss 1.17166662\n",
      "Trained batch 337 batch loss 1.20249867 epoch total loss 1.17175806\n",
      "Trained batch 338 batch loss 1.25092351 epoch total loss 1.1719923\n",
      "Trained batch 339 batch loss 1.40398777 epoch total loss 1.17267668\n",
      "Trained batch 340 batch loss 1.50902367 epoch total loss 1.17366588\n",
      "Trained batch 341 batch loss 1.31608307 epoch total loss 1.17408359\n",
      "Trained batch 342 batch loss 1.11780667 epoch total loss 1.17391896\n",
      "Trained batch 343 batch loss 1.20227885 epoch total loss 1.17400157\n",
      "Trained batch 344 batch loss 1.17335987 epoch total loss 1.17399979\n",
      "Trained batch 345 batch loss 1.31426632 epoch total loss 1.17440641\n",
      "Trained batch 346 batch loss 1.27924275 epoch total loss 1.17470932\n",
      "Trained batch 347 batch loss 1.28660059 epoch total loss 1.17503178\n",
      "Trained batch 348 batch loss 1.16748309 epoch total loss 1.17501009\n",
      "Trained batch 349 batch loss 1.24923265 epoch total loss 1.17522275\n",
      "Trained batch 350 batch loss 1.22786462 epoch total loss 1.1753732\n",
      "Trained batch 351 batch loss 1.20724225 epoch total loss 1.17546403\n",
      "Trained batch 352 batch loss 1.20635092 epoch total loss 1.17555177\n",
      "Trained batch 353 batch loss 1.21766078 epoch total loss 1.17567098\n",
      "Trained batch 354 batch loss 1.05638433 epoch total loss 1.1753341\n",
      "Trained batch 355 batch loss 1.12208414 epoch total loss 1.17518401\n",
      "Trained batch 356 batch loss 1.13169956 epoch total loss 1.17506194\n",
      "Trained batch 357 batch loss 1.14160299 epoch total loss 1.17496824\n",
      "Trained batch 358 batch loss 1.20891559 epoch total loss 1.17506301\n",
      "Trained batch 359 batch loss 1.09080791 epoch total loss 1.17482841\n",
      "Trained batch 360 batch loss 1.1185081 epoch total loss 1.17467189\n",
      "Trained batch 361 batch loss 1.1051569 epoch total loss 1.17447937\n",
      "Trained batch 362 batch loss 1.31717908 epoch total loss 1.17487359\n",
      "Trained batch 363 batch loss 1.23916698 epoch total loss 1.17505074\n",
      "Trained batch 364 batch loss 1.17560756 epoch total loss 1.17505217\n",
      "Trained batch 365 batch loss 1.01974583 epoch total loss 1.17462671\n",
      "Trained batch 366 batch loss 0.980988324 epoch total loss 1.17409766\n",
      "Trained batch 367 batch loss 0.906057417 epoch total loss 1.17336726\n",
      "Trained batch 368 batch loss 0.884754717 epoch total loss 1.17258298\n",
      "Trained batch 369 batch loss 1.10928071 epoch total loss 1.17241144\n",
      "Trained batch 370 batch loss 1.19903255 epoch total loss 1.17248344\n",
      "Trained batch 371 batch loss 1.11552727 epoch total loss 1.17233\n",
      "Trained batch 372 batch loss 1.15980971 epoch total loss 1.17229629\n",
      "Trained batch 373 batch loss 1.18478274 epoch total loss 1.17232978\n",
      "Trained batch 374 batch loss 1.30807579 epoch total loss 1.17269278\n",
      "Trained batch 375 batch loss 1.16429186 epoch total loss 1.17267036\n",
      "Trained batch 376 batch loss 1.23463774 epoch total loss 1.17283523\n",
      "Trained batch 377 batch loss 1.1532985 epoch total loss 1.17278337\n",
      "Trained batch 378 batch loss 1.01422858 epoch total loss 1.17236388\n",
      "Trained batch 379 batch loss 1.19274116 epoch total loss 1.17241776\n",
      "Trained batch 380 batch loss 1.29865468 epoch total loss 1.17274988\n",
      "Trained batch 381 batch loss 1.28322077 epoch total loss 1.17303991\n",
      "Trained batch 382 batch loss 1.29260504 epoch total loss 1.17335284\n",
      "Trained batch 383 batch loss 1.15934396 epoch total loss 1.17331624\n",
      "Trained batch 384 batch loss 1.20318353 epoch total loss 1.17339408\n",
      "Trained batch 385 batch loss 1.17239141 epoch total loss 1.17339146\n",
      "Trained batch 386 batch loss 1.01913285 epoch total loss 1.17299187\n",
      "Trained batch 387 batch loss 1.17993701 epoch total loss 1.17300975\n",
      "Trained batch 388 batch loss 1.28662658 epoch total loss 1.17330253\n",
      "Trained batch 389 batch loss 1.2224021 epoch total loss 1.17342877\n",
      "Trained batch 390 batch loss 1.17817855 epoch total loss 1.17344105\n",
      "Trained batch 391 batch loss 1.34681964 epoch total loss 1.17388451\n",
      "Trained batch 392 batch loss 1.27799273 epoch total loss 1.17415\n",
      "Trained batch 393 batch loss 1.21744061 epoch total loss 1.17426014\n",
      "Trained batch 394 batch loss 1.30761921 epoch total loss 1.17459869\n",
      "Trained batch 395 batch loss 1.33251333 epoch total loss 1.1749984\n",
      "Trained batch 396 batch loss 1.23861504 epoch total loss 1.1751591\n",
      "Trained batch 397 batch loss 1.20064807 epoch total loss 1.17522335\n",
      "Trained batch 398 batch loss 1.14201689 epoch total loss 1.1751399\n",
      "Trained batch 399 batch loss 1.21410346 epoch total loss 1.17523754\n",
      "Trained batch 400 batch loss 1.19482124 epoch total loss 1.17528653\n",
      "Trained batch 401 batch loss 1.16655993 epoch total loss 1.17526484\n",
      "Trained batch 402 batch loss 1.2354095 epoch total loss 1.17541444\n",
      "Trained batch 403 batch loss 1.28478551 epoch total loss 1.17568588\n",
      "Trained batch 404 batch loss 1.2872318 epoch total loss 1.17596197\n",
      "Trained batch 405 batch loss 1.22020376 epoch total loss 1.17607117\n",
      "Trained batch 406 batch loss 1.12109542 epoch total loss 1.17593575\n",
      "Trained batch 407 batch loss 1.1170491 epoch total loss 1.17579103\n",
      "Trained batch 408 batch loss 1.06221485 epoch total loss 1.17551267\n",
      "Trained batch 409 batch loss 1.1595844 epoch total loss 1.17547381\n",
      "Trained batch 410 batch loss 1.17844772 epoch total loss 1.17548096\n",
      "Trained batch 411 batch loss 1.15355849 epoch total loss 1.17542768\n",
      "Trained batch 412 batch loss 1.08414626 epoch total loss 1.17520607\n",
      "Trained batch 413 batch loss 1.18791366 epoch total loss 1.17523694\n",
      "Trained batch 414 batch loss 1.21376753 epoch total loss 1.17532992\n",
      "Trained batch 415 batch loss 1.23143101 epoch total loss 1.17546523\n",
      "Trained batch 416 batch loss 1.42685747 epoch total loss 1.1760695\n",
      "Trained batch 417 batch loss 1.44009888 epoch total loss 1.17670262\n",
      "Trained batch 418 batch loss 1.31580222 epoch total loss 1.17703533\n",
      "Trained batch 419 batch loss 1.14642179 epoch total loss 1.17696238\n",
      "Trained batch 420 batch loss 1.19244313 epoch total loss 1.17699921\n",
      "Trained batch 421 batch loss 1.13420796 epoch total loss 1.17689753\n",
      "Trained batch 422 batch loss 1.11255 epoch total loss 1.17674506\n",
      "Trained batch 423 batch loss 1.18397987 epoch total loss 1.17676222\n",
      "Trained batch 424 batch loss 1.24724579 epoch total loss 1.17692852\n",
      "Trained batch 425 batch loss 1.17328835 epoch total loss 1.17691994\n",
      "Trained batch 426 batch loss 1.1642108 epoch total loss 1.17689\n",
      "Trained batch 427 batch loss 0.976653516 epoch total loss 1.17642117\n",
      "Trained batch 428 batch loss 1.14630651 epoch total loss 1.17635071\n",
      "Trained batch 429 batch loss 1.04236913 epoch total loss 1.17603838\n",
      "Trained batch 430 batch loss 1.02438879 epoch total loss 1.17568576\n",
      "Trained batch 431 batch loss 1.09605062 epoch total loss 1.17550087\n",
      "Trained batch 432 batch loss 1.05006635 epoch total loss 1.1752106\n",
      "Trained batch 433 batch loss 1.18432975 epoch total loss 1.1752317\n",
      "Trained batch 434 batch loss 1.1748364 epoch total loss 1.17523074\n",
      "Trained batch 435 batch loss 1.16310155 epoch total loss 1.17520285\n",
      "Trained batch 436 batch loss 1.12089586 epoch total loss 1.17507839\n",
      "Trained batch 437 batch loss 1.26866341 epoch total loss 1.17529249\n",
      "Trained batch 438 batch loss 1.18958783 epoch total loss 1.17532516\n",
      "Trained batch 439 batch loss 1.04439902 epoch total loss 1.17502689\n",
      "Trained batch 440 batch loss 0.932501435 epoch total loss 1.17447567\n",
      "Trained batch 441 batch loss 1.20194542 epoch total loss 1.17453802\n",
      "Trained batch 442 batch loss 1.16724658 epoch total loss 1.17452145\n",
      "Trained batch 443 batch loss 1.34356916 epoch total loss 1.17490304\n",
      "Trained batch 444 batch loss 1.22357965 epoch total loss 1.17501271\n",
      "Trained batch 445 batch loss 1.41432428 epoch total loss 1.17555046\n",
      "Trained batch 446 batch loss 1.31866503 epoch total loss 1.17587125\n",
      "Trained batch 447 batch loss 1.26124275 epoch total loss 1.17606223\n",
      "Trained batch 448 batch loss 1.26122189 epoch total loss 1.17625237\n",
      "Trained batch 449 batch loss 1.21571994 epoch total loss 1.17634022\n",
      "Trained batch 450 batch loss 1.27004957 epoch total loss 1.17654836\n",
      "Trained batch 451 batch loss 1.2120086 epoch total loss 1.17662704\n",
      "Trained batch 452 batch loss 1.3732971 epoch total loss 1.17706215\n",
      "Trained batch 453 batch loss 1.23119724 epoch total loss 1.17718172\n",
      "Trained batch 454 batch loss 1.16299284 epoch total loss 1.17715037\n",
      "Trained batch 455 batch loss 1.27681398 epoch total loss 1.17736936\n",
      "Trained batch 456 batch loss 1.22772074 epoch total loss 1.17747974\n",
      "Trained batch 457 batch loss 1.15836132 epoch total loss 1.17743802\n",
      "Trained batch 458 batch loss 1.06797481 epoch total loss 1.17719901\n",
      "Trained batch 459 batch loss 0.999449492 epoch total loss 1.17681181\n",
      "Trained batch 460 batch loss 0.954505265 epoch total loss 1.17632854\n",
      "Trained batch 461 batch loss 1.14482725 epoch total loss 1.17626023\n",
      "Trained batch 462 batch loss 1.18909562 epoch total loss 1.17628801\n",
      "Trained batch 463 batch loss 1.38199 epoch total loss 1.17673242\n",
      "Trained batch 464 batch loss 1.32782614 epoch total loss 1.17705798\n",
      "Trained batch 465 batch loss 1.11148858 epoch total loss 1.17691708\n",
      "Trained batch 466 batch loss 1.10237026 epoch total loss 1.17675698\n",
      "Trained batch 467 batch loss 1.05819523 epoch total loss 1.17650306\n",
      "Trained batch 468 batch loss 1.3099252 epoch total loss 1.17678821\n",
      "Trained batch 469 batch loss 1.11444283 epoch total loss 1.17665529\n",
      "Trained batch 470 batch loss 1.21309698 epoch total loss 1.17673278\n",
      "Trained batch 471 batch loss 1.25520301 epoch total loss 1.17689931\n",
      "Trained batch 472 batch loss 1.33655286 epoch total loss 1.17723751\n",
      "Trained batch 473 batch loss 1.28353977 epoch total loss 1.17746234\n",
      "Trained batch 474 batch loss 1.19288766 epoch total loss 1.17749488\n",
      "Trained batch 475 batch loss 1.10539818 epoch total loss 1.17734313\n",
      "Trained batch 476 batch loss 1.00665581 epoch total loss 1.17698455\n",
      "Trained batch 477 batch loss 1.1542747 epoch total loss 1.17693698\n",
      "Trained batch 478 batch loss 1.30087924 epoch total loss 1.17719626\n",
      "Trained batch 479 batch loss 1.24392354 epoch total loss 1.1773355\n",
      "Trained batch 480 batch loss 1.21408474 epoch total loss 1.17741215\n",
      "Trained batch 481 batch loss 1.24913216 epoch total loss 1.17756128\n",
      "Trained batch 482 batch loss 1.25486362 epoch total loss 1.17772174\n",
      "Trained batch 483 batch loss 1.25181568 epoch total loss 1.17787516\n",
      "Trained batch 484 batch loss 0.99487555 epoch total loss 1.17749703\n",
      "Trained batch 485 batch loss 1.16595459 epoch total loss 1.17747319\n",
      "Trained batch 486 batch loss 1.04972112 epoch total loss 1.17721045\n",
      "Trained batch 487 batch loss 0.997226238 epoch total loss 1.1768409\n",
      "Trained batch 488 batch loss 1.0197022 epoch total loss 1.17651892\n",
      "Trained batch 489 batch loss 1.00118792 epoch total loss 1.17616034\n",
      "Trained batch 490 batch loss 0.994548619 epoch total loss 1.17578971\n",
      "Trained batch 491 batch loss 0.905315518 epoch total loss 1.17523885\n",
      "Trained batch 492 batch loss 0.877000034 epoch total loss 1.17463279\n",
      "Trained batch 493 batch loss 0.886811733 epoch total loss 1.17404902\n",
      "Trained batch 494 batch loss 1.05191946 epoch total loss 1.17380178\n",
      "Trained batch 495 batch loss 1.1088872 epoch total loss 1.17367065\n",
      "Trained batch 496 batch loss 1.15203702 epoch total loss 1.17362702\n",
      "Trained batch 497 batch loss 1.11504292 epoch total loss 1.17350924\n",
      "Trained batch 498 batch loss 1.12277246 epoch total loss 1.17340732\n",
      "Trained batch 499 batch loss 1.16528726 epoch total loss 1.1733911\n",
      "Trained batch 500 batch loss 1.13724327 epoch total loss 1.17331886\n",
      "Trained batch 501 batch loss 1.30933595 epoch total loss 1.1735903\n",
      "Trained batch 502 batch loss 1.1721673 epoch total loss 1.17358756\n",
      "Trained batch 503 batch loss 1.25015986 epoch total loss 1.17373979\n",
      "Trained batch 504 batch loss 1.21446729 epoch total loss 1.17382061\n",
      "Trained batch 505 batch loss 1.12342286 epoch total loss 1.17372084\n",
      "Trained batch 506 batch loss 1.02017403 epoch total loss 1.17341745\n",
      "Trained batch 507 batch loss 1.08119512 epoch total loss 1.17323542\n",
      "Trained batch 508 batch loss 1.05604804 epoch total loss 1.17300475\n",
      "Trained batch 509 batch loss 1.0852983 epoch total loss 1.17283249\n",
      "Trained batch 510 batch loss 1.04843211 epoch total loss 1.17258859\n",
      "Trained batch 511 batch loss 1.11664867 epoch total loss 1.17247915\n",
      "Trained batch 512 batch loss 1.05854523 epoch total loss 1.17225659\n",
      "Trained batch 513 batch loss 1.08014321 epoch total loss 1.17207706\n",
      "Trained batch 514 batch loss 1.02202511 epoch total loss 1.17178512\n",
      "Trained batch 515 batch loss 1.15027905 epoch total loss 1.17174327\n",
      "Trained batch 516 batch loss 1.11536813 epoch total loss 1.17163408\n",
      "Trained batch 517 batch loss 1.21506655 epoch total loss 1.17171812\n",
      "Trained batch 518 batch loss 1.1883738 epoch total loss 1.17175019\n",
      "Trained batch 519 batch loss 1.19470322 epoch total loss 1.17179441\n",
      "Trained batch 520 batch loss 1.18209517 epoch total loss 1.1718142\n",
      "Trained batch 521 batch loss 1.11129642 epoch total loss 1.17169797\n",
      "Trained batch 522 batch loss 1.07026279 epoch total loss 1.17150366\n",
      "Trained batch 523 batch loss 1.14422941 epoch total loss 1.17145145\n",
      "Trained batch 524 batch loss 1.14960957 epoch total loss 1.17140973\n",
      "Trained batch 525 batch loss 1.03094053 epoch total loss 1.17114222\n",
      "Trained batch 526 batch loss 1.15838265 epoch total loss 1.17111802\n",
      "Trained batch 527 batch loss 1.11794877 epoch total loss 1.17101705\n",
      "Trained batch 528 batch loss 1.2069366 epoch total loss 1.171085\n",
      "Trained batch 529 batch loss 1.1116457 epoch total loss 1.17097259\n",
      "Trained batch 530 batch loss 1.04595721 epoch total loss 1.17073679\n",
      "Trained batch 531 batch loss 1.30648422 epoch total loss 1.17099237\n",
      "Trained batch 532 batch loss 1.08409679 epoch total loss 1.17082906\n",
      "Trained batch 533 batch loss 1.10414624 epoch total loss 1.17070389\n",
      "Trained batch 534 batch loss 1.23761678 epoch total loss 1.17082918\n",
      "Trained batch 535 batch loss 1.11825228 epoch total loss 1.17073083\n",
      "Trained batch 536 batch loss 1.15593898 epoch total loss 1.17070329\n",
      "Trained batch 537 batch loss 1.27617955 epoch total loss 1.17089963\n",
      "Trained batch 538 batch loss 1.10830057 epoch total loss 1.17078328\n",
      "Trained batch 539 batch loss 1.16081524 epoch total loss 1.1707648\n",
      "Trained batch 540 batch loss 1.07612264 epoch total loss 1.17058957\n",
      "Trained batch 541 batch loss 1.1205554 epoch total loss 1.17049706\n",
      "Trained batch 542 batch loss 1.13228893 epoch total loss 1.17042649\n",
      "Trained batch 543 batch loss 1.1375761 epoch total loss 1.17036593\n",
      "Trained batch 544 batch loss 1.07016063 epoch total loss 1.17018187\n",
      "Trained batch 545 batch loss 1.09373164 epoch total loss 1.17004156\n",
      "Trained batch 546 batch loss 1.06139946 epoch total loss 1.1698426\n",
      "Trained batch 547 batch loss 1.13428974 epoch total loss 1.16977763\n",
      "Trained batch 548 batch loss 1.2601459 epoch total loss 1.1699425\n",
      "Trained batch 549 batch loss 1.12013018 epoch total loss 1.16985178\n",
      "Trained batch 550 batch loss 1.22306633 epoch total loss 1.16994846\n",
      "Trained batch 551 batch loss 1.29691982 epoch total loss 1.17017901\n",
      "Trained batch 552 batch loss 1.18493652 epoch total loss 1.17020571\n",
      "Trained batch 553 batch loss 1.29770613 epoch total loss 1.17043626\n",
      "Trained batch 554 batch loss 1.22083509 epoch total loss 1.17052722\n",
      "Trained batch 555 batch loss 1.08800507 epoch total loss 1.17037857\n",
      "Trained batch 556 batch loss 1.05675542 epoch total loss 1.17017424\n",
      "Trained batch 557 batch loss 1.24640131 epoch total loss 1.17031109\n",
      "Trained batch 558 batch loss 1.13410044 epoch total loss 1.17024624\n",
      "Trained batch 559 batch loss 1.29354024 epoch total loss 1.17046666\n",
      "Trained batch 560 batch loss 1.25542 epoch total loss 1.17061841\n",
      "Trained batch 561 batch loss 1.28531921 epoch total loss 1.17082298\n",
      "Trained batch 562 batch loss 1.07298064 epoch total loss 1.17064881\n",
      "Trained batch 563 batch loss 1.1994921 epoch total loss 1.17070007\n",
      "Trained batch 564 batch loss 1.03345251 epoch total loss 1.17045665\n",
      "Trained batch 565 batch loss 0.978645742 epoch total loss 1.17011714\n",
      "Trained batch 566 batch loss 1.23069835 epoch total loss 1.17022419\n",
      "Trained batch 567 batch loss 1.15418744 epoch total loss 1.17019594\n",
      "Trained batch 568 batch loss 1.16377389 epoch total loss 1.17018461\n",
      "Trained batch 569 batch loss 1.25101066 epoch total loss 1.17032671\n",
      "Trained batch 570 batch loss 1.28924775 epoch total loss 1.17053533\n",
      "Trained batch 571 batch loss 1.1238023 epoch total loss 1.17045343\n",
      "Trained batch 572 batch loss 1.09186113 epoch total loss 1.1703161\n",
      "Trained batch 573 batch loss 1.02272189 epoch total loss 1.17005849\n",
      "Trained batch 574 batch loss 1.14921665 epoch total loss 1.17002213\n",
      "Trained batch 575 batch loss 1.15557027 epoch total loss 1.16999698\n",
      "Trained batch 576 batch loss 1.11864841 epoch total loss 1.16990793\n",
      "Trained batch 577 batch loss 1.28846753 epoch total loss 1.17011333\n",
      "Trained batch 578 batch loss 1.20423543 epoch total loss 1.17017233\n",
      "Trained batch 579 batch loss 1.3406924 epoch total loss 1.1704669\n",
      "Trained batch 580 batch loss 1.23802066 epoch total loss 1.17058337\n",
      "Trained batch 581 batch loss 1.26353288 epoch total loss 1.17074335\n",
      "Trained batch 582 batch loss 1.25325155 epoch total loss 1.17088509\n",
      "Trained batch 583 batch loss 1.25682306 epoch total loss 1.17103255\n",
      "Trained batch 584 batch loss 1.29771423 epoch total loss 1.17124951\n",
      "Trained batch 585 batch loss 1.24798334 epoch total loss 1.17138064\n",
      "Trained batch 586 batch loss 1.19679177 epoch total loss 1.17142403\n",
      "Trained batch 587 batch loss 1.24929273 epoch total loss 1.17155659\n",
      "Trained batch 588 batch loss 1.16882598 epoch total loss 1.17155194\n",
      "Trained batch 589 batch loss 1.1579721 epoch total loss 1.17152894\n",
      "Trained batch 590 batch loss 1.05912089 epoch total loss 1.17133844\n",
      "Trained batch 591 batch loss 1.04916191 epoch total loss 1.17113161\n",
      "Trained batch 592 batch loss 1.25975132 epoch total loss 1.17128134\n",
      "Trained batch 593 batch loss 1.17521167 epoch total loss 1.17128801\n",
      "Trained batch 594 batch loss 1.26472843 epoch total loss 1.17144525\n",
      "Trained batch 595 batch loss 1.24847233 epoch total loss 1.17157471\n",
      "Trained batch 596 batch loss 1.21420157 epoch total loss 1.17164624\n",
      "Trained batch 597 batch loss 1.28684521 epoch total loss 1.17183924\n",
      "Trained batch 598 batch loss 1.2458632 epoch total loss 1.17196298\n",
      "Trained batch 599 batch loss 1.26898313 epoch total loss 1.17212498\n",
      "Trained batch 600 batch loss 1.26475441 epoch total loss 1.17227936\n",
      "Trained batch 601 batch loss 1.16813755 epoch total loss 1.17227244\n",
      "Trained batch 602 batch loss 1.10807204 epoch total loss 1.17216587\n",
      "Trained batch 603 batch loss 1.14385033 epoch total loss 1.1721189\n",
      "Trained batch 604 batch loss 1.10990167 epoch total loss 1.17201602\n",
      "Trained batch 605 batch loss 0.985095263 epoch total loss 1.17170703\n",
      "Trained batch 606 batch loss 1.23476088 epoch total loss 1.17181098\n",
      "Trained batch 607 batch loss 1.15951252 epoch total loss 1.17179072\n",
      "Trained batch 608 batch loss 1.09182429 epoch total loss 1.17165911\n",
      "Trained batch 609 batch loss 1.07039881 epoch total loss 1.17149282\n",
      "Trained batch 610 batch loss 1.04280603 epoch total loss 1.17128181\n",
      "Trained batch 611 batch loss 1.19646096 epoch total loss 1.17132306\n",
      "Trained batch 612 batch loss 1.10674691 epoch total loss 1.17121756\n",
      "Trained batch 613 batch loss 1.22732067 epoch total loss 1.17130911\n",
      "Trained batch 614 batch loss 1.2847563 epoch total loss 1.17149377\n",
      "Trained batch 615 batch loss 1.30785966 epoch total loss 1.1717155\n",
      "Trained batch 616 batch loss 1.32264256 epoch total loss 1.17196047\n",
      "Trained batch 617 batch loss 1.31844187 epoch total loss 1.17219794\n",
      "Trained batch 618 batch loss 1.19223 epoch total loss 1.17223024\n",
      "Trained batch 619 batch loss 1.11342359 epoch total loss 1.17213523\n",
      "Trained batch 620 batch loss 1.25737488 epoch total loss 1.17227268\n",
      "Trained batch 621 batch loss 1.2741909 epoch total loss 1.17243683\n",
      "Trained batch 622 batch loss 1.13539553 epoch total loss 1.17237723\n",
      "Trained batch 623 batch loss 1.08374882 epoch total loss 1.17223489\n",
      "Trained batch 624 batch loss 1.09855556 epoch total loss 1.17211688\n",
      "Trained batch 625 batch loss 1.12110925 epoch total loss 1.17203522\n",
      "Trained batch 626 batch loss 1.0993619 epoch total loss 1.17191923\n",
      "Trained batch 627 batch loss 1.05677938 epoch total loss 1.17173553\n",
      "Trained batch 628 batch loss 1.16754103 epoch total loss 1.17172885\n",
      "Trained batch 629 batch loss 1.20439148 epoch total loss 1.17178082\n",
      "Trained batch 630 batch loss 1.17902148 epoch total loss 1.17179227\n",
      "Trained batch 631 batch loss 1.03146887 epoch total loss 1.17157\n",
      "Trained batch 632 batch loss 0.960737 epoch total loss 1.1712364\n",
      "Trained batch 633 batch loss 1.13256717 epoch total loss 1.17117524\n",
      "Trained batch 634 batch loss 1.26048279 epoch total loss 1.17131615\n",
      "Trained batch 635 batch loss 1.31348109 epoch total loss 1.17154\n",
      "Trained batch 636 batch loss 1.12969112 epoch total loss 1.17147422\n",
      "Trained batch 637 batch loss 1.14142048 epoch total loss 1.17142701\n",
      "Trained batch 638 batch loss 1.23554349 epoch total loss 1.1715275\n",
      "Trained batch 639 batch loss 1.27086926 epoch total loss 1.17168295\n",
      "Trained batch 640 batch loss 1.1806004 epoch total loss 1.1716969\n",
      "Trained batch 641 batch loss 1.12311184 epoch total loss 1.1716212\n",
      "Trained batch 642 batch loss 1.10988903 epoch total loss 1.171525\n",
      "Trained batch 643 batch loss 1.18599367 epoch total loss 1.17154741\n",
      "Trained batch 644 batch loss 1.1653769 epoch total loss 1.17153788\n",
      "Trained batch 645 batch loss 1.0292151 epoch total loss 1.17131722\n",
      "Trained batch 646 batch loss 1.23967242 epoch total loss 1.17142308\n",
      "Trained batch 647 batch loss 1.05613172 epoch total loss 1.17124498\n",
      "Trained batch 648 batch loss 1.12328553 epoch total loss 1.17117095\n",
      "Trained batch 649 batch loss 1.02307618 epoch total loss 1.17094278\n",
      "Trained batch 650 batch loss 1.03813386 epoch total loss 1.17073846\n",
      "Trained batch 651 batch loss 1.01519811 epoch total loss 1.17049956\n",
      "Trained batch 652 batch loss 0.905059099 epoch total loss 1.17009234\n",
      "Trained batch 653 batch loss 0.965511322 epoch total loss 1.16977906\n",
      "Trained batch 654 batch loss 1.10808933 epoch total loss 1.16968477\n",
      "Trained batch 655 batch loss 1.37319803 epoch total loss 1.16999543\n",
      "Trained batch 656 batch loss 1.49823844 epoch total loss 1.17049575\n",
      "Trained batch 657 batch loss 1.25346732 epoch total loss 1.17062211\n",
      "Trained batch 658 batch loss 1.14607048 epoch total loss 1.17058468\n",
      "Trained batch 659 batch loss 1.21844029 epoch total loss 1.1706574\n",
      "Trained batch 660 batch loss 1.15319431 epoch total loss 1.17063093\n",
      "Trained batch 661 batch loss 1.31241989 epoch total loss 1.17084539\n",
      "Trained batch 662 batch loss 1.17055094 epoch total loss 1.17084491\n",
      "Trained batch 663 batch loss 1.10079 epoch total loss 1.17073929\n",
      "Trained batch 664 batch loss 1.32661986 epoch total loss 1.17097402\n",
      "Trained batch 665 batch loss 1.13980818 epoch total loss 1.17092717\n",
      "Trained batch 666 batch loss 1.22966671 epoch total loss 1.17101538\n",
      "Trained batch 667 batch loss 1.01476574 epoch total loss 1.17078114\n",
      "Trained batch 668 batch loss 1.09984374 epoch total loss 1.17067492\n",
      "Trained batch 669 batch loss 1.07060182 epoch total loss 1.17052543\n",
      "Trained batch 670 batch loss 1.2102865 epoch total loss 1.17058468\n",
      "Trained batch 671 batch loss 1.27382624 epoch total loss 1.17073858\n",
      "Trained batch 672 batch loss 0.935658693 epoch total loss 1.1703887\n",
      "Trained batch 673 batch loss 1.20026755 epoch total loss 1.17043304\n",
      "Trained batch 674 batch loss 1.11165857 epoch total loss 1.1703459\n",
      "Trained batch 675 batch loss 1.19619644 epoch total loss 1.17038417\n",
      "Trained batch 676 batch loss 1.22233891 epoch total loss 1.17046094\n",
      "Trained batch 677 batch loss 1.23595595 epoch total loss 1.17055774\n",
      "Trained batch 678 batch loss 1.36673927 epoch total loss 1.17084718\n",
      "Trained batch 679 batch loss 1.12966132 epoch total loss 1.17078638\n",
      "Trained batch 680 batch loss 1.14520168 epoch total loss 1.17074883\n",
      "Trained batch 681 batch loss 1.34042978 epoch total loss 1.17099798\n",
      "Trained batch 682 batch loss 1.34048343 epoch total loss 1.17124653\n",
      "Trained batch 683 batch loss 1.24478567 epoch total loss 1.17135417\n",
      "Trained batch 684 batch loss 1.0903666 epoch total loss 1.1712358\n",
      "Trained batch 685 batch loss 1.1966176 epoch total loss 1.17127287\n",
      "Trained batch 686 batch loss 1.13889194 epoch total loss 1.17122567\n",
      "Trained batch 687 batch loss 1.18033504 epoch total loss 1.17123902\n",
      "Trained batch 688 batch loss 1.22344649 epoch total loss 1.17131484\n",
      "Trained batch 689 batch loss 1.25857282 epoch total loss 1.17144144\n",
      "Trained batch 690 batch loss 1.02428854 epoch total loss 1.17122817\n",
      "Trained batch 691 batch loss 1.1413461 epoch total loss 1.17118502\n",
      "Trained batch 692 batch loss 1.14704752 epoch total loss 1.17115009\n",
      "Trained batch 693 batch loss 1.11549 epoch total loss 1.17106974\n",
      "Trained batch 694 batch loss 1.22957909 epoch total loss 1.17115402\n",
      "Trained batch 695 batch loss 1.24288559 epoch total loss 1.17125714\n",
      "Trained batch 696 batch loss 1.21830583 epoch total loss 1.17132485\n",
      "Trained batch 697 batch loss 1.26375175 epoch total loss 1.17145741\n",
      "Trained batch 698 batch loss 1.21940815 epoch total loss 1.17152607\n",
      "Trained batch 699 batch loss 1.26766658 epoch total loss 1.17166364\n",
      "Trained batch 700 batch loss 1.20156026 epoch total loss 1.17170632\n",
      "Trained batch 701 batch loss 1.22110486 epoch total loss 1.17177677\n",
      "Trained batch 702 batch loss 1.11110449 epoch total loss 1.17169034\n",
      "Trained batch 703 batch loss 1.13796091 epoch total loss 1.1716423\n",
      "Trained batch 704 batch loss 1.33963346 epoch total loss 1.17188096\n",
      "Trained batch 705 batch loss 1.32359695 epoch total loss 1.17209625\n",
      "Trained batch 706 batch loss 1.22790742 epoch total loss 1.17217529\n",
      "Trained batch 707 batch loss 1.11974549 epoch total loss 1.17210114\n",
      "Trained batch 708 batch loss 1.15951443 epoch total loss 1.17208326\n",
      "Trained batch 709 batch loss 1.1481638 epoch total loss 1.17204964\n",
      "Trained batch 710 batch loss 1.2608161 epoch total loss 1.17217457\n",
      "Trained batch 711 batch loss 1.30815148 epoch total loss 1.1723659\n",
      "Trained batch 712 batch loss 1.34298563 epoch total loss 1.1726054\n",
      "Trained batch 713 batch loss 1.28805363 epoch total loss 1.17276728\n",
      "Trained batch 714 batch loss 1.05426371 epoch total loss 1.17260134\n",
      "Trained batch 715 batch loss 1.04261732 epoch total loss 1.17241955\n",
      "Trained batch 716 batch loss 1.02784491 epoch total loss 1.17221761\n",
      "Trained batch 717 batch loss 1.28082299 epoch total loss 1.17236912\n",
      "Trained batch 718 batch loss 1.19732583 epoch total loss 1.17240381\n",
      "Trained batch 719 batch loss 1.27790976 epoch total loss 1.17255056\n",
      "Trained batch 720 batch loss 1.21231115 epoch total loss 1.17260575\n",
      "Trained batch 721 batch loss 1.24596202 epoch total loss 1.17270756\n",
      "Trained batch 722 batch loss 1.17832613 epoch total loss 1.17271543\n",
      "Trained batch 723 batch loss 1.22274017 epoch total loss 1.17278457\n",
      "Trained batch 724 batch loss 1.18579745 epoch total loss 1.17280245\n",
      "Trained batch 725 batch loss 1.13594151 epoch total loss 1.17275167\n",
      "Trained batch 726 batch loss 1.15500212 epoch total loss 1.17272723\n",
      "Trained batch 727 batch loss 1.14266 epoch total loss 1.17268586\n",
      "Trained batch 728 batch loss 1.12786877 epoch total loss 1.17262423\n",
      "Trained batch 729 batch loss 1.10477686 epoch total loss 1.17253125\n",
      "Trained batch 730 batch loss 1.05219781 epoch total loss 1.17236638\n",
      "Trained batch 731 batch loss 1.03543472 epoch total loss 1.1721791\n",
      "Trained batch 732 batch loss 1.06332672 epoch total loss 1.17203045\n",
      "Trained batch 733 batch loss 1.14927924 epoch total loss 1.17199945\n",
      "Trained batch 734 batch loss 1.10975528 epoch total loss 1.17191458\n",
      "Trained batch 735 batch loss 1.22645116 epoch total loss 1.17198873\n",
      "Trained batch 736 batch loss 1.1746434 epoch total loss 1.1719923\n",
      "Trained batch 737 batch loss 1.23204017 epoch total loss 1.17207384\n",
      "Trained batch 738 batch loss 1.14084661 epoch total loss 1.17203152\n",
      "Trained batch 739 batch loss 1.23823023 epoch total loss 1.17212117\n",
      "Trained batch 740 batch loss 1.22789693 epoch total loss 1.17219651\n",
      "Trained batch 741 batch loss 1.2043469 epoch total loss 1.1722399\n",
      "Trained batch 742 batch loss 1.38278508 epoch total loss 1.17252374\n",
      "Trained batch 743 batch loss 1.36570907 epoch total loss 1.17278373\n",
      "Trained batch 744 batch loss 1.26549196 epoch total loss 1.17290831\n",
      "Trained batch 745 batch loss 1.25614429 epoch total loss 1.17302012\n",
      "Trained batch 746 batch loss 1.1494956 epoch total loss 1.17298853\n",
      "Trained batch 747 batch loss 0.94942081 epoch total loss 1.1726892\n",
      "Trained batch 748 batch loss 1.05257404 epoch total loss 1.17252862\n",
      "Trained batch 749 batch loss 1.11632156 epoch total loss 1.17245352\n",
      "Trained batch 750 batch loss 0.894360423 epoch total loss 1.17208278\n",
      "Trained batch 751 batch loss 0.953460932 epoch total loss 1.17179167\n",
      "Trained batch 752 batch loss 0.917134047 epoch total loss 1.171453\n",
      "Trained batch 753 batch loss 0.98385942 epoch total loss 1.17120397\n",
      "Trained batch 754 batch loss 1.08185077 epoch total loss 1.17108548\n",
      "Trained batch 755 batch loss 1.10654032 epoch total loss 1.171\n",
      "Trained batch 756 batch loss 1.23069882 epoch total loss 1.17107892\n",
      "Trained batch 757 batch loss 1.22243297 epoch total loss 1.17114675\n",
      "Trained batch 758 batch loss 1.19736958 epoch total loss 1.17118144\n",
      "Trained batch 759 batch loss 1.21010184 epoch total loss 1.1712327\n",
      "Trained batch 760 batch loss 1.13067627 epoch total loss 1.17117929\n",
      "Trained batch 761 batch loss 1.36206949 epoch total loss 1.17143011\n",
      "Trained batch 762 batch loss 1.16679633 epoch total loss 1.17142403\n",
      "Trained batch 763 batch loss 1.22420514 epoch total loss 1.17149317\n",
      "Trained batch 764 batch loss 1.20882475 epoch total loss 1.17154205\n",
      "Trained batch 765 batch loss 1.19526172 epoch total loss 1.17157304\n",
      "Trained batch 766 batch loss 1.20963264 epoch total loss 1.17162275\n",
      "Trained batch 767 batch loss 1.2661854 epoch total loss 1.17174602\n",
      "Trained batch 768 batch loss 1.41665399 epoch total loss 1.1720649\n",
      "Trained batch 769 batch loss 1.37810755 epoch total loss 1.17233276\n",
      "Trained batch 770 batch loss 1.18153453 epoch total loss 1.17234468\n",
      "Trained batch 771 batch loss 1.22083879 epoch total loss 1.17240763\n",
      "Trained batch 772 batch loss 1.22401822 epoch total loss 1.17247438\n",
      "Trained batch 773 batch loss 1.27711713 epoch total loss 1.17260981\n",
      "Trained batch 774 batch loss 1.2602216 epoch total loss 1.17272294\n",
      "Trained batch 775 batch loss 1.17026 epoch total loss 1.17271984\n",
      "Trained batch 776 batch loss 1.18154645 epoch total loss 1.17273116\n",
      "Trained batch 777 batch loss 1.18211627 epoch total loss 1.1727432\n",
      "Trained batch 778 batch loss 1.21190715 epoch total loss 1.17279363\n",
      "Trained batch 779 batch loss 1.20774901 epoch total loss 1.17283845\n",
      "Trained batch 780 batch loss 1.16690993 epoch total loss 1.17283094\n",
      "Trained batch 781 batch loss 1.1139034 epoch total loss 1.17275548\n",
      "Trained batch 782 batch loss 1.18623126 epoch total loss 1.17277265\n",
      "Trained batch 783 batch loss 1.28087735 epoch total loss 1.17291069\n",
      "Trained batch 784 batch loss 1.27100325 epoch total loss 1.17303586\n",
      "Trained batch 785 batch loss 1.19664276 epoch total loss 1.1730659\n",
      "Trained batch 786 batch loss 1.20713031 epoch total loss 1.17310929\n",
      "Trained batch 787 batch loss 1.2525239 epoch total loss 1.17321014\n",
      "Trained batch 788 batch loss 1.17177951 epoch total loss 1.17320836\n",
      "Trained batch 789 batch loss 1.23597527 epoch total loss 1.17328787\n",
      "Trained batch 790 batch loss 1.28544211 epoch total loss 1.17342985\n",
      "Trained batch 791 batch loss 1.19123554 epoch total loss 1.17345238\n",
      "Trained batch 792 batch loss 1.19063342 epoch total loss 1.17347395\n",
      "Trained batch 793 batch loss 1.25513577 epoch total loss 1.17357695\n",
      "Trained batch 794 batch loss 1.20599794 epoch total loss 1.17361784\n",
      "Trained batch 795 batch loss 1.3134594 epoch total loss 1.17379367\n",
      "Trained batch 796 batch loss 1.1237992 epoch total loss 1.17373085\n",
      "Trained batch 797 batch loss 0.994162917 epoch total loss 1.17350554\n",
      "Trained batch 798 batch loss 1.06833029 epoch total loss 1.17337382\n",
      "Trained batch 799 batch loss 1.14454496 epoch total loss 1.1733377\n",
      "Trained batch 800 batch loss 1.09274828 epoch total loss 1.17323697\n",
      "Trained batch 801 batch loss 1.19423378 epoch total loss 1.17326319\n",
      "Trained batch 802 batch loss 1.32453763 epoch total loss 1.17345178\n",
      "Trained batch 803 batch loss 1.48254097 epoch total loss 1.17383671\n",
      "Trained batch 804 batch loss 1.49916279 epoch total loss 1.1742413\n",
      "Trained batch 805 batch loss 1.25917184 epoch total loss 1.1743468\n",
      "Trained batch 806 batch loss 1.17740369 epoch total loss 1.17435062\n",
      "Trained batch 807 batch loss 1.20342517 epoch total loss 1.17438662\n",
      "Trained batch 808 batch loss 1.22235167 epoch total loss 1.17444599\n",
      "Trained batch 809 batch loss 1.19415665 epoch total loss 1.17447042\n",
      "Trained batch 810 batch loss 1.20616984 epoch total loss 1.17450953\n",
      "Trained batch 811 batch loss 1.2660718 epoch total loss 1.17462242\n",
      "Trained batch 812 batch loss 1.26666749 epoch total loss 1.17473578\n",
      "Trained batch 813 batch loss 1.29187596 epoch total loss 1.17487979\n",
      "Trained batch 814 batch loss 1.37743068 epoch total loss 1.1751287\n",
      "Trained batch 815 batch loss 1.31667662 epoch total loss 1.17530239\n",
      "Trained batch 816 batch loss 1.17852688 epoch total loss 1.17530632\n",
      "Trained batch 817 batch loss 1.15627396 epoch total loss 1.17528296\n",
      "Trained batch 818 batch loss 1.20494187 epoch total loss 1.17531919\n",
      "Trained batch 819 batch loss 1.20775151 epoch total loss 1.17535889\n",
      "Trained batch 820 batch loss 1.22583747 epoch total loss 1.1754204\n",
      "Trained batch 821 batch loss 1.11706221 epoch total loss 1.17534935\n",
      "Trained batch 822 batch loss 1.11786723 epoch total loss 1.17527938\n",
      "Trained batch 823 batch loss 1.14503479 epoch total loss 1.17524266\n",
      "Trained batch 824 batch loss 1.18243062 epoch total loss 1.17525136\n",
      "Trained batch 825 batch loss 1.07764649 epoch total loss 1.17513299\n",
      "Trained batch 826 batch loss 1.06314683 epoch total loss 1.17499745\n",
      "Trained batch 827 batch loss 1.09301901 epoch total loss 1.17489839\n",
      "Trained batch 828 batch loss 1.09847569 epoch total loss 1.174806\n",
      "Trained batch 829 batch loss 1.10449851 epoch total loss 1.17472124\n",
      "Trained batch 830 batch loss 1.15220654 epoch total loss 1.17469406\n",
      "Trained batch 831 batch loss 1.24002337 epoch total loss 1.17477274\n",
      "Trained batch 832 batch loss 1.29858661 epoch total loss 1.17492151\n",
      "Trained batch 833 batch loss 1.36824536 epoch total loss 1.17515361\n",
      "Trained batch 834 batch loss 1.14617693 epoch total loss 1.17511892\n",
      "Trained batch 835 batch loss 1.18940496 epoch total loss 1.17513597\n",
      "Trained batch 836 batch loss 1.066975 epoch total loss 1.17500651\n",
      "Trained batch 837 batch loss 1.05145216 epoch total loss 1.17485893\n",
      "Trained batch 838 batch loss 1.06411147 epoch total loss 1.17472672\n",
      "Trained batch 839 batch loss 1.33798921 epoch total loss 1.17492139\n",
      "Trained batch 840 batch loss 1.17683172 epoch total loss 1.17492366\n",
      "Trained batch 841 batch loss 1.23154092 epoch total loss 1.17499101\n",
      "Trained batch 842 batch loss 1.18154955 epoch total loss 1.17499876\n",
      "Trained batch 843 batch loss 1.14905882 epoch total loss 1.174968\n",
      "Trained batch 844 batch loss 1.12245703 epoch total loss 1.17490578\n",
      "Trained batch 845 batch loss 1.07404637 epoch total loss 1.17478645\n",
      "Trained batch 846 batch loss 1.09785855 epoch total loss 1.17469549\n",
      "Trained batch 847 batch loss 1.13141894 epoch total loss 1.17464435\n",
      "Trained batch 848 batch loss 1.05258417 epoch total loss 1.17450047\n",
      "Trained batch 849 batch loss 1.09667611 epoch total loss 1.17440879\n",
      "Trained batch 850 batch loss 1.17786372 epoch total loss 1.17441285\n",
      "Trained batch 851 batch loss 1.0801512 epoch total loss 1.1743021\n",
      "Trained batch 852 batch loss 1.19110894 epoch total loss 1.17432177\n",
      "Trained batch 853 batch loss 1.12112713 epoch total loss 1.17425942\n",
      "Trained batch 854 batch loss 1.10042286 epoch total loss 1.174173\n",
      "Trained batch 855 batch loss 0.961511195 epoch total loss 1.17392421\n",
      "Trained batch 856 batch loss 0.935530782 epoch total loss 1.17364573\n",
      "Trained batch 857 batch loss 1.05166757 epoch total loss 1.1735034\n",
      "Trained batch 858 batch loss 1.18411672 epoch total loss 1.1735158\n",
      "Trained batch 859 batch loss 1.19975305 epoch total loss 1.17354643\n",
      "Trained batch 860 batch loss 1.31457067 epoch total loss 1.17371035\n",
      "Trained batch 861 batch loss 1.1757617 epoch total loss 1.17371273\n",
      "Trained batch 862 batch loss 1.04197848 epoch total loss 1.17356\n",
      "Trained batch 863 batch loss 1.0243814 epoch total loss 1.17338705\n",
      "Trained batch 864 batch loss 1.08526516 epoch total loss 1.17328513\n",
      "Trained batch 865 batch loss 1.0925349 epoch total loss 1.17319179\n",
      "Trained batch 866 batch loss 1.25857615 epoch total loss 1.17329037\n",
      "Trained batch 867 batch loss 1.10124826 epoch total loss 1.17320728\n",
      "Trained batch 868 batch loss 1.30665088 epoch total loss 1.17336106\n",
      "Trained batch 869 batch loss 1.09702325 epoch total loss 1.17327321\n",
      "Trained batch 870 batch loss 1.18108582 epoch total loss 1.17328215\n",
      "Trained batch 871 batch loss 1.18746436 epoch total loss 1.17329848\n",
      "Trained batch 872 batch loss 1.26933181 epoch total loss 1.17340863\n",
      "Trained batch 873 batch loss 1.15238237 epoch total loss 1.17338443\n",
      "Trained batch 874 batch loss 1.03808129 epoch total loss 1.17322969\n",
      "Trained batch 875 batch loss 1.1129427 epoch total loss 1.17316067\n",
      "Trained batch 876 batch loss 1.0456593 epoch total loss 1.17301512\n",
      "Trained batch 877 batch loss 1.15748048 epoch total loss 1.17299747\n",
      "Trained batch 878 batch loss 1.19082499 epoch total loss 1.17301774\n",
      "Trained batch 879 batch loss 1.18122 epoch total loss 1.17302716\n",
      "Trained batch 880 batch loss 1.29244089 epoch total loss 1.17316282\n",
      "Trained batch 881 batch loss 1.08877897 epoch total loss 1.17306697\n",
      "Trained batch 882 batch loss 1.22644663 epoch total loss 1.17312753\n",
      "Trained batch 883 batch loss 1.23557138 epoch total loss 1.17319822\n",
      "Trained batch 884 batch loss 1.12713575 epoch total loss 1.17314613\n",
      "Trained batch 885 batch loss 1.17205012 epoch total loss 1.17314482\n",
      "Trained batch 886 batch loss 1.1185981 epoch total loss 1.17308331\n",
      "Trained batch 887 batch loss 1.13219047 epoch total loss 1.17303717\n",
      "Trained batch 888 batch loss 1.02260745 epoch total loss 1.17286777\n",
      "Trained batch 889 batch loss 1.18311667 epoch total loss 1.17287934\n",
      "Trained batch 890 batch loss 1.14732933 epoch total loss 1.17285061\n",
      "Trained batch 891 batch loss 1.31311464 epoch total loss 1.17300797\n",
      "Trained batch 892 batch loss 1.34524667 epoch total loss 1.17320108\n",
      "Trained batch 893 batch loss 1.28090048 epoch total loss 1.1733216\n",
      "Trained batch 894 batch loss 1.20479774 epoch total loss 1.17335689\n",
      "Trained batch 895 batch loss 1.15226865 epoch total loss 1.17333329\n",
      "Trained batch 896 batch loss 1.23302805 epoch total loss 1.17339993\n",
      "Trained batch 897 batch loss 1.0597502 epoch total loss 1.17327321\n",
      "Trained batch 898 batch loss 1.18628561 epoch total loss 1.17328763\n",
      "Trained batch 899 batch loss 1.25092638 epoch total loss 1.17337406\n",
      "Trained batch 900 batch loss 1.12671888 epoch total loss 1.1733222\n",
      "Trained batch 901 batch loss 1.00095057 epoch total loss 1.17313087\n",
      "Trained batch 902 batch loss 0.953129113 epoch total loss 1.17288697\n",
      "Trained batch 903 batch loss 0.983275056 epoch total loss 1.17267704\n",
      "Trained batch 904 batch loss 1.0425328 epoch total loss 1.17253304\n",
      "Trained batch 905 batch loss 1.18806779 epoch total loss 1.1725502\n",
      "Trained batch 906 batch loss 1.44641805 epoch total loss 1.17285252\n",
      "Trained batch 907 batch loss 1.49474096 epoch total loss 1.1732074\n",
      "Trained batch 908 batch loss 1.29877448 epoch total loss 1.1733458\n",
      "Trained batch 909 batch loss 1.22527432 epoch total loss 1.17340279\n",
      "Trained batch 910 batch loss 1.42719567 epoch total loss 1.17368174\n",
      "Trained batch 911 batch loss 1.30463469 epoch total loss 1.17382562\n",
      "Trained batch 912 batch loss 1.2783289 epoch total loss 1.17394018\n",
      "Trained batch 913 batch loss 1.16993105 epoch total loss 1.17393577\n",
      "Trained batch 914 batch loss 1.13383245 epoch total loss 1.17389178\n",
      "Trained batch 915 batch loss 1.15412152 epoch total loss 1.17387033\n",
      "Trained batch 916 batch loss 1.16350591 epoch total loss 1.17385888\n",
      "Trained batch 917 batch loss 1.15642452 epoch total loss 1.17383981\n",
      "Trained batch 918 batch loss 1.0371592 epoch total loss 1.17369092\n",
      "Trained batch 919 batch loss 1.13556981 epoch total loss 1.17364943\n",
      "Trained batch 920 batch loss 1.02594817 epoch total loss 1.17348897\n",
      "Trained batch 921 batch loss 1.09754562 epoch total loss 1.17340648\n",
      "Trained batch 922 batch loss 1.02414882 epoch total loss 1.1732446\n",
      "Trained batch 923 batch loss 1.06575632 epoch total loss 1.17312825\n",
      "Trained batch 924 batch loss 1.14897275 epoch total loss 1.17310202\n",
      "Trained batch 925 batch loss 1.29909384 epoch total loss 1.17323828\n",
      "Trained batch 926 batch loss 1.11815739 epoch total loss 1.17317879\n",
      "Trained batch 927 batch loss 1.14542794 epoch total loss 1.17314875\n",
      "Trained batch 928 batch loss 1.21763885 epoch total loss 1.17319667\n",
      "Trained batch 929 batch loss 1.17502975 epoch total loss 1.1731987\n",
      "Trained batch 930 batch loss 1.08814192 epoch total loss 1.17310727\n",
      "Trained batch 931 batch loss 1.19671893 epoch total loss 1.17313266\n",
      "Trained batch 932 batch loss 1.1746037 epoch total loss 1.17313421\n",
      "Trained batch 933 batch loss 1.02369058 epoch total loss 1.17297399\n",
      "Trained batch 934 batch loss 0.930430055 epoch total loss 1.17271435\n",
      "Trained batch 935 batch loss 1.05941486 epoch total loss 1.17259324\n",
      "Trained batch 936 batch loss 1.02209544 epoch total loss 1.17243242\n",
      "Trained batch 937 batch loss 1.12879121 epoch total loss 1.17238581\n",
      "Trained batch 938 batch loss 1.07505202 epoch total loss 1.1722821\n",
      "Trained batch 939 batch loss 1.06356621 epoch total loss 1.17216635\n",
      "Trained batch 940 batch loss 1.05846858 epoch total loss 1.17204535\n",
      "Trained batch 941 batch loss 1.18596733 epoch total loss 1.17206013\n",
      "Trained batch 942 batch loss 1.08985317 epoch total loss 1.17197287\n",
      "Trained batch 943 batch loss 1.20925927 epoch total loss 1.17201233\n",
      "Trained batch 944 batch loss 0.952181101 epoch total loss 1.17177939\n",
      "Trained batch 945 batch loss 1.10050309 epoch total loss 1.17170393\n",
      "Trained batch 946 batch loss 1.0767858 epoch total loss 1.17160368\n",
      "Trained batch 947 batch loss 1.08035684 epoch total loss 1.17150724\n",
      "Trained batch 948 batch loss 1.19716144 epoch total loss 1.1715343\n",
      "Trained batch 949 batch loss 1.30771041 epoch total loss 1.17167783\n",
      "Trained batch 950 batch loss 1.38829648 epoch total loss 1.17190588\n",
      "Trained batch 951 batch loss 1.33671355 epoch total loss 1.17207909\n",
      "Trained batch 952 batch loss 1.09684193 epoch total loss 1.172\n",
      "Trained batch 953 batch loss 1.08623 epoch total loss 1.17190993\n",
      "Trained batch 954 batch loss 1.16911817 epoch total loss 1.17190695\n",
      "Trained batch 955 batch loss 1.22964287 epoch total loss 1.17196739\n",
      "Trained batch 956 batch loss 1.13141978 epoch total loss 1.17192507\n",
      "Trained batch 957 batch loss 1.139799 epoch total loss 1.17189145\n",
      "Trained batch 958 batch loss 1.20425153 epoch total loss 1.17192519\n",
      "Trained batch 959 batch loss 1.13122916 epoch total loss 1.17188275\n",
      "Trained batch 960 batch loss 1.18875396 epoch total loss 1.17190027\n",
      "Trained batch 961 batch loss 1.07452583 epoch total loss 1.17179906\n",
      "Trained batch 962 batch loss 1.17663014 epoch total loss 1.17180407\n",
      "Trained batch 963 batch loss 1.15487063 epoch total loss 1.17178655\n",
      "Trained batch 964 batch loss 1.19447458 epoch total loss 1.17181\n",
      "Trained batch 965 batch loss 1.21595454 epoch total loss 1.17185581\n",
      "Trained batch 966 batch loss 1.02563739 epoch total loss 1.17170441\n",
      "Trained batch 967 batch loss 1.1287936 epoch total loss 1.17166007\n",
      "Trained batch 968 batch loss 1.11726 epoch total loss 1.17160392\n",
      "Trained batch 969 batch loss 1.16421723 epoch total loss 1.17159617\n",
      "Trained batch 970 batch loss 1.22480989 epoch total loss 1.17165112\n",
      "Trained batch 971 batch loss 1.13295698 epoch total loss 1.17161119\n",
      "Trained batch 972 batch loss 1.09638059 epoch total loss 1.17153394\n",
      "Trained batch 973 batch loss 1.11747944 epoch total loss 1.17147827\n",
      "Trained batch 974 batch loss 1.11656618 epoch total loss 1.17142189\n",
      "Trained batch 975 batch loss 1.15353966 epoch total loss 1.17140365\n",
      "Trained batch 976 batch loss 1.11003792 epoch total loss 1.1713407\n",
      "Trained batch 977 batch loss 1.0413264 epoch total loss 1.17120767\n",
      "Trained batch 978 batch loss 1.04974401 epoch total loss 1.17108357\n",
      "Trained batch 979 batch loss 1.31868553 epoch total loss 1.17123437\n",
      "Trained batch 980 batch loss 1.23249257 epoch total loss 1.17129695\n",
      "Trained batch 981 batch loss 1.33197498 epoch total loss 1.17146075\n",
      "Trained batch 982 batch loss 1.26566589 epoch total loss 1.17155659\n",
      "Trained batch 983 batch loss 1.13264823 epoch total loss 1.17151713\n",
      "Trained batch 984 batch loss 1.27259541 epoch total loss 1.17161977\n",
      "Trained batch 985 batch loss 1.1964891 epoch total loss 1.17164516\n",
      "Trained batch 986 batch loss 1.29831958 epoch total loss 1.17177355\n",
      "Trained batch 987 batch loss 1.26778436 epoch total loss 1.17187095\n",
      "Trained batch 988 batch loss 1.23642945 epoch total loss 1.17193627\n",
      "Trained batch 989 batch loss 1.30806375 epoch total loss 1.17207396\n",
      "Trained batch 990 batch loss 1.18621194 epoch total loss 1.17208815\n",
      "Trained batch 991 batch loss 1.17407274 epoch total loss 1.17209017\n",
      "Trained batch 992 batch loss 1.15842438 epoch total loss 1.17207646\n",
      "Trained batch 993 batch loss 1.21527183 epoch total loss 1.17212\n",
      "Trained batch 994 batch loss 1.30617142 epoch total loss 1.1722548\n",
      "Trained batch 995 batch loss 1.10439658 epoch total loss 1.17218661\n",
      "Trained batch 996 batch loss 1.17122138 epoch total loss 1.17218566\n",
      "Trained batch 997 batch loss 1.1184659 epoch total loss 1.17213178\n",
      "Trained batch 998 batch loss 1.23343337 epoch total loss 1.17219317\n",
      "Trained batch 999 batch loss 1.3078469 epoch total loss 1.17232895\n",
      "Trained batch 1000 batch loss 1.22711599 epoch total loss 1.17238379\n",
      "Trained batch 1001 batch loss 1.23858047 epoch total loss 1.17244983\n",
      "Trained batch 1002 batch loss 1.15750277 epoch total loss 1.17243493\n",
      "Trained batch 1003 batch loss 1.16207504 epoch total loss 1.17242467\n",
      "Trained batch 1004 batch loss 1.26996136 epoch total loss 1.17252183\n",
      "Trained batch 1005 batch loss 1.13355541 epoch total loss 1.17248309\n",
      "Trained batch 1006 batch loss 1.14278924 epoch total loss 1.17245352\n",
      "Trained batch 1007 batch loss 1.16372383 epoch total loss 1.17244482\n",
      "Trained batch 1008 batch loss 1.22771716 epoch total loss 1.17249966\n",
      "Trained batch 1009 batch loss 1.28755927 epoch total loss 1.17261374\n",
      "Trained batch 1010 batch loss 1.15639269 epoch total loss 1.17259765\n",
      "Trained batch 1011 batch loss 1.16234684 epoch total loss 1.17258751\n",
      "Trained batch 1012 batch loss 1.02251744 epoch total loss 1.1724391\n",
      "Trained batch 1013 batch loss 1.06682801 epoch total loss 1.17233479\n",
      "Trained batch 1014 batch loss 1.16856885 epoch total loss 1.17233109\n",
      "Trained batch 1015 batch loss 1.07500577 epoch total loss 1.17223525\n",
      "Trained batch 1016 batch loss 1.11773121 epoch total loss 1.17218149\n",
      "Trained batch 1017 batch loss 1.07216477 epoch total loss 1.17208314\n",
      "Trained batch 1018 batch loss 1.10863721 epoch total loss 1.17202079\n",
      "Trained batch 1019 batch loss 1.24389148 epoch total loss 1.17209136\n",
      "Trained batch 1020 batch loss 1.35095465 epoch total loss 1.17226672\n",
      "Trained batch 1021 batch loss 1.32582283 epoch total loss 1.17241704\n",
      "Trained batch 1022 batch loss 1.21964288 epoch total loss 1.1724633\n",
      "Trained batch 1023 batch loss 1.10967338 epoch total loss 1.17240179\n",
      "Trained batch 1024 batch loss 1.20427418 epoch total loss 1.1724329\n",
      "Trained batch 1025 batch loss 1.20964122 epoch total loss 1.17246914\n",
      "Trained batch 1026 batch loss 1.22402549 epoch total loss 1.17251933\n",
      "Trained batch 1027 batch loss 1.06178105 epoch total loss 1.17241156\n",
      "Trained batch 1028 batch loss 0.924430728 epoch total loss 1.17217028\n",
      "Trained batch 1029 batch loss 0.941697836 epoch total loss 1.17194629\n",
      "Trained batch 1030 batch loss 1.15025735 epoch total loss 1.17192531\n",
      "Trained batch 1031 batch loss 1.09232128 epoch total loss 1.17184806\n",
      "Trained batch 1032 batch loss 1.02544713 epoch total loss 1.17170608\n",
      "Trained batch 1033 batch loss 1.12759352 epoch total loss 1.1716634\n",
      "Trained batch 1034 batch loss 1.1120677 epoch total loss 1.17160571\n",
      "Trained batch 1035 batch loss 1.10268366 epoch total loss 1.17153907\n",
      "Trained batch 1036 batch loss 1.18240166 epoch total loss 1.17154956\n",
      "Trained batch 1037 batch loss 1.16453528 epoch total loss 1.17154276\n",
      "Trained batch 1038 batch loss 1.15163231 epoch total loss 1.17152357\n",
      "Trained batch 1039 batch loss 1.1976316 epoch total loss 1.17154872\n",
      "Trained batch 1040 batch loss 1.21410286 epoch total loss 1.17158961\n",
      "Trained batch 1041 batch loss 1.14429 epoch total loss 1.17156339\n",
      "Trained batch 1042 batch loss 1.30223906 epoch total loss 1.1716888\n",
      "Trained batch 1043 batch loss 1.19393349 epoch total loss 1.17171025\n",
      "Trained batch 1044 batch loss 1.08857954 epoch total loss 1.17163062\n",
      "Trained batch 1045 batch loss 1.04406214 epoch total loss 1.17150855\n",
      "Trained batch 1046 batch loss 1.0256573 epoch total loss 1.17136908\n",
      "Trained batch 1047 batch loss 1.05434918 epoch total loss 1.17125726\n",
      "Trained batch 1048 batch loss 1.02062988 epoch total loss 1.17111361\n",
      "Trained batch 1049 batch loss 1.08596039 epoch total loss 1.17103243\n",
      "Trained batch 1050 batch loss 1.07222939 epoch total loss 1.17093837\n",
      "Trained batch 1051 batch loss 1.12787831 epoch total loss 1.17089736\n",
      "Trained batch 1052 batch loss 1.03819728 epoch total loss 1.17077124\n",
      "Trained batch 1053 batch loss 1.04386353 epoch total loss 1.17065072\n",
      "Trained batch 1054 batch loss 1.16808367 epoch total loss 1.17064822\n",
      "Trained batch 1055 batch loss 1.22716224 epoch total loss 1.17070186\n",
      "Trained batch 1056 batch loss 1.2199142 epoch total loss 1.17074847\n",
      "Trained batch 1057 batch loss 1.31249189 epoch total loss 1.17088258\n",
      "Trained batch 1058 batch loss 1.18516898 epoch total loss 1.17089617\n",
      "Trained batch 1059 batch loss 1.05553699 epoch total loss 1.17078722\n",
      "Trained batch 1060 batch loss 1.1330055 epoch total loss 1.17075157\n",
      "Trained batch 1061 batch loss 1.24475515 epoch total loss 1.17082131\n",
      "Trained batch 1062 batch loss 1.08786225 epoch total loss 1.17074323\n",
      "Trained batch 1063 batch loss 1.06664062 epoch total loss 1.17064536\n",
      "Trained batch 1064 batch loss 1.14945126 epoch total loss 1.17062533\n",
      "Trained batch 1065 batch loss 1.18541861 epoch total loss 1.17063928\n",
      "Trained batch 1066 batch loss 1.1715486 epoch total loss 1.17064011\n",
      "Trained batch 1067 batch loss 1.04000545 epoch total loss 1.17051768\n",
      "Trained batch 1068 batch loss 1.1501379 epoch total loss 1.17049861\n",
      "Trained batch 1069 batch loss 1.11556971 epoch total loss 1.17044723\n",
      "Trained batch 1070 batch loss 1.19197416 epoch total loss 1.17046738\n",
      "Trained batch 1071 batch loss 1.22068739 epoch total loss 1.17051435\n",
      "Trained batch 1072 batch loss 1.14888835 epoch total loss 1.1704942\n",
      "Trained batch 1073 batch loss 1.08968389 epoch total loss 1.17041886\n",
      "Trained batch 1074 batch loss 1.20099485 epoch total loss 1.17044747\n",
      "Trained batch 1075 batch loss 1.20046544 epoch total loss 1.17047536\n",
      "Trained batch 1076 batch loss 1.22349703 epoch total loss 1.1705246\n",
      "Trained batch 1077 batch loss 1.15703511 epoch total loss 1.17051208\n",
      "Trained batch 1078 batch loss 1.25132275 epoch total loss 1.17058706\n",
      "Trained batch 1079 batch loss 1.10366702 epoch total loss 1.17052495\n",
      "Trained batch 1080 batch loss 1.01981187 epoch total loss 1.17038536\n",
      "Trained batch 1081 batch loss 1.1314 epoch total loss 1.17034924\n",
      "Trained batch 1082 batch loss 1.25610435 epoch total loss 1.17042851\n",
      "Trained batch 1083 batch loss 1.16086793 epoch total loss 1.17041969\n",
      "Trained batch 1084 batch loss 1.19311929 epoch total loss 1.17044067\n",
      "Trained batch 1085 batch loss 1.16878688 epoch total loss 1.17043912\n",
      "Trained batch 1086 batch loss 1.1360085 epoch total loss 1.17040741\n",
      "Trained batch 1087 batch loss 1.14310813 epoch total loss 1.17038226\n",
      "Trained batch 1088 batch loss 1.24676597 epoch total loss 1.17045259\n",
      "Trained batch 1089 batch loss 1.06980789 epoch total loss 1.17036021\n",
      "Trained batch 1090 batch loss 1.11027515 epoch total loss 1.17030501\n",
      "Trained batch 1091 batch loss 1.08748627 epoch total loss 1.17022908\n",
      "Trained batch 1092 batch loss 1.04441261 epoch total loss 1.17011392\n",
      "Trained batch 1093 batch loss 1.10554588 epoch total loss 1.17005491\n",
      "Trained batch 1094 batch loss 0.926693559 epoch total loss 1.16983235\n",
      "Trained batch 1095 batch loss 1.04042745 epoch total loss 1.16971421\n",
      "Trained batch 1096 batch loss 1.06361949 epoch total loss 1.16961741\n",
      "Trained batch 1097 batch loss 1.04766142 epoch total loss 1.16950619\n",
      "Trained batch 1098 batch loss 1.05902028 epoch total loss 1.16940546\n",
      "Trained batch 1099 batch loss 1.02710319 epoch total loss 1.169276\n",
      "Trained batch 1100 batch loss 1.08840573 epoch total loss 1.16920245\n",
      "Trained batch 1101 batch loss 1.14169896 epoch total loss 1.16917753\n",
      "Trained batch 1102 batch loss 1.05725515 epoch total loss 1.16907597\n",
      "Trained batch 1103 batch loss 1.35051477 epoch total loss 1.16924036\n",
      "Trained batch 1104 batch loss 1.18526256 epoch total loss 1.1692549\n",
      "Trained batch 1105 batch loss 1.34923506 epoch total loss 1.16941774\n",
      "Trained batch 1106 batch loss 1.26968944 epoch total loss 1.16950846\n",
      "Trained batch 1107 batch loss 1.1965785 epoch total loss 1.1695329\n",
      "Trained batch 1108 batch loss 1.19761527 epoch total loss 1.16955817\n",
      "Trained batch 1109 batch loss 1.29941154 epoch total loss 1.16967535\n",
      "Trained batch 1110 batch loss 1.10586596 epoch total loss 1.16961777\n",
      "Trained batch 1111 batch loss 1.07244647 epoch total loss 1.16953027\n",
      "Trained batch 1112 batch loss 1.22246742 epoch total loss 1.16957784\n",
      "Trained batch 1113 batch loss 1.11122918 epoch total loss 1.16952538\n",
      "Trained batch 1114 batch loss 1.20299792 epoch total loss 1.16955543\n",
      "Trained batch 1115 batch loss 1.09194767 epoch total loss 1.16948581\n",
      "Trained batch 1116 batch loss 1.25667286 epoch total loss 1.16956401\n",
      "Trained batch 1117 batch loss 1.18616033 epoch total loss 1.16957879\n",
      "Trained batch 1118 batch loss 1.04947126 epoch total loss 1.16947138\n",
      "Trained batch 1119 batch loss 1.11023641 epoch total loss 1.16941845\n",
      "Trained batch 1120 batch loss 1.2241075 epoch total loss 1.16946733\n",
      "Trained batch 1121 batch loss 1.11956048 epoch total loss 1.16942275\n",
      "Trained batch 1122 batch loss 1.10785413 epoch total loss 1.16936791\n",
      "Trained batch 1123 batch loss 1.16089594 epoch total loss 1.16936028\n",
      "Trained batch 1124 batch loss 1.11214077 epoch total loss 1.1693095\n",
      "Trained batch 1125 batch loss 1.19749105 epoch total loss 1.16933453\n",
      "Trained batch 1126 batch loss 1.1061964 epoch total loss 1.1692785\n",
      "Trained batch 1127 batch loss 1.12536359 epoch total loss 1.16923952\n",
      "Trained batch 1128 batch loss 1.10028088 epoch total loss 1.16917837\n",
      "Trained batch 1129 batch loss 1.19324791 epoch total loss 1.16919971\n",
      "Trained batch 1130 batch loss 1.17184329 epoch total loss 1.16920209\n",
      "Trained batch 1131 batch loss 1.10447526 epoch total loss 1.16914487\n",
      "Trained batch 1132 batch loss 1.25074124 epoch total loss 1.16921699\n",
      "Trained batch 1133 batch loss 1.39839673 epoch total loss 1.16941929\n",
      "Trained batch 1134 batch loss 1.27419317 epoch total loss 1.16951168\n",
      "Trained batch 1135 batch loss 1.24255991 epoch total loss 1.16957605\n",
      "Trained batch 1136 batch loss 1.10229945 epoch total loss 1.1695168\n",
      "Trained batch 1137 batch loss 1.11302173 epoch total loss 1.16946709\n",
      "Trained batch 1138 batch loss 1.11602581 epoch total loss 1.16942012\n",
      "Trained batch 1139 batch loss 1.29512227 epoch total loss 1.16953051\n",
      "Trained batch 1140 batch loss 1.33623815 epoch total loss 1.16967666\n",
      "Trained batch 1141 batch loss 1.40626884 epoch total loss 1.16988397\n",
      "Trained batch 1142 batch loss 1.39587045 epoch total loss 1.17008185\n",
      "Trained batch 1143 batch loss 1.23946238 epoch total loss 1.17014265\n",
      "Trained batch 1144 batch loss 1.24184418 epoch total loss 1.17020524\n",
      "Trained batch 1145 batch loss 1.17821848 epoch total loss 1.17021227\n",
      "Trained batch 1146 batch loss 1.26667166 epoch total loss 1.17029655\n",
      "Trained batch 1147 batch loss 1.30395949 epoch total loss 1.17041302\n",
      "Trained batch 1148 batch loss 1.18060315 epoch total loss 1.17042196\n",
      "Trained batch 1149 batch loss 1.12235928 epoch total loss 1.17038012\n",
      "Trained batch 1150 batch loss 1.10903049 epoch total loss 1.17032671\n",
      "Trained batch 1151 batch loss 1.26851845 epoch total loss 1.17041206\n",
      "Trained batch 1152 batch loss 1.19932771 epoch total loss 1.17043722\n",
      "Trained batch 1153 batch loss 1.22708774 epoch total loss 1.17048633\n",
      "Trained batch 1154 batch loss 1.29596 epoch total loss 1.17059505\n",
      "Trained batch 1155 batch loss 1.2127552 epoch total loss 1.17063153\n",
      "Trained batch 1156 batch loss 1.11601865 epoch total loss 1.17058432\n",
      "Trained batch 1157 batch loss 1.15292835 epoch total loss 1.17056906\n",
      "Trained batch 1158 batch loss 1.21520352 epoch total loss 1.17060757\n",
      "Trained batch 1159 batch loss 1.10125732 epoch total loss 1.17054772\n",
      "Trained batch 1160 batch loss 1.10708845 epoch total loss 1.17049301\n",
      "Trained batch 1161 batch loss 1.19346642 epoch total loss 1.1705128\n",
      "Trained batch 1162 batch loss 1.17947328 epoch total loss 1.17052042\n",
      "Trained batch 1163 batch loss 1.35846114 epoch total loss 1.17068207\n",
      "Trained batch 1164 batch loss 1.35274768 epoch total loss 1.17083859\n",
      "Trained batch 1165 batch loss 1.28080094 epoch total loss 1.17093289\n",
      "Trained batch 1166 batch loss 1.23775029 epoch total loss 1.17099023\n",
      "Trained batch 1167 batch loss 1.21828413 epoch total loss 1.17103076\n",
      "Trained batch 1168 batch loss 0.991782784 epoch total loss 1.17087734\n",
      "Trained batch 1169 batch loss 0.956820905 epoch total loss 1.17069423\n",
      "Trained batch 1170 batch loss 0.901696146 epoch total loss 1.17046428\n",
      "Trained batch 1171 batch loss 0.993570328 epoch total loss 1.17031324\n",
      "Trained batch 1172 batch loss 1.10426307 epoch total loss 1.17025685\n",
      "Trained batch 1173 batch loss 1.31419516 epoch total loss 1.17037952\n",
      "Trained batch 1174 batch loss 1.32127023 epoch total loss 1.17050815\n",
      "Trained batch 1175 batch loss 1.36665857 epoch total loss 1.17067504\n",
      "Trained batch 1176 batch loss 1.3253212 epoch total loss 1.17080653\n",
      "Trained batch 1177 batch loss 1.29441977 epoch total loss 1.17091155\n",
      "Trained batch 1178 batch loss 1.23890245 epoch total loss 1.17096937\n",
      "Trained batch 1179 batch loss 1.2255826 epoch total loss 1.17101562\n",
      "Trained batch 1180 batch loss 1.2311542 epoch total loss 1.17106664\n",
      "Trained batch 1181 batch loss 1.10520577 epoch total loss 1.17101085\n",
      "Trained batch 1182 batch loss 1.24933457 epoch total loss 1.17107725\n",
      "Trained batch 1183 batch loss 1.21231151 epoch total loss 1.17111206\n",
      "Trained batch 1184 batch loss 1.09870386 epoch total loss 1.17105091\n",
      "Trained batch 1185 batch loss 1.21517563 epoch total loss 1.17108822\n",
      "Trained batch 1186 batch loss 1.04753327 epoch total loss 1.17098391\n",
      "Trained batch 1187 batch loss 1.0128088 epoch total loss 1.17085075\n",
      "Trained batch 1188 batch loss 0.985539436 epoch total loss 1.17069483\n",
      "Trained batch 1189 batch loss 1.26108074 epoch total loss 1.17077076\n",
      "Trained batch 1190 batch loss 1.21818972 epoch total loss 1.17081058\n",
      "Trained batch 1191 batch loss 1.29964149 epoch total loss 1.17091882\n",
      "Trained batch 1192 batch loss 1.27811909 epoch total loss 1.17100871\n",
      "Trained batch 1193 batch loss 1.3674438 epoch total loss 1.17117333\n",
      "Trained batch 1194 batch loss 1.32782555 epoch total loss 1.17130458\n",
      "Trained batch 1195 batch loss 1.26093256 epoch total loss 1.17137969\n",
      "Trained batch 1196 batch loss 1.09134078 epoch total loss 1.17131269\n",
      "Trained batch 1197 batch loss 1.05212736 epoch total loss 1.17121315\n",
      "Trained batch 1198 batch loss 1.32564688 epoch total loss 1.17134213\n",
      "Trained batch 1199 batch loss 1.21792161 epoch total loss 1.17138088\n",
      "Trained batch 1200 batch loss 1.30837464 epoch total loss 1.17149508\n",
      "Trained batch 1201 batch loss 1.29472327 epoch total loss 1.1715976\n",
      "Trained batch 1202 batch loss 1.21706343 epoch total loss 1.17163539\n",
      "Trained batch 1203 batch loss 1.23350501 epoch total loss 1.17168689\n",
      "Trained batch 1204 batch loss 1.27287984 epoch total loss 1.17177093\n",
      "Trained batch 1205 batch loss 1.18518877 epoch total loss 1.17178202\n",
      "Trained batch 1206 batch loss 1.254614 epoch total loss 1.17185068\n",
      "Trained batch 1207 batch loss 1.1853503 epoch total loss 1.17186189\n",
      "Trained batch 1208 batch loss 1.12776089 epoch total loss 1.17182541\n",
      "Trained batch 1209 batch loss 1.19832814 epoch total loss 1.17184734\n",
      "Trained batch 1210 batch loss 1.24652636 epoch total loss 1.17190909\n",
      "Trained batch 1211 batch loss 1.26502419 epoch total loss 1.17198598\n",
      "Trained batch 1212 batch loss 1.24176943 epoch total loss 1.17204356\n",
      "Trained batch 1213 batch loss 1.26056945 epoch total loss 1.17211664\n",
      "Trained batch 1214 batch loss 1.17760062 epoch total loss 1.17212117\n",
      "Trained batch 1215 batch loss 1.20665896 epoch total loss 1.17214954\n",
      "Trained batch 1216 batch loss 1.11158538 epoch total loss 1.17209971\n",
      "Trained batch 1217 batch loss 1.10451961 epoch total loss 1.17204416\n",
      "Trained batch 1218 batch loss 1.13230062 epoch total loss 1.17201161\n",
      "Trained batch 1219 batch loss 1.06940222 epoch total loss 1.17192745\n",
      "Trained batch 1220 batch loss 1.10836291 epoch total loss 1.17187536\n",
      "Trained batch 1221 batch loss 0.938551545 epoch total loss 1.17168438\n",
      "Trained batch 1222 batch loss 1.08610392 epoch total loss 1.17161429\n",
      "Trained batch 1223 batch loss 1.0643965 epoch total loss 1.17152667\n",
      "Trained batch 1224 batch loss 1.01972556 epoch total loss 1.17140269\n",
      "Trained batch 1225 batch loss 1.09390068 epoch total loss 1.17133939\n",
      "Trained batch 1226 batch loss 1.08190668 epoch total loss 1.17126644\n",
      "Trained batch 1227 batch loss 1.11409891 epoch total loss 1.17121983\n",
      "Trained batch 1228 batch loss 1.27260196 epoch total loss 1.17130244\n",
      "Trained batch 1229 batch loss 1.27248883 epoch total loss 1.17138469\n",
      "Trained batch 1230 batch loss 1.15690529 epoch total loss 1.17137289\n",
      "Trained batch 1231 batch loss 1.2896049 epoch total loss 1.17146897\n",
      "Trained batch 1232 batch loss 1.32729709 epoch total loss 1.17159534\n",
      "Trained batch 1233 batch loss 1.23138177 epoch total loss 1.17164385\n",
      "Trained batch 1234 batch loss 1.14950061 epoch total loss 1.17162597\n",
      "Trained batch 1235 batch loss 1.11631215 epoch total loss 1.17158115\n",
      "Trained batch 1236 batch loss 1.10142183 epoch total loss 1.17152441\n",
      "Trained batch 1237 batch loss 1.30336857 epoch total loss 1.17163098\n",
      "Trained batch 1238 batch loss 1.17914677 epoch total loss 1.17163706\n",
      "Trained batch 1239 batch loss 1.28710532 epoch total loss 1.17173028\n",
      "Trained batch 1240 batch loss 1.11064076 epoch total loss 1.17168093\n",
      "Trained batch 1241 batch loss 1.17752016 epoch total loss 1.1716857\n",
      "Trained batch 1242 batch loss 1.1149981 epoch total loss 1.17164\n",
      "Trained batch 1243 batch loss 1.09566259 epoch total loss 1.17157888\n",
      "Trained batch 1244 batch loss 1.24313486 epoch total loss 1.17163646\n",
      "Trained batch 1245 batch loss 1.28327787 epoch total loss 1.17172611\n",
      "Trained batch 1246 batch loss 1.06862009 epoch total loss 1.17164338\n",
      "Trained batch 1247 batch loss 1.2119056 epoch total loss 1.17167568\n",
      "Trained batch 1248 batch loss 1.14823973 epoch total loss 1.17165685\n",
      "Trained batch 1249 batch loss 1.21737075 epoch total loss 1.17169356\n",
      "Trained batch 1250 batch loss 1.10213029 epoch total loss 1.17163789\n",
      "Trained batch 1251 batch loss 1.14817655 epoch total loss 1.17161918\n",
      "Trained batch 1252 batch loss 1.06587315 epoch total loss 1.17153478\n",
      "Trained batch 1253 batch loss 1.29149735 epoch total loss 1.1716305\n",
      "Trained batch 1254 batch loss 1.22017038 epoch total loss 1.17166924\n",
      "Trained batch 1255 batch loss 1.03962231 epoch total loss 1.17156398\n",
      "Trained batch 1256 batch loss 1.142658 epoch total loss 1.17154109\n",
      "Trained batch 1257 batch loss 1.09319186 epoch total loss 1.17147863\n",
      "Trained batch 1258 batch loss 1.21571374 epoch total loss 1.1715138\n",
      "Trained batch 1259 batch loss 1.2504878 epoch total loss 1.17157662\n",
      "Trained batch 1260 batch loss 1.35878205 epoch total loss 1.17172515\n",
      "Trained batch 1261 batch loss 1.18486297 epoch total loss 1.17173553\n",
      "Trained batch 1262 batch loss 1.16349971 epoch total loss 1.17172897\n",
      "Trained batch 1263 batch loss 0.988588214 epoch total loss 1.17158401\n",
      "Trained batch 1264 batch loss 1.01340377 epoch total loss 1.17145884\n",
      "Trained batch 1265 batch loss 1.1540153 epoch total loss 1.17144513\n",
      "Trained batch 1266 batch loss 1.25163698 epoch total loss 1.17150843\n",
      "Trained batch 1267 batch loss 1.1105473 epoch total loss 1.17146027\n",
      "Trained batch 1268 batch loss 1.23431206 epoch total loss 1.17150986\n",
      "Trained batch 1269 batch loss 1.16433239 epoch total loss 1.17150414\n",
      "Trained batch 1270 batch loss 1.20727932 epoch total loss 1.17153239\n",
      "Trained batch 1271 batch loss 1.06829298 epoch total loss 1.17145109\n",
      "Trained batch 1272 batch loss 1.10027027 epoch total loss 1.17139506\n",
      "Trained batch 1273 batch loss 1.17150009 epoch total loss 1.17139518\n",
      "Trained batch 1274 batch loss 1.35870111 epoch total loss 1.17154217\n",
      "Trained batch 1275 batch loss 1.27974319 epoch total loss 1.17162704\n",
      "Trained batch 1276 batch loss 1.03558445 epoch total loss 1.17152047\n",
      "Trained batch 1277 batch loss 1.09372354 epoch total loss 1.17145956\n",
      "Trained batch 1278 batch loss 1.03054941 epoch total loss 1.17134929\n",
      "Trained batch 1279 batch loss 1.17785764 epoch total loss 1.17135441\n",
      "Trained batch 1280 batch loss 1.16611421 epoch total loss 1.17135024\n",
      "Trained batch 1281 batch loss 1.00735497 epoch total loss 1.17122221\n",
      "Trained batch 1282 batch loss 1.13430047 epoch total loss 1.17119348\n",
      "Trained batch 1283 batch loss 1.09055281 epoch total loss 1.17113054\n",
      "Trained batch 1284 batch loss 1.14025617 epoch total loss 1.17110658\n",
      "Trained batch 1285 batch loss 1.13699412 epoch total loss 1.17108\n",
      "Trained batch 1286 batch loss 1.35256648 epoch total loss 1.17122114\n",
      "Trained batch 1287 batch loss 1.25405288 epoch total loss 1.17128539\n",
      "Trained batch 1288 batch loss 1.30749941 epoch total loss 1.17139113\n",
      "Trained batch 1289 batch loss 1.1547128 epoch total loss 1.17137825\n",
      "Trained batch 1290 batch loss 1.24835932 epoch total loss 1.17143786\n",
      "Trained batch 1291 batch loss 1.08531666 epoch total loss 1.17137122\n",
      "Trained batch 1292 batch loss 1.21991062 epoch total loss 1.17140877\n",
      "Trained batch 1293 batch loss 1.22094393 epoch total loss 1.17144716\n",
      "Trained batch 1294 batch loss 1.24757957 epoch total loss 1.17150593\n",
      "Trained batch 1295 batch loss 1.24697208 epoch total loss 1.17156422\n",
      "Trained batch 1296 batch loss 1.1759584 epoch total loss 1.17156756\n",
      "Trained batch 1297 batch loss 1.24309695 epoch total loss 1.17162263\n",
      "Trained batch 1298 batch loss 1.40194631 epoch total loss 1.17180014\n",
      "Trained batch 1299 batch loss 1.21027184 epoch total loss 1.17182982\n",
      "Trained batch 1300 batch loss 1.17407715 epoch total loss 1.17183149\n",
      "Trained batch 1301 batch loss 1.13736439 epoch total loss 1.17180502\n",
      "Trained batch 1302 batch loss 1.07744491 epoch total loss 1.17173254\n",
      "Trained batch 1303 batch loss 1.21560025 epoch total loss 1.17176616\n",
      "Trained batch 1304 batch loss 1.06031573 epoch total loss 1.17168069\n",
      "Trained batch 1305 batch loss 1.08249199 epoch total loss 1.17161238\n",
      "Trained batch 1306 batch loss 1.25581872 epoch total loss 1.17167687\n",
      "Trained batch 1307 batch loss 1.13849807 epoch total loss 1.17165148\n",
      "Trained batch 1308 batch loss 1.4552753 epoch total loss 1.17186832\n",
      "Trained batch 1309 batch loss 1.21378 epoch total loss 1.17190039\n",
      "Trained batch 1310 batch loss 1.20839119 epoch total loss 1.17192817\n",
      "Trained batch 1311 batch loss 1.21481109 epoch total loss 1.17196095\n",
      "Trained batch 1312 batch loss 1.25384307 epoch total loss 1.1720233\n",
      "Trained batch 1313 batch loss 1.27796125 epoch total loss 1.172104\n",
      "Trained batch 1314 batch loss 1.06455612 epoch total loss 1.1720221\n",
      "Trained batch 1315 batch loss 1.14609909 epoch total loss 1.17200243\n",
      "Trained batch 1316 batch loss 1.19344032 epoch total loss 1.17201877\n",
      "Trained batch 1317 batch loss 1.08441377 epoch total loss 1.17195225\n",
      "Trained batch 1318 batch loss 1.18449783 epoch total loss 1.17196178\n",
      "Trained batch 1319 batch loss 1.18337727 epoch total loss 1.17197037\n",
      "Trained batch 1320 batch loss 1.15207553 epoch total loss 1.17195535\n",
      "Trained batch 1321 batch loss 1.07446313 epoch total loss 1.17188156\n",
      "Trained batch 1322 batch loss 1.09744596 epoch total loss 1.17182517\n",
      "Trained batch 1323 batch loss 1.12428856 epoch total loss 1.17178929\n",
      "Trained batch 1324 batch loss 0.978218555 epoch total loss 1.17164314\n",
      "Trained batch 1325 batch loss 0.984947562 epoch total loss 1.17150223\n",
      "Trained batch 1326 batch loss 1.01823401 epoch total loss 1.1713866\n",
      "Trained batch 1327 batch loss 1.13212752 epoch total loss 1.17135704\n",
      "Trained batch 1328 batch loss 1.22876656 epoch total loss 1.17140019\n",
      "Trained batch 1329 batch loss 1.25219071 epoch total loss 1.17146099\n",
      "Trained batch 1330 batch loss 1.20377946 epoch total loss 1.1714853\n",
      "Trained batch 1331 batch loss 1.06603229 epoch total loss 1.17140603\n",
      "Trained batch 1332 batch loss 1.11184549 epoch total loss 1.17136133\n",
      "Trained batch 1333 batch loss 1.02888238 epoch total loss 1.17125452\n",
      "Trained batch 1334 batch loss 1.0850575 epoch total loss 1.1711899\n",
      "Trained batch 1335 batch loss 1.1750803 epoch total loss 1.17119277\n",
      "Trained batch 1336 batch loss 1.30056381 epoch total loss 1.17128956\n",
      "Trained batch 1337 batch loss 1.19648528 epoch total loss 1.17130852\n",
      "Trained batch 1338 batch loss 1.17913401 epoch total loss 1.17131424\n",
      "Trained batch 1339 batch loss 1.23735034 epoch total loss 1.17136359\n",
      "Trained batch 1340 batch loss 1.12338054 epoch total loss 1.17132783\n",
      "Trained batch 1341 batch loss 1.09618127 epoch total loss 1.1712718\n",
      "Trained batch 1342 batch loss 1.12960839 epoch total loss 1.17124069\n",
      "Trained batch 1343 batch loss 1.2141484 epoch total loss 1.17127264\n",
      "Trained batch 1344 batch loss 1.17254233 epoch total loss 1.17127359\n",
      "Trained batch 1345 batch loss 1.21904016 epoch total loss 1.17130899\n",
      "Trained batch 1346 batch loss 1.1471529 epoch total loss 1.17129099\n",
      "Trained batch 1347 batch loss 1.05246174 epoch total loss 1.1712029\n",
      "Trained batch 1348 batch loss 0.952296 epoch total loss 1.17104042\n",
      "Trained batch 1349 batch loss 1.01777363 epoch total loss 1.17092681\n",
      "Trained batch 1350 batch loss 1.06069422 epoch total loss 1.17084515\n",
      "Trained batch 1351 batch loss 1.04459143 epoch total loss 1.17075169\n",
      "Trained batch 1352 batch loss 1.08636606 epoch total loss 1.17068934\n",
      "Trained batch 1353 batch loss 1.1428802 epoch total loss 1.17066872\n",
      "Trained batch 1354 batch loss 1.1571877 epoch total loss 1.17065883\n",
      "Trained batch 1355 batch loss 1.34383416 epoch total loss 1.17078662\n",
      "Trained batch 1356 batch loss 1.14801896 epoch total loss 1.17076993\n",
      "Trained batch 1357 batch loss 1.39380717 epoch total loss 1.1709342\n",
      "Trained batch 1358 batch loss 1.04605305 epoch total loss 1.17084229\n",
      "Trained batch 1359 batch loss 1.16188228 epoch total loss 1.17083561\n",
      "Trained batch 1360 batch loss 1.06803298 epoch total loss 1.17076\n",
      "Trained batch 1361 batch loss 1.22563732 epoch total loss 1.17080033\n",
      "Trained batch 1362 batch loss 1.20258093 epoch total loss 1.17082369\n",
      "Trained batch 1363 batch loss 1.29143441 epoch total loss 1.17091215\n",
      "Trained batch 1364 batch loss 1.23054862 epoch total loss 1.1709559\n",
      "Trained batch 1365 batch loss 1.14185 epoch total loss 1.17093456\n",
      "Trained batch 1366 batch loss 1.0389483 epoch total loss 1.17083788\n",
      "Trained batch 1367 batch loss 0.954274356 epoch total loss 1.17067945\n",
      "Trained batch 1368 batch loss 1.0816499 epoch total loss 1.17061436\n",
      "Trained batch 1369 batch loss 1.15178537 epoch total loss 1.17060065\n",
      "Trained batch 1370 batch loss 1.32066107 epoch total loss 1.17071021\n",
      "Trained batch 1371 batch loss 1.22630119 epoch total loss 1.17075074\n",
      "Trained batch 1372 batch loss 1.1155982 epoch total loss 1.17071056\n",
      "Trained batch 1373 batch loss 1.44187987 epoch total loss 1.17090809\n",
      "Trained batch 1374 batch loss 1.25615036 epoch total loss 1.17097008\n",
      "Trained batch 1375 batch loss 1.20065606 epoch total loss 1.17099166\n",
      "Trained batch 1376 batch loss 1.22774041 epoch total loss 1.17103291\n",
      "Trained batch 1377 batch loss 1.29196095 epoch total loss 1.17112076\n",
      "Trained batch 1378 batch loss 1.23823059 epoch total loss 1.17116952\n",
      "Trained batch 1379 batch loss 1.12992358 epoch total loss 1.1711396\n",
      "Trained batch 1380 batch loss 1.03875744 epoch total loss 1.17104363\n",
      "Trained batch 1381 batch loss 1.01101255 epoch total loss 1.17092776\n",
      "Trained batch 1382 batch loss 1.09929574 epoch total loss 1.17087591\n",
      "Trained batch 1383 batch loss 1.11537921 epoch total loss 1.17083573\n",
      "Trained batch 1384 batch loss 1.1422205 epoch total loss 1.17081511\n",
      "Trained batch 1385 batch loss 1.22756231 epoch total loss 1.170856\n",
      "Trained batch 1386 batch loss 1.30627263 epoch total loss 1.17095375\n",
      "Trained batch 1387 batch loss 1.2006408 epoch total loss 1.17097521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 03:59:27.631378: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "2025-03-19 03:59:27.631424: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "\t [[RemoteCall]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1388 batch loss 1.16759408 epoch total loss 1.1709727\n",
      "Epoch 5 train loss 1.1709727048873901\n",
      "Validated batch 1 batch loss 1.05834222\n",
      "Validated batch 2 batch loss 1.15227628\n",
      "Validated batch 3 batch loss 1.24042273\n",
      "Validated batch 4 batch loss 1.16678989\n",
      "Validated batch 5 batch loss 1.28759193\n",
      "Validated batch 6 batch loss 1.37742496\n",
      "Validated batch 7 batch loss 1.07071495\n",
      "Validated batch 8 batch loss 1.23097968\n",
      "Validated batch 9 batch loss 1.16877055\n",
      "Validated batch 10 batch loss 1.31084299\n",
      "Validated batch 11 batch loss 1.18809044\n",
      "Validated batch 12 batch loss 1.02018189\n",
      "Validated batch 13 batch loss 1.07504785\n",
      "Validated batch 14 batch loss 1.11836553\n",
      "Validated batch 15 batch loss 1.10750663\n",
      "Validated batch 16 batch loss 1.18060303\n",
      "Validated batch 17 batch loss 1.12301755\n",
      "Validated batch 18 batch loss 1.04727423\n",
      "Validated batch 19 batch loss 1.20629644\n",
      "Validated batch 20 batch loss 1.24776876\n",
      "Validated batch 21 batch loss 1.27645481\n",
      "Validated batch 22 batch loss 1.20474696\n",
      "Validated batch 23 batch loss 1.09410155\n",
      "Validated batch 24 batch loss 1.11773038\n",
      "Validated batch 25 batch loss 1.13169932\n",
      "Validated batch 26 batch loss 1.16479206\n",
      "Validated batch 27 batch loss 1.16091657\n",
      "Validated batch 28 batch loss 1.12724185\n",
      "Validated batch 29 batch loss 1.25070989\n",
      "Validated batch 30 batch loss 1.12564695\n",
      "Validated batch 31 batch loss 0.980116\n",
      "Validated batch 32 batch loss 1.08642793\n",
      "Validated batch 33 batch loss 1.09729743\n",
      "Validated batch 34 batch loss 1.07329\n",
      "Validated batch 35 batch loss 1.0972687\n",
      "Validated batch 36 batch loss 1.1284616\n",
      "Validated batch 37 batch loss 1.12312198\n",
      "Validated batch 38 batch loss 1.28962874\n",
      "Validated batch 39 batch loss 1.1822691\n",
      "Validated batch 40 batch loss 1.18351698\n",
      "Validated batch 41 batch loss 1.25262809\n",
      "Validated batch 42 batch loss 0.967234969\n",
      "Validated batch 43 batch loss 1.09300673\n",
      "Validated batch 44 batch loss 1.07121372\n",
      "Validated batch 45 batch loss 1.19560969\n",
      "Validated batch 46 batch loss 1.30448782\n",
      "Validated batch 47 batch loss 1.15426528\n",
      "Validated batch 48 batch loss 1.05110359\n",
      "Validated batch 49 batch loss 1.11743772\n",
      "Validated batch 50 batch loss 1.0710535\n",
      "Validated batch 51 batch loss 1.16287041\n",
      "Validated batch 52 batch loss 1.21022105\n",
      "Validated batch 53 batch loss 1.20244741\n",
      "Validated batch 54 batch loss 1.12776446\n",
      "Validated batch 55 batch loss 1.2238754\n",
      "Validated batch 56 batch loss 1.1854434\n",
      "Validated batch 57 batch loss 1.15224695\n",
      "Validated batch 58 batch loss 1.19642782\n",
      "Validated batch 59 batch loss 1.12185621\n",
      "Validated batch 60 batch loss 1.1217494\n",
      "Validated batch 61 batch loss 1.21225154\n",
      "Validated batch 62 batch loss 1.13220561\n",
      "Validated batch 63 batch loss 1.33352804\n",
      "Validated batch 64 batch loss 1.2326169\n",
      "Validated batch 65 batch loss 1.04680157\n",
      "Validated batch 66 batch loss 1.2493968\n",
      "Validated batch 67 batch loss 1.14481032\n",
      "Validated batch 68 batch loss 1.09073877\n",
      "Validated batch 69 batch loss 1.18853486\n",
      "Validated batch 70 batch loss 1.20840609\n",
      "Validated batch 71 batch loss 1.15727615\n",
      "Validated batch 72 batch loss 1.07631969\n",
      "Validated batch 73 batch loss 1.12561131\n",
      "Validated batch 74 batch loss 1.16286719\n",
      "Validated batch 75 batch loss 1.18436098\n",
      "Validated batch 76 batch loss 1.20872211\n",
      "Validated batch 77 batch loss 1.16315722\n",
      "Validated batch 78 batch loss 1.15924942\n",
      "Validated batch 79 batch loss 1.28061104\n",
      "Validated batch 80 batch loss 1.05196512\n",
      "Validated batch 81 batch loss 1.04339314\n",
      "Validated batch 82 batch loss 1.22769165\n",
      "Validated batch 83 batch loss 1.27771187\n",
      "Validated batch 84 batch loss 1.3421427\n",
      "Validated batch 85 batch loss 1.38540494\n",
      "Validated batch 86 batch loss 1.16664982\n",
      "Validated batch 87 batch loss 1.37208676\n",
      "Validated batch 88 batch loss 1.15665436\n",
      "Validated batch 89 batch loss 1.19489145\n",
      "Validated batch 90 batch loss 1.18065238\n",
      "Validated batch 91 batch loss 0.899286032\n",
      "Validated batch 92 batch loss 1.13671851\n",
      "Validated batch 93 batch loss 1.17042947\n",
      "Validated batch 94 batch loss 1.12334836\n",
      "Validated batch 95 batch loss 1.17239833\n",
      "Validated batch 96 batch loss 1.09429455\n",
      "Validated batch 97 batch loss 1.13220882\n",
      "Validated batch 98 batch loss 1.24999678\n",
      "Validated batch 99 batch loss 1.18840766\n",
      "Validated batch 100 batch loss 1.28216124\n",
      "Validated batch 101 batch loss 1.2537446\n",
      "Validated batch 102 batch loss 1.19966125\n",
      "Validated batch 103 batch loss 1.23737562\n",
      "Validated batch 104 batch loss 1.37650764\n",
      "Validated batch 105 batch loss 1.28360653\n",
      "Validated batch 106 batch loss 1.28081477\n",
      "Validated batch 107 batch loss 1.27911782\n",
      "Validated batch 108 batch loss 1.30234933\n",
      "Validated batch 109 batch loss 1.2918905\n",
      "Validated batch 110 batch loss 1.10262835\n",
      "Validated batch 111 batch loss 1.19390225\n",
      "Validated batch 112 batch loss 1.24430239\n",
      "Validated batch 113 batch loss 1.29047656\n",
      "Validated batch 114 batch loss 1.14926302\n",
      "Validated batch 115 batch loss 1.20573342\n",
      "Validated batch 116 batch loss 1.24808979\n",
      "Validated batch 117 batch loss 1.23257029\n",
      "Validated batch 118 batch loss 1.17973685\n",
      "Validated batch 119 batch loss 1.19790888\n",
      "Validated batch 120 batch loss 1.26627386\n",
      "Validated batch 121 batch loss 1.38609648\n",
      "Validated batch 122 batch loss 1.20299852\n",
      "Validated batch 123 batch loss 1.24274957\n",
      "Validated batch 124 batch loss 1.16847777\n",
      "Validated batch 125 batch loss 1.25610793\n",
      "Validated batch 126 batch loss 1.2277633\n",
      "Validated batch 127 batch loss 1.15027\n",
      "Validated batch 128 batch loss 1.26291871\n",
      "Validated batch 129 batch loss 1.27655149\n",
      "Validated batch 130 batch loss 1.32998872\n",
      "Validated batch 131 batch loss 1.23176515\n",
      "Validated batch 132 batch loss 1.24894309\n",
      "Validated batch 133 batch loss 1.15833116\n",
      "Validated batch 134 batch loss 1.15058446\n",
      "Validated batch 135 batch loss 1.23440433\n",
      "Validated batch 136 batch loss 1.15030694\n",
      "Validated batch 137 batch loss 1.2650969\n",
      "Validated batch 138 batch loss 1.20885611\n",
      "Validated batch 139 batch loss 1.36508584\n",
      "Validated batch 140 batch loss 1.19302988\n",
      "Validated batch 141 batch loss 1.1847316\n",
      "Validated batch 142 batch loss 1.19340372\n",
      "Validated batch 143 batch loss 1.10147262\n",
      "Validated batch 144 batch loss 1.21839738\n",
      "Validated batch 145 batch loss 1.22545588\n",
      "Validated batch 146 batch loss 1.12906408\n",
      "Validated batch 147 batch loss 1.14592445\n",
      "Validated batch 148 batch loss 1.27739\n",
      "Validated batch 149 batch loss 1.21269298\n",
      "Validated batch 150 batch loss 1.26140225\n",
      "Validated batch 151 batch loss 1.24485695\n",
      "Validated batch 152 batch loss 1.09981859\n",
      "Validated batch 153 batch loss 1.13328505\n",
      "Validated batch 154 batch loss 1.21987748\n",
      "Validated batch 155 batch loss 1.14337087\n",
      "Validated batch 156 batch loss 1.21081102\n",
      "Validated batch 157 batch loss 1.16457427\n",
      "Validated batch 158 batch loss 1.31793499\n",
      "Validated batch 159 batch loss 1.31284761\n",
      "Validated batch 160 batch loss 1.20944762\n",
      "Validated batch 161 batch loss 1.06889153\n",
      "Validated batch 162 batch loss 1.09190977\n",
      "Validated batch 163 batch loss 1.19697046\n",
      "Validated batch 164 batch loss 1.24735904\n",
      "Validated batch 165 batch loss 1.07932186\n",
      "Validated batch 166 batch loss 1.19642735\n",
      "Validated batch 167 batch loss 1.23149538\n",
      "Validated batch 168 batch loss 1.26485562\n",
      "Validated batch 169 batch loss 1.33325517\n",
      "Validated batch 170 batch loss 1.23307192\n",
      "Validated batch 171 batch loss 1.11259854\n",
      "Validated batch 172 batch loss 1.13062775\n",
      "Validated batch 173 batch loss 1.15837955\n",
      "Validated batch 174 batch loss 1.13458979\n",
      "Validated batch 175 batch loss 1.29256451\n",
      "Validated batch 176 batch loss 1.37245882\n",
      "Validated batch 177 batch loss 1.17801261\n",
      "Validated batch 178 batch loss 1.27141392\n",
      "Validated batch 179 batch loss 1.20546794\n",
      "Validated batch 180 batch loss 1.11986732\n",
      "Validated batch 181 batch loss 1.16513622\n",
      "Validated batch 182 batch loss 1.00314748\n",
      "Validated batch 183 batch loss 1.21091509\n",
      "Validated batch 184 batch loss 1.13701475\n",
      "Validated batch 185 batch loss 1.17850041\n",
      "Epoch 5 val loss 1.1870808601379395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 03:59:43.579567: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "2025-03-19 03:59:43.579603: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "\t [[RemoteCall]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model /home/minho/Desktop/aiffel/pose_estimation/models/model-epoch-5-loss-1.1871.weights.h5 saved.\n",
      "Start epoch 6 with learning rate 0.001\n",
      "Start distributed training...\n",
      "Trained batch 1 batch loss 1.11196578 epoch total loss 1.11196578\n",
      "Trained batch 2 batch loss 1.35055149 epoch total loss 1.23125863\n",
      "Trained batch 3 batch loss 1.21495008 epoch total loss 1.22582245\n",
      "Trained batch 4 batch loss 1.24900365 epoch total loss 1.23161769\n",
      "Trained batch 5 batch loss 1.30181813 epoch total loss 1.24565768\n",
      "Trained batch 6 batch loss 1.38528287 epoch total loss 1.26892865\n",
      "Trained batch 7 batch loss 1.16894042 epoch total loss 1.25464451\n",
      "Trained batch 8 batch loss 1.2235868 epoch total loss 1.25076234\n",
      "Trained batch 9 batch loss 1.15292287 epoch total loss 1.23989129\n",
      "Trained batch 10 batch loss 1.13342488 epoch total loss 1.22924459\n",
      "Trained batch 11 batch loss 1.24407828 epoch total loss 1.2305932\n",
      "Trained batch 12 batch loss 1.24408102 epoch total loss 1.23171711\n",
      "Trained batch 13 batch loss 1.2186451 epoch total loss 1.23071158\n",
      "Trained batch 14 batch loss 1.36266172 epoch total loss 1.2401365\n",
      "Trained batch 15 batch loss 1.26442766 epoch total loss 1.24175596\n",
      "Trained batch 16 batch loss 1.35953426 epoch total loss 1.24911714\n",
      "Trained batch 17 batch loss 1.33915615 epoch total loss 1.2544136\n",
      "Trained batch 18 batch loss 1.30055141 epoch total loss 1.25697684\n",
      "Trained batch 19 batch loss 1.34803605 epoch total loss 1.26176941\n",
      "Trained batch 20 batch loss 1.30056179 epoch total loss 1.26370907\n",
      "Trained batch 21 batch loss 1.23430872 epoch total loss 1.26230907\n",
      "Trained batch 22 batch loss 1.16418087 epoch total loss 1.25784862\n",
      "Trained batch 23 batch loss 1.12862539 epoch total loss 1.25223029\n",
      "Trained batch 24 batch loss 1.01722348 epoch total loss 1.24243832\n",
      "Trained batch 25 batch loss 1.05384135 epoch total loss 1.23489439\n",
      "Trained batch 26 batch loss 1.13613963 epoch total loss 1.23109615\n",
      "Trained batch 27 batch loss 1.14161754 epoch total loss 1.22778213\n",
      "Trained batch 28 batch loss 1.22129679 epoch total loss 1.22755051\n",
      "Trained batch 29 batch loss 1.263363 epoch total loss 1.2287854\n",
      "Trained batch 30 batch loss 1.18301868 epoch total loss 1.22725987\n",
      "Trained batch 31 batch loss 1.11037636 epoch total loss 1.22348928\n",
      "Trained batch 32 batch loss 1.1506331 epoch total loss 1.22121263\n",
      "Trained batch 33 batch loss 1.10941648 epoch total loss 1.21782494\n",
      "Trained batch 34 batch loss 1.22409916 epoch total loss 1.21800935\n",
      "Trained batch 35 batch loss 1.13965416 epoch total loss 1.2157706\n",
      "Trained batch 36 batch loss 1.19515634 epoch total loss 1.21519804\n",
      "Trained batch 37 batch loss 1.21517825 epoch total loss 1.21519744\n",
      "Trained batch 38 batch loss 1.12818193 epoch total loss 1.21290755\n",
      "Trained batch 39 batch loss 1.19565225 epoch total loss 1.21246517\n",
      "Trained batch 40 batch loss 1.22909451 epoch total loss 1.21288085\n",
      "Trained batch 41 batch loss 1.38408601 epoch total loss 1.21705663\n",
      "Trained batch 42 batch loss 1.1114738 epoch total loss 1.21454275\n",
      "Trained batch 43 batch loss 1.06218183 epoch total loss 1.21099949\n",
      "Trained batch 44 batch loss 1.24793947 epoch total loss 1.21183908\n",
      "Trained batch 45 batch loss 1.1566329 epoch total loss 1.21061218\n",
      "Trained batch 46 batch loss 1.20386553 epoch total loss 1.21046555\n",
      "Trained batch 47 batch loss 1.22577286 epoch total loss 1.21079123\n",
      "Trained batch 48 batch loss 1.09662867 epoch total loss 1.20841289\n",
      "Trained batch 49 batch loss 1.02433491 epoch total loss 1.20465612\n",
      "Trained batch 50 batch loss 1.16662192 epoch total loss 1.20389545\n",
      "Trained batch 51 batch loss 1.0038476 epoch total loss 1.19997299\n",
      "Trained batch 52 batch loss 1.1601367 epoch total loss 1.19920695\n",
      "Trained batch 53 batch loss 1.13415611 epoch total loss 1.19797957\n",
      "Trained batch 54 batch loss 1.12580907 epoch total loss 1.19664311\n",
      "Trained batch 55 batch loss 1.18334293 epoch total loss 1.19640124\n",
      "Trained batch 56 batch loss 1.14968717 epoch total loss 1.19556713\n",
      "Trained batch 57 batch loss 1.08216321 epoch total loss 1.19357753\n",
      "Trained batch 58 batch loss 1.07458258 epoch total loss 1.19152594\n",
      "Trained batch 59 batch loss 1.07239842 epoch total loss 1.18950677\n",
      "Trained batch 60 batch loss 1.17511821 epoch total loss 1.18926692\n",
      "Trained batch 61 batch loss 1.13537347 epoch total loss 1.18838346\n",
      "Trained batch 62 batch loss 1.15517 epoch total loss 1.18784773\n",
      "Trained batch 63 batch loss 1.01437 epoch total loss 1.18509424\n",
      "Trained batch 64 batch loss 1.06666887 epoch total loss 1.18324375\n",
      "Trained batch 65 batch loss 1.19231343 epoch total loss 1.18338335\n",
      "Trained batch 66 batch loss 1.26802921 epoch total loss 1.1846658\n",
      "Trained batch 67 batch loss 1.19322419 epoch total loss 1.18479347\n",
      "Trained batch 68 batch loss 1.12552 epoch total loss 1.18392181\n",
      "Trained batch 69 batch loss 1.22092199 epoch total loss 1.18445814\n",
      "Trained batch 70 batch loss 1.17969227 epoch total loss 1.18439007\n",
      "Trained batch 71 batch loss 1.1659019 epoch total loss 1.1841296\n",
      "Trained batch 72 batch loss 1.13416529 epoch total loss 1.18343568\n",
      "Trained batch 73 batch loss 1.07873452 epoch total loss 1.18200135\n",
      "Trained batch 74 batch loss 1.14074945 epoch total loss 1.18144393\n",
      "Trained batch 75 batch loss 1.08037245 epoch total loss 1.18009639\n",
      "Trained batch 76 batch loss 1.18632174 epoch total loss 1.18017828\n",
      "Trained batch 77 batch loss 1.27800918 epoch total loss 1.18144882\n",
      "Trained batch 78 batch loss 1.18627608 epoch total loss 1.18151069\n",
      "Trained batch 79 batch loss 1.23992205 epoch total loss 1.18225014\n",
      "Trained batch 80 batch loss 1.04197383 epoch total loss 1.18049669\n",
      "Trained batch 81 batch loss 1.10958207 epoch total loss 1.17962122\n",
      "Trained batch 82 batch loss 1.14593387 epoch total loss 1.17921042\n",
      "Trained batch 83 batch loss 1.07263517 epoch total loss 1.1779263\n",
      "Trained batch 84 batch loss 1.10486054 epoch total loss 1.17705643\n",
      "Trained batch 85 batch loss 1.25515568 epoch total loss 1.1779753\n",
      "Trained batch 86 batch loss 1.20576346 epoch total loss 1.17829847\n",
      "Trained batch 87 batch loss 1.10687137 epoch total loss 1.17747748\n",
      "Trained batch 88 batch loss 1.02472556 epoch total loss 1.17574167\n",
      "Trained batch 89 batch loss 1.0331645 epoch total loss 1.17413962\n",
      "Trained batch 90 batch loss 1.137097 epoch total loss 1.17372811\n",
      "Trained batch 91 batch loss 1.12240911 epoch total loss 1.17316413\n",
      "Trained batch 92 batch loss 1.1049149 epoch total loss 1.17242229\n",
      "Trained batch 93 batch loss 1.24754834 epoch total loss 1.17323\n",
      "Trained batch 94 batch loss 1.24403393 epoch total loss 1.17398334\n",
      "Trained batch 95 batch loss 1.35307658 epoch total loss 1.17586851\n",
      "Trained batch 96 batch loss 1.33321333 epoch total loss 1.17750752\n",
      "Trained batch 97 batch loss 1.23587143 epoch total loss 1.17810917\n",
      "Trained batch 98 batch loss 1.05442095 epoch total loss 1.17684698\n",
      "Trained batch 99 batch loss 1.11443281 epoch total loss 1.1762166\n",
      "Trained batch 100 batch loss 1.39388537 epoch total loss 1.17839324\n",
      "Trained batch 101 batch loss 1.13076925 epoch total loss 1.17792165\n",
      "Trained batch 102 batch loss 1.11528206 epoch total loss 1.17730761\n",
      "Trained batch 103 batch loss 1.04479885 epoch total loss 1.1760211\n",
      "Trained batch 104 batch loss 1.11322582 epoch total loss 1.1754173\n",
      "Trained batch 105 batch loss 1.06196749 epoch total loss 1.17433679\n",
      "Trained batch 106 batch loss 1.06198096 epoch total loss 1.1732769\n",
      "Trained batch 107 batch loss 1.09988248 epoch total loss 1.17259097\n",
      "Trained batch 108 batch loss 1.1490469 epoch total loss 1.17237294\n",
      "Trained batch 109 batch loss 1.30903113 epoch total loss 1.17362666\n",
      "Trained batch 110 batch loss 1.17466354 epoch total loss 1.1736362\n",
      "Trained batch 111 batch loss 1.19067299 epoch total loss 1.17378962\n",
      "Trained batch 112 batch loss 1.03323269 epoch total loss 1.1725347\n",
      "Trained batch 113 batch loss 1.17970693 epoch total loss 1.17259812\n",
      "Trained batch 114 batch loss 1.19188583 epoch total loss 1.17276728\n",
      "Trained batch 115 batch loss 1.23420238 epoch total loss 1.17330146\n",
      "Trained batch 116 batch loss 1.24708271 epoch total loss 1.17393756\n",
      "Trained batch 117 batch loss 1.21112823 epoch total loss 1.17425537\n",
      "Trained batch 118 batch loss 1.21169758 epoch total loss 1.17457271\n",
      "Trained batch 119 batch loss 1.20299 epoch total loss 1.17481148\n",
      "Trained batch 120 batch loss 1.16920507 epoch total loss 1.17476475\n",
      "Trained batch 121 batch loss 1.17602921 epoch total loss 1.17477512\n",
      "Trained batch 122 batch loss 1.2104001 epoch total loss 1.17506719\n",
      "Trained batch 123 batch loss 1.10004663 epoch total loss 1.17445731\n",
      "Trained batch 124 batch loss 1.17470622 epoch total loss 1.17445934\n",
      "Trained batch 125 batch loss 1.18813896 epoch total loss 1.17456889\n",
      "Trained batch 126 batch loss 1.20964801 epoch total loss 1.17484725\n",
      "Trained batch 127 batch loss 1.16335869 epoch total loss 1.17475677\n",
      "Trained batch 128 batch loss 1.18087256 epoch total loss 1.17480457\n",
      "Trained batch 129 batch loss 1.24005163 epoch total loss 1.17531037\n",
      "Trained batch 130 batch loss 1.21029472 epoch total loss 1.17557943\n",
      "Trained batch 131 batch loss 1.07325816 epoch total loss 1.17479837\n",
      "Trained batch 132 batch loss 1.07598293 epoch total loss 1.17404985\n",
      "Trained batch 133 batch loss 1.05229616 epoch total loss 1.17313433\n",
      "Trained batch 134 batch loss 1.11607742 epoch total loss 1.17270851\n",
      "Trained batch 135 batch loss 1.16695 epoch total loss 1.17266583\n",
      "Trained batch 136 batch loss 1.11164 epoch total loss 1.17221713\n",
      "Trained batch 137 batch loss 1.22355759 epoch total loss 1.17259181\n",
      "Trained batch 138 batch loss 1.11864579 epoch total loss 1.17220092\n",
      "Trained batch 139 batch loss 0.96334362 epoch total loss 1.1706984\n",
      "Trained batch 140 batch loss 1.06792545 epoch total loss 1.16996443\n",
      "Trained batch 141 batch loss 1.01625526 epoch total loss 1.16887426\n",
      "Trained batch 142 batch loss 1.05824709 epoch total loss 1.16809511\n",
      "Trained batch 143 batch loss 1.17652225 epoch total loss 1.16815412\n",
      "Trained batch 144 batch loss 1.17988384 epoch total loss 1.16823554\n",
      "Trained batch 145 batch loss 1.13076019 epoch total loss 1.16797721\n",
      "Trained batch 146 batch loss 1.07748199 epoch total loss 1.16735733\n",
      "Trained batch 147 batch loss 1.19061375 epoch total loss 1.16751552\n",
      "Trained batch 148 batch loss 1.06375134 epoch total loss 1.16681445\n",
      "Trained batch 149 batch loss 1.12315619 epoch total loss 1.16652143\n",
      "Trained batch 150 batch loss 1.15779972 epoch total loss 1.16646338\n",
      "Trained batch 151 batch loss 1.17313325 epoch total loss 1.16650748\n",
      "Trained batch 152 batch loss 1.08418584 epoch total loss 1.1659658\n",
      "Trained batch 153 batch loss 1.18220091 epoch total loss 1.16607201\n",
      "Trained batch 154 batch loss 1.08593965 epoch total loss 1.16555166\n",
      "Trained batch 155 batch loss 0.881393373 epoch total loss 1.16371834\n",
      "Trained batch 156 batch loss 0.892263651 epoch total loss 1.16197824\n",
      "Trained batch 157 batch loss 1.04286265 epoch total loss 1.16121948\n",
      "Trained batch 158 batch loss 1.01441646 epoch total loss 1.16029036\n",
      "Trained batch 159 batch loss 1.03328085 epoch total loss 1.15949154\n",
      "Trained batch 160 batch loss 1.07216156 epoch total loss 1.1589458\n",
      "Trained batch 161 batch loss 1.17371202 epoch total loss 1.15903747\n",
      "Trained batch 162 batch loss 1.18982768 epoch total loss 1.15922749\n",
      "Trained batch 163 batch loss 1.23378706 epoch total loss 1.1596849\n",
      "Trained batch 164 batch loss 1.25764561 epoch total loss 1.16028225\n",
      "Trained batch 165 batch loss 1.1519419 epoch total loss 1.16023171\n",
      "Trained batch 166 batch loss 0.963337243 epoch total loss 1.15904558\n",
      "Trained batch 167 batch loss 1.05768204 epoch total loss 1.15843856\n",
      "Trained batch 168 batch loss 1.12714875 epoch total loss 1.15825236\n",
      "Trained batch 169 batch loss 1.25018072 epoch total loss 1.15879631\n",
      "Trained batch 170 batch loss 1.24687338 epoch total loss 1.15931439\n",
      "Trained batch 171 batch loss 1.21571267 epoch total loss 1.15964425\n",
      "Trained batch 172 batch loss 1.21545208 epoch total loss 1.15996873\n",
      "Trained batch 173 batch loss 1.07838321 epoch total loss 1.15949714\n",
      "Trained batch 174 batch loss 1.10457432 epoch total loss 1.15918148\n",
      "Trained batch 175 batch loss 1.17579877 epoch total loss 1.15927637\n",
      "Trained batch 176 batch loss 1.02888548 epoch total loss 1.15853548\n",
      "Trained batch 177 batch loss 0.991514444 epoch total loss 1.15759194\n",
      "Trained batch 178 batch loss 0.967283487 epoch total loss 1.15652275\n",
      "Trained batch 179 batch loss 1.08820319 epoch total loss 1.15614104\n",
      "Trained batch 180 batch loss 1.19691539 epoch total loss 1.15636754\n",
      "Trained batch 181 batch loss 1.19571817 epoch total loss 1.15658498\n",
      "Trained batch 182 batch loss 1.38158417 epoch total loss 1.1578213\n",
      "Trained batch 183 batch loss 1.08875358 epoch total loss 1.15744388\n",
      "Trained batch 184 batch loss 1.158728 epoch total loss 1.15745091\n",
      "Trained batch 185 batch loss 1.09552038 epoch total loss 1.15711617\n",
      "Trained batch 186 batch loss 1.14723217 epoch total loss 1.15706301\n",
      "Trained batch 187 batch loss 1.09008062 epoch total loss 1.15670478\n",
      "Trained batch 188 batch loss 1.15050936 epoch total loss 1.15667188\n",
      "Trained batch 189 batch loss 1.1916436 epoch total loss 1.15685701\n",
      "Trained batch 190 batch loss 1.40778124 epoch total loss 1.15817761\n",
      "Trained batch 191 batch loss 1.26219368 epoch total loss 1.15872216\n",
      "Trained batch 192 batch loss 1.23314106 epoch total loss 1.15910971\n",
      "Trained batch 193 batch loss 1.09992146 epoch total loss 1.15880299\n",
      "Trained batch 194 batch loss 1.00226223 epoch total loss 1.15799606\n",
      "Trained batch 195 batch loss 1.04894114 epoch total loss 1.15743685\n",
      "Trained batch 196 batch loss 1.10394382 epoch total loss 1.15716386\n",
      "Trained batch 197 batch loss 1.15429747 epoch total loss 1.15714931\n",
      "Trained batch 198 batch loss 1.11922169 epoch total loss 1.15695775\n",
      "Trained batch 199 batch loss 1.08906126 epoch total loss 1.15661657\n",
      "Trained batch 200 batch loss 0.957975566 epoch total loss 1.15562344\n",
      "Trained batch 201 batch loss 1.10990548 epoch total loss 1.15539598\n",
      "Trained batch 202 batch loss 1.17182827 epoch total loss 1.15547729\n",
      "Trained batch 203 batch loss 1.1766994 epoch total loss 1.15558183\n",
      "Trained batch 204 batch loss 1.15806806 epoch total loss 1.15559399\n",
      "Trained batch 205 batch loss 1.0719378 epoch total loss 1.15518594\n",
      "Trained batch 206 batch loss 1.13479531 epoch total loss 1.15508699\n",
      "Trained batch 207 batch loss 1.19226587 epoch total loss 1.15526652\n",
      "Trained batch 208 batch loss 1.2338109 epoch total loss 1.15564418\n",
      "Trained batch 209 batch loss 1.1354003 epoch total loss 1.15554738\n",
      "Trained batch 210 batch loss 1.14609194 epoch total loss 1.15550232\n",
      "Trained batch 211 batch loss 1.12660491 epoch total loss 1.15536535\n",
      "Trained batch 212 batch loss 1.11662126 epoch total loss 1.1551826\n",
      "Trained batch 213 batch loss 1.07359421 epoch total loss 1.15479958\n",
      "Trained batch 214 batch loss 1.06130433 epoch total loss 1.15436268\n",
      "Trained batch 215 batch loss 1.14295053 epoch total loss 1.15430963\n",
      "Trained batch 216 batch loss 1.1355536 epoch total loss 1.15422273\n",
      "Trained batch 217 batch loss 1.04771948 epoch total loss 1.15373194\n",
      "Trained batch 218 batch loss 1.11124218 epoch total loss 1.15353703\n",
      "Trained batch 219 batch loss 1.15095747 epoch total loss 1.15352523\n",
      "Trained batch 220 batch loss 1.2173866 epoch total loss 1.15381551\n",
      "Trained batch 221 batch loss 1.16111624 epoch total loss 1.15384853\n",
      "Trained batch 222 batch loss 1.12389743 epoch total loss 1.15371358\n",
      "Trained batch 223 batch loss 1.16161752 epoch total loss 1.15374911\n",
      "Trained batch 224 batch loss 1.15327609 epoch total loss 1.15374696\n",
      "Trained batch 225 batch loss 1.06996477 epoch total loss 1.15337467\n",
      "Trained batch 226 batch loss 1.10399425 epoch total loss 1.15315628\n",
      "Trained batch 227 batch loss 1.23944569 epoch total loss 1.15353632\n",
      "Trained batch 228 batch loss 1.00656617 epoch total loss 1.15289176\n",
      "Trained batch 229 batch loss 0.981013834 epoch total loss 1.15214121\n",
      "Trained batch 230 batch loss 1.15851378 epoch total loss 1.15216887\n",
      "Trained batch 231 batch loss 1.3226825 epoch total loss 1.15290701\n",
      "Trained batch 232 batch loss 1.08529067 epoch total loss 1.15261567\n",
      "Trained batch 233 batch loss 1.05743408 epoch total loss 1.15220714\n",
      "Trained batch 234 batch loss 1.02419364 epoch total loss 1.15166008\n",
      "Trained batch 235 batch loss 1.02419341 epoch total loss 1.15111768\n",
      "Trained batch 236 batch loss 0.944920301 epoch total loss 1.150244\n",
      "Trained batch 237 batch loss 0.874590099 epoch total loss 1.14908099\n",
      "Trained batch 238 batch loss 0.977922 epoch total loss 1.1483618\n",
      "Trained batch 239 batch loss 1.0846411 epoch total loss 1.14809525\n",
      "Trained batch 240 batch loss 1.07205248 epoch total loss 1.14777839\n",
      "Trained batch 241 batch loss 1.09811926 epoch total loss 1.1475724\n",
      "Trained batch 242 batch loss 1.14393616 epoch total loss 1.14755726\n",
      "Trained batch 243 batch loss 1.20020103 epoch total loss 1.14777386\n",
      "Trained batch 244 batch loss 1.08118558 epoch total loss 1.14750099\n",
      "Trained batch 245 batch loss 1.05262136 epoch total loss 1.14711368\n",
      "Trained batch 246 batch loss 1.06991172 epoch total loss 1.1467998\n",
      "Trained batch 247 batch loss 1.10494876 epoch total loss 1.14663041\n",
      "Trained batch 248 batch loss 1.16334152 epoch total loss 1.14669776\n",
      "Trained batch 249 batch loss 1.22227657 epoch total loss 1.14700139\n",
      "Trained batch 250 batch loss 1.09805572 epoch total loss 1.14680552\n",
      "Trained batch 251 batch loss 1.02955914 epoch total loss 1.14633846\n",
      "Trained batch 252 batch loss 1.08217335 epoch total loss 1.14608395\n",
      "Trained batch 253 batch loss 1.12416053 epoch total loss 1.14599717\n",
      "Trained batch 254 batch loss 1.11117828 epoch total loss 1.14586008\n",
      "Trained batch 255 batch loss 1.18275785 epoch total loss 1.1460048\n",
      "Trained batch 256 batch loss 1.09383452 epoch total loss 1.14580107\n",
      "Trained batch 257 batch loss 0.963486373 epoch total loss 1.14509177\n",
      "Trained batch 258 batch loss 0.993284702 epoch total loss 1.14450336\n",
      "Trained batch 259 batch loss 1.12661958 epoch total loss 1.14443433\n",
      "Trained batch 260 batch loss 1.07732463 epoch total loss 1.14417624\n",
      "Trained batch 261 batch loss 1.12371719 epoch total loss 1.14409781\n",
      "Trained batch 262 batch loss 1.1693306 epoch total loss 1.14419413\n",
      "Trained batch 263 batch loss 1.06838083 epoch total loss 1.14390588\n",
      "Trained batch 264 batch loss 1.11032486 epoch total loss 1.14377868\n",
      "Trained batch 265 batch loss 1.08662879 epoch total loss 1.14356315\n",
      "Trained batch 266 batch loss 1.01740491 epoch total loss 1.14308882\n",
      "Trained batch 267 batch loss 1.09993887 epoch total loss 1.14292717\n",
      "Trained batch 268 batch loss 1.05248952 epoch total loss 1.14258969\n",
      "Trained batch 269 batch loss 1.00228667 epoch total loss 1.14206815\n",
      "Trained batch 270 batch loss 1.03017879 epoch total loss 1.14165378\n",
      "Trained batch 271 batch loss 1.03618658 epoch total loss 1.14126468\n",
      "Trained batch 272 batch loss 0.881628871 epoch total loss 1.14031\n",
      "Trained batch 273 batch loss 1.0517714 epoch total loss 1.13998568\n",
      "Trained batch 274 batch loss 1.31811237 epoch total loss 1.14063585\n",
      "Trained batch 275 batch loss 1.23645306 epoch total loss 1.14098418\n",
      "Trained batch 276 batch loss 1.10822928 epoch total loss 1.14086545\n",
      "Trained batch 277 batch loss 1.21027231 epoch total loss 1.14111602\n",
      "Trained batch 278 batch loss 1.21107125 epoch total loss 1.14136767\n",
      "Trained batch 279 batch loss 0.968341351 epoch total loss 1.14074755\n",
      "Trained batch 280 batch loss 1.11568129 epoch total loss 1.14065802\n",
      "Trained batch 281 batch loss 1.11244512 epoch total loss 1.14055765\n",
      "Trained batch 282 batch loss 1.07471061 epoch total loss 1.14032412\n",
      "Trained batch 283 batch loss 1.21575141 epoch total loss 1.14059067\n",
      "Trained batch 284 batch loss 1.13691223 epoch total loss 1.14057767\n",
      "Trained batch 285 batch loss 1.12817693 epoch total loss 1.14053416\n",
      "Trained batch 286 batch loss 1.18825364 epoch total loss 1.14070106\n",
      "Trained batch 287 batch loss 1.30765939 epoch total loss 1.1412828\n",
      "Trained batch 288 batch loss 1.3470515 epoch total loss 1.14199722\n",
      "Trained batch 289 batch loss 1.14091468 epoch total loss 1.1419934\n",
      "Trained batch 290 batch loss 1.17182875 epoch total loss 1.14209628\n",
      "Trained batch 291 batch loss 1.09185982 epoch total loss 1.14192367\n",
      "Trained batch 292 batch loss 1.1206249 epoch total loss 1.14185071\n",
      "Trained batch 293 batch loss 1.20515323 epoch total loss 1.14206672\n",
      "Trained batch 294 batch loss 1.25553215 epoch total loss 1.1424526\n",
      "Trained batch 295 batch loss 1.4024533 epoch total loss 1.14333403\n",
      "Trained batch 296 batch loss 1.24403 epoch total loss 1.14367414\n",
      "Trained batch 297 batch loss 1.15034723 epoch total loss 1.14369667\n",
      "Trained batch 298 batch loss 1.26179099 epoch total loss 1.14409292\n",
      "Trained batch 299 batch loss 1.10011399 epoch total loss 1.14394593\n",
      "Trained batch 300 batch loss 1.29539526 epoch total loss 1.14445078\n",
      "Trained batch 301 batch loss 1.29860318 epoch total loss 1.14496291\n",
      "Trained batch 302 batch loss 1.15336239 epoch total loss 1.14499068\n",
      "Trained batch 303 batch loss 0.990069568 epoch total loss 1.14447951\n",
      "Trained batch 304 batch loss 1.12417817 epoch total loss 1.14441264\n",
      "Trained batch 305 batch loss 1.02167976 epoch total loss 1.14401031\n",
      "Trained batch 306 batch loss 0.984055579 epoch total loss 1.14348757\n",
      "Trained batch 307 batch loss 1.09213626 epoch total loss 1.14332032\n",
      "Trained batch 308 batch loss 1.28309488 epoch total loss 1.14377403\n",
      "Trained batch 309 batch loss 1.19162881 epoch total loss 1.14392889\n",
      "Trained batch 310 batch loss 1.17427731 epoch total loss 1.14402688\n",
      "Trained batch 311 batch loss 1.20316982 epoch total loss 1.1442169\n",
      "Trained batch 312 batch loss 1.16623 epoch total loss 1.14428747\n",
      "Trained batch 313 batch loss 1.21725607 epoch total loss 1.14452064\n",
      "Trained batch 314 batch loss 1.27271962 epoch total loss 1.14492881\n",
      "Trained batch 315 batch loss 1.17806268 epoch total loss 1.14503407\n",
      "Trained batch 316 batch loss 1.10479987 epoch total loss 1.14490676\n",
      "Trained batch 317 batch loss 1.0735755 epoch total loss 1.14468169\n",
      "Trained batch 318 batch loss 1.09871137 epoch total loss 1.14453721\n",
      "Trained batch 319 batch loss 0.964358568 epoch total loss 1.1439724\n",
      "Trained batch 320 batch loss 1.02356696 epoch total loss 1.14359605\n",
      "Trained batch 321 batch loss 1.13937128 epoch total loss 1.14358294\n",
      "Trained batch 322 batch loss 1.07339716 epoch total loss 1.14336491\n",
      "Trained batch 323 batch loss 1.05037725 epoch total loss 1.14307714\n",
      "Trained batch 324 batch loss 1.02244878 epoch total loss 1.14270484\n",
      "Trained batch 325 batch loss 1.14438379 epoch total loss 1.14271\n",
      "Trained batch 326 batch loss 1.17835963 epoch total loss 1.14281929\n",
      "Trained batch 327 batch loss 1.13792944 epoch total loss 1.14280438\n",
      "Trained batch 328 batch loss 1.25071681 epoch total loss 1.14313328\n",
      "Trained batch 329 batch loss 1.11422765 epoch total loss 1.14304543\n",
      "Trained batch 330 batch loss 1.22955322 epoch total loss 1.14330757\n",
      "Trained batch 331 batch loss 1.16592193 epoch total loss 1.14337587\n",
      "Trained batch 332 batch loss 1.04867113 epoch total loss 1.14309072\n",
      "Trained batch 333 batch loss 1.15664494 epoch total loss 1.14313138\n",
      "Trained batch 334 batch loss 1.19934988 epoch total loss 1.1432997\n",
      "Trained batch 335 batch loss 1.18342817 epoch total loss 1.1434195\n",
      "Trained batch 336 batch loss 1.1911881 epoch total loss 1.14356172\n",
      "Trained batch 337 batch loss 1.22318244 epoch total loss 1.14379787\n",
      "Trained batch 338 batch loss 1.30023921 epoch total loss 1.14426076\n",
      "Trained batch 339 batch loss 1.2128284 epoch total loss 1.14446306\n",
      "Trained batch 340 batch loss 1.2544626 epoch total loss 1.14478648\n",
      "Trained batch 341 batch loss 1.23233294 epoch total loss 1.14504325\n",
      "Trained batch 342 batch loss 1.15747762 epoch total loss 1.14507961\n",
      "Trained batch 343 batch loss 1.12949765 epoch total loss 1.14503407\n",
      "Trained batch 344 batch loss 1.15179086 epoch total loss 1.14505374\n",
      "Trained batch 345 batch loss 1.07812321 epoch total loss 1.14485979\n",
      "Trained batch 346 batch loss 1.03693628 epoch total loss 1.14454782\n",
      "Trained batch 347 batch loss 1.06165195 epoch total loss 1.14430892\n",
      "Trained batch 348 batch loss 1.13582301 epoch total loss 1.14428461\n",
      "Trained batch 349 batch loss 1.15178752 epoch total loss 1.14430606\n",
      "Trained batch 350 batch loss 1.00090289 epoch total loss 1.14389634\n",
      "Trained batch 351 batch loss 0.937001586 epoch total loss 1.14330697\n",
      "Trained batch 352 batch loss 1.05647779 epoch total loss 1.14306033\n",
      "Trained batch 353 batch loss 1.04910314 epoch total loss 1.14279413\n",
      "Trained batch 354 batch loss 0.990608096 epoch total loss 1.14236426\n",
      "Trained batch 355 batch loss 0.984968901 epoch total loss 1.1419208\n",
      "Trained batch 356 batch loss 1.01862 epoch total loss 1.1415745\n",
      "Trained batch 357 batch loss 1.04043877 epoch total loss 1.14129114\n",
      "Trained batch 358 batch loss 1.00707066 epoch total loss 1.14091623\n",
      "Trained batch 359 batch loss 1.00049663 epoch total loss 1.1405251\n",
      "Trained batch 360 batch loss 1.11565757 epoch total loss 1.14045608\n",
      "Trained batch 361 batch loss 1.09596658 epoch total loss 1.14033282\n",
      "Trained batch 362 batch loss 1.17319071 epoch total loss 1.14042366\n",
      "Trained batch 363 batch loss 1.22809792 epoch total loss 1.14066505\n",
      "Trained batch 364 batch loss 1.20282638 epoch total loss 1.14083588\n",
      "Trained batch 365 batch loss 1.07835579 epoch total loss 1.1406647\n",
      "Trained batch 366 batch loss 1.13444614 epoch total loss 1.14064777\n",
      "Trained batch 367 batch loss 1.14033628 epoch total loss 1.14064693\n",
      "Trained batch 368 batch loss 1.04552758 epoch total loss 1.14038849\n",
      "Trained batch 369 batch loss 1.11151266 epoch total loss 1.14031029\n",
      "Trained batch 370 batch loss 1.05443156 epoch total loss 1.14007819\n",
      "Trained batch 371 batch loss 1.14294755 epoch total loss 1.14008594\n",
      "Trained batch 372 batch loss 1.35006475 epoch total loss 1.14065039\n",
      "Trained batch 373 batch loss 1.23644567 epoch total loss 1.14090717\n",
      "Trained batch 374 batch loss 1.11757064 epoch total loss 1.14084482\n",
      "Trained batch 375 batch loss 1.11448884 epoch total loss 1.14077461\n",
      "Trained batch 376 batch loss 1.07396317 epoch total loss 1.14059687\n",
      "Trained batch 377 batch loss 0.96257 epoch total loss 1.14012468\n",
      "Trained batch 378 batch loss 1.14354384 epoch total loss 1.14013374\n",
      "Trained batch 379 batch loss 1.14884543 epoch total loss 1.14015675\n",
      "Trained batch 380 batch loss 1.13602686 epoch total loss 1.14014578\n",
      "Trained batch 381 batch loss 1.05060112 epoch total loss 1.13991082\n",
      "Trained batch 382 batch loss 0.994482398 epoch total loss 1.13953006\n",
      "Trained batch 383 batch loss 1.12242949 epoch total loss 1.13948536\n",
      "Trained batch 384 batch loss 1.29507303 epoch total loss 1.13989055\n",
      "Trained batch 385 batch loss 1.24782288 epoch total loss 1.14017093\n",
      "Trained batch 386 batch loss 1.12233305 epoch total loss 1.1401248\n",
      "Trained batch 387 batch loss 0.961825 epoch total loss 1.13966405\n",
      "Trained batch 388 batch loss 0.953317165 epoch total loss 1.13918376\n",
      "Trained batch 389 batch loss 0.797200561 epoch total loss 1.13830459\n",
      "Trained batch 390 batch loss 0.875138 epoch total loss 1.13762987\n",
      "Trained batch 391 batch loss 0.986556649 epoch total loss 1.13724351\n",
      "Trained batch 392 batch loss 1.31658161 epoch total loss 1.13770103\n",
      "Trained batch 393 batch loss 1.12619925 epoch total loss 1.13767171\n",
      "Trained batch 394 batch loss 1.12628651 epoch total loss 1.13764274\n",
      "Trained batch 395 batch loss 1.02775395 epoch total loss 1.13736451\n",
      "Trained batch 396 batch loss 1.11494398 epoch total loss 1.13730788\n",
      "Trained batch 397 batch loss 1.18815947 epoch total loss 1.13743603\n",
      "Trained batch 398 batch loss 1.21744895 epoch total loss 1.13763702\n",
      "Trained batch 399 batch loss 1.18577743 epoch total loss 1.13775778\n",
      "Trained batch 400 batch loss 1.28559768 epoch total loss 1.13812733\n",
      "Trained batch 401 batch loss 1.24043643 epoch total loss 1.13838243\n",
      "Trained batch 402 batch loss 1.27139151 epoch total loss 1.13871336\n",
      "Trained batch 403 batch loss 1.25593781 epoch total loss 1.13900423\n",
      "Trained batch 404 batch loss 1.1983645 epoch total loss 1.13915122\n",
      "Trained batch 405 batch loss 1.27689576 epoch total loss 1.1394912\n",
      "Trained batch 406 batch loss 1.18406796 epoch total loss 1.13960111\n",
      "Trained batch 407 batch loss 1.07704091 epoch total loss 1.13944733\n",
      "Trained batch 408 batch loss 1.11477196 epoch total loss 1.13938689\n",
      "Trained batch 409 batch loss 1.07466102 epoch total loss 1.13922858\n",
      "Trained batch 410 batch loss 1.0322547 epoch total loss 1.13896763\n",
      "Trained batch 411 batch loss 1.14742589 epoch total loss 1.13898826\n",
      "Trained batch 412 batch loss 1.13500285 epoch total loss 1.1389786\n",
      "Trained batch 413 batch loss 1.20889676 epoch total loss 1.13914788\n",
      "Trained batch 414 batch loss 1.17106104 epoch total loss 1.13922501\n",
      "Trained batch 415 batch loss 1.1586144 epoch total loss 1.13927162\n",
      "Trained batch 416 batch loss 1.22019327 epoch total loss 1.13946617\n",
      "Trained batch 417 batch loss 1.22174406 epoch total loss 1.13966346\n",
      "Trained batch 418 batch loss 1.04467881 epoch total loss 1.13943624\n",
      "Trained batch 419 batch loss 1.12122381 epoch total loss 1.13939273\n",
      "Trained batch 420 batch loss 1.15839589 epoch total loss 1.13943791\n",
      "Trained batch 421 batch loss 1.12088871 epoch total loss 1.13939381\n",
      "Trained batch 422 batch loss 1.0326035 epoch total loss 1.13914073\n",
      "Trained batch 423 batch loss 1.08700454 epoch total loss 1.13901758\n",
      "Trained batch 424 batch loss 1.08354235 epoch total loss 1.13888669\n",
      "Trained batch 425 batch loss 1.2597363 epoch total loss 1.13917112\n",
      "Trained batch 426 batch loss 1.26389503 epoch total loss 1.13946378\n",
      "Trained batch 427 batch loss 1.35681665 epoch total loss 1.13997281\n",
      "Trained batch 428 batch loss 1.30910778 epoch total loss 1.14036798\n",
      "Trained batch 429 batch loss 1.18206203 epoch total loss 1.14046526\n",
      "Trained batch 430 batch loss 1.02753913 epoch total loss 1.14020252\n",
      "Trained batch 431 batch loss 0.981792927 epoch total loss 1.139835\n",
      "Trained batch 432 batch loss 0.875483274 epoch total loss 1.1392231\n",
      "Trained batch 433 batch loss 0.963907659 epoch total loss 1.13881814\n",
      "Trained batch 434 batch loss 1.00696039 epoch total loss 1.1385144\n",
      "Trained batch 435 batch loss 1.10076046 epoch total loss 1.13842762\n",
      "Trained batch 436 batch loss 1.18855929 epoch total loss 1.13854265\n",
      "Trained batch 437 batch loss 1.27452612 epoch total loss 1.13885379\n",
      "Trained batch 438 batch loss 1.38669169 epoch total loss 1.13941967\n",
      "Trained batch 439 batch loss 1.22435439 epoch total loss 1.13961315\n",
      "Trained batch 440 batch loss 1.25192869 epoch total loss 1.13986838\n",
      "Trained batch 441 batch loss 1.20917177 epoch total loss 1.1400255\n",
      "Trained batch 442 batch loss 1.09632695 epoch total loss 1.13992667\n",
      "Trained batch 443 batch loss 1.04082167 epoch total loss 1.13970292\n",
      "Trained batch 444 batch loss 1.08436656 epoch total loss 1.13957834\n",
      "Trained batch 445 batch loss 1.17856622 epoch total loss 1.13966596\n",
      "Trained batch 446 batch loss 1.21676612 epoch total loss 1.13983881\n",
      "Trained batch 447 batch loss 1.16557574 epoch total loss 1.13989639\n",
      "Trained batch 448 batch loss 1.05047786 epoch total loss 1.13969684\n",
      "Trained batch 449 batch loss 1.16798842 epoch total loss 1.13975978\n",
      "Trained batch 450 batch loss 0.996557713 epoch total loss 1.13944161\n",
      "Trained batch 451 batch loss 1.05920506 epoch total loss 1.13926363\n",
      "Trained batch 452 batch loss 1.08872783 epoch total loss 1.13915193\n",
      "Trained batch 453 batch loss 1.02080333 epoch total loss 1.13889074\n",
      "Trained batch 454 batch loss 1.05527508 epoch total loss 1.13870656\n",
      "Trained batch 455 batch loss 1.10606456 epoch total loss 1.1386348\n",
      "Trained batch 456 batch loss 1.0591445 epoch total loss 1.13846052\n",
      "Trained batch 457 batch loss 1.08075655 epoch total loss 1.13833427\n",
      "Trained batch 458 batch loss 1.18122411 epoch total loss 1.13842785\n",
      "Trained batch 459 batch loss 1.11332417 epoch total loss 1.13837326\n",
      "Trained batch 460 batch loss 1.16811657 epoch total loss 1.13843787\n",
      "Trained batch 461 batch loss 1.10968292 epoch total loss 1.1383754\n",
      "Trained batch 462 batch loss 1.16899037 epoch total loss 1.13844168\n",
      "Trained batch 463 batch loss 1.13537419 epoch total loss 1.13843513\n",
      "Trained batch 464 batch loss 1.1894412 epoch total loss 1.13854504\n",
      "Trained batch 465 batch loss 1.21623099 epoch total loss 1.13871217\n",
      "Trained batch 466 batch loss 1.16017222 epoch total loss 1.13875818\n",
      "Trained batch 467 batch loss 1.22141445 epoch total loss 1.13893521\n",
      "Trained batch 468 batch loss 1.04988074 epoch total loss 1.13874495\n",
      "Trained batch 469 batch loss 1.09182 epoch total loss 1.13864481\n",
      "Trained batch 470 batch loss 1.20399261 epoch total loss 1.13878381\n",
      "Trained batch 471 batch loss 1.16017175 epoch total loss 1.13882923\n",
      "Trained batch 472 batch loss 1.25803518 epoch total loss 1.13908184\n",
      "Trained batch 473 batch loss 1.3461231 epoch total loss 1.13951957\n",
      "Trained batch 474 batch loss 1.25364971 epoch total loss 1.13976038\n",
      "Trained batch 475 batch loss 1.31381881 epoch total loss 1.14012682\n",
      "Trained batch 476 batch loss 1.17595959 epoch total loss 1.14020216\n",
      "Trained batch 477 batch loss 1.06060147 epoch total loss 1.14003527\n",
      "Trained batch 478 batch loss 1.1584959 epoch total loss 1.1400739\n",
      "Trained batch 479 batch loss 1.11264694 epoch total loss 1.14001667\n",
      "Trained batch 480 batch loss 1.17886198 epoch total loss 1.1400975\n",
      "Trained batch 481 batch loss 1.12104 epoch total loss 1.14005792\n",
      "Trained batch 482 batch loss 1.13626015 epoch total loss 1.14004993\n",
      "Trained batch 483 batch loss 1.11816835 epoch total loss 1.14000463\n",
      "Trained batch 484 batch loss 1.02873123 epoch total loss 1.1397748\n",
      "Trained batch 485 batch loss 1.0319556 epoch total loss 1.13955259\n",
      "Trained batch 486 batch loss 1.22798216 epoch total loss 1.13973451\n",
      "Trained batch 487 batch loss 1.1068362 epoch total loss 1.13966691\n",
      "Trained batch 488 batch loss 1.15589368 epoch total loss 1.1397\n",
      "Trained batch 489 batch loss 1.1503607 epoch total loss 1.13972199\n",
      "Trained batch 490 batch loss 1.04426765 epoch total loss 1.13952708\n",
      "Trained batch 491 batch loss 1.05393481 epoch total loss 1.1393528\n",
      "Trained batch 492 batch loss 1.16080284 epoch total loss 1.13939643\n",
      "Trained batch 493 batch loss 1.09927177 epoch total loss 1.13931501\n",
      "Trained batch 494 batch loss 1.08312583 epoch total loss 1.13920128\n",
      "Trained batch 495 batch loss 1.10787296 epoch total loss 1.13913798\n",
      "Trained batch 496 batch loss 1.08159351 epoch total loss 1.13902199\n",
      "Trained batch 497 batch loss 1.09674573 epoch total loss 1.13893688\n",
      "Trained batch 498 batch loss 1.16143012 epoch total loss 1.13898206\n",
      "Trained batch 499 batch loss 1.16222572 epoch total loss 1.13902867\n",
      "Trained batch 500 batch loss 1.15424156 epoch total loss 1.13905907\n",
      "Trained batch 501 batch loss 1.04400396 epoch total loss 1.1388694\n",
      "Trained batch 502 batch loss 0.994523466 epoch total loss 1.13858175\n",
      "Trained batch 503 batch loss 0.916562438 epoch total loss 1.13814044\n",
      "Trained batch 504 batch loss 1.16963911 epoch total loss 1.13820291\n",
      "Trained batch 505 batch loss 1.10711324 epoch total loss 1.13814127\n",
      "Trained batch 506 batch loss 1.18943226 epoch total loss 1.13824272\n",
      "Trained batch 507 batch loss 1.3061161 epoch total loss 1.13857377\n",
      "Trained batch 508 batch loss 1.33736634 epoch total loss 1.13896501\n",
      "Trained batch 509 batch loss 1.05631876 epoch total loss 1.13880265\n",
      "Trained batch 510 batch loss 1.11506081 epoch total loss 1.13875616\n",
      "Trained batch 511 batch loss 1.12827969 epoch total loss 1.13873565\n",
      "Trained batch 512 batch loss 1.1087867 epoch total loss 1.13867712\n",
      "Trained batch 513 batch loss 1.1736778 epoch total loss 1.13874543\n",
      "Trained batch 514 batch loss 1.00063825 epoch total loss 1.13847661\n",
      "Trained batch 515 batch loss 0.885703385 epoch total loss 1.13798583\n",
      "Trained batch 516 batch loss 0.960278094 epoch total loss 1.13764143\n",
      "Trained batch 517 batch loss 1.2131325 epoch total loss 1.13778734\n",
      "Trained batch 518 batch loss 1.26692092 epoch total loss 1.13803661\n",
      "Trained batch 519 batch loss 1.29106879 epoch total loss 1.13833153\n",
      "Trained batch 520 batch loss 1.25152504 epoch total loss 1.13854921\n",
      "Trained batch 521 batch loss 1.20926249 epoch total loss 1.13868499\n",
      "Trained batch 522 batch loss 1.18031502 epoch total loss 1.13876474\n",
      "Trained batch 523 batch loss 1.15194225 epoch total loss 1.13878989\n",
      "Trained batch 524 batch loss 1.07782972 epoch total loss 1.13867354\n",
      "Trained batch 525 batch loss 0.939953446 epoch total loss 1.13829494\n",
      "Trained batch 526 batch loss 0.914161742 epoch total loss 1.13786888\n",
      "Trained batch 527 batch loss 1.18472791 epoch total loss 1.13795781\n",
      "Trained batch 528 batch loss 1.12107718 epoch total loss 1.13792598\n",
      "Trained batch 529 batch loss 1.20365334 epoch total loss 1.1380502\n",
      "Trained batch 530 batch loss 1.33601379 epoch total loss 1.13842368\n",
      "Trained batch 531 batch loss 1.23664093 epoch total loss 1.13860869\n",
      "Trained batch 532 batch loss 1.32165444 epoch total loss 1.13895273\n",
      "Trained batch 533 batch loss 1.2361002 epoch total loss 1.139135\n",
      "Trained batch 534 batch loss 1.0023005 epoch total loss 1.1388787\n",
      "Trained batch 535 batch loss 1.01067579 epoch total loss 1.13863909\n",
      "Trained batch 536 batch loss 1.29462528 epoch total loss 1.13893008\n",
      "Trained batch 537 batch loss 1.08547091 epoch total loss 1.13883054\n",
      "Trained batch 538 batch loss 1.13592863 epoch total loss 1.13882518\n",
      "Trained batch 539 batch loss 1.13486648 epoch total loss 1.13881779\n",
      "Trained batch 540 batch loss 1.20242095 epoch total loss 1.13893557\n",
      "Trained batch 541 batch loss 1.19210708 epoch total loss 1.13903379\n",
      "Trained batch 542 batch loss 1.18199313 epoch total loss 1.13911307\n",
      "Trained batch 543 batch loss 1.09404445 epoch total loss 1.1390301\n",
      "Trained batch 544 batch loss 1.03963518 epoch total loss 1.13884735\n",
      "Trained batch 545 batch loss 1.08306479 epoch total loss 1.13874495\n",
      "Trained batch 546 batch loss 0.996687233 epoch total loss 1.13848484\n",
      "Trained batch 547 batch loss 1.10094297 epoch total loss 1.13841629\n",
      "Trained batch 548 batch loss 0.936671853 epoch total loss 1.13804805\n",
      "Trained batch 549 batch loss 0.960068226 epoch total loss 1.13772392\n",
      "Trained batch 550 batch loss 1.04418099 epoch total loss 1.13755381\n",
      "Trained batch 551 batch loss 1.01640213 epoch total loss 1.13733399\n",
      "Trained batch 552 batch loss 0.980838299 epoch total loss 1.13705051\n",
      "Trained batch 553 batch loss 0.99631685 epoch total loss 1.136796\n",
      "Trained batch 554 batch loss 1.11479878 epoch total loss 1.1367563\n",
      "Trained batch 555 batch loss 1.13161874 epoch total loss 1.136747\n",
      "Trained batch 556 batch loss 1.25738204 epoch total loss 1.13696396\n",
      "Trained batch 557 batch loss 1.27931881 epoch total loss 1.13721955\n",
      "Trained batch 558 batch loss 1.16619182 epoch total loss 1.1372714\n",
      "Trained batch 559 batch loss 1.23588419 epoch total loss 1.13744795\n",
      "Trained batch 560 batch loss 1.25470114 epoch total loss 1.13765728\n",
      "Trained batch 561 batch loss 1.17696571 epoch total loss 1.13772726\n",
      "Trained batch 562 batch loss 1.12083793 epoch total loss 1.13769722\n",
      "Trained batch 563 batch loss 1.06758535 epoch total loss 1.13757265\n",
      "Trained batch 564 batch loss 1.04884267 epoch total loss 1.13741541\n",
      "Trained batch 565 batch loss 1.17510855 epoch total loss 1.13748205\n",
      "Trained batch 566 batch loss 1.2097702 epoch total loss 1.13760984\n",
      "Trained batch 567 batch loss 1.21270657 epoch total loss 1.13774228\n",
      "Trained batch 568 batch loss 1.19966543 epoch total loss 1.13785124\n",
      "Trained batch 569 batch loss 1.17566514 epoch total loss 1.13791764\n",
      "Trained batch 570 batch loss 1.24211431 epoch total loss 1.1381005\n",
      "Trained batch 571 batch loss 1.3460896 epoch total loss 1.13846469\n",
      "Trained batch 572 batch loss 1.16841769 epoch total loss 1.13851702\n",
      "Trained batch 573 batch loss 1.24618924 epoch total loss 1.13870502\n",
      "Trained batch 574 batch loss 1.16625834 epoch total loss 1.13875306\n",
      "Trained batch 575 batch loss 1.13299847 epoch total loss 1.13874304\n",
      "Trained batch 576 batch loss 1.20390165 epoch total loss 1.13885617\n",
      "Trained batch 577 batch loss 1.18669236 epoch total loss 1.13893902\n",
      "Trained batch 578 batch loss 1.16538715 epoch total loss 1.13898492\n",
      "Trained batch 579 batch loss 1.1940397 epoch total loss 1.13907993\n",
      "Trained batch 580 batch loss 1.23899674 epoch total loss 1.13925219\n",
      "Trained batch 581 batch loss 1.26102 epoch total loss 1.13946187\n",
      "Trained batch 582 batch loss 1.231848 epoch total loss 1.13962066\n",
      "Trained batch 583 batch loss 1.28160357 epoch total loss 1.13986421\n",
      "Trained batch 584 batch loss 1.30760598 epoch total loss 1.1401515\n",
      "Trained batch 585 batch loss 1.08349037 epoch total loss 1.14005458\n",
      "Trained batch 586 batch loss 1.13171208 epoch total loss 1.1400404\n",
      "Trained batch 587 batch loss 1.07269657 epoch total loss 1.1399256\n",
      "Trained batch 588 batch loss 1.03751111 epoch total loss 1.13975155\n",
      "Trained batch 589 batch loss 1.12750435 epoch total loss 1.13973069\n",
      "Trained batch 590 batch loss 1.06852663 epoch total loss 1.13961\n",
      "Trained batch 591 batch loss 1.17693806 epoch total loss 1.13967323\n",
      "Trained batch 592 batch loss 1.16497672 epoch total loss 1.13971591\n",
      "Trained batch 593 batch loss 1.22304237 epoch total loss 1.13985646\n",
      "Trained batch 594 batch loss 1.30786681 epoch total loss 1.14013934\n",
      "Trained batch 595 batch loss 1.14881551 epoch total loss 1.14015388\n",
      "Trained batch 596 batch loss 1.10166955 epoch total loss 1.14008927\n",
      "Trained batch 597 batch loss 1.08188701 epoch total loss 1.13999188\n",
      "Trained batch 598 batch loss 1.04335368 epoch total loss 1.13983023\n",
      "Trained batch 599 batch loss 1.06318653 epoch total loss 1.1397022\n",
      "Trained batch 600 batch loss 1.14466953 epoch total loss 1.13971055\n",
      "Trained batch 601 batch loss 1.28083944 epoch total loss 1.13994527\n",
      "Trained batch 602 batch loss 1.19089043 epoch total loss 1.14002991\n",
      "Trained batch 603 batch loss 1.13560498 epoch total loss 1.14002264\n",
      "Trained batch 604 batch loss 1.09933639 epoch total loss 1.13995528\n",
      "Trained batch 605 batch loss 1.14526916 epoch total loss 1.1399641\n",
      "Trained batch 606 batch loss 1.13541389 epoch total loss 1.13995659\n",
      "Trained batch 607 batch loss 1.19311583 epoch total loss 1.14004421\n",
      "Trained batch 608 batch loss 1.29016376 epoch total loss 1.14029109\n",
      "Trained batch 609 batch loss 1.31242228 epoch total loss 1.14057374\n",
      "Trained batch 610 batch loss 1.2112956 epoch total loss 1.14068973\n",
      "Trained batch 611 batch loss 1.15093517 epoch total loss 1.14070654\n",
      "Trained batch 612 batch loss 1.06771982 epoch total loss 1.14058733\n",
      "Trained batch 613 batch loss 1.1369828 epoch total loss 1.14058137\n",
      "Trained batch 614 batch loss 1.22907221 epoch total loss 1.14072549\n",
      "Trained batch 615 batch loss 1.14357257 epoch total loss 1.14073014\n",
      "Trained batch 616 batch loss 1.17843401 epoch total loss 1.1407913\n",
      "Trained batch 617 batch loss 1.27253616 epoch total loss 1.1410048\n",
      "Trained batch 618 batch loss 1.24946737 epoch total loss 1.14118028\n",
      "Trained batch 619 batch loss 1.33296633 epoch total loss 1.1414901\n",
      "Trained batch 620 batch loss 1.2546351 epoch total loss 1.14167249\n",
      "Trained batch 621 batch loss 1.21581328 epoch total loss 1.14179194\n",
      "Trained batch 622 batch loss 1.08009839 epoch total loss 1.14169276\n",
      "Trained batch 623 batch loss 1.20229375 epoch total loss 1.14178991\n",
      "Trained batch 624 batch loss 1.20179498 epoch total loss 1.14188612\n",
      "Trained batch 625 batch loss 1.17418242 epoch total loss 1.14193785\n",
      "Trained batch 626 batch loss 1.17512536 epoch total loss 1.14199078\n",
      "Trained batch 627 batch loss 1.08152926 epoch total loss 1.14189434\n",
      "Trained batch 628 batch loss 1.12945008 epoch total loss 1.14187455\n",
      "Trained batch 629 batch loss 0.976848066 epoch total loss 1.14161229\n",
      "Trained batch 630 batch loss 1.09593105 epoch total loss 1.14153969\n",
      "Trained batch 631 batch loss 0.910932779 epoch total loss 1.14117432\n",
      "Trained batch 632 batch loss 1.15652502 epoch total loss 1.14119864\n",
      "Trained batch 633 batch loss 0.993005753 epoch total loss 1.14096451\n",
      "Trained batch 634 batch loss 1.11718535 epoch total loss 1.14092696\n",
      "Trained batch 635 batch loss 1.06692886 epoch total loss 1.14081049\n",
      "Trained batch 636 batch loss 1.04841375 epoch total loss 1.14066517\n",
      "Trained batch 637 batch loss 1.11021221 epoch total loss 1.14061749\n",
      "Trained batch 638 batch loss 1.11096656 epoch total loss 1.140571\n",
      "Trained batch 639 batch loss 1.14362741 epoch total loss 1.14057577\n",
      "Trained batch 640 batch loss 1.02870846 epoch total loss 1.14040089\n",
      "Trained batch 641 batch loss 0.991449237 epoch total loss 1.14016855\n",
      "Trained batch 642 batch loss 1.0205009 epoch total loss 1.1399821\n",
      "Trained batch 643 batch loss 1.13685226 epoch total loss 1.13997722\n",
      "Trained batch 644 batch loss 1.12227035 epoch total loss 1.13994968\n",
      "Trained batch 645 batch loss 1.27512562 epoch total loss 1.14015937\n",
      "Trained batch 646 batch loss 1.40714288 epoch total loss 1.14057267\n",
      "Trained batch 647 batch loss 1.22234225 epoch total loss 1.14069903\n",
      "Trained batch 648 batch loss 1.13078761 epoch total loss 1.14068377\n",
      "Trained batch 649 batch loss 1.13333201 epoch total loss 1.14067245\n",
      "Trained batch 650 batch loss 1.07274616 epoch total loss 1.14056802\n",
      "Trained batch 651 batch loss 1.10484123 epoch total loss 1.14051318\n",
      "Trained batch 652 batch loss 1.2038002 epoch total loss 1.14061022\n",
      "Trained batch 653 batch loss 1.31680715 epoch total loss 1.14088011\n",
      "Trained batch 654 batch loss 1.13463151 epoch total loss 1.14087057\n",
      "Trained batch 655 batch loss 1.28559446 epoch total loss 1.14109147\n",
      "Trained batch 656 batch loss 1.10357 epoch total loss 1.14103425\n",
      "Trained batch 657 batch loss 1.16404772 epoch total loss 1.14106929\n",
      "Trained batch 658 batch loss 1.20842385 epoch total loss 1.14117169\n",
      "Trained batch 659 batch loss 1.25167584 epoch total loss 1.1413393\n",
      "Trained batch 660 batch loss 1.30183733 epoch total loss 1.14158249\n",
      "Trained batch 661 batch loss 1.17993546 epoch total loss 1.14164054\n",
      "Trained batch 662 batch loss 1.16130781 epoch total loss 1.14167023\n",
      "Trained batch 663 batch loss 1.18007803 epoch total loss 1.14172816\n",
      "Trained batch 664 batch loss 1.14031351 epoch total loss 1.14172602\n",
      "Trained batch 665 batch loss 1.27475059 epoch total loss 1.14192605\n",
      "Trained batch 666 batch loss 1.09855247 epoch total loss 1.14186096\n",
      "Trained batch 667 batch loss 1.2214427 epoch total loss 1.14198029\n",
      "Trained batch 668 batch loss 1.20365155 epoch total loss 1.14207268\n",
      "Trained batch 669 batch loss 1.14858031 epoch total loss 1.14208233\n",
      "Trained batch 670 batch loss 1.28428948 epoch total loss 1.14229465\n",
      "Trained batch 671 batch loss 1.18488419 epoch total loss 1.14235806\n",
      "Trained batch 672 batch loss 1.1097523 epoch total loss 1.14230955\n",
      "Trained batch 673 batch loss 1.12806058 epoch total loss 1.14228833\n",
      "Trained batch 674 batch loss 1.0274955 epoch total loss 1.14211798\n",
      "Trained batch 675 batch loss 1.10495663 epoch total loss 1.14206302\n",
      "Trained batch 676 batch loss 1.07019114 epoch total loss 1.14195669\n",
      "Trained batch 677 batch loss 1.11154509 epoch total loss 1.14191175\n",
      "Trained batch 678 batch loss 1.11538768 epoch total loss 1.14187276\n",
      "Trained batch 679 batch loss 1.05913961 epoch total loss 1.14175081\n",
      "Trained batch 680 batch loss 1.06573105 epoch total loss 1.14163911\n",
      "Trained batch 681 batch loss 1.11682892 epoch total loss 1.14160264\n",
      "Trained batch 682 batch loss 0.881998718 epoch total loss 1.141222\n",
      "Trained batch 683 batch loss 1.05971706 epoch total loss 1.14110267\n",
      "Trained batch 684 batch loss 1.05722106 epoch total loss 1.14098\n",
      "Trained batch 685 batch loss 1.05479455 epoch total loss 1.14085424\n",
      "Trained batch 686 batch loss 1.11699009 epoch total loss 1.14081955\n",
      "Trained batch 687 batch loss 1.21828806 epoch total loss 1.1409322\n",
      "Trained batch 688 batch loss 1.14034033 epoch total loss 1.14093137\n",
      "Trained batch 689 batch loss 1.07231188 epoch total loss 1.14083171\n",
      "Trained batch 690 batch loss 0.945000768 epoch total loss 1.14054799\n",
      "Trained batch 691 batch loss 1.07032478 epoch total loss 1.14044631\n",
      "Trained batch 692 batch loss 1.13772511 epoch total loss 1.14044237\n",
      "Trained batch 693 batch loss 1.2212764 epoch total loss 1.14055896\n",
      "Trained batch 694 batch loss 1.05195284 epoch total loss 1.14043128\n",
      "Trained batch 695 batch loss 1.10692477 epoch total loss 1.14038301\n",
      "Trained batch 696 batch loss 1.05527294 epoch total loss 1.14026082\n",
      "Trained batch 697 batch loss 1.05324292 epoch total loss 1.14013588\n",
      "Trained batch 698 batch loss 1.09310579 epoch total loss 1.14006853\n",
      "Trained batch 699 batch loss 1.21268749 epoch total loss 1.14017248\n",
      "Trained batch 700 batch loss 1.30013764 epoch total loss 1.14040089\n",
      "Trained batch 701 batch loss 1.42748475 epoch total loss 1.14081049\n",
      "Trained batch 702 batch loss 1.27105379 epoch total loss 1.14099598\n",
      "Trained batch 703 batch loss 1.16382802 epoch total loss 1.1410284\n",
      "Trained batch 704 batch loss 1.15094495 epoch total loss 1.14104259\n",
      "Trained batch 705 batch loss 1.28024805 epoch total loss 1.14124\n",
      "Trained batch 706 batch loss 1.09101248 epoch total loss 1.14116883\n",
      "Trained batch 707 batch loss 1.17847323 epoch total loss 1.14122164\n",
      "Trained batch 708 batch loss 1.17858016 epoch total loss 1.14127445\n",
      "Trained batch 709 batch loss 1.15674305 epoch total loss 1.14129627\n",
      "Trained batch 710 batch loss 1.17859268 epoch total loss 1.14134872\n",
      "Trained batch 711 batch loss 1.12216604 epoch total loss 1.14132178\n",
      "Trained batch 712 batch loss 1.12100458 epoch total loss 1.14129329\n",
      "Trained batch 713 batch loss 1.08620179 epoch total loss 1.14121604\n",
      "Trained batch 714 batch loss 1.04195821 epoch total loss 1.14107692\n",
      "Trained batch 715 batch loss 1.12965631 epoch total loss 1.14106095\n",
      "Trained batch 716 batch loss 1.19600356 epoch total loss 1.14113772\n",
      "Trained batch 717 batch loss 1.06498885 epoch total loss 1.1410315\n",
      "Trained batch 718 batch loss 1.09940565 epoch total loss 1.14097357\n",
      "Trained batch 719 batch loss 1.2465142 epoch total loss 1.14112031\n",
      "Trained batch 720 batch loss 1.06136787 epoch total loss 1.14100957\n",
      "Trained batch 721 batch loss 1.0752064 epoch total loss 1.14091825\n",
      "Trained batch 722 batch loss 1.03985095 epoch total loss 1.1407783\n",
      "Trained batch 723 batch loss 1.15312219 epoch total loss 1.14079535\n",
      "Trained batch 724 batch loss 1.01293612 epoch total loss 1.1406188\n",
      "Trained batch 725 batch loss 0.925383568 epoch total loss 1.14032185\n",
      "Trained batch 726 batch loss 0.910099626 epoch total loss 1.14000475\n",
      "Trained batch 727 batch loss 0.953085542 epoch total loss 1.13974762\n",
      "Trained batch 728 batch loss 1.12749159 epoch total loss 1.13973081\n",
      "Trained batch 729 batch loss 0.998456836 epoch total loss 1.13953698\n",
      "Trained batch 730 batch loss 1.03768706 epoch total loss 1.13939738\n",
      "Trained batch 731 batch loss 1.05531502 epoch total loss 1.13928235\n",
      "Trained batch 732 batch loss 1.08067048 epoch total loss 1.13920236\n",
      "Trained batch 733 batch loss 1.16483796 epoch total loss 1.1392374\n",
      "Trained batch 734 batch loss 1.15635729 epoch total loss 1.13926065\n",
      "Trained batch 735 batch loss 1.19161415 epoch total loss 1.13933194\n",
      "Trained batch 736 batch loss 1.3135072 epoch total loss 1.13956857\n",
      "Trained batch 737 batch loss 1.25658989 epoch total loss 1.13972735\n",
      "Trained batch 738 batch loss 1.27755642 epoch total loss 1.13991404\n",
      "Trained batch 739 batch loss 1.35591483 epoch total loss 1.14020634\n",
      "Trained batch 740 batch loss 1.14977694 epoch total loss 1.14021933\n",
      "Trained batch 741 batch loss 1.20004416 epoch total loss 1.1403\n",
      "Trained batch 742 batch loss 1.25637054 epoch total loss 1.14045644\n",
      "Trained batch 743 batch loss 1.26413059 epoch total loss 1.14062297\n",
      "Trained batch 744 batch loss 1.15000904 epoch total loss 1.14063561\n",
      "Trained batch 745 batch loss 1.07777584 epoch total loss 1.14055121\n",
      "Trained batch 746 batch loss 1.20057619 epoch total loss 1.14063168\n",
      "Trained batch 747 batch loss 1.15135181 epoch total loss 1.14064598\n",
      "Trained batch 748 batch loss 1.14260292 epoch total loss 1.1406486\n",
      "Trained batch 749 batch loss 1.23827386 epoch total loss 1.1407789\n",
      "Trained batch 750 batch loss 1.24586 epoch total loss 1.14091897\n",
      "Trained batch 751 batch loss 1.24850667 epoch total loss 1.14106238\n",
      "Trained batch 752 batch loss 1.17872477 epoch total loss 1.14111245\n",
      "Trained batch 753 batch loss 1.13275576 epoch total loss 1.14110124\n",
      "Trained batch 754 batch loss 1.09092689 epoch total loss 1.14103472\n",
      "Trained batch 755 batch loss 0.987726092 epoch total loss 1.14083171\n",
      "Trained batch 756 batch loss 1.06369925 epoch total loss 1.14072967\n",
      "Trained batch 757 batch loss 1.18075693 epoch total loss 1.14078259\n",
      "Trained batch 758 batch loss 1.153561 epoch total loss 1.14079952\n",
      "Trained batch 759 batch loss 1.15812075 epoch total loss 1.14082229\n",
      "Trained batch 760 batch loss 1.19446683 epoch total loss 1.14089286\n",
      "Trained batch 761 batch loss 1.22333562 epoch total loss 1.14100122\n",
      "Trained batch 762 batch loss 1.24694598 epoch total loss 1.14114022\n",
      "Trained batch 763 batch loss 1.23251116 epoch total loss 1.14126\n",
      "Trained batch 764 batch loss 1.23989749 epoch total loss 1.14138901\n",
      "Trained batch 765 batch loss 1.30266678 epoch total loss 1.14159989\n",
      "Trained batch 766 batch loss 1.15303135 epoch total loss 1.14161479\n",
      "Trained batch 767 batch loss 1.13396382 epoch total loss 1.14160478\n",
      "Trained batch 768 batch loss 1.1274786 epoch total loss 1.14158642\n",
      "Trained batch 769 batch loss 1.07501447 epoch total loss 1.14149988\n",
      "Trained batch 770 batch loss 1.12412119 epoch total loss 1.14147735\n",
      "Trained batch 771 batch loss 1.08669591 epoch total loss 1.1414063\n",
      "Trained batch 772 batch loss 1.1625334 epoch total loss 1.1414336\n",
      "Trained batch 773 batch loss 1.03972721 epoch total loss 1.14130211\n",
      "Trained batch 774 batch loss 1.00220418 epoch total loss 1.14112234\n",
      "Trained batch 775 batch loss 0.984551728 epoch total loss 1.14092028\n",
      "Trained batch 776 batch loss 1.0955596 epoch total loss 1.14086187\n",
      "Trained batch 777 batch loss 1.10056484 epoch total loss 1.14081\n",
      "Trained batch 778 batch loss 1.17974126 epoch total loss 1.14086008\n",
      "Trained batch 779 batch loss 1.12329733 epoch total loss 1.14083755\n",
      "Trained batch 780 batch loss 1.18021154 epoch total loss 1.14088809\n",
      "Trained batch 781 batch loss 1.22948849 epoch total loss 1.14100158\n",
      "Trained batch 782 batch loss 1.19775724 epoch total loss 1.14107406\n",
      "Trained batch 783 batch loss 1.07372355 epoch total loss 1.14098811\n",
      "Trained batch 784 batch loss 1.18630242 epoch total loss 1.14104581\n",
      "Trained batch 785 batch loss 1.29512525 epoch total loss 1.14124215\n",
      "Trained batch 786 batch loss 1.03136301 epoch total loss 1.14110231\n",
      "Trained batch 787 batch loss 0.961396694 epoch total loss 1.14087403\n",
      "Trained batch 788 batch loss 1.13939989 epoch total loss 1.14087212\n",
      "Trained batch 789 batch loss 1.14084411 epoch total loss 1.14087212\n",
      "Trained batch 790 batch loss 1.27125692 epoch total loss 1.14103723\n",
      "Trained batch 791 batch loss 1.20828331 epoch total loss 1.14112222\n",
      "Trained batch 792 batch loss 1.31773674 epoch total loss 1.14134526\n",
      "Trained batch 793 batch loss 1.3731395 epoch total loss 1.14163756\n",
      "Trained batch 794 batch loss 1.14706731 epoch total loss 1.14164448\n",
      "Trained batch 795 batch loss 1.25344443 epoch total loss 1.14178503\n",
      "Trained batch 796 batch loss 1.22369444 epoch total loss 1.1418879\n",
      "Trained batch 797 batch loss 1.15999794 epoch total loss 1.14191067\n",
      "Trained batch 798 batch loss 1.14403391 epoch total loss 1.14191329\n",
      "Trained batch 799 batch loss 1.28611434 epoch total loss 1.14209378\n",
      "Trained batch 800 batch loss 1.21418846 epoch total loss 1.1421839\n",
      "Trained batch 801 batch loss 1.22140121 epoch total loss 1.14228272\n",
      "Trained batch 802 batch loss 1.2183913 epoch total loss 1.14237761\n",
      "Trained batch 803 batch loss 0.920083702 epoch total loss 1.14210081\n",
      "Trained batch 804 batch loss 1.0131005 epoch total loss 1.14194047\n",
      "Trained batch 805 batch loss 1.14298153 epoch total loss 1.14194179\n",
      "Trained batch 806 batch loss 1.23888755 epoch total loss 1.14206207\n",
      "Trained batch 807 batch loss 1.16636777 epoch total loss 1.14209223\n",
      "Trained batch 808 batch loss 1.29769969 epoch total loss 1.14228475\n",
      "Trained batch 809 batch loss 1.08555973 epoch total loss 1.14221466\n",
      "Trained batch 810 batch loss 1.10801208 epoch total loss 1.14217246\n",
      "Trained batch 811 batch loss 0.989509761 epoch total loss 1.14198422\n",
      "Trained batch 812 batch loss 1.0713172 epoch total loss 1.1418972\n",
      "Trained batch 813 batch loss 1.10902476 epoch total loss 1.14185679\n",
      "Trained batch 814 batch loss 1.12685966 epoch total loss 1.14183831\n",
      "Trained batch 815 batch loss 1.21710241 epoch total loss 1.14193058\n",
      "Trained batch 816 batch loss 1.21304274 epoch total loss 1.14201772\n",
      "Trained batch 817 batch loss 1.20943594 epoch total loss 1.14210021\n",
      "Trained batch 818 batch loss 1.19490254 epoch total loss 1.14216471\n",
      "Trained batch 819 batch loss 1.31096148 epoch total loss 1.14237082\n",
      "Trained batch 820 batch loss 1.3178165 epoch total loss 1.1425848\n",
      "Trained batch 821 batch loss 1.2477479 epoch total loss 1.14271295\n",
      "Trained batch 822 batch loss 1.32274723 epoch total loss 1.14293194\n",
      "Trained batch 823 batch loss 1.29406095 epoch total loss 1.14311552\n",
      "Trained batch 824 batch loss 1.14040291 epoch total loss 1.1431123\n",
      "Trained batch 825 batch loss 1.15659666 epoch total loss 1.14312863\n",
      "Trained batch 826 batch loss 1.15615559 epoch total loss 1.14314437\n",
      "Trained batch 827 batch loss 1.18104649 epoch total loss 1.14319015\n",
      "Trained batch 828 batch loss 1.1815486 epoch total loss 1.14323652\n",
      "Trained batch 829 batch loss 1.06265736 epoch total loss 1.14313924\n",
      "Trained batch 830 batch loss 1.19246316 epoch total loss 1.14319873\n",
      "Trained batch 831 batch loss 1.20805144 epoch total loss 1.14327681\n",
      "Trained batch 832 batch loss 1.31716037 epoch total loss 1.14348578\n",
      "Trained batch 833 batch loss 1.09337234 epoch total loss 1.14342558\n",
      "Trained batch 834 batch loss 1.03628969 epoch total loss 1.1432972\n",
      "Trained batch 835 batch loss 1.03565073 epoch total loss 1.14316821\n",
      "Trained batch 836 batch loss 1.19583797 epoch total loss 1.14323127\n",
      "Trained batch 837 batch loss 1.0363524 epoch total loss 1.1431036\n",
      "Trained batch 838 batch loss 1.06877017 epoch total loss 1.14301491\n",
      "Trained batch 839 batch loss 1.14238691 epoch total loss 1.14301419\n",
      "Trained batch 840 batch loss 1.17663288 epoch total loss 1.14305425\n",
      "Trained batch 841 batch loss 1.23241556 epoch total loss 1.14316046\n",
      "Trained batch 842 batch loss 1.20821667 epoch total loss 1.14323771\n",
      "Trained batch 843 batch loss 1.20116138 epoch total loss 1.14330637\n",
      "Trained batch 844 batch loss 1.29426682 epoch total loss 1.14348531\n",
      "Trained batch 845 batch loss 1.39504409 epoch total loss 1.14378297\n",
      "Trained batch 846 batch loss 1.33048558 epoch total loss 1.14400363\n",
      "Trained batch 847 batch loss 1.1211499 epoch total loss 1.14397669\n",
      "Trained batch 848 batch loss 1.22575653 epoch total loss 1.14407313\n",
      "Trained batch 849 batch loss 1.16554379 epoch total loss 1.1440984\n",
      "Trained batch 850 batch loss 1.23048258 epoch total loss 1.1442\n",
      "Trained batch 851 batch loss 1.24064541 epoch total loss 1.14431334\n",
      "Trained batch 852 batch loss 1.12593436 epoch total loss 1.14429176\n",
      "Trained batch 853 batch loss 1.01760292 epoch total loss 1.14414322\n",
      "Trained batch 854 batch loss 1.02771974 epoch total loss 1.14400685\n",
      "Trained batch 855 batch loss 1.09908938 epoch total loss 1.14395428\n",
      "Trained batch 856 batch loss 0.967950344 epoch total loss 1.14374876\n",
      "Trained batch 857 batch loss 1.03319681 epoch total loss 1.14361966\n",
      "Trained batch 858 batch loss 1.02576387 epoch total loss 1.14348233\n",
      "Trained batch 859 batch loss 1.17204952 epoch total loss 1.14351559\n",
      "Trained batch 860 batch loss 1.0654906 epoch total loss 1.14342487\n",
      "Trained batch 861 batch loss 1.03227353 epoch total loss 1.14329576\n",
      "Trained batch 862 batch loss 1.05953062 epoch total loss 1.14319861\n",
      "Trained batch 863 batch loss 1.04275131 epoch total loss 1.14308214\n",
      "Trained batch 864 batch loss 1.21650696 epoch total loss 1.14316714\n",
      "Trained batch 865 batch loss 1.22974277 epoch total loss 1.14326727\n",
      "Trained batch 866 batch loss 1.21729708 epoch total loss 1.14335275\n",
      "Trained batch 867 batch loss 1.11739898 epoch total loss 1.14332271\n",
      "Trained batch 868 batch loss 1.15808558 epoch total loss 1.14333975\n",
      "Trained batch 869 batch loss 1.01837492 epoch total loss 1.14319587\n",
      "Trained batch 870 batch loss 1.15834 epoch total loss 1.14321327\n",
      "Trained batch 871 batch loss 1.13162303 epoch total loss 1.1432\n",
      "Trained batch 872 batch loss 1.01801956 epoch total loss 1.14305651\n",
      "Trained batch 873 batch loss 1.02301121 epoch total loss 1.14291894\n",
      "Trained batch 874 batch loss 1.115165 epoch total loss 1.14288723\n",
      "Trained batch 875 batch loss 1.10275173 epoch total loss 1.14284134\n",
      "Trained batch 876 batch loss 1.07342672 epoch total loss 1.14276206\n",
      "Trained batch 877 batch loss 1.11303079 epoch total loss 1.14272821\n",
      "Trained batch 878 batch loss 1.0985117 epoch total loss 1.14267778\n",
      "Trained batch 879 batch loss 1.17026615 epoch total loss 1.14270926\n",
      "Trained batch 880 batch loss 1.11863637 epoch total loss 1.14268184\n",
      "Trained batch 881 batch loss 1.13395154 epoch total loss 1.14267194\n",
      "Trained batch 882 batch loss 1.19689035 epoch total loss 1.14273345\n",
      "Trained batch 883 batch loss 1.13142085 epoch total loss 1.1427207\n",
      "Trained batch 884 batch loss 1.08270955 epoch total loss 1.14265275\n",
      "Trained batch 885 batch loss 1.16289485 epoch total loss 1.14267564\n",
      "Trained batch 886 batch loss 1.23295105 epoch total loss 1.14277756\n",
      "Trained batch 887 batch loss 1.07613027 epoch total loss 1.14270234\n",
      "Trained batch 888 batch loss 1.08260834 epoch total loss 1.14263475\n",
      "Trained batch 889 batch loss 1.12827849 epoch total loss 1.14261854\n",
      "Trained batch 890 batch loss 1.22676039 epoch total loss 1.14271307\n",
      "Trained batch 891 batch loss 1.1511023 epoch total loss 1.14272249\n",
      "Trained batch 892 batch loss 1.05428934 epoch total loss 1.14262331\n",
      "Trained batch 893 batch loss 1.13815033 epoch total loss 1.1426183\n",
      "Trained batch 894 batch loss 1.21905899 epoch total loss 1.14270377\n",
      "Trained batch 895 batch loss 1.04999614 epoch total loss 1.14260018\n",
      "Trained batch 896 batch loss 1.06525874 epoch total loss 1.14251387\n",
      "Trained batch 897 batch loss 1.12096119 epoch total loss 1.14248979\n",
      "Trained batch 898 batch loss 0.970910668 epoch total loss 1.14229882\n",
      "Trained batch 899 batch loss 1.14856863 epoch total loss 1.14230573\n",
      "Trained batch 900 batch loss 1.20668852 epoch total loss 1.14237726\n",
      "Trained batch 901 batch loss 1.16581059 epoch total loss 1.14240324\n",
      "Trained batch 902 batch loss 1.16813302 epoch total loss 1.14243174\n",
      "Trained batch 903 batch loss 1.19634449 epoch total loss 1.14249134\n",
      "Trained batch 904 batch loss 1.11674523 epoch total loss 1.14246285\n",
      "Trained batch 905 batch loss 1.10452402 epoch total loss 1.14242089\n",
      "Trained batch 906 batch loss 1.07313871 epoch total loss 1.14234436\n",
      "Trained batch 907 batch loss 1.05520439 epoch total loss 1.14224827\n",
      "Trained batch 908 batch loss 1.15534735 epoch total loss 1.1422627\n",
      "Trained batch 909 batch loss 1.09626102 epoch total loss 1.14221215\n",
      "Trained batch 910 batch loss 1.08053517 epoch total loss 1.14214444\n",
      "Trained batch 911 batch loss 1.154989 epoch total loss 1.14215863\n",
      "Trained batch 912 batch loss 1.17977619 epoch total loss 1.14219987\n",
      "Trained batch 913 batch loss 1.1143012 epoch total loss 1.14216924\n",
      "Trained batch 914 batch loss 1.16524553 epoch total loss 1.14219451\n",
      "Trained batch 915 batch loss 1.1487689 epoch total loss 1.14220178\n",
      "Trained batch 916 batch loss 1.22910166 epoch total loss 1.14229667\n",
      "Trained batch 917 batch loss 1.21763706 epoch total loss 1.14237881\n",
      "Trained batch 918 batch loss 1.10018933 epoch total loss 1.14233291\n",
      "Trained batch 919 batch loss 0.999611378 epoch total loss 1.1421777\n",
      "Trained batch 920 batch loss 1.22416401 epoch total loss 1.14226675\n",
      "Trained batch 921 batch loss 1.26183462 epoch total loss 1.14239657\n",
      "Trained batch 922 batch loss 1.21522284 epoch total loss 1.14247549\n",
      "Trained batch 923 batch loss 1.15444243 epoch total loss 1.14248848\n",
      "Trained batch 924 batch loss 1.14469385 epoch total loss 1.14249074\n",
      "Trained batch 925 batch loss 1.1967895 epoch total loss 1.14254951\n",
      "Trained batch 926 batch loss 1.13020694 epoch total loss 1.14253616\n",
      "Trained batch 927 batch loss 1.20520175 epoch total loss 1.14260375\n",
      "Trained batch 928 batch loss 1.22984529 epoch total loss 1.14269781\n",
      "Trained batch 929 batch loss 1.28892851 epoch total loss 1.14285529\n",
      "Trained batch 930 batch loss 1.19922733 epoch total loss 1.14291584\n",
      "Trained batch 931 batch loss 1.04777586 epoch total loss 1.14281356\n",
      "Trained batch 932 batch loss 1.00743794 epoch total loss 1.14266837\n",
      "Trained batch 933 batch loss 1.147995 epoch total loss 1.14267397\n",
      "Trained batch 934 batch loss 1.23922348 epoch total loss 1.14277744\n",
      "Trained batch 935 batch loss 1.30250812 epoch total loss 1.14294827\n",
      "Trained batch 936 batch loss 1.16044796 epoch total loss 1.14296687\n",
      "Trained batch 937 batch loss 1.25332499 epoch total loss 1.14308465\n",
      "Trained batch 938 batch loss 1.28811836 epoch total loss 1.14323926\n",
      "Trained batch 939 batch loss 1.08903 epoch total loss 1.14318144\n",
      "Trained batch 940 batch loss 1.10710037 epoch total loss 1.14314306\n",
      "Trained batch 941 batch loss 1.03905666 epoch total loss 1.14303243\n",
      "Trained batch 942 batch loss 1.0346148 epoch total loss 1.14291739\n",
      "Trained batch 943 batch loss 0.98894918 epoch total loss 1.14275408\n",
      "Trained batch 944 batch loss 1.02961028 epoch total loss 1.14263427\n",
      "Trained batch 945 batch loss 1.01158285 epoch total loss 1.14249563\n",
      "Trained batch 946 batch loss 0.952457786 epoch total loss 1.14229476\n",
      "Trained batch 947 batch loss 0.881789744 epoch total loss 1.14201975\n",
      "Trained batch 948 batch loss 0.860722303 epoch total loss 1.14172304\n",
      "Trained batch 949 batch loss 1.01512325 epoch total loss 1.14158964\n",
      "Trained batch 950 batch loss 1.03919435 epoch total loss 1.14148176\n",
      "Trained batch 951 batch loss 1.10446572 epoch total loss 1.14144289\n",
      "Trained batch 952 batch loss 1.19293737 epoch total loss 1.14149702\n",
      "Trained batch 953 batch loss 1.02290189 epoch total loss 1.14137268\n",
      "Trained batch 954 batch loss 1.12572312 epoch total loss 1.14135623\n",
      "Trained batch 955 batch loss 1.32980514 epoch total loss 1.14155364\n",
      "Trained batch 956 batch loss 1.1746285 epoch total loss 1.14158833\n",
      "Trained batch 957 batch loss 1.20457506 epoch total loss 1.14165413\n",
      "Trained batch 958 batch loss 1.20883417 epoch total loss 1.14172423\n",
      "Trained batch 959 batch loss 1.25086093 epoch total loss 1.14183807\n",
      "Trained batch 960 batch loss 1.10980606 epoch total loss 1.14180481\n",
      "Trained batch 961 batch loss 1.1636641 epoch total loss 1.14182758\n",
      "Trained batch 962 batch loss 1.11995947 epoch total loss 1.14180481\n",
      "Trained batch 963 batch loss 1.3302865 epoch total loss 1.14200056\n",
      "Trained batch 964 batch loss 1.2562077 epoch total loss 1.14211905\n",
      "Trained batch 965 batch loss 1.27173805 epoch total loss 1.1422534\n",
      "Trained batch 966 batch loss 1.25756359 epoch total loss 1.14237273\n",
      "Trained batch 967 batch loss 1.26659322 epoch total loss 1.14250124\n",
      "Trained batch 968 batch loss 1.05615735 epoch total loss 1.14241207\n",
      "Trained batch 969 batch loss 1.18826163 epoch total loss 1.14245927\n",
      "Trained batch 970 batch loss 1.15244472 epoch total loss 1.14246964\n",
      "Trained batch 971 batch loss 1.23321068 epoch total loss 1.14256299\n",
      "Trained batch 972 batch loss 1.1575681 epoch total loss 1.14257848\n",
      "Trained batch 973 batch loss 1.20123649 epoch total loss 1.1426388\n",
      "Trained batch 974 batch loss 1.23016596 epoch total loss 1.14272881\n",
      "Trained batch 975 batch loss 1.14074397 epoch total loss 1.14272678\n",
      "Trained batch 976 batch loss 1.33655834 epoch total loss 1.14292538\n",
      "Trained batch 977 batch loss 1.21192348 epoch total loss 1.14299595\n",
      "Trained batch 978 batch loss 1.2366457 epoch total loss 1.1430918\n",
      "Trained batch 979 batch loss 1.24794185 epoch total loss 1.14319885\n",
      "Trained batch 980 batch loss 1.12111247 epoch total loss 1.14317632\n",
      "Trained batch 981 batch loss 1.03627753 epoch total loss 1.14306724\n",
      "Trained batch 982 batch loss 1.11531186 epoch total loss 1.14303911\n",
      "Trained batch 983 batch loss 1.13026297 epoch total loss 1.14302599\n",
      "Trained batch 984 batch loss 1.19852281 epoch total loss 1.14308238\n",
      "Trained batch 985 batch loss 1.22769594 epoch total loss 1.14316833\n",
      "Trained batch 986 batch loss 1.16581666 epoch total loss 1.14319122\n",
      "Trained batch 987 batch loss 1.11753154 epoch total loss 1.14316523\n",
      "Trained batch 988 batch loss 1.05167985 epoch total loss 1.14307261\n",
      "Trained batch 989 batch loss 1.09739685 epoch total loss 1.14302647\n",
      "Trained batch 990 batch loss 1.13656199 epoch total loss 1.14301991\n",
      "Trained batch 991 batch loss 1.07931042 epoch total loss 1.14295566\n",
      "Trained batch 992 batch loss 1.15772521 epoch total loss 1.14297056\n",
      "Trained batch 993 batch loss 1.15362692 epoch total loss 1.14298129\n",
      "Trained batch 994 batch loss 1.10973549 epoch total loss 1.14294791\n",
      "Trained batch 995 batch loss 1.19585812 epoch total loss 1.14300096\n",
      "Trained batch 996 batch loss 1.05880094 epoch total loss 1.14291656\n",
      "Trained batch 997 batch loss 1.00627422 epoch total loss 1.14277947\n",
      "Trained batch 998 batch loss 1.12461829 epoch total loss 1.14276123\n",
      "Trained batch 999 batch loss 1.16602445 epoch total loss 1.14278448\n",
      "Trained batch 1000 batch loss 1.04461741 epoch total loss 1.14268637\n",
      "Trained batch 1001 batch loss 1.20066977 epoch total loss 1.1427443\n",
      "Trained batch 1002 batch loss 1.3097235 epoch total loss 1.14291096\n",
      "Trained batch 1003 batch loss 1.01189494 epoch total loss 1.1427803\n",
      "Trained batch 1004 batch loss 1.28725946 epoch total loss 1.14292419\n",
      "Trained batch 1005 batch loss 1.27249992 epoch total loss 1.14305305\n",
      "Trained batch 1006 batch loss 1.32700074 epoch total loss 1.14323592\n",
      "Trained batch 1007 batch loss 1.0621202 epoch total loss 1.14315534\n",
      "Trained batch 1008 batch loss 0.924597383 epoch total loss 1.14293849\n",
      "Trained batch 1009 batch loss 0.949172556 epoch total loss 1.14274657\n",
      "Trained batch 1010 batch loss 0.982420921 epoch total loss 1.14258778\n",
      "Trained batch 1011 batch loss 0.998632491 epoch total loss 1.14244545\n",
      "Trained batch 1012 batch loss 0.940796912 epoch total loss 1.14224613\n",
      "Trained batch 1013 batch loss 1.06657243 epoch total loss 1.14217138\n",
      "Trained batch 1014 batch loss 1.03950644 epoch total loss 1.14207017\n",
      "Trained batch 1015 batch loss 1.10307753 epoch total loss 1.14203179\n",
      "Trained batch 1016 batch loss 1.08476543 epoch total loss 1.14197528\n",
      "Trained batch 1017 batch loss 1.14563346 epoch total loss 1.14197898\n",
      "Trained batch 1018 batch loss 1.27255225 epoch total loss 1.14210725\n",
      "Trained batch 1019 batch loss 1.15054107 epoch total loss 1.14211547\n",
      "Trained batch 1020 batch loss 1.11030841 epoch total loss 1.14208436\n",
      "Trained batch 1021 batch loss 1.40003 epoch total loss 1.14233696\n",
      "Trained batch 1022 batch loss 1.36654735 epoch total loss 1.14255643\n",
      "Trained batch 1023 batch loss 1.33897436 epoch total loss 1.14274836\n",
      "Trained batch 1024 batch loss 1.30633664 epoch total loss 1.14290822\n",
      "Trained batch 1025 batch loss 1.34983349 epoch total loss 1.14311016\n",
      "Trained batch 1026 batch loss 1.22796011 epoch total loss 1.14319277\n",
      "Trained batch 1027 batch loss 1.13268554 epoch total loss 1.14318252\n",
      "Trained batch 1028 batch loss 1.24378908 epoch total loss 1.14328039\n",
      "Trained batch 1029 batch loss 1.22859764 epoch total loss 1.14336336\n",
      "Trained batch 1030 batch loss 1.11165583 epoch total loss 1.1433326\n",
      "Trained batch 1031 batch loss 1.17388856 epoch total loss 1.14336216\n",
      "Trained batch 1032 batch loss 1.15682983 epoch total loss 1.14337528\n",
      "Trained batch 1033 batch loss 1.12809086 epoch total loss 1.14336038\n",
      "Trained batch 1034 batch loss 1.14798367 epoch total loss 1.14336491\n",
      "Trained batch 1035 batch loss 0.969684124 epoch total loss 1.14319706\n",
      "Trained batch 1036 batch loss 0.979501 epoch total loss 1.14303911\n",
      "Trained batch 1037 batch loss 1.0787915 epoch total loss 1.14297712\n",
      "Trained batch 1038 batch loss 1.08368027 epoch total loss 1.14292\n",
      "Trained batch 1039 batch loss 1.34914839 epoch total loss 1.1431185\n",
      "Trained batch 1040 batch loss 1.27700591 epoch total loss 1.14324713\n",
      "Trained batch 1041 batch loss 1.21661329 epoch total loss 1.14331758\n",
      "Trained batch 1042 batch loss 1.23094368 epoch total loss 1.14340174\n",
      "Trained batch 1043 batch loss 1.14992666 epoch total loss 1.14340794\n",
      "Trained batch 1044 batch loss 1.14529657 epoch total loss 1.14340973\n",
      "Trained batch 1045 batch loss 1.05598927 epoch total loss 1.14332604\n",
      "Trained batch 1046 batch loss 1.10686612 epoch total loss 1.14329112\n",
      "Trained batch 1047 batch loss 1.16566706 epoch total loss 1.14331257\n",
      "Trained batch 1048 batch loss 1.1000104 epoch total loss 1.14327121\n",
      "Trained batch 1049 batch loss 1.05075741 epoch total loss 1.14318299\n",
      "Trained batch 1050 batch loss 1.09690988 epoch total loss 1.143139\n",
      "Trained batch 1051 batch loss 1.10500836 epoch total loss 1.14310265\n",
      "Trained batch 1052 batch loss 1.08370161 epoch total loss 1.14304626\n",
      "Trained batch 1053 batch loss 1.07792783 epoch total loss 1.14298427\n",
      "Trained batch 1054 batch loss 1.06736135 epoch total loss 1.14291263\n",
      "Trained batch 1055 batch loss 1.09174037 epoch total loss 1.14286411\n",
      "Trained batch 1056 batch loss 1.17613125 epoch total loss 1.1428957\n",
      "Trained batch 1057 batch loss 1.11006892 epoch total loss 1.1428647\n",
      "Trained batch 1058 batch loss 1.24130213 epoch total loss 1.14295769\n",
      "Trained batch 1059 batch loss 1.1552012 epoch total loss 1.14296925\n",
      "Trained batch 1060 batch loss 1.26335 epoch total loss 1.14308274\n",
      "Trained batch 1061 batch loss 1.15565896 epoch total loss 1.14309454\n",
      "Trained batch 1062 batch loss 1.24063849 epoch total loss 1.14318645\n",
      "Trained batch 1063 batch loss 1.15315604 epoch total loss 1.14319587\n",
      "Trained batch 1064 batch loss 1.24768686 epoch total loss 1.14329398\n",
      "Trained batch 1065 batch loss 1.23036575 epoch total loss 1.14337575\n",
      "Trained batch 1066 batch loss 1.13497484 epoch total loss 1.14336789\n",
      "Trained batch 1067 batch loss 1.29629111 epoch total loss 1.14351118\n",
      "Trained batch 1068 batch loss 1.37437057 epoch total loss 1.14372742\n",
      "Trained batch 1069 batch loss 1.3430618 epoch total loss 1.14391387\n",
      "Trained batch 1070 batch loss 1.18021774 epoch total loss 1.14394772\n",
      "Trained batch 1071 batch loss 1.16514671 epoch total loss 1.14396751\n",
      "Trained batch 1072 batch loss 0.947188616 epoch total loss 1.14378393\n",
      "Trained batch 1073 batch loss 0.890207052 epoch total loss 1.14354765\n",
      "Trained batch 1074 batch loss 1.10125732 epoch total loss 1.14350832\n",
      "Trained batch 1075 batch loss 1.02976012 epoch total loss 1.14340258\n",
      "Trained batch 1076 batch loss 0.884686112 epoch total loss 1.14316201\n",
      "Trained batch 1077 batch loss 0.918579 epoch total loss 1.14295352\n",
      "Trained batch 1078 batch loss 0.93690604 epoch total loss 1.1427623\n",
      "Trained batch 1079 batch loss 1.02554274 epoch total loss 1.1426537\n",
      "Trained batch 1080 batch loss 1.06501412 epoch total loss 1.14258182\n",
      "Trained batch 1081 batch loss 1.21503234 epoch total loss 1.14264894\n",
      "Trained batch 1082 batch loss 1.0939219 epoch total loss 1.14260387\n",
      "Trained batch 1083 batch loss 1.16834068 epoch total loss 1.1426276\n",
      "Trained batch 1084 batch loss 1.09445739 epoch total loss 1.14258325\n",
      "Trained batch 1085 batch loss 1.01044405 epoch total loss 1.14246142\n",
      "Trained batch 1086 batch loss 0.936874688 epoch total loss 1.14227211\n",
      "Trained batch 1087 batch loss 1.12774467 epoch total loss 1.14225876\n",
      "Trained batch 1088 batch loss 1.30410171 epoch total loss 1.14240742\n",
      "Trained batch 1089 batch loss 1.20610714 epoch total loss 1.14246595\n",
      "Trained batch 1090 batch loss 1.17353785 epoch total loss 1.14249444\n",
      "Trained batch 1091 batch loss 1.34553134 epoch total loss 1.14268064\n",
      "Trained batch 1092 batch loss 1.22558451 epoch total loss 1.14275658\n",
      "Trained batch 1093 batch loss 1.16715419 epoch total loss 1.14277887\n",
      "Trained batch 1094 batch loss 1.23090541 epoch total loss 1.14285946\n",
      "Trained batch 1095 batch loss 1.26882172 epoch total loss 1.14297438\n",
      "Trained batch 1096 batch loss 1.21529555 epoch total loss 1.14304042\n",
      "Trained batch 1097 batch loss 1.0695591 epoch total loss 1.14297354\n",
      "Trained batch 1098 batch loss 1.06015921 epoch total loss 1.14289808\n",
      "Trained batch 1099 batch loss 0.880360425 epoch total loss 1.14265919\n",
      "Trained batch 1100 batch loss 1.0522995 epoch total loss 1.14257705\n",
      "Trained batch 1101 batch loss 1.083601 epoch total loss 1.14252341\n",
      "Trained batch 1102 batch loss 1.14760768 epoch total loss 1.14252806\n",
      "Trained batch 1103 batch loss 1.13010252 epoch total loss 1.14251685\n",
      "Trained batch 1104 batch loss 1.29690146 epoch total loss 1.14265668\n",
      "Trained batch 1105 batch loss 1.18182969 epoch total loss 1.14269209\n",
      "Trained batch 1106 batch loss 1.20225048 epoch total loss 1.14274597\n",
      "Trained batch 1107 batch loss 1.26229358 epoch total loss 1.14285398\n",
      "Trained batch 1108 batch loss 1.16371334 epoch total loss 1.14287281\n",
      "Trained batch 1109 batch loss 1.18218207 epoch total loss 1.14290822\n",
      "Trained batch 1110 batch loss 1.24045551 epoch total loss 1.14299619\n",
      "Trained batch 1111 batch loss 1.17304504 epoch total loss 1.14302325\n",
      "Trained batch 1112 batch loss 1.18795443 epoch total loss 1.14306366\n",
      "Trained batch 1113 batch loss 1.07962739 epoch total loss 1.14300668\n",
      "Trained batch 1114 batch loss 1.19997048 epoch total loss 1.1430577\n",
      "Trained batch 1115 batch loss 1.20374274 epoch total loss 1.14311218\n",
      "Trained batch 1116 batch loss 1.12496364 epoch total loss 1.14309597\n",
      "Trained batch 1117 batch loss 1.20627379 epoch total loss 1.14315248\n",
      "Trained batch 1118 batch loss 1.18965602 epoch total loss 1.1431942\n",
      "Trained batch 1119 batch loss 1.24891734 epoch total loss 1.14328861\n",
      "Trained batch 1120 batch loss 1.08240783 epoch total loss 1.14323425\n",
      "Trained batch 1121 batch loss 1.01074278 epoch total loss 1.14311612\n",
      "Trained batch 1122 batch loss 0.922486305 epoch total loss 1.14291942\n",
      "Trained batch 1123 batch loss 0.99348563 epoch total loss 1.14278638\n",
      "Trained batch 1124 batch loss 0.985228479 epoch total loss 1.14264619\n",
      "Trained batch 1125 batch loss 1.09801257 epoch total loss 1.1426065\n",
      "Trained batch 1126 batch loss 0.987676442 epoch total loss 1.14246893\n",
      "Trained batch 1127 batch loss 0.961607039 epoch total loss 1.14230847\n",
      "Trained batch 1128 batch loss 1.12321258 epoch total loss 1.14229143\n",
      "Trained batch 1129 batch loss 1.0716114 epoch total loss 1.14222884\n",
      "Trained batch 1130 batch loss 1.29677606 epoch total loss 1.14236569\n",
      "Trained batch 1131 batch loss 1.15498567 epoch total loss 1.14237678\n",
      "Trained batch 1132 batch loss 1.35085607 epoch total loss 1.14256096\n",
      "Trained batch 1133 batch loss 1.30352342 epoch total loss 1.14270294\n",
      "Trained batch 1134 batch loss 1.32086134 epoch total loss 1.14286\n",
      "Trained batch 1135 batch loss 1.48139524 epoch total loss 1.14315832\n",
      "Trained batch 1136 batch loss 1.10543942 epoch total loss 1.14312518\n",
      "Trained batch 1137 batch loss 1.07227302 epoch total loss 1.14306283\n",
      "Trained batch 1138 batch loss 1.12866902 epoch total loss 1.14305019\n",
      "Trained batch 1139 batch loss 1.00279176 epoch total loss 1.14292705\n",
      "Trained batch 1140 batch loss 0.977334499 epoch total loss 1.14278173\n",
      "Trained batch 1141 batch loss 0.971820951 epoch total loss 1.14263189\n",
      "Trained batch 1142 batch loss 1.088552 epoch total loss 1.14258456\n",
      "Trained batch 1143 batch loss 0.933708131 epoch total loss 1.14240181\n",
      "Trained batch 1144 batch loss 0.960088491 epoch total loss 1.14224243\n",
      "Trained batch 1145 batch loss 0.893767118 epoch total loss 1.14202547\n",
      "Trained batch 1146 batch loss 1.22879314 epoch total loss 1.14210117\n",
      "Trained batch 1147 batch loss 1.03506899 epoch total loss 1.14200783\n",
      "Trained batch 1148 batch loss 0.87667346 epoch total loss 1.14177668\n",
      "Trained batch 1149 batch loss 1.05362225 epoch total loss 1.14169991\n",
      "Trained batch 1150 batch loss 1.11248136 epoch total loss 1.14167452\n",
      "Trained batch 1151 batch loss 1.19392598 epoch total loss 1.14171994\n",
      "Trained batch 1152 batch loss 1.17938244 epoch total loss 1.14175272\n",
      "Trained batch 1153 batch loss 0.95475316 epoch total loss 1.14159048\n",
      "Trained batch 1154 batch loss 1.05183685 epoch total loss 1.14151275\n",
      "Trained batch 1155 batch loss 1.0308435 epoch total loss 1.14141691\n",
      "Trained batch 1156 batch loss 1.12179017 epoch total loss 1.1414\n",
      "Trained batch 1157 batch loss 1.00285971 epoch total loss 1.14128017\n",
      "Trained batch 1158 batch loss 1.0769155 epoch total loss 1.14122462\n",
      "Trained batch 1159 batch loss 1.22414505 epoch total loss 1.14129615\n",
      "Trained batch 1160 batch loss 1.15998602 epoch total loss 1.14131224\n",
      "Trained batch 1161 batch loss 1.10524607 epoch total loss 1.14128125\n",
      "Trained batch 1162 batch loss 1.24121237 epoch total loss 1.1413672\n",
      "Trained batch 1163 batch loss 1.3518244 epoch total loss 1.14154816\n",
      "Trained batch 1164 batch loss 1.38976884 epoch total loss 1.14176142\n",
      "Trained batch 1165 batch loss 1.33645833 epoch total loss 1.14192843\n",
      "Trained batch 1166 batch loss 1.2044003 epoch total loss 1.14198196\n",
      "Trained batch 1167 batch loss 1.04937339 epoch total loss 1.14190257\n",
      "Trained batch 1168 batch loss 1.26283574 epoch total loss 1.14200616\n",
      "Trained batch 1169 batch loss 1.31209207 epoch total loss 1.14215171\n",
      "Trained batch 1170 batch loss 1.19263625 epoch total loss 1.14219475\n",
      "Trained batch 1171 batch loss 1.26449144 epoch total loss 1.14229929\n",
      "Trained batch 1172 batch loss 1.21485662 epoch total loss 1.14236116\n",
      "Trained batch 1173 batch loss 1.20273066 epoch total loss 1.14241266\n",
      "Trained batch 1174 batch loss 1.14593458 epoch total loss 1.14241564\n",
      "Trained batch 1175 batch loss 1.17435026 epoch total loss 1.1424427\n",
      "Trained batch 1176 batch loss 1.32461667 epoch total loss 1.14259768\n",
      "Trained batch 1177 batch loss 1.22172654 epoch total loss 1.14266479\n",
      "Trained batch 1178 batch loss 1.11017954 epoch total loss 1.14263725\n",
      "Trained batch 1179 batch loss 1.34420502 epoch total loss 1.14280832\n",
      "Trained batch 1180 batch loss 1.22160435 epoch total loss 1.14287508\n",
      "Trained batch 1181 batch loss 1.22283828 epoch total loss 1.14294267\n",
      "Trained batch 1182 batch loss 1.20900214 epoch total loss 1.14299858\n",
      "Trained batch 1183 batch loss 1.14925 epoch total loss 1.14300382\n",
      "Trained batch 1184 batch loss 1.10263813 epoch total loss 1.14296985\n",
      "Trained batch 1185 batch loss 1.10659468 epoch total loss 1.14293909\n",
      "Trained batch 1186 batch loss 1.21015728 epoch total loss 1.14299583\n",
      "Trained batch 1187 batch loss 1.14769185 epoch total loss 1.14299977\n",
      "Trained batch 1188 batch loss 1.17405844 epoch total loss 1.14302588\n",
      "Trained batch 1189 batch loss 1.07938409 epoch total loss 1.14297235\n",
      "Trained batch 1190 batch loss 1.1371448 epoch total loss 1.14296746\n",
      "Trained batch 1191 batch loss 1.20111978 epoch total loss 1.14301634\n",
      "Trained batch 1192 batch loss 1.10953546 epoch total loss 1.1429882\n",
      "Trained batch 1193 batch loss 1.16631126 epoch total loss 1.14300764\n",
      "Trained batch 1194 batch loss 1.18497705 epoch total loss 1.1430428\n",
      "Trained batch 1195 batch loss 1.16914511 epoch total loss 1.14306462\n",
      "Trained batch 1196 batch loss 0.943215311 epoch total loss 1.14289761\n",
      "Trained batch 1197 batch loss 0.974108 epoch total loss 1.14275658\n",
      "Trained batch 1198 batch loss 1.00965309 epoch total loss 1.14264548\n",
      "Trained batch 1199 batch loss 1.2921741 epoch total loss 1.14277017\n",
      "Trained batch 1200 batch loss 1.30645752 epoch total loss 1.14290655\n",
      "Trained batch 1201 batch loss 1.18486261 epoch total loss 1.14294136\n",
      "Trained batch 1202 batch loss 1.02271688 epoch total loss 1.14284134\n",
      "Trained batch 1203 batch loss 1.13233542 epoch total loss 1.14283264\n",
      "Trained batch 1204 batch loss 1.07222712 epoch total loss 1.14277399\n",
      "Trained batch 1205 batch loss 0.989055514 epoch total loss 1.14264643\n",
      "Trained batch 1206 batch loss 1.13291216 epoch total loss 1.14263833\n",
      "Trained batch 1207 batch loss 1.0977267 epoch total loss 1.14260113\n",
      "Trained batch 1208 batch loss 1.08227372 epoch total loss 1.1425513\n",
      "Trained batch 1209 batch loss 1.11603105 epoch total loss 1.14252937\n",
      "Trained batch 1210 batch loss 1.19249964 epoch total loss 1.14257061\n",
      "Trained batch 1211 batch loss 1.11207366 epoch total loss 1.14254546\n",
      "Trained batch 1212 batch loss 1.10019302 epoch total loss 1.14251053\n",
      "Trained batch 1213 batch loss 1.14257932 epoch total loss 1.14251065\n",
      "Trained batch 1214 batch loss 1.12453008 epoch total loss 1.14249575\n",
      "Trained batch 1215 batch loss 1.25768 epoch total loss 1.14259064\n",
      "Trained batch 1216 batch loss 1.26989615 epoch total loss 1.14269531\n",
      "Trained batch 1217 batch loss 1.22291672 epoch total loss 1.14276123\n",
      "Trained batch 1218 batch loss 1.25426602 epoch total loss 1.14285278\n",
      "Trained batch 1219 batch loss 1.11173046 epoch total loss 1.14282715\n",
      "Trained batch 1220 batch loss 1.11054862 epoch total loss 1.14280081\n",
      "Trained batch 1221 batch loss 1.13414168 epoch total loss 1.14279366\n",
      "Trained batch 1222 batch loss 1.09550714 epoch total loss 1.14275491\n",
      "Trained batch 1223 batch loss 1.06524181 epoch total loss 1.14269149\n",
      "Trained batch 1224 batch loss 1.26072133 epoch total loss 1.14278793\n",
      "Trained batch 1225 batch loss 1.18402314 epoch total loss 1.14282167\n",
      "Trained batch 1226 batch loss 1.26285863 epoch total loss 1.14291954\n",
      "Trained batch 1227 batch loss 1.13041699 epoch total loss 1.14290929\n",
      "Trained batch 1228 batch loss 1.12443805 epoch total loss 1.14289427\n",
      "Trained batch 1229 batch loss 0.989936233 epoch total loss 1.14276981\n",
      "Trained batch 1230 batch loss 1.09420371 epoch total loss 1.14273036\n",
      "Trained batch 1231 batch loss 1.17627358 epoch total loss 1.14275765\n",
      "Trained batch 1232 batch loss 1.02807593 epoch total loss 1.14266455\n",
      "Trained batch 1233 batch loss 0.995329738 epoch total loss 1.1425451\n",
      "Trained batch 1234 batch loss 0.988038063 epoch total loss 1.14241982\n",
      "Trained batch 1235 batch loss 1.09745383 epoch total loss 1.14238346\n",
      "Trained batch 1236 batch loss 1.24560261 epoch total loss 1.1424669\n",
      "Trained batch 1237 batch loss 1.40567064 epoch total loss 1.14267969\n",
      "Trained batch 1238 batch loss 1.23812246 epoch total loss 1.14275682\n",
      "Trained batch 1239 batch loss 1.25924623 epoch total loss 1.14285088\n",
      "Trained batch 1240 batch loss 1.16879642 epoch total loss 1.14287174\n",
      "Trained batch 1241 batch loss 1.05644631 epoch total loss 1.14280212\n",
      "Trained batch 1242 batch loss 1.21806288 epoch total loss 1.14286268\n",
      "Trained batch 1243 batch loss 1.08357823 epoch total loss 1.14281499\n",
      "Trained batch 1244 batch loss 1.018929 epoch total loss 1.14271545\n",
      "Trained batch 1245 batch loss 0.956313491 epoch total loss 1.14256573\n",
      "Trained batch 1246 batch loss 0.975557327 epoch total loss 1.14243162\n",
      "Trained batch 1247 batch loss 1.08227551 epoch total loss 1.14238346\n",
      "Trained batch 1248 batch loss 0.976004481 epoch total loss 1.14225006\n",
      "Trained batch 1249 batch loss 0.96033144 epoch total loss 1.14210439\n",
      "Trained batch 1250 batch loss 1.12065649 epoch total loss 1.14208722\n",
      "Trained batch 1251 batch loss 1.17107117 epoch total loss 1.14211035\n",
      "Trained batch 1252 batch loss 1.10671568 epoch total loss 1.1420821\n",
      "Trained batch 1253 batch loss 1.05781555 epoch total loss 1.14201486\n",
      "Trained batch 1254 batch loss 1.06469405 epoch total loss 1.14195323\n",
      "Trained batch 1255 batch loss 1.26159871 epoch total loss 1.14204848\n",
      "Trained batch 1256 batch loss 1.18095207 epoch total loss 1.14207947\n",
      "Trained batch 1257 batch loss 1.17562127 epoch total loss 1.14210618\n",
      "Trained batch 1258 batch loss 1.02003956 epoch total loss 1.14200914\n",
      "Trained batch 1259 batch loss 1.26280546 epoch total loss 1.1421051\n",
      "Trained batch 1260 batch loss 1.2185266 epoch total loss 1.14216566\n",
      "Trained batch 1261 batch loss 1.26864791 epoch total loss 1.14226604\n",
      "Trained batch 1262 batch loss 1.20323098 epoch total loss 1.14231431\n",
      "Trained batch 1263 batch loss 1.2101903 epoch total loss 1.14236808\n",
      "Trained batch 1264 batch loss 1.18672121 epoch total loss 1.14240324\n",
      "Trained batch 1265 batch loss 1.29819524 epoch total loss 1.14252639\n",
      "Trained batch 1266 batch loss 1.22582674 epoch total loss 1.14259219\n",
      "Trained batch 1267 batch loss 1.1905688 epoch total loss 1.1426301\n",
      "Trained batch 1268 batch loss 1.18534124 epoch total loss 1.14266372\n",
      "Trained batch 1269 batch loss 1.15888834 epoch total loss 1.14267647\n",
      "Trained batch 1270 batch loss 1.28888297 epoch total loss 1.14279175\n",
      "Trained batch 1271 batch loss 1.22584629 epoch total loss 1.14285707\n",
      "Trained batch 1272 batch loss 1.19744086 epoch total loss 1.14289987\n",
      "Trained batch 1273 batch loss 1.10777473 epoch total loss 1.14287233\n",
      "Trained batch 1274 batch loss 0.984063566 epoch total loss 1.14274764\n",
      "Trained batch 1275 batch loss 0.957613647 epoch total loss 1.14260244\n",
      "Trained batch 1276 batch loss 1.10536408 epoch total loss 1.14257324\n",
      "Trained batch 1277 batch loss 1.09106064 epoch total loss 1.14253294\n",
      "Trained batch 1278 batch loss 1.22103012 epoch total loss 1.14259434\n",
      "Trained batch 1279 batch loss 1.21383643 epoch total loss 1.14265013\n",
      "Trained batch 1280 batch loss 1.20355964 epoch total loss 1.14269769\n",
      "Trained batch 1281 batch loss 1.16397119 epoch total loss 1.14271426\n",
      "Trained batch 1282 batch loss 1.41431725 epoch total loss 1.1429261\n",
      "Trained batch 1283 batch loss 1.19930935 epoch total loss 1.14297009\n",
      "Trained batch 1284 batch loss 1.07391214 epoch total loss 1.14291632\n",
      "Trained batch 1285 batch loss 1.01848137 epoch total loss 1.1428194\n",
      "Trained batch 1286 batch loss 1.08228219 epoch total loss 1.14277232\n",
      "Trained batch 1287 batch loss 1.08198726 epoch total loss 1.14272511\n",
      "Trained batch 1288 batch loss 1.20724642 epoch total loss 1.1427753\n",
      "Trained batch 1289 batch loss 1.23739886 epoch total loss 1.14284873\n",
      "Trained batch 1290 batch loss 1.14580977 epoch total loss 1.142851\n",
      "Trained batch 1291 batch loss 1.09764385 epoch total loss 1.14281595\n",
      "Trained batch 1292 batch loss 1.20659041 epoch total loss 1.1428653\n",
      "Trained batch 1293 batch loss 1.06942606 epoch total loss 1.14280844\n",
      "Trained batch 1294 batch loss 1.1167047 epoch total loss 1.14278829\n",
      "Trained batch 1295 batch loss 1.20822692 epoch total loss 1.14283884\n",
      "Trained batch 1296 batch loss 1.19950056 epoch total loss 1.14288259\n",
      "Trained batch 1297 batch loss 1.12516105 epoch total loss 1.14286888\n",
      "Trained batch 1298 batch loss 1.02888787 epoch total loss 1.14278102\n",
      "Trained batch 1299 batch loss 1.12273586 epoch total loss 1.14276564\n",
      "Trained batch 1300 batch loss 0.978478193 epoch total loss 1.14263928\n",
      "Trained batch 1301 batch loss 1.11011958 epoch total loss 1.14261425\n",
      "Trained batch 1302 batch loss 0.975219846 epoch total loss 1.14248574\n",
      "Trained batch 1303 batch loss 0.922337532 epoch total loss 1.1423167\n",
      "Trained batch 1304 batch loss 0.996403515 epoch total loss 1.14220488\n",
      "Trained batch 1305 batch loss 1.20293498 epoch total loss 1.14225137\n",
      "Trained batch 1306 batch loss 1.37351727 epoch total loss 1.14242852\n",
      "Trained batch 1307 batch loss 1.39238119 epoch total loss 1.14261973\n",
      "Trained batch 1308 batch loss 1.18668056 epoch total loss 1.14265335\n",
      "Trained batch 1309 batch loss 1.05278516 epoch total loss 1.14258468\n",
      "Trained batch 1310 batch loss 1.25455046 epoch total loss 1.14267015\n",
      "Trained batch 1311 batch loss 1.17091191 epoch total loss 1.14269161\n",
      "Trained batch 1312 batch loss 1.18138885 epoch total loss 1.14272118\n",
      "Trained batch 1313 batch loss 1.18231928 epoch total loss 1.14275134\n",
      "Trained batch 1314 batch loss 1.21152699 epoch total loss 1.14280367\n",
      "Trained batch 1315 batch loss 1.26293468 epoch total loss 1.1428951\n",
      "Trained batch 1316 batch loss 1.00768363 epoch total loss 1.14279234\n",
      "Trained batch 1317 batch loss 1.10754538 epoch total loss 1.14276552\n",
      "Trained batch 1318 batch loss 1.08564007 epoch total loss 1.14272225\n",
      "Trained batch 1319 batch loss 1.0726397 epoch total loss 1.14266908\n",
      "Trained batch 1320 batch loss 1.1344806 epoch total loss 1.14266288\n",
      "Trained batch 1321 batch loss 1.14626968 epoch total loss 1.14266562\n",
      "Trained batch 1322 batch loss 0.919138193 epoch total loss 1.14249659\n",
      "Trained batch 1323 batch loss 1.04857707 epoch total loss 1.14242566\n",
      "Trained batch 1324 batch loss 1.16321158 epoch total loss 1.14244127\n",
      "Trained batch 1325 batch loss 1.26565599 epoch total loss 1.14253426\n",
      "Trained batch 1326 batch loss 1.33904076 epoch total loss 1.14268243\n",
      "Trained batch 1327 batch loss 1.12900484 epoch total loss 1.14267218\n",
      "Trained batch 1328 batch loss 1.240062 epoch total loss 1.14274549\n",
      "Trained batch 1329 batch loss 1.12555957 epoch total loss 1.14273262\n",
      "Trained batch 1330 batch loss 1.18293571 epoch total loss 1.1427629\n",
      "Trained batch 1331 batch loss 1.08595943 epoch total loss 1.14272022\n",
      "Trained batch 1332 batch loss 1.10198021 epoch total loss 1.14268959\n",
      "Trained batch 1333 batch loss 1.12052369 epoch total loss 1.1426729\n",
      "Trained batch 1334 batch loss 1.05405569 epoch total loss 1.1426065\n",
      "Trained batch 1335 batch loss 1.0663538 epoch total loss 1.1425494\n",
      "Trained batch 1336 batch loss 0.958615184 epoch total loss 1.14241171\n",
      "Trained batch 1337 batch loss 0.967087388 epoch total loss 1.14228058\n",
      "Trained batch 1338 batch loss 0.931629837 epoch total loss 1.1421231\n",
      "Trained batch 1339 batch loss 1.26901269 epoch total loss 1.14221787\n",
      "Trained batch 1340 batch loss 1.43463135 epoch total loss 1.14243615\n",
      "Trained batch 1341 batch loss 1.44362402 epoch total loss 1.14266074\n",
      "Trained batch 1342 batch loss 1.26529682 epoch total loss 1.14275217\n",
      "Trained batch 1343 batch loss 1.18746924 epoch total loss 1.14278543\n",
      "Trained batch 1344 batch loss 1.35329938 epoch total loss 1.14294207\n",
      "Trained batch 1345 batch loss 1.17198956 epoch total loss 1.14296365\n",
      "Trained batch 1346 batch loss 1.133968 epoch total loss 1.14295697\n",
      "Trained batch 1347 batch loss 1.18853068 epoch total loss 1.14299071\n",
      "Trained batch 1348 batch loss 1.14621484 epoch total loss 1.14299309\n",
      "Trained batch 1349 batch loss 1.28297424 epoch total loss 1.14309692\n",
      "Trained batch 1350 batch loss 1.16563463 epoch total loss 1.14311361\n",
      "Trained batch 1351 batch loss 1.17410481 epoch total loss 1.1431365\n",
      "Trained batch 1352 batch loss 1.22730064 epoch total loss 1.14319873\n",
      "Trained batch 1353 batch loss 1.2502588 epoch total loss 1.14327788\n",
      "Trained batch 1354 batch loss 1.18861389 epoch total loss 1.14331138\n",
      "Trained batch 1355 batch loss 1.29370928 epoch total loss 1.14342237\n",
      "Trained batch 1356 batch loss 1.23326266 epoch total loss 1.14348865\n",
      "Trained batch 1357 batch loss 1.28087759 epoch total loss 1.14358985\n",
      "Trained batch 1358 batch loss 1.25478506 epoch total loss 1.14367175\n",
      "Trained batch 1359 batch loss 1.16620612 epoch total loss 1.14368832\n",
      "Trained batch 1360 batch loss 1.11803162 epoch total loss 1.14366949\n",
      "Trained batch 1361 batch loss 1.21767282 epoch total loss 1.14372385\n",
      "Trained batch 1362 batch loss 1.20364499 epoch total loss 1.14376783\n",
      "Trained batch 1363 batch loss 1.14576638 epoch total loss 1.14376926\n",
      "Trained batch 1364 batch loss 1.11568141 epoch total loss 1.14374876\n",
      "Trained batch 1365 batch loss 1.1171298 epoch total loss 1.14372921\n",
      "Trained batch 1366 batch loss 1.00556111 epoch total loss 1.14362812\n",
      "Trained batch 1367 batch loss 0.923288286 epoch total loss 1.14346695\n",
      "Trained batch 1368 batch loss 1.1490736 epoch total loss 1.14347112\n",
      "Trained batch 1369 batch loss 1.12234926 epoch total loss 1.14345562\n",
      "Trained batch 1370 batch loss 1.01027453 epoch total loss 1.14335835\n",
      "Trained batch 1371 batch loss 1.22731364 epoch total loss 1.14341962\n",
      "Trained batch 1372 batch loss 1.17378211 epoch total loss 1.1434418\n",
      "Trained batch 1373 batch loss 1.09931469 epoch total loss 1.14340973\n",
      "Trained batch 1374 batch loss 1.17135727 epoch total loss 1.14343\n",
      "Trained batch 1375 batch loss 1.25349879 epoch total loss 1.1435101\n",
      "Trained batch 1376 batch loss 1.14439297 epoch total loss 1.14351082\n",
      "Trained batch 1377 batch loss 1.26673651 epoch total loss 1.14360023\n",
      "Trained batch 1378 batch loss 1.19122779 epoch total loss 1.14363492\n",
      "Trained batch 1379 batch loss 1.14960456 epoch total loss 1.14363921\n",
      "Trained batch 1380 batch loss 1.06953418 epoch total loss 1.14358556\n",
      "Trained batch 1381 batch loss 1.03847468 epoch total loss 1.14350939\n",
      "Trained batch 1382 batch loss 1.17445433 epoch total loss 1.1435318\n",
      "Trained batch 1383 batch loss 1.28666043 epoch total loss 1.14363527\n",
      "Trained batch 1384 batch loss 1.24556363 epoch total loss 1.14370894\n",
      "Trained batch 1385 batch loss 1.23683357 epoch total loss 1.14377618\n",
      "Trained batch 1386 batch loss 1.02297723 epoch total loss 1.14368904\n",
      "Trained batch 1387 batch loss 1.19873428 epoch total loss 1.14372873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 04:07:59.440989: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "2025-03-19 04:07:59.441041: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "\t [[RemoteCall]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1388 batch loss 1.1699748 epoch total loss 1.14374757\n",
      "Epoch 6 train loss 1.1437475681304932\n",
      "Validated batch 1 batch loss 1.0891614\n",
      "Validated batch 2 batch loss 1.077775\n",
      "Validated batch 3 batch loss 1.1233716\n",
      "Validated batch 4 batch loss 1.10852671\n",
      "Validated batch 5 batch loss 1.13185918\n",
      "Validated batch 6 batch loss 1.19895458\n",
      "Validated batch 7 batch loss 1.15650058\n",
      "Validated batch 8 batch loss 1.15506339\n",
      "Validated batch 9 batch loss 1.14186561\n",
      "Validated batch 10 batch loss 1.1635263\n",
      "Validated batch 11 batch loss 1.1311599\n",
      "Validated batch 12 batch loss 1.15029609\n",
      "Validated batch 13 batch loss 1.12557769\n",
      "Validated batch 14 batch loss 1.13728046\n",
      "Validated batch 15 batch loss 1.15784955\n",
      "Validated batch 16 batch loss 1.13740838\n",
      "Validated batch 17 batch loss 1.28614151\n",
      "Validated batch 18 batch loss 1.20492244\n",
      "Validated batch 19 batch loss 1.04488182\n",
      "Validated batch 20 batch loss 1.27147853\n",
      "Validated batch 21 batch loss 1.10950708\n",
      "Validated batch 22 batch loss 1.08822668\n",
      "Validated batch 23 batch loss 1.15596747\n",
      "Validated batch 24 batch loss 1.13909984\n",
      "Validated batch 25 batch loss 1.11065602\n",
      "Validated batch 26 batch loss 1.14340961\n",
      "Validated batch 27 batch loss 1.15348744\n",
      "Validated batch 28 batch loss 1.07628155\n",
      "Validated batch 29 batch loss 1.23147535\n",
      "Validated batch 30 batch loss 1.1506685\n",
      "Validated batch 31 batch loss 1.02741742\n",
      "Validated batch 32 batch loss 1.07141244\n",
      "Validated batch 33 batch loss 1.07253671\n",
      "Validated batch 34 batch loss 1.06195641\n",
      "Validated batch 35 batch loss 1.11911952\n",
      "Validated batch 36 batch loss 1.14452744\n",
      "Validated batch 37 batch loss 1.128824\n",
      "Validated batch 38 batch loss 1.28047931\n",
      "Validated batch 39 batch loss 1.19234347\n",
      "Validated batch 40 batch loss 1.15480411\n",
      "Validated batch 41 batch loss 1.25008035\n",
      "Validated batch 42 batch loss 0.962975264\n",
      "Validated batch 43 batch loss 1.10581493\n",
      "Validated batch 44 batch loss 1.03287399\n",
      "Validated batch 45 batch loss 1.16874623\n",
      "Validated batch 46 batch loss 1.28226578\n",
      "Validated batch 47 batch loss 1.26400471\n",
      "Validated batch 48 batch loss 1.16412473\n",
      "Validated batch 49 batch loss 1.10600686\n",
      "Validated batch 50 batch loss 1.14870036\n",
      "Validated batch 51 batch loss 1.07510114\n",
      "Validated batch 52 batch loss 1.16835928\n",
      "Validated batch 53 batch loss 1.20580065\n",
      "Validated batch 54 batch loss 1.02444518\n",
      "Validated batch 55 batch loss 1.17779589\n",
      "Validated batch 56 batch loss 1.13874221\n",
      "Validated batch 57 batch loss 1.18893123\n",
      "Validated batch 58 batch loss 1.21058583\n",
      "Validated batch 59 batch loss 1.16244006\n",
      "Validated batch 60 batch loss 1.0716188\n",
      "Validated batch 61 batch loss 1.08392692\n",
      "Validated batch 62 batch loss 1.14658618\n",
      "Validated batch 63 batch loss 1.11300981\n",
      "Validated batch 64 batch loss 1.1679318\n",
      "Validated batch 65 batch loss 1.16618514\n",
      "Validated batch 66 batch loss 1.34549904\n",
      "Validated batch 67 batch loss 1.17585826\n",
      "Validated batch 68 batch loss 1.18626058\n",
      "Validated batch 69 batch loss 1.03408945\n",
      "Validated batch 70 batch loss 1.14049649\n",
      "Validated batch 71 batch loss 1.16033924\n",
      "Validated batch 72 batch loss 1.08506441\n",
      "Validated batch 73 batch loss 1.14921665\n",
      "Validated batch 74 batch loss 1.16749454\n",
      "Validated batch 75 batch loss 1.16598201\n",
      "Validated batch 76 batch loss 1.23241949\n",
      "Validated batch 77 batch loss 1.13771296\n",
      "Validated batch 78 batch loss 1.14444327\n",
      "Validated batch 79 batch loss 1.25990975\n",
      "Validated batch 80 batch loss 1.05170572\n",
      "Validated batch 81 batch loss 1.04772198\n",
      "Validated batch 82 batch loss 1.23466659\n",
      "Validated batch 83 batch loss 1.26243126\n",
      "Validated batch 84 batch loss 1.28615355\n",
      "Validated batch 85 batch loss 1.33058\n",
      "Validated batch 86 batch loss 1.1394136\n",
      "Validated batch 87 batch loss 1.34441686\n",
      "Validated batch 88 batch loss 1.17485619\n",
      "Validated batch 89 batch loss 1.23769569\n",
      "Validated batch 90 batch loss 1.22451055\n",
      "Validated batch 91 batch loss 0.929547369\n",
      "Validated batch 92 batch loss 1.15535665\n",
      "Validated batch 93 batch loss 1.19166446\n",
      "Validated batch 94 batch loss 1.18504703\n",
      "Validated batch 95 batch loss 1.11652696\n",
      "Validated batch 96 batch loss 1.14967966\n",
      "Validated batch 97 batch loss 1.23570442\n",
      "Validated batch 98 batch loss 1.353966\n",
      "Validated batch 99 batch loss 1.11896336\n",
      "Validated batch 100 batch loss 1.19699812\n",
      "Validated batch 101 batch loss 1.12144542\n",
      "Validated batch 102 batch loss 1.24265909\n",
      "Validated batch 103 batch loss 1.19656277\n",
      "Validated batch 104 batch loss 1.10464096\n",
      "Validated batch 105 batch loss 1.27192712\n",
      "Validated batch 106 batch loss 1.17861283\n",
      "Validated batch 107 batch loss 1.24577904\n",
      "Validated batch 108 batch loss 1.21669877\n",
      "Validated batch 109 batch loss 1.22053516\n",
      "Validated batch 110 batch loss 1.10835528\n",
      "Validated batch 111 batch loss 1.17724133\n",
      "Validated batch 112 batch loss 1.17136836\n",
      "Validated batch 113 batch loss 1.09423184\n",
      "Validated batch 114 batch loss 1.21283329\n",
      "Validated batch 115 batch loss 1.13409662\n",
      "Validated batch 116 batch loss 1.16839385\n",
      "Validated batch 117 batch loss 1.09990704\n",
      "Validated batch 118 batch loss 1.20909584\n",
      "Validated batch 119 batch loss 1.07379234\n",
      "Validated batch 120 batch loss 1.18071473\n",
      "Validated batch 121 batch loss 1.32272196\n",
      "Validated batch 122 batch loss 1.01864302\n",
      "Validated batch 123 batch loss 1.0812825\n",
      "Validated batch 124 batch loss 1.13621366\n",
      "Validated batch 125 batch loss 1.15765727\n",
      "Validated batch 126 batch loss 1.16792238\n",
      "Validated batch 127 batch loss 1.07568181\n",
      "Validated batch 128 batch loss 0.899454415\n",
      "Validated batch 129 batch loss 1.14376819\n",
      "Validated batch 130 batch loss 1.02404785\n",
      "Validated batch 131 batch loss 1.10002768\n",
      "Validated batch 132 batch loss 1.18282\n",
      "Validated batch 133 batch loss 0.98577559\n",
      "Validated batch 134 batch loss 1.15304768\n",
      "Validated batch 135 batch loss 1.24702787\n",
      "Validated batch 136 batch loss 1.17745\n",
      "Validated batch 137 batch loss 1.15552664\n",
      "Validated batch 138 batch loss 0.999357224\n",
      "Validated batch 139 batch loss 1.14366853\n",
      "Validated batch 140 batch loss 1.18867064\n",
      "Validated batch 141 batch loss 1.19485474\n",
      "Validated batch 142 batch loss 1.06122625\n",
      "Validated batch 143 batch loss 1.18327069\n",
      "Validated batch 144 batch loss 1.18275905\n",
      "Validated batch 145 batch loss 1.23283243\n",
      "Validated batch 146 batch loss 1.26789224\n",
      "Validated batch 147 batch loss 1.22462296\n",
      "Validated batch 148 batch loss 1.11346173\n",
      "Validated batch 149 batch loss 1.16272759\n",
      "Validated batch 150 batch loss 1.19043517\n",
      "Validated batch 151 batch loss 1.16789615\n",
      "Validated batch 152 batch loss 1.24664664\n",
      "Validated batch 153 batch loss 1.28103256\n",
      "Validated batch 154 batch loss 1.18039632\n",
      "Validated batch 155 batch loss 1.27155626\n",
      "Validated batch 156 batch loss 1.15239763\n",
      "Validated batch 157 batch loss 1.14555085\n",
      "Validated batch 158 batch loss 1.10717201\n",
      "Validated batch 159 batch loss 1.03334916\n",
      "Validated batch 160 batch loss 1.22784245\n",
      "Validated batch 161 batch loss 1.11636543\n",
      "Validated batch 162 batch loss 1.15694666\n",
      "Validated batch 163 batch loss 1.06195903\n",
      "Validated batch 164 batch loss 1.10557318\n",
      "Validated batch 165 batch loss 1.11228609\n",
      "Validated batch 166 batch loss 1.09093642\n",
      "Validated batch 167 batch loss 1.16413391\n",
      "Validated batch 168 batch loss 1.17963362\n",
      "Validated batch 169 batch loss 1.19095087\n",
      "Validated batch 170 batch loss 1.23904192\n",
      "Validated batch 171 batch loss 1.21381521\n",
      "Validated batch 172 batch loss 1.15643287\n",
      "Validated batch 173 batch loss 1.32992601\n",
      "Validated batch 174 batch loss 1.23948979\n",
      "Validated batch 175 batch loss 1.27175808\n",
      "Validated batch 176 batch loss 1.23769486\n",
      "Validated batch 177 batch loss 1.31643069\n",
      "Validated batch 178 batch loss 1.2463336\n",
      "Validated batch 179 batch loss 1.08706331\n",
      "Validated batch 180 batch loss 1.1615355\n",
      "Validated batch 181 batch loss 1.26025271\n",
      "Validated batch 182 batch loss 1.3017112\n",
      "Validated batch 183 batch loss 1.10876727\n",
      "Validated batch 184 batch loss 1.22593892\n",
      "Validated batch 185 batch loss 1.11397552\n",
      "Epoch 6 val loss 1.1602801084518433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 04:08:15.323485: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "2025-03-19 04:08:15.323518: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "\t [[RemoteCall]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model /home/minho/Desktop/aiffel/pose_estimation/models/model-epoch-6-loss-1.1603.weights.h5 saved.\n",
      "Start epoch 7 with learning rate 0.001\n",
      "Start distributed training...\n",
      "Trained batch 1 batch loss 1.25415432 epoch total loss 1.25415432\n",
      "Trained batch 2 batch loss 1.123788 epoch total loss 1.18897116\n",
      "Trained batch 3 batch loss 1.17348671 epoch total loss 1.18380964\n",
      "Trained batch 4 batch loss 1.22563362 epoch total loss 1.1942656\n",
      "Trained batch 5 batch loss 0.979790092 epoch total loss 1.15137053\n",
      "Trained batch 6 batch loss 0.952592909 epoch total loss 1.11824095\n",
      "Trained batch 7 batch loss 1.00382245 epoch total loss 1.10189545\n",
      "Trained batch 8 batch loss 1.24585426 epoch total loss 1.11989021\n",
      "Trained batch 9 batch loss 1.19071865 epoch total loss 1.12776\n",
      "Trained batch 10 batch loss 1.18352962 epoch total loss 1.13333702\n",
      "Trained batch 11 batch loss 1.19694352 epoch total loss 1.13911939\n",
      "Trained batch 12 batch loss 1.33700192 epoch total loss 1.15560961\n",
      "Trained batch 13 batch loss 1.06901526 epoch total loss 1.14894855\n",
      "Trained batch 14 batch loss 1.16133666 epoch total loss 1.14983344\n",
      "Trained batch 15 batch loss 1.1430645 epoch total loss 1.14938211\n",
      "Trained batch 16 batch loss 1.24226832 epoch total loss 1.15518749\n",
      "Trained batch 17 batch loss 1.22309828 epoch total loss 1.15918231\n",
      "Trained batch 18 batch loss 1.10524726 epoch total loss 1.15618587\n",
      "Trained batch 19 batch loss 1.21263349 epoch total loss 1.1591568\n",
      "Trained batch 20 batch loss 1.23816752 epoch total loss 1.16310728\n",
      "Trained batch 21 batch loss 1.16864657 epoch total loss 1.16337109\n",
      "Trained batch 22 batch loss 1.24281359 epoch total loss 1.16698205\n",
      "Trained batch 23 batch loss 1.15681291 epoch total loss 1.16653991\n",
      "Trained batch 24 batch loss 1.13361454 epoch total loss 1.16516793\n",
      "Trained batch 25 batch loss 1.12323058 epoch total loss 1.16349041\n",
      "Trained batch 26 batch loss 1.09916234 epoch total loss 1.16101635\n",
      "Trained batch 27 batch loss 1.15987265 epoch total loss 1.16097391\n",
      "Trained batch 28 batch loss 1.1707474 epoch total loss 1.16132295\n",
      "Trained batch 29 batch loss 1.14120066 epoch total loss 1.16062915\n",
      "Trained batch 30 batch loss 1.21407247 epoch total loss 1.16241062\n",
      "Trained batch 31 batch loss 1.19969368 epoch total loss 1.16361332\n",
      "Trained batch 32 batch loss 1.30599952 epoch total loss 1.16806293\n",
      "Trained batch 33 batch loss 1.26554036 epoch total loss 1.17101681\n",
      "Trained batch 34 batch loss 1.18951726 epoch total loss 1.171561\n",
      "Trained batch 35 batch loss 1.10302806 epoch total loss 1.16960287\n",
      "Trained batch 36 batch loss 1.09495771 epoch total loss 1.16752946\n",
      "Trained batch 37 batch loss 1.14680648 epoch total loss 1.1669693\n",
      "Trained batch 38 batch loss 1.19464421 epoch total loss 1.16769755\n",
      "Trained batch 39 batch loss 1.13806856 epoch total loss 1.16693795\n",
      "Trained batch 40 batch loss 1.18751097 epoch total loss 1.16745222\n",
      "Trained batch 41 batch loss 1.1569612 epoch total loss 1.16719627\n",
      "Trained batch 42 batch loss 1.17615747 epoch total loss 1.16740966\n",
      "Trained batch 43 batch loss 1.21506047 epoch total loss 1.16851795\n",
      "Trained batch 44 batch loss 1.17146659 epoch total loss 1.16858494\n",
      "Trained batch 45 batch loss 1.33377087 epoch total loss 1.17225575\n",
      "Trained batch 46 batch loss 1.23423862 epoch total loss 1.17360318\n",
      "Trained batch 47 batch loss 1.25568342 epoch total loss 1.17534959\n",
      "Trained batch 48 batch loss 1.11736202 epoch total loss 1.17414153\n",
      "Trained batch 49 batch loss 1.02397525 epoch total loss 1.17107689\n",
      "Trained batch 50 batch loss 1.12668478 epoch total loss 1.17018902\n",
      "Trained batch 51 batch loss 1.06575859 epoch total loss 1.16814137\n",
      "Trained batch 52 batch loss 1.08409703 epoch total loss 1.16652513\n",
      "Trained batch 53 batch loss 1.25002146 epoch total loss 1.1681006\n",
      "Trained batch 54 batch loss 1.12804353 epoch total loss 1.16735888\n",
      "Trained batch 55 batch loss 1.13607466 epoch total loss 1.16679013\n",
      "Trained batch 56 batch loss 1.13838148 epoch total loss 1.16628277\n",
      "Trained batch 57 batch loss 1.0649761 epoch total loss 1.16450548\n",
      "Trained batch 58 batch loss 0.951505125 epoch total loss 1.16083312\n",
      "Trained batch 59 batch loss 1.02123308 epoch total loss 1.15846705\n",
      "Trained batch 60 batch loss 1.0387032 epoch total loss 1.15647101\n",
      "Trained batch 61 batch loss 1.04752183 epoch total loss 1.15468502\n",
      "Trained batch 62 batch loss 1.00736809 epoch total loss 1.15230894\n",
      "Trained batch 63 batch loss 1.0388608 epoch total loss 1.15050817\n",
      "Trained batch 64 batch loss 0.995497346 epoch total loss 1.14808619\n",
      "Trained batch 65 batch loss 1.13820755 epoch total loss 1.1479342\n",
      "Trained batch 66 batch loss 1.08973372 epoch total loss 1.14705241\n",
      "Trained batch 67 batch loss 1.13060164 epoch total loss 1.14680684\n",
      "Trained batch 68 batch loss 1.28861356 epoch total loss 1.14889228\n",
      "Trained batch 69 batch loss 1.03299952 epoch total loss 1.14721262\n",
      "Trained batch 70 batch loss 1.38192737 epoch total loss 1.15056562\n",
      "Trained batch 71 batch loss 1.30518377 epoch total loss 1.15274334\n",
      "Trained batch 72 batch loss 1.18728709 epoch total loss 1.15322316\n",
      "Trained batch 73 batch loss 1.10702145 epoch total loss 1.15259027\n",
      "Trained batch 74 batch loss 1.02797425 epoch total loss 1.15090632\n",
      "Trained batch 75 batch loss 0.884128213 epoch total loss 1.14734924\n",
      "Trained batch 76 batch loss 1.02039671 epoch total loss 1.14567876\n",
      "Trained batch 77 batch loss 1.08766484 epoch total loss 1.14492536\n",
      "Trained batch 78 batch loss 0.827538669 epoch total loss 1.14085627\n",
      "Trained batch 79 batch loss 0.777961195 epoch total loss 1.13626266\n",
      "Trained batch 80 batch loss 0.842796206 epoch total loss 1.13259435\n",
      "Trained batch 81 batch loss 0.940994 epoch total loss 1.13022888\n",
      "Trained batch 82 batch loss 1.0015099 epoch total loss 1.12865913\n",
      "Trained batch 83 batch loss 1.07192063 epoch total loss 1.12797558\n",
      "Trained batch 84 batch loss 1.10298991 epoch total loss 1.12767816\n",
      "Trained batch 85 batch loss 1.03762984 epoch total loss 1.12661874\n",
      "Trained batch 86 batch loss 1.09928131 epoch total loss 1.12630081\n",
      "Trained batch 87 batch loss 1.15020084 epoch total loss 1.12657547\n",
      "Trained batch 88 batch loss 1.16762221 epoch total loss 1.12704194\n",
      "Trained batch 89 batch loss 1.12384367 epoch total loss 1.12700605\n",
      "Trained batch 90 batch loss 1.07721281 epoch total loss 1.12645268\n",
      "Trained batch 91 batch loss 1.20190954 epoch total loss 1.1272819\n",
      "Trained batch 92 batch loss 1.30887365 epoch total loss 1.12925577\n",
      "Trained batch 93 batch loss 1.21782911 epoch total loss 1.13020813\n",
      "Trained batch 94 batch loss 1.18992698 epoch total loss 1.13084352\n",
      "Trained batch 95 batch loss 1.10274315 epoch total loss 1.13054764\n",
      "Trained batch 96 batch loss 1.05001318 epoch total loss 1.12970877\n",
      "Trained batch 97 batch loss 1.14273965 epoch total loss 1.12984312\n",
      "Trained batch 98 batch loss 1.26727831 epoch total loss 1.13124549\n",
      "Trained batch 99 batch loss 1.19006491 epoch total loss 1.13183963\n",
      "Trained batch 100 batch loss 1.3892839 epoch total loss 1.13441408\n",
      "Trained batch 101 batch loss 1.19419551 epoch total loss 1.13500595\n",
      "Trained batch 102 batch loss 1.24058378 epoch total loss 1.13604105\n",
      "Trained batch 103 batch loss 1.10880494 epoch total loss 1.13577664\n",
      "Trained batch 104 batch loss 1.21169305 epoch total loss 1.13650656\n",
      "Trained batch 105 batch loss 1.11988831 epoch total loss 1.13634837\n",
      "Trained batch 106 batch loss 1.33877861 epoch total loss 1.13825798\n",
      "Trained batch 107 batch loss 1.07814312 epoch total loss 1.13769615\n",
      "Trained batch 108 batch loss 0.984601915 epoch total loss 1.13627863\n",
      "Trained batch 109 batch loss 1.24374604 epoch total loss 1.13726461\n",
      "Trained batch 110 batch loss 1.19558656 epoch total loss 1.13779473\n",
      "Trained batch 111 batch loss 1.26243794 epoch total loss 1.13891768\n",
      "Trained batch 112 batch loss 1.20965064 epoch total loss 1.13954914\n",
      "Trained batch 113 batch loss 1.22637141 epoch total loss 1.14031756\n",
      "Trained batch 114 batch loss 1.22967541 epoch total loss 1.14110136\n",
      "Trained batch 115 batch loss 1.10504544 epoch total loss 1.14078784\n",
      "Trained batch 116 batch loss 1.15086579 epoch total loss 1.14087462\n",
      "Trained batch 117 batch loss 1.01650119 epoch total loss 1.13981164\n",
      "Trained batch 118 batch loss 0.955075383 epoch total loss 1.13824606\n",
      "Trained batch 119 batch loss 1.00019228 epoch total loss 1.13708603\n",
      "Trained batch 120 batch loss 1.01041818 epoch total loss 1.13603044\n",
      "Trained batch 121 batch loss 0.94330579 epoch total loss 1.13443768\n",
      "Trained batch 122 batch loss 0.842494965 epoch total loss 1.13204467\n",
      "Trained batch 123 batch loss 0.872615576 epoch total loss 1.1299355\n",
      "Trained batch 124 batch loss 0.774674416 epoch total loss 1.12707055\n",
      "Trained batch 125 batch loss 0.947978854 epoch total loss 1.12563777\n",
      "Trained batch 126 batch loss 1.08955944 epoch total loss 1.12535143\n",
      "Trained batch 127 batch loss 1.04772711 epoch total loss 1.12474024\n",
      "Trained batch 128 batch loss 1.04466593 epoch total loss 1.12411463\n",
      "Trained batch 129 batch loss 1.04369247 epoch total loss 1.12349117\n",
      "Trained batch 130 batch loss 1.07900763 epoch total loss 1.12314904\n",
      "Trained batch 131 batch loss 1.15060723 epoch total loss 1.12335861\n",
      "Trained batch 132 batch loss 1.21697927 epoch total loss 1.12406778\n",
      "Trained batch 133 batch loss 1.09832501 epoch total loss 1.12387431\n",
      "Trained batch 134 batch loss 0.986834466 epoch total loss 1.12285161\n",
      "Trained batch 135 batch loss 1.05475 epoch total loss 1.12234712\n",
      "Trained batch 136 batch loss 1.10300922 epoch total loss 1.1222049\n",
      "Trained batch 137 batch loss 1.28738844 epoch total loss 1.12341058\n",
      "Trained batch 138 batch loss 1.10952961 epoch total loss 1.12331009\n",
      "Trained batch 139 batch loss 1.15230298 epoch total loss 1.12351859\n",
      "Trained batch 140 batch loss 1.02340841 epoch total loss 1.12280345\n",
      "Trained batch 141 batch loss 1.07749116 epoch total loss 1.12248206\n",
      "Trained batch 142 batch loss 1.17692828 epoch total loss 1.12286544\n",
      "Trained batch 143 batch loss 1.19998813 epoch total loss 1.12340474\n",
      "Trained batch 144 batch loss 1.19879174 epoch total loss 1.12392831\n",
      "Trained batch 145 batch loss 1.12881088 epoch total loss 1.12396193\n",
      "Trained batch 146 batch loss 1.11494684 epoch total loss 1.12390018\n",
      "Trained batch 147 batch loss 1.08806455 epoch total loss 1.12365639\n",
      "Trained batch 148 batch loss 1.04797769 epoch total loss 1.12314498\n",
      "Trained batch 149 batch loss 1.12667656 epoch total loss 1.12316871\n",
      "Trained batch 150 batch loss 1.11416519 epoch total loss 1.12310874\n",
      "Trained batch 151 batch loss 1.08857083 epoch total loss 1.12288\n",
      "Trained batch 152 batch loss 1.22913647 epoch total loss 1.12357914\n",
      "Trained batch 153 batch loss 1.22060061 epoch total loss 1.12421322\n",
      "Trained batch 154 batch loss 1.12839735 epoch total loss 1.1242404\n",
      "Trained batch 155 batch loss 1.13178778 epoch total loss 1.12428916\n",
      "Trained batch 156 batch loss 1.28437686 epoch total loss 1.12531531\n",
      "Trained batch 157 batch loss 1.00322556 epoch total loss 1.12453771\n",
      "Trained batch 158 batch loss 1.19122767 epoch total loss 1.12495971\n",
      "Trained batch 159 batch loss 1.16803026 epoch total loss 1.12523055\n",
      "Trained batch 160 batch loss 1.24207032 epoch total loss 1.12596083\n",
      "Trained batch 161 batch loss 1.09998357 epoch total loss 1.12579954\n",
      "Trained batch 162 batch loss 1.12468505 epoch total loss 1.12579262\n",
      "Trained batch 163 batch loss 1.11906254 epoch total loss 1.12575138\n",
      "Trained batch 164 batch loss 1.14862478 epoch total loss 1.12589073\n",
      "Trained batch 165 batch loss 1.09645498 epoch total loss 1.12571239\n",
      "Trained batch 166 batch loss 1.07584083 epoch total loss 1.12541187\n",
      "Trained batch 167 batch loss 1.0169785 epoch total loss 1.12476265\n",
      "Trained batch 168 batch loss 0.895583868 epoch total loss 1.12339842\n",
      "Trained batch 169 batch loss 0.966698885 epoch total loss 1.12247133\n",
      "Trained batch 170 batch loss 1.0187819 epoch total loss 1.12186134\n",
      "Trained batch 171 batch loss 1.07419896 epoch total loss 1.12158263\n",
      "Trained batch 172 batch loss 0.901061475 epoch total loss 1.12030053\n",
      "Trained batch 173 batch loss 0.989127755 epoch total loss 1.11954224\n",
      "Trained batch 174 batch loss 0.913794756 epoch total loss 1.1183598\n",
      "Trained batch 175 batch loss 1.15112066 epoch total loss 1.11854696\n",
      "Trained batch 176 batch loss 1.26721895 epoch total loss 1.11939168\n",
      "Trained batch 177 batch loss 1.28449786 epoch total loss 1.12032449\n",
      "Trained batch 178 batch loss 1.39334035 epoch total loss 1.12185836\n",
      "Trained batch 179 batch loss 1.22958028 epoch total loss 1.12246013\n",
      "Trained batch 180 batch loss 1.229213 epoch total loss 1.12305319\n",
      "Trained batch 181 batch loss 1.17334056 epoch total loss 1.12333107\n",
      "Trained batch 182 batch loss 1.02159357 epoch total loss 1.1227721\n",
      "Trained batch 183 batch loss 0.971057594 epoch total loss 1.121943\n",
      "Trained batch 184 batch loss 1.0941534 epoch total loss 1.12179196\n",
      "Trained batch 185 batch loss 1.16919374 epoch total loss 1.12204814\n",
      "Trained batch 186 batch loss 1.18968177 epoch total loss 1.12241173\n",
      "Trained batch 187 batch loss 1.20953834 epoch total loss 1.1228776\n",
      "Trained batch 188 batch loss 1.05995977 epoch total loss 1.12254298\n",
      "Trained batch 189 batch loss 1.06885147 epoch total loss 1.1222589\n",
      "Trained batch 190 batch loss 0.916317225 epoch total loss 1.12117505\n",
      "Trained batch 191 batch loss 1.02497232 epoch total loss 1.12067139\n",
      "Trained batch 192 batch loss 1.08255792 epoch total loss 1.12047291\n",
      "Trained batch 193 batch loss 1.0664947 epoch total loss 1.12019324\n",
      "Trained batch 194 batch loss 1.0471046 epoch total loss 1.11981654\n",
      "Trained batch 195 batch loss 1.12943316 epoch total loss 1.11986589\n",
      "Trained batch 196 batch loss 0.979415536 epoch total loss 1.11914921\n",
      "Trained batch 197 batch loss 1.08563566 epoch total loss 1.1189791\n",
      "Trained batch 198 batch loss 1.16135085 epoch total loss 1.11919308\n",
      "Trained batch 199 batch loss 1.25483525 epoch total loss 1.11987472\n",
      "Trained batch 200 batch loss 1.18664181 epoch total loss 1.12020862\n",
      "Trained batch 201 batch loss 1.15135503 epoch total loss 1.12036347\n",
      "Trained batch 202 batch loss 1.18092346 epoch total loss 1.12066329\n",
      "Trained batch 203 batch loss 1.19761443 epoch total loss 1.12104237\n",
      "Trained batch 204 batch loss 1.15029383 epoch total loss 1.12118578\n",
      "Trained batch 205 batch loss 1.20218372 epoch total loss 1.12158096\n",
      "Trained batch 206 batch loss 1.2430824 epoch total loss 1.12217081\n",
      "Trained batch 207 batch loss 1.21135902 epoch total loss 1.12260163\n",
      "Trained batch 208 batch loss 1.23857725 epoch total loss 1.12315917\n",
      "Trained batch 209 batch loss 1.13739395 epoch total loss 1.12322724\n",
      "Trained batch 210 batch loss 1.0847646 epoch total loss 1.12304413\n",
      "Trained batch 211 batch loss 1.12641215 epoch total loss 1.12306011\n",
      "Trained batch 212 batch loss 1.12766254 epoch total loss 1.1230818\n",
      "Trained batch 213 batch loss 1.22433615 epoch total loss 1.12355709\n",
      "Trained batch 214 batch loss 1.09886777 epoch total loss 1.1234417\n",
      "Trained batch 215 batch loss 1.10941327 epoch total loss 1.12337649\n",
      "Trained batch 216 batch loss 1.05427265 epoch total loss 1.12305665\n",
      "Trained batch 217 batch loss 1.16624737 epoch total loss 1.12325561\n",
      "Trained batch 218 batch loss 1.00412571 epoch total loss 1.12270916\n",
      "Trained batch 219 batch loss 1.10281777 epoch total loss 1.12261832\n",
      "Trained batch 220 batch loss 1.11166346 epoch total loss 1.12256849\n",
      "Trained batch 221 batch loss 1.28013456 epoch total loss 1.12328148\n",
      "Trained batch 222 batch loss 1.39511991 epoch total loss 1.124506\n",
      "Trained batch 223 batch loss 1.31535935 epoch total loss 1.1253618\n",
      "Trained batch 224 batch loss 1.17798913 epoch total loss 1.12559676\n",
      "Trained batch 225 batch loss 1.12911856 epoch total loss 1.1256125\n",
      "Trained batch 226 batch loss 1.09639561 epoch total loss 1.12548316\n",
      "Trained batch 227 batch loss 1.14824724 epoch total loss 1.12558341\n",
      "Trained batch 228 batch loss 1.19987011 epoch total loss 1.12590921\n",
      "Trained batch 229 batch loss 1.22837508 epoch total loss 1.1263566\n",
      "Trained batch 230 batch loss 1.23547781 epoch total loss 1.12683105\n",
      "Trained batch 231 batch loss 1.34383714 epoch total loss 1.12777054\n",
      "Trained batch 232 batch loss 1.26958442 epoch total loss 1.12838185\n",
      "Trained batch 233 batch loss 1.35217929 epoch total loss 1.12934232\n",
      "Trained batch 234 batch loss 1.2987932 epoch total loss 1.13006639\n",
      "Trained batch 235 batch loss 1.32438934 epoch total loss 1.13089335\n",
      "Trained batch 236 batch loss 1.25223374 epoch total loss 1.1314075\n",
      "Trained batch 237 batch loss 1.27696657 epoch total loss 1.13202178\n",
      "Trained batch 238 batch loss 1.2144959 epoch total loss 1.13236833\n",
      "Trained batch 239 batch loss 1.17321074 epoch total loss 1.13253927\n",
      "Trained batch 240 batch loss 1.19688773 epoch total loss 1.13280737\n",
      "Trained batch 241 batch loss 1.14345241 epoch total loss 1.1328516\n",
      "Trained batch 242 batch loss 1.15217686 epoch total loss 1.13293159\n",
      "Trained batch 243 batch loss 1.18771088 epoch total loss 1.13315701\n",
      "Trained batch 244 batch loss 1.20457017 epoch total loss 1.13344967\n",
      "Trained batch 245 batch loss 1.1453414 epoch total loss 1.13349819\n",
      "Trained batch 246 batch loss 1.09306085 epoch total loss 1.1333338\n",
      "Trained batch 247 batch loss 0.986186504 epoch total loss 1.13273799\n",
      "Trained batch 248 batch loss 1.09722352 epoch total loss 1.13259482\n",
      "Trained batch 249 batch loss 1.13105905 epoch total loss 1.13258874\n",
      "Trained batch 250 batch loss 1.17303908 epoch total loss 1.13275051\n",
      "Trained batch 251 batch loss 1.21663344 epoch total loss 1.13308477\n",
      "Trained batch 252 batch loss 1.19148147 epoch total loss 1.1333164\n",
      "Trained batch 253 batch loss 1.14023602 epoch total loss 1.1333437\n",
      "Trained batch 254 batch loss 1.10172749 epoch total loss 1.13321924\n",
      "Trained batch 255 batch loss 1.11956692 epoch total loss 1.13316572\n",
      "Trained batch 256 batch loss 1.03215 epoch total loss 1.13277102\n",
      "Trained batch 257 batch loss 1.02732015 epoch total loss 1.1323607\n",
      "Trained batch 258 batch loss 1.24859798 epoch total loss 1.13281119\n",
      "Trained batch 259 batch loss 1.1802026 epoch total loss 1.13299417\n",
      "Trained batch 260 batch loss 1.14482164 epoch total loss 1.13303971\n",
      "Trained batch 261 batch loss 1.09534383 epoch total loss 1.13289523\n",
      "Trained batch 262 batch loss 1.01038182 epoch total loss 1.13242769\n",
      "Trained batch 263 batch loss 0.963817179 epoch total loss 1.13178647\n",
      "Trained batch 264 batch loss 1.05181038 epoch total loss 1.13148355\n",
      "Trained batch 265 batch loss 1.0219053 epoch total loss 1.13107014\n",
      "Trained batch 266 batch loss 1.2420963 epoch total loss 1.13148749\n",
      "Trained batch 267 batch loss 1.08813274 epoch total loss 1.13132513\n",
      "Trained batch 268 batch loss 1.10188055 epoch total loss 1.13121521\n",
      "Trained batch 269 batch loss 1.19232869 epoch total loss 1.13144243\n",
      "Trained batch 270 batch loss 1.13223183 epoch total loss 1.13144529\n",
      "Trained batch 271 batch loss 1.23286796 epoch total loss 1.13181961\n",
      "Trained batch 272 batch loss 1.11777616 epoch total loss 1.13176799\n",
      "Trained batch 273 batch loss 1.06971693 epoch total loss 1.13154066\n",
      "Trained batch 274 batch loss 1.01203847 epoch total loss 1.13110435\n",
      "Trained batch 275 batch loss 1.09819043 epoch total loss 1.13098478\n",
      "Trained batch 276 batch loss 1.1862694 epoch total loss 1.13118517\n",
      "Trained batch 277 batch loss 1.33833945 epoch total loss 1.13193297\n",
      "Trained batch 278 batch loss 1.22804952 epoch total loss 1.1322788\n",
      "Trained batch 279 batch loss 1.16306508 epoch total loss 1.13238907\n",
      "Trained batch 280 batch loss 1.028584 epoch total loss 1.13201845\n",
      "Trained batch 281 batch loss 1.12038612 epoch total loss 1.13197696\n",
      "Trained batch 282 batch loss 1.18991208 epoch total loss 1.13218248\n",
      "Trained batch 283 batch loss 1.34075367 epoch total loss 1.13291943\n",
      "Trained batch 284 batch loss 1.14454651 epoch total loss 1.13296032\n",
      "Trained batch 285 batch loss 1.05100965 epoch total loss 1.13267279\n",
      "Trained batch 286 batch loss 0.985235631 epoch total loss 1.13215721\n",
      "Trained batch 287 batch loss 1.19269061 epoch total loss 1.13236809\n",
      "Trained batch 288 batch loss 1.16194224 epoch total loss 1.13247085\n",
      "Trained batch 289 batch loss 1.26787066 epoch total loss 1.13293946\n",
      "Trained batch 290 batch loss 1.15305531 epoch total loss 1.13300872\n",
      "Trained batch 291 batch loss 1.23220778 epoch total loss 1.13334966\n",
      "Trained batch 292 batch loss 1.14574099 epoch total loss 1.1333921\n",
      "Trained batch 293 batch loss 1.14138222 epoch total loss 1.13341939\n",
      "Trained batch 294 batch loss 1.232324 epoch total loss 1.1337558\n",
      "Trained batch 295 batch loss 1.02329445 epoch total loss 1.13338137\n",
      "Trained batch 296 batch loss 1.17609048 epoch total loss 1.13352561\n",
      "Trained batch 297 batch loss 1.18907082 epoch total loss 1.13371265\n",
      "Trained batch 298 batch loss 1.11564302 epoch total loss 1.13365197\n",
      "Trained batch 299 batch loss 1.18436313 epoch total loss 1.13382149\n",
      "Trained batch 300 batch loss 1.21666288 epoch total loss 1.1340977\n",
      "Trained batch 301 batch loss 1.1521678 epoch total loss 1.13415766\n",
      "Trained batch 302 batch loss 1.2065711 epoch total loss 1.13439751\n",
      "Trained batch 303 batch loss 1.24207115 epoch total loss 1.13475287\n",
      "Trained batch 304 batch loss 1.15320742 epoch total loss 1.13481355\n",
      "Trained batch 305 batch loss 1.11674356 epoch total loss 1.13475418\n",
      "Trained batch 306 batch loss 1.07676971 epoch total loss 1.13456476\n",
      "Trained batch 307 batch loss 1.19889116 epoch total loss 1.13477433\n",
      "Trained batch 308 batch loss 1.07729268 epoch total loss 1.13458765\n",
      "Trained batch 309 batch loss 1.14565015 epoch total loss 1.13462353\n",
      "Trained batch 310 batch loss 1.04952669 epoch total loss 1.13434899\n",
      "Trained batch 311 batch loss 1.05724907 epoch total loss 1.13410115\n",
      "Trained batch 312 batch loss 1.05927014 epoch total loss 1.1338613\n",
      "Trained batch 313 batch loss 1.15030992 epoch total loss 1.13391376\n",
      "Trained batch 314 batch loss 1.10867107 epoch total loss 1.13383341\n",
      "Trained batch 315 batch loss 1.13807046 epoch total loss 1.13384676\n",
      "Trained batch 316 batch loss 1.07362866 epoch total loss 1.13365626\n",
      "Trained batch 317 batch loss 1.08813429 epoch total loss 1.13351262\n",
      "Trained batch 318 batch loss 1.11788428 epoch total loss 1.1334635\n",
      "Trained batch 319 batch loss 1.13177347 epoch total loss 1.13345826\n",
      "Trained batch 320 batch loss 1.13273764 epoch total loss 1.13345599\n",
      "Trained batch 321 batch loss 1.12757444 epoch total loss 1.13343763\n",
      "Trained batch 322 batch loss 1.04368448 epoch total loss 1.13315892\n",
      "Trained batch 323 batch loss 1.01131833 epoch total loss 1.13278174\n",
      "Trained batch 324 batch loss 1.0645597 epoch total loss 1.1325711\n",
      "Trained batch 325 batch loss 1.00313163 epoch total loss 1.13217282\n",
      "Trained batch 326 batch loss 1.07653832 epoch total loss 1.13200223\n",
      "Trained batch 327 batch loss 1.13992608 epoch total loss 1.13202643\n",
      "Trained batch 328 batch loss 1.16329062 epoch total loss 1.1321218\n",
      "Trained batch 329 batch loss 1.17907858 epoch total loss 1.13226449\n",
      "Trained batch 330 batch loss 1.1663003 epoch total loss 1.13236761\n",
      "Trained batch 331 batch loss 1.05809617 epoch total loss 1.13214326\n",
      "Trained batch 332 batch loss 1.17646194 epoch total loss 1.13227665\n",
      "Trained batch 333 batch loss 1.23213625 epoch total loss 1.13257658\n",
      "Trained batch 334 batch loss 1.19012046 epoch total loss 1.13274896\n",
      "Trained batch 335 batch loss 0.985491633 epoch total loss 1.13230932\n",
      "Trained batch 336 batch loss 0.868303359 epoch total loss 1.13152373\n",
      "Trained batch 337 batch loss 1.05417609 epoch total loss 1.13129413\n",
      "Trained batch 338 batch loss 1.10604596 epoch total loss 1.13121939\n",
      "Trained batch 339 batch loss 1.04993403 epoch total loss 1.13097966\n",
      "Trained batch 340 batch loss 1.07551265 epoch total loss 1.13081646\n",
      "Trained batch 341 batch loss 1.12544346 epoch total loss 1.13080072\n",
      "Trained batch 342 batch loss 1.19024026 epoch total loss 1.13097453\n",
      "Trained batch 343 batch loss 1.10231495 epoch total loss 1.13089108\n",
      "Trained batch 344 batch loss 1.06649351 epoch total loss 1.13070381\n",
      "Trained batch 345 batch loss 1.40116608 epoch total loss 1.13148773\n",
      "Trained batch 346 batch loss 1.28156042 epoch total loss 1.13192153\n",
      "Trained batch 347 batch loss 1.33174205 epoch total loss 1.13249743\n",
      "Trained batch 348 batch loss 1.18059278 epoch total loss 1.13263559\n",
      "Trained batch 349 batch loss 1.06443679 epoch total loss 1.13244021\n",
      "Trained batch 350 batch loss 0.986554 epoch total loss 1.13202333\n",
      "Trained batch 351 batch loss 0.904034 epoch total loss 1.13137376\n",
      "Trained batch 352 batch loss 0.976745605 epoch total loss 1.13093448\n",
      "Trained batch 353 batch loss 1.14740717 epoch total loss 1.13098109\n",
      "Trained batch 354 batch loss 1.15121615 epoch total loss 1.13103831\n",
      "Trained batch 355 batch loss 1.2739712 epoch total loss 1.13144088\n",
      "Trained batch 356 batch loss 1.10511875 epoch total loss 1.13136697\n",
      "Trained batch 357 batch loss 0.934834182 epoch total loss 1.13081646\n",
      "Trained batch 358 batch loss 0.984281898 epoch total loss 1.13040709\n",
      "Trained batch 359 batch loss 1.20683777 epoch total loss 1.13062012\n",
      "Trained batch 360 batch loss 1.09877932 epoch total loss 1.13053167\n",
      "Trained batch 361 batch loss 1.22382236 epoch total loss 1.13079011\n",
      "Trained batch 362 batch loss 1.09306145 epoch total loss 1.13068581\n",
      "Trained batch 363 batch loss 1.1842351 epoch total loss 1.13083327\n",
      "Trained batch 364 batch loss 1.27069509 epoch total loss 1.13121748\n",
      "Trained batch 365 batch loss 1.15710151 epoch total loss 1.13128841\n",
      "Trained batch 366 batch loss 1.10096633 epoch total loss 1.13120556\n",
      "Trained batch 367 batch loss 1.1225158 epoch total loss 1.13118196\n",
      "Trained batch 368 batch loss 0.998093247 epoch total loss 1.13082027\n",
      "Trained batch 369 batch loss 1.06033134 epoch total loss 1.1306293\n",
      "Trained batch 370 batch loss 1.11946464 epoch total loss 1.13059914\n",
      "Trained batch 371 batch loss 1.21363938 epoch total loss 1.13082302\n",
      "Trained batch 372 batch loss 1.19237256 epoch total loss 1.13098848\n",
      "Trained batch 373 batch loss 1.10251522 epoch total loss 1.13091218\n",
      "Trained batch 374 batch loss 1.17753315 epoch total loss 1.13103676\n",
      "Trained batch 375 batch loss 1.17476034 epoch total loss 1.13115335\n",
      "Trained batch 376 batch loss 1.36501062 epoch total loss 1.13177538\n",
      "Trained batch 377 batch loss 1.01639342 epoch total loss 1.13146937\n",
      "Trained batch 378 batch loss 1.22953153 epoch total loss 1.13172877\n",
      "Trained batch 379 batch loss 1.21621585 epoch total loss 1.13195169\n",
      "Trained batch 380 batch loss 1.0583055 epoch total loss 1.13175786\n",
      "Trained batch 381 batch loss 1.10149145 epoch total loss 1.13167846\n",
      "Trained batch 382 batch loss 1.05063856 epoch total loss 1.13146627\n",
      "Trained batch 383 batch loss 1.17744935 epoch total loss 1.13158643\n",
      "Trained batch 384 batch loss 1.23446655 epoch total loss 1.1318543\n",
      "Trained batch 385 batch loss 1.27364182 epoch total loss 1.13222265\n",
      "Trained batch 386 batch loss 1.20019865 epoch total loss 1.13239872\n",
      "Trained batch 387 batch loss 1.28791285 epoch total loss 1.13280046\n",
      "Trained batch 388 batch loss 1.06697226 epoch total loss 1.13263094\n",
      "Trained batch 389 batch loss 1.25584304 epoch total loss 1.13294756\n",
      "Trained batch 390 batch loss 1.13832688 epoch total loss 1.13296139\n",
      "Trained batch 391 batch loss 1.21621656 epoch total loss 1.1331743\n",
      "Trained batch 392 batch loss 1.28143752 epoch total loss 1.13355255\n",
      "Trained batch 393 batch loss 1.17791033 epoch total loss 1.13366544\n",
      "Trained batch 394 batch loss 1.11382592 epoch total loss 1.13361514\n",
      "Trained batch 395 batch loss 1.01869547 epoch total loss 1.13332415\n",
      "Trained batch 396 batch loss 1.03285265 epoch total loss 1.13307047\n",
      "Trained batch 397 batch loss 1.00617671 epoch total loss 1.13275087\n",
      "Trained batch 398 batch loss 1.28187287 epoch total loss 1.13312554\n",
      "Trained batch 399 batch loss 1.1185286 epoch total loss 1.13308895\n",
      "Trained batch 400 batch loss 1.18618512 epoch total loss 1.13322163\n",
      "Trained batch 401 batch loss 1.15939796 epoch total loss 1.13328695\n",
      "Trained batch 402 batch loss 1.02893817 epoch total loss 1.13302732\n",
      "Trained batch 403 batch loss 1.1192838 epoch total loss 1.13299322\n",
      "Trained batch 404 batch loss 1.0764972 epoch total loss 1.13285339\n",
      "Trained batch 405 batch loss 1.06361556 epoch total loss 1.13268256\n",
      "Trained batch 406 batch loss 1.03001952 epoch total loss 1.13242972\n",
      "Trained batch 407 batch loss 1.16403425 epoch total loss 1.13250732\n",
      "Trained batch 408 batch loss 1.09498191 epoch total loss 1.13241529\n",
      "Trained batch 409 batch loss 1.13421679 epoch total loss 1.13241971\n",
      "Trained batch 410 batch loss 1.05240667 epoch total loss 1.13222456\n",
      "Trained batch 411 batch loss 1.04394054 epoch total loss 1.13200974\n",
      "Trained batch 412 batch loss 0.967849433 epoch total loss 1.13161123\n",
      "Trained batch 413 batch loss 0.959218621 epoch total loss 1.13119388\n",
      "Trained batch 414 batch loss 1.19903064 epoch total loss 1.13135779\n",
      "Trained batch 415 batch loss 1.28191555 epoch total loss 1.13172054\n",
      "Trained batch 416 batch loss 1.23733473 epoch total loss 1.13197446\n",
      "Trained batch 417 batch loss 0.97620821 epoch total loss 1.13160086\n",
      "Trained batch 418 batch loss 0.975009382 epoch total loss 1.1312263\n",
      "Trained batch 419 batch loss 0.987582922 epoch total loss 1.13088346\n",
      "Trained batch 420 batch loss 1.21251905 epoch total loss 1.13107777\n",
      "Trained batch 421 batch loss 1.23502791 epoch total loss 1.13132465\n",
      "Trained batch 422 batch loss 1.1347456 epoch total loss 1.13133276\n",
      "Trained batch 423 batch loss 1.19020736 epoch total loss 1.13147199\n",
      "Trained batch 424 batch loss 1.0424794 epoch total loss 1.13126206\n",
      "Trained batch 425 batch loss 1.09045267 epoch total loss 1.1311661\n",
      "Trained batch 426 batch loss 1.14520538 epoch total loss 1.131199\n",
      "Trained batch 427 batch loss 1.11099219 epoch total loss 1.13115168\n",
      "Trained batch 428 batch loss 1.22930479 epoch total loss 1.13138103\n",
      "Trained batch 429 batch loss 1.14300525 epoch total loss 1.1314081\n",
      "Trained batch 430 batch loss 1.03174281 epoch total loss 1.13117635\n",
      "Trained batch 431 batch loss 0.989376426 epoch total loss 1.13084733\n",
      "Trained batch 432 batch loss 0.94918 epoch total loss 1.13042688\n",
      "Trained batch 433 batch loss 1.13709736 epoch total loss 1.13044226\n",
      "Trained batch 434 batch loss 1.15013719 epoch total loss 1.13048756\n",
      "Trained batch 435 batch loss 1.10389292 epoch total loss 1.13042641\n",
      "Trained batch 436 batch loss 1.16137469 epoch total loss 1.13049746\n",
      "Trained batch 437 batch loss 0.944776297 epoch total loss 1.13007236\n",
      "Trained batch 438 batch loss 0.980280697 epoch total loss 1.12973046\n",
      "Trained batch 439 batch loss 1.0001359 epoch total loss 1.12943518\n",
      "Trained batch 440 batch loss 1.06199408 epoch total loss 1.12928188\n",
      "Trained batch 441 batch loss 0.936965704 epoch total loss 1.12884581\n",
      "Trained batch 442 batch loss 1.07820058 epoch total loss 1.12873113\n",
      "Trained batch 443 batch loss 0.993468583 epoch total loss 1.12842584\n",
      "Trained batch 444 batch loss 1.11977422 epoch total loss 1.12840641\n",
      "Trained batch 445 batch loss 0.908385277 epoch total loss 1.12791193\n",
      "Trained batch 446 batch loss 1.0530045 epoch total loss 1.12774396\n",
      "Trained batch 447 batch loss 1.06961203 epoch total loss 1.1276139\n",
      "Trained batch 448 batch loss 1.09182882 epoch total loss 1.12753403\n",
      "Trained batch 449 batch loss 1.10255694 epoch total loss 1.12747848\n",
      "Trained batch 450 batch loss 1.31735325 epoch total loss 1.12790036\n",
      "Trained batch 451 batch loss 1.1492281 epoch total loss 1.12794769\n",
      "Trained batch 452 batch loss 1.10948372 epoch total loss 1.12790692\n",
      "Trained batch 453 batch loss 1.1021148 epoch total loss 1.12784994\n",
      "Trained batch 454 batch loss 1.07562232 epoch total loss 1.1277349\n",
      "Trained batch 455 batch loss 0.968614936 epoch total loss 1.12738526\n",
      "Trained batch 456 batch loss 1.09931469 epoch total loss 1.12732363\n",
      "Trained batch 457 batch loss 1.08275032 epoch total loss 1.12722611\n",
      "Trained batch 458 batch loss 1.05302429 epoch total loss 1.12706411\n",
      "Trained batch 459 batch loss 0.892738938 epoch total loss 1.12655365\n",
      "Trained batch 460 batch loss 0.930037737 epoch total loss 1.12612653\n",
      "Trained batch 461 batch loss 0.92043227 epoch total loss 1.12568021\n",
      "Trained batch 462 batch loss 1.08425903 epoch total loss 1.12559056\n",
      "Trained batch 463 batch loss 1.14183486 epoch total loss 1.12562561\n",
      "Trained batch 464 batch loss 1.08529568 epoch total loss 1.12553871\n",
      "Trained batch 465 batch loss 1.06473553 epoch total loss 1.12540793\n",
      "Trained batch 466 batch loss 1.25384951 epoch total loss 1.12568355\n",
      "Trained batch 467 batch loss 1.21423435 epoch total loss 1.12587321\n",
      "Trained batch 468 batch loss 1.19653845 epoch total loss 1.12602413\n",
      "Trained batch 469 batch loss 1.07504666 epoch total loss 1.12591553\n",
      "Trained batch 470 batch loss 1.09326601 epoch total loss 1.12584603\n",
      "Trained batch 471 batch loss 1.04886723 epoch total loss 1.12568271\n",
      "Trained batch 472 batch loss 1.19275641 epoch total loss 1.12582481\n",
      "Trained batch 473 batch loss 1.28510296 epoch total loss 1.12616146\n",
      "Trained batch 474 batch loss 1.24208462 epoch total loss 1.12640595\n",
      "Trained batch 475 batch loss 1.15736902 epoch total loss 1.12647116\n",
      "Trained batch 476 batch loss 1.0984906 epoch total loss 1.12641239\n",
      "Trained batch 477 batch loss 1.13381732 epoch total loss 1.12642789\n",
      "Trained batch 478 batch loss 1.20336223 epoch total loss 1.12658882\n",
      "Trained batch 479 batch loss 1.08907866 epoch total loss 1.1265105\n",
      "Trained batch 480 batch loss 1.18546677 epoch total loss 1.12663329\n",
      "Trained batch 481 batch loss 1.2332983 epoch total loss 1.12685502\n",
      "Trained batch 482 batch loss 1.05765796 epoch total loss 1.12671149\n",
      "Trained batch 483 batch loss 1.08961523 epoch total loss 1.12663472\n",
      "Trained batch 484 batch loss 1.0397954 epoch total loss 1.12645531\n",
      "Trained batch 485 batch loss 1.01197147 epoch total loss 1.12621915\n",
      "Trained batch 486 batch loss 1.01082587 epoch total loss 1.12598169\n",
      "Trained batch 487 batch loss 0.942045927 epoch total loss 1.12560391\n",
      "Trained batch 488 batch loss 0.905181646 epoch total loss 1.12515223\n",
      "Trained batch 489 batch loss 1.09644067 epoch total loss 1.12509346\n",
      "Trained batch 490 batch loss 1.05599022 epoch total loss 1.12495244\n",
      "Trained batch 491 batch loss 0.930807233 epoch total loss 1.12455702\n",
      "Trained batch 492 batch loss 1.0632025 epoch total loss 1.12443233\n",
      "Trained batch 493 batch loss 0.936919212 epoch total loss 1.12405193\n",
      "Trained batch 494 batch loss 1.112854 epoch total loss 1.12402928\n",
      "Trained batch 495 batch loss 1.21218526 epoch total loss 1.12420726\n",
      "Trained batch 496 batch loss 1.16223252 epoch total loss 1.12428391\n",
      "Trained batch 497 batch loss 1.19735825 epoch total loss 1.12443101\n",
      "Trained batch 498 batch loss 1.14598465 epoch total loss 1.12447429\n",
      "Trained batch 499 batch loss 1.19952464 epoch total loss 1.12462473\n",
      "Trained batch 500 batch loss 1.11250615 epoch total loss 1.12460041\n",
      "Trained batch 501 batch loss 1.06465733 epoch total loss 1.12448072\n",
      "Trained batch 502 batch loss 0.972164869 epoch total loss 1.12417734\n",
      "Trained batch 503 batch loss 1.03512383 epoch total loss 1.12400031\n",
      "Trained batch 504 batch loss 1.16229796 epoch total loss 1.12407625\n",
      "Trained batch 505 batch loss 1.10757113 epoch total loss 1.12404346\n",
      "Trained batch 506 batch loss 1.22343028 epoch total loss 1.12423992\n",
      "Trained batch 507 batch loss 1.28404701 epoch total loss 1.12455523\n",
      "Trained batch 508 batch loss 1.22288132 epoch total loss 1.12474883\n",
      "Trained batch 509 batch loss 1.11310625 epoch total loss 1.12472594\n",
      "Trained batch 510 batch loss 1.10991859 epoch total loss 1.12469685\n",
      "Trained batch 511 batch loss 1.10234225 epoch total loss 1.1246531\n",
      "Trained batch 512 batch loss 1.1242733 epoch total loss 1.12465239\n",
      "Trained batch 513 batch loss 1.13246202 epoch total loss 1.12466753\n",
      "Trained batch 514 batch loss 0.989882052 epoch total loss 1.12440526\n",
      "Trained batch 515 batch loss 0.860168815 epoch total loss 1.12389219\n",
      "Trained batch 516 batch loss 0.952282965 epoch total loss 1.12355959\n",
      "Trained batch 517 batch loss 1.15866053 epoch total loss 1.12362742\n",
      "Trained batch 518 batch loss 1.19468844 epoch total loss 1.12376463\n",
      "Trained batch 519 batch loss 1.2648741 epoch total loss 1.12403655\n",
      "Trained batch 520 batch loss 1.25914645 epoch total loss 1.12429643\n",
      "Trained batch 521 batch loss 1.12220168 epoch total loss 1.12429237\n",
      "Trained batch 522 batch loss 1.24236166 epoch total loss 1.12451863\n",
      "Trained batch 523 batch loss 1.17627347 epoch total loss 1.12461758\n",
      "Trained batch 524 batch loss 1.17351592 epoch total loss 1.12471092\n",
      "Trained batch 525 batch loss 0.986307323 epoch total loss 1.12444735\n",
      "Trained batch 526 batch loss 1.08838701 epoch total loss 1.1243788\n",
      "Trained batch 527 batch loss 1.1631372 epoch total loss 1.12445235\n",
      "Trained batch 528 batch loss 1.16531873 epoch total loss 1.12452972\n",
      "Trained batch 529 batch loss 1.17997241 epoch total loss 1.12463462\n",
      "Trained batch 530 batch loss 1.20060205 epoch total loss 1.12477791\n",
      "Trained batch 531 batch loss 1.17237568 epoch total loss 1.12486756\n",
      "Trained batch 532 batch loss 1.20545518 epoch total loss 1.12501907\n",
      "Trained batch 533 batch loss 1.18189776 epoch total loss 1.12512577\n",
      "Trained batch 534 batch loss 1.30461884 epoch total loss 1.12546194\n",
      "Trained batch 535 batch loss 1.15226126 epoch total loss 1.125512\n",
      "Trained batch 536 batch loss 1.16730487 epoch total loss 1.12559\n",
      "Trained batch 537 batch loss 1.14121044 epoch total loss 1.12561905\n",
      "Trained batch 538 batch loss 1.07097769 epoch total loss 1.12551761\n",
      "Trained batch 539 batch loss 1.07922018 epoch total loss 1.12543166\n",
      "Trained batch 540 batch loss 0.930061698 epoch total loss 1.12506986\n",
      "Trained batch 541 batch loss 1.10907412 epoch total loss 1.12504029\n",
      "Trained batch 542 batch loss 0.994807243 epoch total loss 1.1248\n",
      "Trained batch 543 batch loss 0.909655 epoch total loss 1.12440383\n",
      "Trained batch 544 batch loss 0.868486583 epoch total loss 1.12393332\n",
      "Trained batch 545 batch loss 1.05553043 epoch total loss 1.12380791\n",
      "Trained batch 546 batch loss 1.15626538 epoch total loss 1.12386727\n",
      "Trained batch 547 batch loss 1.32307875 epoch total loss 1.12423146\n",
      "Trained batch 548 batch loss 1.36717343 epoch total loss 1.1246748\n",
      "Trained batch 549 batch loss 1.17018461 epoch total loss 1.12475765\n",
      "Trained batch 550 batch loss 1.04500246 epoch total loss 1.12461257\n",
      "Trained batch 551 batch loss 1.07146621 epoch total loss 1.12451613\n",
      "Trained batch 552 batch loss 1.20998096 epoch total loss 1.12467098\n",
      "Trained batch 553 batch loss 1.16852224 epoch total loss 1.12475026\n",
      "Trained batch 554 batch loss 1.13360894 epoch total loss 1.12476623\n",
      "Trained batch 555 batch loss 1.13711405 epoch total loss 1.1247884\n",
      "Trained batch 556 batch loss 1.20519924 epoch total loss 1.124933\n",
      "Trained batch 557 batch loss 1.12759948 epoch total loss 1.12493789\n",
      "Trained batch 558 batch loss 1.06248033 epoch total loss 1.12482595\n",
      "Trained batch 559 batch loss 0.944488406 epoch total loss 1.12450325\n",
      "Trained batch 560 batch loss 0.966108739 epoch total loss 1.12422049\n",
      "Trained batch 561 batch loss 1.14883637 epoch total loss 1.12426448\n",
      "Trained batch 562 batch loss 1.13700354 epoch total loss 1.12428713\n",
      "Trained batch 563 batch loss 0.916067958 epoch total loss 1.12391734\n",
      "Trained batch 564 batch loss 0.912483215 epoch total loss 1.12354243\n",
      "Trained batch 565 batch loss 0.994520545 epoch total loss 1.12331402\n",
      "Trained batch 566 batch loss 1.16146719 epoch total loss 1.12338138\n",
      "Trained batch 567 batch loss 1.05768096 epoch total loss 1.1232655\n",
      "Trained batch 568 batch loss 0.916999 epoch total loss 1.12290239\n",
      "Trained batch 569 batch loss 0.932063 epoch total loss 1.12256694\n",
      "Trained batch 570 batch loss 0.968577087 epoch total loss 1.12229681\n",
      "Trained batch 571 batch loss 1.10067189 epoch total loss 1.1222589\n",
      "Trained batch 572 batch loss 1.33026159 epoch total loss 1.12262249\n",
      "Trained batch 573 batch loss 1.30627489 epoch total loss 1.12294304\n",
      "Trained batch 574 batch loss 1.27322257 epoch total loss 1.12320483\n",
      "Trained batch 575 batch loss 1.23932517 epoch total loss 1.12340677\n",
      "Trained batch 576 batch loss 1.12311089 epoch total loss 1.12340617\n",
      "Trained batch 577 batch loss 1.1196208 epoch total loss 1.12339962\n",
      "Trained batch 578 batch loss 1.01338601 epoch total loss 1.12320924\n",
      "Trained batch 579 batch loss 1.13909113 epoch total loss 1.12323678\n",
      "Trained batch 580 batch loss 1.11352515 epoch total loss 1.12322\n",
      "Trained batch 581 batch loss 1.15636384 epoch total loss 1.12327707\n",
      "Trained batch 582 batch loss 1.11442125 epoch total loss 1.12326181\n",
      "Trained batch 583 batch loss 1.18186951 epoch total loss 1.12336242\n",
      "Trained batch 584 batch loss 1.15036273 epoch total loss 1.12340868\n",
      "Trained batch 585 batch loss 1.09099805 epoch total loss 1.12335336\n",
      "Trained batch 586 batch loss 1.13846576 epoch total loss 1.12337911\n",
      "Trained batch 587 batch loss 1.12858009 epoch total loss 1.12338805\n",
      "Trained batch 588 batch loss 1.23379183 epoch total loss 1.12357569\n",
      "Trained batch 589 batch loss 1.21281481 epoch total loss 1.12372732\n",
      "Trained batch 590 batch loss 1.17875516 epoch total loss 1.12382054\n",
      "Trained batch 591 batch loss 1.16645813 epoch total loss 1.12389266\n",
      "Trained batch 592 batch loss 1.27933693 epoch total loss 1.12415528\n",
      "Trained batch 593 batch loss 1.18271816 epoch total loss 1.12425411\n",
      "Trained batch 594 batch loss 1.19365466 epoch total loss 1.12437093\n",
      "Trained batch 595 batch loss 1.07067621 epoch total loss 1.12428069\n",
      "Trained batch 596 batch loss 1.08877885 epoch total loss 1.12422121\n",
      "Trained batch 597 batch loss 1.26178277 epoch total loss 1.12445164\n",
      "Trained batch 598 batch loss 1.23629093 epoch total loss 1.12463856\n",
      "Trained batch 599 batch loss 1.1214807 epoch total loss 1.12463331\n",
      "Trained batch 600 batch loss 1.25285041 epoch total loss 1.12484705\n",
      "Trained batch 601 batch loss 1.0120616 epoch total loss 1.12465942\n",
      "Trained batch 602 batch loss 0.958703518 epoch total loss 1.12438369\n",
      "Trained batch 603 batch loss 1.01331282 epoch total loss 1.12419951\n",
      "Trained batch 604 batch loss 1.05437303 epoch total loss 1.12408388\n",
      "Trained batch 605 batch loss 1.0862956 epoch total loss 1.12402141\n",
      "Trained batch 606 batch loss 1.28822684 epoch total loss 1.12429237\n",
      "Trained batch 607 batch loss 1.17588377 epoch total loss 1.12437737\n",
      "Trained batch 608 batch loss 1.06572819 epoch total loss 1.12428093\n",
      "Trained batch 609 batch loss 1.14203644 epoch total loss 1.12431\n",
      "Trained batch 610 batch loss 1.18429756 epoch total loss 1.12440848\n",
      "Trained batch 611 batch loss 1.2474525 epoch total loss 1.12460983\n",
      "Trained batch 612 batch loss 1.31506085 epoch total loss 1.12492096\n",
      "Trained batch 613 batch loss 1.3043921 epoch total loss 1.12521374\n",
      "Trained batch 614 batch loss 1.36267269 epoch total loss 1.12560046\n",
      "Trained batch 615 batch loss 1.29581845 epoch total loss 1.12587726\n",
      "Trained batch 616 batch loss 1.07895291 epoch total loss 1.12580121\n",
      "Trained batch 617 batch loss 1.07766366 epoch total loss 1.12572312\n",
      "Trained batch 618 batch loss 1.08238959 epoch total loss 1.12565303\n",
      "Trained batch 619 batch loss 0.963407159 epoch total loss 1.12539089\n",
      "Trained batch 620 batch loss 0.965022564 epoch total loss 1.1251322\n",
      "Trained batch 621 batch loss 0.977769911 epoch total loss 1.12489498\n",
      "Trained batch 622 batch loss 0.973534763 epoch total loss 1.12465155\n",
      "Trained batch 623 batch loss 0.972837687 epoch total loss 1.12440789\n",
      "Trained batch 624 batch loss 0.922791362 epoch total loss 1.12408471\n",
      "Trained batch 625 batch loss 0.9832744 epoch total loss 1.12385952\n",
      "Trained batch 626 batch loss 1.09540009 epoch total loss 1.12381399\n",
      "Trained batch 627 batch loss 1.04948771 epoch total loss 1.12369549\n",
      "Trained batch 628 batch loss 0.944336653 epoch total loss 1.12340987\n",
      "Trained batch 629 batch loss 1.08446145 epoch total loss 1.123348\n",
      "Trained batch 630 batch loss 1.17036271 epoch total loss 1.12342262\n",
      "Trained batch 631 batch loss 1.12491393 epoch total loss 1.12342501\n",
      "Trained batch 632 batch loss 1.04589367 epoch total loss 1.12330234\n",
      "Trained batch 633 batch loss 0.89781487 epoch total loss 1.12294614\n",
      "Trained batch 634 batch loss 1.02085888 epoch total loss 1.12278509\n",
      "Trained batch 635 batch loss 1.24321628 epoch total loss 1.12297475\n",
      "Trained batch 636 batch loss 1.2901572 epoch total loss 1.12323761\n",
      "Trained batch 637 batch loss 1.11257243 epoch total loss 1.12322092\n",
      "Trained batch 638 batch loss 0.9456653 epoch total loss 1.12294257\n",
      "Trained batch 639 batch loss 1.09517431 epoch total loss 1.12289906\n",
      "Trained batch 640 batch loss 1.07332301 epoch total loss 1.12282157\n",
      "Trained batch 641 batch loss 1.10202777 epoch total loss 1.12278926\n",
      "Trained batch 642 batch loss 0.996020317 epoch total loss 1.12259173\n",
      "Trained batch 643 batch loss 1.11354554 epoch total loss 1.12257767\n",
      "Trained batch 644 batch loss 1.1052264 epoch total loss 1.12255073\n",
      "Trained batch 645 batch loss 1.10374761 epoch total loss 1.12252164\n",
      "Trained batch 646 batch loss 1.18492389 epoch total loss 1.1226182\n",
      "Trained batch 647 batch loss 1.16614 epoch total loss 1.12268543\n",
      "Trained batch 648 batch loss 0.987930477 epoch total loss 1.12247753\n",
      "Trained batch 649 batch loss 0.983843446 epoch total loss 1.12226391\n",
      "Trained batch 650 batch loss 1.11582899 epoch total loss 1.12225401\n",
      "Trained batch 651 batch loss 1.17532611 epoch total loss 1.12233555\n",
      "Trained batch 652 batch loss 1.28328562 epoch total loss 1.12258232\n",
      "Trained batch 653 batch loss 1.1620636 epoch total loss 1.12264276\n",
      "Trained batch 654 batch loss 1.29147124 epoch total loss 1.12290096\n",
      "Trained batch 655 batch loss 1.22476268 epoch total loss 1.12305653\n",
      "Trained batch 656 batch loss 1.27543712 epoch total loss 1.12328875\n",
      "Trained batch 657 batch loss 1.19438744 epoch total loss 1.12339699\n",
      "Trained batch 658 batch loss 1.17569578 epoch total loss 1.12347651\n",
      "Trained batch 659 batch loss 1.13953888 epoch total loss 1.12350094\n",
      "Trained batch 660 batch loss 1.03473735 epoch total loss 1.12336636\n",
      "Trained batch 661 batch loss 1.05894554 epoch total loss 1.12326896\n",
      "Trained batch 662 batch loss 1.08275402 epoch total loss 1.12320781\n",
      "Trained batch 663 batch loss 1.09792626 epoch total loss 1.12316954\n",
      "Trained batch 664 batch loss 1.04025972 epoch total loss 1.12304473\n",
      "Trained batch 665 batch loss 1.03252459 epoch total loss 1.12290859\n",
      "Trained batch 666 batch loss 0.934924662 epoch total loss 1.12262642\n",
      "Trained batch 667 batch loss 1.00069833 epoch total loss 1.12244356\n",
      "Trained batch 668 batch loss 1.19222879 epoch total loss 1.12254798\n",
      "Trained batch 669 batch loss 1.17471647 epoch total loss 1.12262607\n",
      "Trained batch 670 batch loss 1.15811539 epoch total loss 1.122679\n",
      "Trained batch 671 batch loss 1.20687294 epoch total loss 1.1228044\n",
      "Trained batch 672 batch loss 1.17323482 epoch total loss 1.12287951\n",
      "Trained batch 673 batch loss 1.11207199 epoch total loss 1.12286341\n",
      "Trained batch 674 batch loss 1.15427351 epoch total loss 1.12291\n",
      "Trained batch 675 batch loss 0.99875617 epoch total loss 1.1227262\n",
      "Trained batch 676 batch loss 1.09383678 epoch total loss 1.12268341\n",
      "Trained batch 677 batch loss 1.19305539 epoch total loss 1.12278736\n",
      "Trained batch 678 batch loss 1.18545175 epoch total loss 1.12287974\n",
      "Trained batch 679 batch loss 1.13660479 epoch total loss 1.12289989\n",
      "Trained batch 680 batch loss 1.19443691 epoch total loss 1.12300515\n",
      "Trained batch 681 batch loss 1.15560699 epoch total loss 1.12305295\n",
      "Trained batch 682 batch loss 1.28756976 epoch total loss 1.12329423\n",
      "Trained batch 683 batch loss 1.13820267 epoch total loss 1.12331605\n",
      "Trained batch 684 batch loss 1.08742774 epoch total loss 1.12326348\n",
      "Trained batch 685 batch loss 1.05398476 epoch total loss 1.12316239\n",
      "Trained batch 686 batch loss 1.04195786 epoch total loss 1.12304389\n",
      "Trained batch 687 batch loss 1.10924554 epoch total loss 1.12302387\n",
      "Trained batch 688 batch loss 0.96075511 epoch total loss 1.12278795\n",
      "Trained batch 689 batch loss 1.06522858 epoch total loss 1.12270451\n",
      "Trained batch 690 batch loss 0.955519438 epoch total loss 1.12246215\n",
      "Trained batch 691 batch loss 1.02360106 epoch total loss 1.1223191\n",
      "Trained batch 692 batch loss 0.981773555 epoch total loss 1.12211597\n",
      "Trained batch 693 batch loss 1.15258634 epoch total loss 1.12216\n",
      "Trained batch 694 batch loss 1.07911849 epoch total loss 1.12209797\n",
      "Trained batch 695 batch loss 1.07238078 epoch total loss 1.12202644\n",
      "Trained batch 696 batch loss 1.22965479 epoch total loss 1.12218106\n",
      "Trained batch 697 batch loss 1.13335025 epoch total loss 1.12219715\n",
      "Trained batch 698 batch loss 1.266855 epoch total loss 1.12240434\n",
      "Trained batch 699 batch loss 1.2015034 epoch total loss 1.12251747\n",
      "Trained batch 700 batch loss 1.09521759 epoch total loss 1.12247849\n",
      "Trained batch 701 batch loss 1.1693697 epoch total loss 1.12254536\n",
      "Trained batch 702 batch loss 1.04576385 epoch total loss 1.12243605\n",
      "Trained batch 703 batch loss 1.1336838 epoch total loss 1.12245202\n",
      "Trained batch 704 batch loss 1.08703613 epoch total loss 1.12240171\n",
      "Trained batch 705 batch loss 1.01242578 epoch total loss 1.12224567\n",
      "Trained batch 706 batch loss 0.91636 epoch total loss 1.12195408\n",
      "Trained batch 707 batch loss 0.999084175 epoch total loss 1.12178028\n",
      "Trained batch 708 batch loss 0.979563832 epoch total loss 1.12157941\n",
      "Trained batch 709 batch loss 0.995305419 epoch total loss 1.12140131\n",
      "Trained batch 710 batch loss 1.25810874 epoch total loss 1.12159395\n",
      "Trained batch 711 batch loss 1.2453481 epoch total loss 1.121768\n",
      "Trained batch 712 batch loss 1.3494271 epoch total loss 1.12208772\n",
      "Trained batch 713 batch loss 1.26895356 epoch total loss 1.12229371\n",
      "Trained batch 714 batch loss 1.21933258 epoch total loss 1.12242973\n",
      "Trained batch 715 batch loss 1.2730006 epoch total loss 1.12264025\n",
      "Trained batch 716 batch loss 1.23878825 epoch total loss 1.1228025\n",
      "Trained batch 717 batch loss 1.13530469 epoch total loss 1.1228199\n",
      "Trained batch 718 batch loss 1.19238245 epoch total loss 1.12291682\n",
      "Trained batch 719 batch loss 1.22893751 epoch total loss 1.12306428\n",
      "Trained batch 720 batch loss 1.17331362 epoch total loss 1.12313414\n",
      "Trained batch 721 batch loss 1.12594128 epoch total loss 1.12313795\n",
      "Trained batch 722 batch loss 1.04686129 epoch total loss 1.12303233\n",
      "Trained batch 723 batch loss 1.0611105 epoch total loss 1.12294662\n",
      "Trained batch 724 batch loss 1.06117141 epoch total loss 1.12286127\n",
      "Trained batch 725 batch loss 1.08120513 epoch total loss 1.12280381\n",
      "Trained batch 726 batch loss 1.17317903 epoch total loss 1.12287319\n",
      "Trained batch 727 batch loss 1.09544718 epoch total loss 1.12283552\n",
      "Trained batch 728 batch loss 1.03113079 epoch total loss 1.12270951\n",
      "Trained batch 729 batch loss 1.01700842 epoch total loss 1.12256455\n",
      "Trained batch 730 batch loss 1.14310062 epoch total loss 1.12259269\n",
      "Trained batch 731 batch loss 1.18553007 epoch total loss 1.12267888\n",
      "Trained batch 732 batch loss 1.06585813 epoch total loss 1.12260115\n",
      "Trained batch 733 batch loss 1.23054326 epoch total loss 1.12274849\n",
      "Trained batch 734 batch loss 1.0139246 epoch total loss 1.1226002\n",
      "Trained batch 735 batch loss 0.794216096 epoch total loss 1.1221534\n",
      "Trained batch 736 batch loss 0.808082938 epoch total loss 1.12172663\n",
      "Trained batch 737 batch loss 0.809455216 epoch total loss 1.12130296\n",
      "Trained batch 738 batch loss 1.01083267 epoch total loss 1.12115324\n",
      "Trained batch 739 batch loss 0.871381283 epoch total loss 1.12081528\n",
      "Trained batch 740 batch loss 0.94797647 epoch total loss 1.12058175\n",
      "Trained batch 741 batch loss 0.95072782 epoch total loss 1.12035251\n",
      "Trained batch 742 batch loss 1.17835 epoch total loss 1.12043071\n",
      "Trained batch 743 batch loss 1.15727758 epoch total loss 1.1204803\n",
      "Trained batch 744 batch loss 1.22538471 epoch total loss 1.12062132\n",
      "Trained batch 745 batch loss 1.12183833 epoch total loss 1.12062287\n",
      "Trained batch 746 batch loss 1.05300105 epoch total loss 1.12053227\n",
      "Trained batch 747 batch loss 1.00204384 epoch total loss 1.12037361\n",
      "Trained batch 748 batch loss 1.09033799 epoch total loss 1.12033343\n",
      "Trained batch 749 batch loss 1.10618305 epoch total loss 1.1203146\n",
      "Trained batch 750 batch loss 1.11043048 epoch total loss 1.12030137\n",
      "Trained batch 751 batch loss 1.14806819 epoch total loss 1.12033832\n",
      "Trained batch 752 batch loss 1.22122741 epoch total loss 1.12047255\n",
      "Trained batch 753 batch loss 1.10921609 epoch total loss 1.12045753\n",
      "Trained batch 754 batch loss 1.13472736 epoch total loss 1.12047648\n",
      "Trained batch 755 batch loss 1.10385537 epoch total loss 1.12045443\n",
      "Trained batch 756 batch loss 1.13164854 epoch total loss 1.12046921\n",
      "Trained batch 757 batch loss 1.07443595 epoch total loss 1.12040854\n",
      "Trained batch 758 batch loss 1.19022942 epoch total loss 1.12050068\n",
      "Trained batch 759 batch loss 1.14316332 epoch total loss 1.12053049\n",
      "Trained batch 760 batch loss 1.07314551 epoch total loss 1.12046814\n",
      "Trained batch 761 batch loss 1.06747615 epoch total loss 1.12039852\n",
      "Trained batch 762 batch loss 1.10449433 epoch total loss 1.12037766\n",
      "Trained batch 763 batch loss 1.04928744 epoch total loss 1.12028456\n",
      "Trained batch 764 batch loss 1.06085 epoch total loss 1.12020671\n",
      "Trained batch 765 batch loss 1.07083595 epoch total loss 1.12014222\n",
      "Trained batch 766 batch loss 1.07176125 epoch total loss 1.12007916\n",
      "Trained batch 767 batch loss 1.05489671 epoch total loss 1.11999404\n",
      "Trained batch 768 batch loss 0.98215425 epoch total loss 1.11981463\n",
      "Trained batch 769 batch loss 1.11327386 epoch total loss 1.11980617\n",
      "Trained batch 770 batch loss 1.09356093 epoch total loss 1.11977208\n",
      "Trained batch 771 batch loss 1.10524106 epoch total loss 1.11975324\n",
      "Trained batch 772 batch loss 1.0837785 epoch total loss 1.11970663\n",
      "Trained batch 773 batch loss 1.07248235 epoch total loss 1.1196456\n",
      "Trained batch 774 batch loss 1.12604833 epoch total loss 1.11965382\n",
      "Trained batch 775 batch loss 1.16669834 epoch total loss 1.1197145\n",
      "Trained batch 776 batch loss 1.13496876 epoch total loss 1.11973417\n",
      "Trained batch 777 batch loss 1.11303031 epoch total loss 1.11972558\n",
      "Trained batch 778 batch loss 1.08968115 epoch total loss 1.11968684\n",
      "Trained batch 779 batch loss 1.05072594 epoch total loss 1.11959839\n",
      "Trained batch 780 batch loss 1.1268332 epoch total loss 1.11960757\n",
      "Trained batch 781 batch loss 1.04785633 epoch total loss 1.11951578\n",
      "Trained batch 782 batch loss 1.04515219 epoch total loss 1.11942065\n",
      "Trained batch 783 batch loss 0.989472151 epoch total loss 1.11925471\n",
      "Trained batch 784 batch loss 1.13223553 epoch total loss 1.11927128\n",
      "Trained batch 785 batch loss 1.09321821 epoch total loss 1.11923814\n",
      "Trained batch 786 batch loss 1.24390721 epoch total loss 1.11939669\n",
      "Trained batch 787 batch loss 1.25831199 epoch total loss 1.11957324\n",
      "Trained batch 788 batch loss 1.22823441 epoch total loss 1.11971104\n",
      "Trained batch 789 batch loss 1.30393934 epoch total loss 1.11994457\n",
      "Trained batch 790 batch loss 1.28962493 epoch total loss 1.12015939\n",
      "Trained batch 791 batch loss 1.27240694 epoch total loss 1.12035179\n",
      "Trained batch 792 batch loss 1.08914089 epoch total loss 1.12031233\n",
      "Trained batch 793 batch loss 1.11899567 epoch total loss 1.12031078\n",
      "Trained batch 794 batch loss 1.186818 epoch total loss 1.12039459\n",
      "Trained batch 795 batch loss 1.14279401 epoch total loss 1.12042272\n",
      "Trained batch 796 batch loss 1.1841259 epoch total loss 1.12050283\n",
      "Trained batch 797 batch loss 1.11807132 epoch total loss 1.12049973\n",
      "Trained batch 798 batch loss 1.07622135 epoch total loss 1.12044418\n",
      "Trained batch 799 batch loss 1.01960564 epoch total loss 1.12031806\n",
      "Trained batch 800 batch loss 1.02232337 epoch total loss 1.12019551\n",
      "Trained batch 801 batch loss 1.06661212 epoch total loss 1.12012863\n",
      "Trained batch 802 batch loss 1.05860221 epoch total loss 1.12005186\n",
      "Trained batch 803 batch loss 1.09480298 epoch total loss 1.12002039\n",
      "Trained batch 804 batch loss 1.14122856 epoch total loss 1.12004685\n",
      "Trained batch 805 batch loss 1.21804667 epoch total loss 1.12016857\n",
      "Trained batch 806 batch loss 1.13662457 epoch total loss 1.12018895\n",
      "Trained batch 807 batch loss 1.03836894 epoch total loss 1.1200875\n",
      "Trained batch 808 batch loss 1.16354728 epoch total loss 1.12014139\n",
      "Trained batch 809 batch loss 1.06090212 epoch total loss 1.12006819\n",
      "Trained batch 810 batch loss 1.05245 epoch total loss 1.11998463\n",
      "Trained batch 811 batch loss 1.10760605 epoch total loss 1.11996937\n",
      "Trained batch 812 batch loss 1.06309795 epoch total loss 1.11989939\n",
      "Trained batch 813 batch loss 1.09046698 epoch total loss 1.11986315\n",
      "Trained batch 814 batch loss 1.12442398 epoch total loss 1.11986876\n",
      "Trained batch 815 batch loss 1.01770413 epoch total loss 1.11974347\n",
      "Trained batch 816 batch loss 1.02072108 epoch total loss 1.11962199\n",
      "Trained batch 817 batch loss 1.13115156 epoch total loss 1.11963618\n",
      "Trained batch 818 batch loss 1.04361606 epoch total loss 1.11954319\n",
      "Trained batch 819 batch loss 1.07176638 epoch total loss 1.1194849\n",
      "Trained batch 820 batch loss 1.09079134 epoch total loss 1.11945\n",
      "Trained batch 821 batch loss 1.08506036 epoch total loss 1.11940813\n",
      "Trained batch 822 batch loss 1.03455615 epoch total loss 1.1193049\n",
      "Trained batch 823 batch loss 1.15322554 epoch total loss 1.11934602\n",
      "Trained batch 824 batch loss 1.25434446 epoch total loss 1.11950982\n",
      "Trained batch 825 batch loss 1.17749834 epoch total loss 1.11958015\n",
      "Trained batch 826 batch loss 1.15970278 epoch total loss 1.11962879\n",
      "Trained batch 827 batch loss 1.11780906 epoch total loss 1.11962652\n",
      "Trained batch 828 batch loss 1.09823287 epoch total loss 1.11960065\n",
      "Trained batch 829 batch loss 0.994017839 epoch total loss 1.11944914\n",
      "Trained batch 830 batch loss 0.982341766 epoch total loss 1.11928403\n",
      "Trained batch 831 batch loss 1.04573464 epoch total loss 1.11919546\n",
      "Trained batch 832 batch loss 0.998826385 epoch total loss 1.11905086\n",
      "Trained batch 833 batch loss 0.920192957 epoch total loss 1.11881208\n",
      "Trained batch 834 batch loss 0.987481594 epoch total loss 1.11865461\n",
      "Trained batch 835 batch loss 1.02513683 epoch total loss 1.11854267\n",
      "Trained batch 836 batch loss 1.01863456 epoch total loss 1.1184231\n",
      "Trained batch 837 batch loss 1.05357194 epoch total loss 1.11834562\n",
      "Trained batch 838 batch loss 1.09119987 epoch total loss 1.11831319\n",
      "Trained batch 839 batch loss 1.07715368 epoch total loss 1.1182642\n",
      "Trained batch 840 batch loss 1.11908269 epoch total loss 1.11826515\n",
      "Trained batch 841 batch loss 1.0673995 epoch total loss 1.11820459\n",
      "Trained batch 842 batch loss 1.20287144 epoch total loss 1.11830521\n",
      "Trained batch 843 batch loss 1.05082607 epoch total loss 1.11822522\n",
      "Trained batch 844 batch loss 1.09282374 epoch total loss 1.11819506\n",
      "Trained batch 845 batch loss 1.14515126 epoch total loss 1.11822701\n",
      "Trained batch 846 batch loss 0.914114237 epoch total loss 1.11798573\n",
      "Trained batch 847 batch loss 0.898670316 epoch total loss 1.1177268\n",
      "Trained batch 848 batch loss 0.966742873 epoch total loss 1.1175487\n",
      "Trained batch 849 batch loss 0.934206784 epoch total loss 1.11733282\n",
      "Trained batch 850 batch loss 1.02761579 epoch total loss 1.1172272\n",
      "Trained batch 851 batch loss 1.03284109 epoch total loss 1.11712801\n",
      "Trained batch 852 batch loss 1.07230425 epoch total loss 1.11707544\n",
      "Trained batch 853 batch loss 1.02070832 epoch total loss 1.11696243\n",
      "Trained batch 854 batch loss 1.11437881 epoch total loss 1.11695945\n",
      "Trained batch 855 batch loss 1.07770634 epoch total loss 1.11691356\n",
      "Trained batch 856 batch loss 1.1035111 epoch total loss 1.11689782\n",
      "Trained batch 857 batch loss 0.907225847 epoch total loss 1.1166532\n",
      "Trained batch 858 batch loss 1.00196135 epoch total loss 1.11651957\n",
      "Trained batch 859 batch loss 1.06653166 epoch total loss 1.11646128\n",
      "Trained batch 860 batch loss 1.15316021 epoch total loss 1.11650395\n",
      "Trained batch 861 batch loss 1.14533198 epoch total loss 1.11653745\n",
      "Trained batch 862 batch loss 1.15881956 epoch total loss 1.11658645\n",
      "Trained batch 863 batch loss 1.27852166 epoch total loss 1.11677408\n",
      "Trained batch 864 batch loss 1.26322496 epoch total loss 1.1169436\n",
      "Trained batch 865 batch loss 1.04729962 epoch total loss 1.11686313\n",
      "Trained batch 866 batch loss 1.07842481 epoch total loss 1.11681879\n",
      "Trained batch 867 batch loss 1.10329568 epoch total loss 1.11680317\n",
      "Trained batch 868 batch loss 1.16980696 epoch total loss 1.1168642\n",
      "Trained batch 869 batch loss 0.991457224 epoch total loss 1.11671984\n",
      "Trained batch 870 batch loss 1.02507365 epoch total loss 1.11661458\n",
      "Trained batch 871 batch loss 1.10409355 epoch total loss 1.11660016\n",
      "Trained batch 872 batch loss 1.07859182 epoch total loss 1.11655653\n",
      "Trained batch 873 batch loss 1.14024246 epoch total loss 1.1165837\n",
      "Trained batch 874 batch loss 1.29285574 epoch total loss 1.11678541\n",
      "Trained batch 875 batch loss 1.23359907 epoch total loss 1.11691892\n",
      "Trained batch 876 batch loss 1.20652318 epoch total loss 1.1170212\n",
      "Trained batch 877 batch loss 1.16821384 epoch total loss 1.11707962\n",
      "Trained batch 878 batch loss 1.15239418 epoch total loss 1.11711979\n",
      "Trained batch 879 batch loss 1.02913678 epoch total loss 1.11701965\n",
      "Trained batch 880 batch loss 1.0137862 epoch total loss 1.11690235\n",
      "Trained batch 881 batch loss 1.07808268 epoch total loss 1.11685824\n",
      "Trained batch 882 batch loss 1.12817097 epoch total loss 1.11687112\n",
      "Trained batch 883 batch loss 1.27912712 epoch total loss 1.11705482\n",
      "Trained batch 884 batch loss 1.14490461 epoch total loss 1.11708629\n",
      "Trained batch 885 batch loss 1.16180766 epoch total loss 1.11713684\n",
      "Trained batch 886 batch loss 1.13888049 epoch total loss 1.11716139\n",
      "Trained batch 887 batch loss 1.1303966 epoch total loss 1.11717629\n",
      "Trained batch 888 batch loss 0.953341424 epoch total loss 1.11699176\n",
      "Trained batch 889 batch loss 0.982115388 epoch total loss 1.11684012\n",
      "Trained batch 890 batch loss 0.946032226 epoch total loss 1.1166482\n",
      "Trained batch 891 batch loss 0.919839263 epoch total loss 1.1164273\n",
      "Trained batch 892 batch loss 1.17690361 epoch total loss 1.11649513\n",
      "Trained batch 893 batch loss 1.2270534 epoch total loss 1.11661887\n",
      "Trained batch 894 batch loss 1.22651982 epoch total loss 1.11674178\n",
      "Trained batch 895 batch loss 1.1686939 epoch total loss 1.11679983\n",
      "Trained batch 896 batch loss 1.31692052 epoch total loss 1.11702323\n",
      "Trained batch 897 batch loss 1.19093561 epoch total loss 1.1171056\n",
      "Trained batch 898 batch loss 1.15237176 epoch total loss 1.11714482\n",
      "Trained batch 899 batch loss 1.21469522 epoch total loss 1.1172533\n",
      "Trained batch 900 batch loss 1.15760529 epoch total loss 1.11729813\n",
      "Trained batch 901 batch loss 1.16716325 epoch total loss 1.11735356\n",
      "Trained batch 902 batch loss 1.10493743 epoch total loss 1.11733973\n",
      "Trained batch 903 batch loss 1.00662673 epoch total loss 1.11721718\n",
      "Trained batch 904 batch loss 0.892062306 epoch total loss 1.11696815\n",
      "Trained batch 905 batch loss 1.00125051 epoch total loss 1.11684024\n",
      "Trained batch 906 batch loss 1.09961224 epoch total loss 1.11682117\n",
      "Trained batch 907 batch loss 1.07871246 epoch total loss 1.11677921\n",
      "Trained batch 908 batch loss 1.1386255 epoch total loss 1.11680329\n",
      "Trained batch 909 batch loss 1.22611928 epoch total loss 1.11692357\n",
      "Trained batch 910 batch loss 1.1484499 epoch total loss 1.11695814\n",
      "Trained batch 911 batch loss 1.14612412 epoch total loss 1.11699021\n",
      "Trained batch 912 batch loss 1.19860172 epoch total loss 1.11707973\n",
      "Trained batch 913 batch loss 1.06482542 epoch total loss 1.1170224\n",
      "Trained batch 914 batch loss 1.11052918 epoch total loss 1.11701536\n",
      "Trained batch 915 batch loss 1.14969373 epoch total loss 1.11705112\n",
      "Trained batch 916 batch loss 1.26090097 epoch total loss 1.11720812\n",
      "Trained batch 917 batch loss 1.04148173 epoch total loss 1.11712563\n",
      "Trained batch 918 batch loss 1.08240521 epoch total loss 1.11708772\n",
      "Trained batch 919 batch loss 1.01319551 epoch total loss 1.11697471\n",
      "Trained batch 920 batch loss 0.963676929 epoch total loss 1.11680806\n",
      "Trained batch 921 batch loss 0.915297389 epoch total loss 1.11658919\n",
      "Trained batch 922 batch loss 1.01367557 epoch total loss 1.11647761\n",
      "Trained batch 923 batch loss 1.05158699 epoch total loss 1.11640728\n",
      "Trained batch 924 batch loss 1.04696548 epoch total loss 1.11633217\n",
      "Trained batch 925 batch loss 1.04035664 epoch total loss 1.11625016\n",
      "Trained batch 926 batch loss 1.07306778 epoch total loss 1.11620355\n",
      "Trained batch 927 batch loss 1.07082605 epoch total loss 1.11615455\n",
      "Trained batch 928 batch loss 0.940382242 epoch total loss 1.11596525\n",
      "Trained batch 929 batch loss 1.14573777 epoch total loss 1.11599731\n",
      "Trained batch 930 batch loss 1.15509582 epoch total loss 1.1160394\n",
      "Trained batch 931 batch loss 1.2155534 epoch total loss 1.11614633\n",
      "Trained batch 932 batch loss 1.10083532 epoch total loss 1.11612988\n",
      "Trained batch 933 batch loss 1.11346698 epoch total loss 1.11612701\n",
      "Trained batch 934 batch loss 1.11940455 epoch total loss 1.11613059\n",
      "Trained batch 935 batch loss 1.1768465 epoch total loss 1.11619556\n",
      "Trained batch 936 batch loss 1.10737407 epoch total loss 1.11618614\n",
      "Trained batch 937 batch loss 1.22723913 epoch total loss 1.11630476\n",
      "Trained batch 938 batch loss 0.950582087 epoch total loss 1.11612809\n",
      "Trained batch 939 batch loss 1.20270538 epoch total loss 1.11622036\n",
      "Trained batch 940 batch loss 1.15537405 epoch total loss 1.11626196\n",
      "Trained batch 941 batch loss 1.25088394 epoch total loss 1.11640501\n",
      "Trained batch 942 batch loss 0.947239 epoch total loss 1.11622548\n",
      "Trained batch 943 batch loss 0.954672039 epoch total loss 1.11605418\n",
      "Trained batch 944 batch loss 0.957958221 epoch total loss 1.11588681\n",
      "Trained batch 945 batch loss 0.942748189 epoch total loss 1.11570358\n",
      "Trained batch 946 batch loss 0.924718857 epoch total loss 1.11550164\n",
      "Trained batch 947 batch loss 0.933208 epoch total loss 1.11530912\n",
      "Trained batch 948 batch loss 1.00143516 epoch total loss 1.11518908\n",
      "Trained batch 949 batch loss 1.09937906 epoch total loss 1.11517239\n",
      "Trained batch 950 batch loss 0.980293095 epoch total loss 1.11503041\n",
      "Trained batch 951 batch loss 1.09403944 epoch total loss 1.11500835\n",
      "Trained batch 952 batch loss 1.13323331 epoch total loss 1.11502743\n",
      "Trained batch 953 batch loss 1.19093442 epoch total loss 1.11510706\n",
      "Trained batch 954 batch loss 1.0988847 epoch total loss 1.11509\n",
      "Trained batch 955 batch loss 1.06681848 epoch total loss 1.11503947\n",
      "Trained batch 956 batch loss 1.10744333 epoch total loss 1.11503148\n",
      "Trained batch 957 batch loss 1.06563056 epoch total loss 1.11497986\n",
      "Trained batch 958 batch loss 1.03279543 epoch total loss 1.11489415\n",
      "Trained batch 959 batch loss 1.20028508 epoch total loss 1.1149832\n",
      "Trained batch 960 batch loss 1.21564043 epoch total loss 1.11508811\n",
      "Trained batch 961 batch loss 1.23449206 epoch total loss 1.11521244\n",
      "Trained batch 962 batch loss 1.24111795 epoch total loss 1.11534321\n",
      "Trained batch 963 batch loss 1.26605642 epoch total loss 1.11549985\n",
      "Trained batch 964 batch loss 1.15675366 epoch total loss 1.11554253\n",
      "Trained batch 965 batch loss 1.16550171 epoch total loss 1.11559439\n",
      "Trained batch 966 batch loss 1.11536849 epoch total loss 1.11559415\n",
      "Trained batch 967 batch loss 1.0539633 epoch total loss 1.11553037\n",
      "Trained batch 968 batch loss 1.20124793 epoch total loss 1.11561894\n",
      "Trained batch 969 batch loss 1.04399419 epoch total loss 1.11554503\n",
      "Trained batch 970 batch loss 1.05075479 epoch total loss 1.11547828\n",
      "Trained batch 971 batch loss 1.2577759 epoch total loss 1.1156249\n",
      "Trained batch 972 batch loss 1.03742671 epoch total loss 1.11554444\n",
      "Trained batch 973 batch loss 1.14506626 epoch total loss 1.11557472\n",
      "Trained batch 974 batch loss 1.03738713 epoch total loss 1.11549449\n",
      "Trained batch 975 batch loss 1.03869069 epoch total loss 1.11541569\n",
      "Trained batch 976 batch loss 1.07926524 epoch total loss 1.11537862\n",
      "Trained batch 977 batch loss 1.12373877 epoch total loss 1.1153872\n",
      "Trained batch 978 batch loss 1.05812442 epoch total loss 1.11532867\n",
      "Trained batch 979 batch loss 1.12417901 epoch total loss 1.11533761\n",
      "Trained batch 980 batch loss 1.1895498 epoch total loss 1.11541343\n",
      "Trained batch 981 batch loss 1.18195963 epoch total loss 1.11548126\n",
      "Trained batch 982 batch loss 1.09124207 epoch total loss 1.11545646\n",
      "Trained batch 983 batch loss 1.06743145 epoch total loss 1.11540759\n",
      "Trained batch 984 batch loss 0.938443 epoch total loss 1.11522782\n",
      "Trained batch 985 batch loss 1.01737571 epoch total loss 1.1151284\n",
      "Trained batch 986 batch loss 1.00143456 epoch total loss 1.11501312\n",
      "Trained batch 987 batch loss 1.07929111 epoch total loss 1.114977\n",
      "Trained batch 988 batch loss 1.16705108 epoch total loss 1.11502969\n",
      "Trained batch 989 batch loss 1.20887935 epoch total loss 1.11512458\n",
      "Trained batch 990 batch loss 1.0700022 epoch total loss 1.11507893\n",
      "Trained batch 991 batch loss 1.13646 epoch total loss 1.1151005\n",
      "Trained batch 992 batch loss 1.04822087 epoch total loss 1.11503303\n",
      "Trained batch 993 batch loss 1.08454669 epoch total loss 1.11500239\n",
      "Trained batch 994 batch loss 1.09635687 epoch total loss 1.11498356\n",
      "Trained batch 995 batch loss 1.11711991 epoch total loss 1.1149857\n",
      "Trained batch 996 batch loss 1.03075385 epoch total loss 1.11490119\n",
      "Trained batch 997 batch loss 1.01791453 epoch total loss 1.11480391\n",
      "Trained batch 998 batch loss 1.09586692 epoch total loss 1.11478484\n",
      "Trained batch 999 batch loss 1.20799756 epoch total loss 1.11487818\n",
      "Trained batch 1000 batch loss 1.23909235 epoch total loss 1.11500239\n",
      "Trained batch 1001 batch loss 1.19204724 epoch total loss 1.1150794\n",
      "Trained batch 1002 batch loss 1.29785991 epoch total loss 1.11526179\n",
      "Trained batch 1003 batch loss 1.19256246 epoch total loss 1.1153388\n",
      "Trained batch 1004 batch loss 1.19810319 epoch total loss 1.1154213\n",
      "Trained batch 1005 batch loss 1.12301207 epoch total loss 1.11542881\n",
      "Trained batch 1006 batch loss 1.24859977 epoch total loss 1.11556125\n",
      "Trained batch 1007 batch loss 1.28702259 epoch total loss 1.11573148\n",
      "Trained batch 1008 batch loss 1.12765646 epoch total loss 1.1157434\n",
      "Trained batch 1009 batch loss 1.13383436 epoch total loss 1.11576128\n",
      "Trained batch 1010 batch loss 1.08807397 epoch total loss 1.11573386\n",
      "Trained batch 1011 batch loss 1.14372277 epoch total loss 1.11576152\n",
      "Trained batch 1012 batch loss 1.17819118 epoch total loss 1.11582327\n",
      "Trained batch 1013 batch loss 1.0905273 epoch total loss 1.11579835\n",
      "Trained batch 1014 batch loss 1.18620384 epoch total loss 1.11586773\n",
      "Trained batch 1015 batch loss 1.17852402 epoch total loss 1.11592937\n",
      "Trained batch 1016 batch loss 1.08101726 epoch total loss 1.11589503\n",
      "Trained batch 1017 batch loss 1.08107924 epoch total loss 1.11586082\n",
      "Trained batch 1018 batch loss 1.0984683 epoch total loss 1.11584377\n",
      "Trained batch 1019 batch loss 1.09757781 epoch total loss 1.11582577\n",
      "Trained batch 1020 batch loss 1.09592128 epoch total loss 1.11580634\n",
      "Trained batch 1021 batch loss 1.09992087 epoch total loss 1.11579084\n",
      "Trained batch 1022 batch loss 0.962159395 epoch total loss 1.11564052\n",
      "Trained batch 1023 batch loss 0.958362281 epoch total loss 1.11548674\n",
      "Trained batch 1024 batch loss 1.02791274 epoch total loss 1.11540127\n",
      "Trained batch 1025 batch loss 1.01979637 epoch total loss 1.11530793\n",
      "Trained batch 1026 batch loss 0.98315 epoch total loss 1.11517918\n",
      "Trained batch 1027 batch loss 0.983674109 epoch total loss 1.11505115\n",
      "Trained batch 1028 batch loss 0.924904108 epoch total loss 1.11486614\n",
      "Trained batch 1029 batch loss 1.02165127 epoch total loss 1.11477554\n",
      "Trained batch 1030 batch loss 1.0470922 epoch total loss 1.11470985\n",
      "Trained batch 1031 batch loss 0.987380385 epoch total loss 1.11458635\n",
      "Trained batch 1032 batch loss 0.987175941 epoch total loss 1.11446297\n",
      "Trained batch 1033 batch loss 1.27970505 epoch total loss 1.11462283\n",
      "Trained batch 1034 batch loss 1.38485241 epoch total loss 1.11488426\n",
      "Trained batch 1035 batch loss 1.05676544 epoch total loss 1.11482811\n",
      "Trained batch 1036 batch loss 1.15475845 epoch total loss 1.11486661\n",
      "Trained batch 1037 batch loss 1.24621034 epoch total loss 1.11499333\n",
      "Trained batch 1038 batch loss 1.05664372 epoch total loss 1.11493707\n",
      "Trained batch 1039 batch loss 1.17083299 epoch total loss 1.11499083\n",
      "Trained batch 1040 batch loss 1.01799059 epoch total loss 1.11489749\n",
      "Trained batch 1041 batch loss 1.12848139 epoch total loss 1.1149106\n",
      "Trained batch 1042 batch loss 1.17148948 epoch total loss 1.11496496\n",
      "Trained batch 1043 batch loss 1.25915074 epoch total loss 1.11510313\n",
      "Trained batch 1044 batch loss 1.02047884 epoch total loss 1.11501253\n",
      "Trained batch 1045 batch loss 0.9363904 epoch total loss 1.1148417\n",
      "Trained batch 1046 batch loss 0.972419143 epoch total loss 1.11470544\n",
      "Trained batch 1047 batch loss 0.953580618 epoch total loss 1.11455166\n",
      "Trained batch 1048 batch loss 0.996034086 epoch total loss 1.11443853\n",
      "Trained batch 1049 batch loss 1.13419616 epoch total loss 1.11445737\n",
      "Trained batch 1050 batch loss 1.12879884 epoch total loss 1.11447108\n",
      "Trained batch 1051 batch loss 1.16969287 epoch total loss 1.11452353\n",
      "Trained batch 1052 batch loss 1.14787292 epoch total loss 1.11455524\n",
      "Trained batch 1053 batch loss 1.1433475 epoch total loss 1.11458254\n",
      "Trained batch 1054 batch loss 1.09348178 epoch total loss 1.11456251\n",
      "Trained batch 1055 batch loss 1.34807491 epoch total loss 1.11478376\n",
      "Trained batch 1056 batch loss 1.42911577 epoch total loss 1.11508143\n",
      "Trained batch 1057 batch loss 1.22752988 epoch total loss 1.11518788\n",
      "Trained batch 1058 batch loss 1.01554668 epoch total loss 1.11509359\n",
      "Trained batch 1059 batch loss 1.0790782 epoch total loss 1.11505961\n",
      "Trained batch 1060 batch loss 1.26136649 epoch total loss 1.11519766\n",
      "Trained batch 1061 batch loss 1.33273804 epoch total loss 1.1154027\n",
      "Trained batch 1062 batch loss 1.2609067 epoch total loss 1.11553967\n",
      "Trained batch 1063 batch loss 1.2120887 epoch total loss 1.11563039\n",
      "Trained batch 1064 batch loss 1.10808611 epoch total loss 1.11562335\n",
      "Trained batch 1065 batch loss 1.15922415 epoch total loss 1.11566424\n",
      "Trained batch 1066 batch loss 1.19078147 epoch total loss 1.1157347\n",
      "Trained batch 1067 batch loss 1.14636827 epoch total loss 1.11576343\n",
      "Trained batch 1068 batch loss 1.13636 epoch total loss 1.11578262\n",
      "Trained batch 1069 batch loss 1.05246377 epoch total loss 1.11572349\n",
      "Trained batch 1070 batch loss 1.14549816 epoch total loss 1.11575127\n",
      "Trained batch 1071 batch loss 1.17153239 epoch total loss 1.11580336\n",
      "Trained batch 1072 batch loss 1.26671302 epoch total loss 1.11594415\n",
      "Trained batch 1073 batch loss 1.23121953 epoch total loss 1.11605155\n",
      "Trained batch 1074 batch loss 1.06811523 epoch total loss 1.11600697\n",
      "Trained batch 1075 batch loss 1.08010578 epoch total loss 1.11597347\n",
      "Trained batch 1076 batch loss 1.07495105 epoch total loss 1.11593533\n",
      "Trained batch 1077 batch loss 1.12124622 epoch total loss 1.11594033\n",
      "Trained batch 1078 batch loss 1.19442987 epoch total loss 1.11601317\n",
      "Trained batch 1079 batch loss 1.24984276 epoch total loss 1.11613715\n",
      "Trained batch 1080 batch loss 1.27873945 epoch total loss 1.11628771\n",
      "Trained batch 1081 batch loss 1.27014136 epoch total loss 1.11643\n",
      "Trained batch 1082 batch loss 1.22840261 epoch total loss 1.11653352\n",
      "Trained batch 1083 batch loss 1.20367837 epoch total loss 1.11661398\n",
      "Trained batch 1084 batch loss 1.08788848 epoch total loss 1.11658752\n",
      "Trained batch 1085 batch loss 1.14198196 epoch total loss 1.11661088\n",
      "Trained batch 1086 batch loss 1.06553531 epoch total loss 1.11656392\n",
      "Trained batch 1087 batch loss 0.975843966 epoch total loss 1.11643445\n",
      "Trained batch 1088 batch loss 1.08386445 epoch total loss 1.11640453\n",
      "Trained batch 1089 batch loss 1.04188144 epoch total loss 1.11633599\n",
      "Trained batch 1090 batch loss 0.96591562 epoch total loss 1.11619806\n",
      "Trained batch 1091 batch loss 1.15971088 epoch total loss 1.11623788\n",
      "Trained batch 1092 batch loss 1.03388476 epoch total loss 1.11616254\n",
      "Trained batch 1093 batch loss 1.11052418 epoch total loss 1.11615729\n",
      "Trained batch 1094 batch loss 0.838114262 epoch total loss 1.11590314\n",
      "Trained batch 1095 batch loss 0.957093596 epoch total loss 1.11575818\n",
      "Trained batch 1096 batch loss 1.14026666 epoch total loss 1.11578059\n",
      "Trained batch 1097 batch loss 1.16961288 epoch total loss 1.11582959\n",
      "Trained batch 1098 batch loss 1.09311056 epoch total loss 1.11580896\n",
      "Trained batch 1099 batch loss 1.11618078 epoch total loss 1.11580932\n",
      "Trained batch 1100 batch loss 1.01318359 epoch total loss 1.11571598\n",
      "Trained batch 1101 batch loss 0.998553336 epoch total loss 1.11560953\n",
      "Trained batch 1102 batch loss 1.05300522 epoch total loss 1.11555278\n",
      "Trained batch 1103 batch loss 1.01939023 epoch total loss 1.11546552\n",
      "Trained batch 1104 batch loss 1.10373127 epoch total loss 1.11545491\n",
      "Trained batch 1105 batch loss 1.09853959 epoch total loss 1.11543965\n",
      "Trained batch 1106 batch loss 1.0614624 epoch total loss 1.1153909\n",
      "Trained batch 1107 batch loss 1.04996157 epoch total loss 1.11533177\n",
      "Trained batch 1108 batch loss 0.995768249 epoch total loss 1.11522377\n",
      "Trained batch 1109 batch loss 1.21144819 epoch total loss 1.11531055\n",
      "Trained batch 1110 batch loss 1.09125519 epoch total loss 1.11528897\n",
      "Trained batch 1111 batch loss 1.12943625 epoch total loss 1.11530161\n",
      "Trained batch 1112 batch loss 1.18971729 epoch total loss 1.11536849\n",
      "Trained batch 1113 batch loss 0.995897591 epoch total loss 1.11526108\n",
      "Trained batch 1114 batch loss 1.09187865 epoch total loss 1.11524022\n",
      "Trained batch 1115 batch loss 1.12655163 epoch total loss 1.11525035\n",
      "Trained batch 1116 batch loss 1.10753512 epoch total loss 1.11524343\n",
      "Trained batch 1117 batch loss 0.924817681 epoch total loss 1.11507297\n",
      "Trained batch 1118 batch loss 1.05608678 epoch total loss 1.11502016\n",
      "Trained batch 1119 batch loss 1.19924271 epoch total loss 1.11509538\n",
      "Trained batch 1120 batch loss 1.08915496 epoch total loss 1.11507225\n",
      "Trained batch 1121 batch loss 1.018484 epoch total loss 1.11498594\n",
      "Trained batch 1122 batch loss 1.14486718 epoch total loss 1.11501265\n",
      "Trained batch 1123 batch loss 1.01505208 epoch total loss 1.1149236\n",
      "Trained batch 1124 batch loss 0.923478246 epoch total loss 1.11475325\n",
      "Trained batch 1125 batch loss 0.863090038 epoch total loss 1.11452949\n",
      "Trained batch 1126 batch loss 0.941073358 epoch total loss 1.11437547\n",
      "Trained batch 1127 batch loss 0.909244299 epoch total loss 1.11419344\n",
      "Trained batch 1128 batch loss 1.08152318 epoch total loss 1.11416447\n",
      "Trained batch 1129 batch loss 1.03641629 epoch total loss 1.11409569\n",
      "Trained batch 1130 batch loss 1.26290917 epoch total loss 1.11422729\n",
      "Trained batch 1131 batch loss 1.07912517 epoch total loss 1.1141963\n",
      "Trained batch 1132 batch loss 1.11479533 epoch total loss 1.11419678\n",
      "Trained batch 1133 batch loss 1.20998859 epoch total loss 1.1142813\n",
      "Trained batch 1134 batch loss 1.2727356 epoch total loss 1.11442101\n",
      "Trained batch 1135 batch loss 1.06146634 epoch total loss 1.1143744\n",
      "Trained batch 1136 batch loss 0.978611708 epoch total loss 1.11425495\n",
      "Trained batch 1137 batch loss 1.15214586 epoch total loss 1.11428821\n",
      "Trained batch 1138 batch loss 1.18809867 epoch total loss 1.11435306\n",
      "Trained batch 1139 batch loss 1.27083671 epoch total loss 1.11449051\n",
      "Trained batch 1140 batch loss 1.06344712 epoch total loss 1.11444569\n",
      "Trained batch 1141 batch loss 1.12520516 epoch total loss 1.11445522\n",
      "Trained batch 1142 batch loss 1.1596663 epoch total loss 1.1144948\n",
      "Trained batch 1143 batch loss 1.08140016 epoch total loss 1.11446583\n",
      "Trained batch 1144 batch loss 1.13554299 epoch total loss 1.11448419\n",
      "Trained batch 1145 batch loss 1.30163825 epoch total loss 1.11464775\n",
      "Trained batch 1146 batch loss 1.26173925 epoch total loss 1.11477602\n",
      "Trained batch 1147 batch loss 1.15052342 epoch total loss 1.11480713\n",
      "Trained batch 1148 batch loss 0.985107839 epoch total loss 1.11469424\n",
      "Trained batch 1149 batch loss 1.04628789 epoch total loss 1.11463463\n",
      "Trained batch 1150 batch loss 1.27453017 epoch total loss 1.11477375\n",
      "Trained batch 1151 batch loss 1.36197519 epoch total loss 1.11498845\n",
      "Trained batch 1152 batch loss 1.10917282 epoch total loss 1.11498332\n",
      "Trained batch 1153 batch loss 1.08832574 epoch total loss 1.11496031\n",
      "Trained batch 1154 batch loss 1.11370397 epoch total loss 1.11495912\n",
      "Trained batch 1155 batch loss 1.06023586 epoch total loss 1.11491168\n",
      "Trained batch 1156 batch loss 1.18362153 epoch total loss 1.11497116\n",
      "Trained batch 1157 batch loss 1.09860766 epoch total loss 1.11495697\n",
      "Trained batch 1158 batch loss 1.21271443 epoch total loss 1.11504149\n",
      "Trained batch 1159 batch loss 1.35063958 epoch total loss 1.11524475\n",
      "Trained batch 1160 batch loss 1.34546852 epoch total loss 1.11544311\n",
      "Trained batch 1161 batch loss 1.330109 epoch total loss 1.115628\n",
      "Trained batch 1162 batch loss 1.18271422 epoch total loss 1.11568582\n",
      "Trained batch 1163 batch loss 1.2370187 epoch total loss 1.11579013\n",
      "Trained batch 1164 batch loss 1.04969585 epoch total loss 1.11573339\n",
      "Trained batch 1165 batch loss 1.26035953 epoch total loss 1.11585748\n",
      "Trained batch 1166 batch loss 1.15469158 epoch total loss 1.11589074\n",
      "Trained batch 1167 batch loss 1.19418764 epoch total loss 1.11595786\n",
      "Trained batch 1168 batch loss 1.11095214 epoch total loss 1.11595368\n",
      "Trained batch 1169 batch loss 1.06678605 epoch total loss 1.1159116\n",
      "Trained batch 1170 batch loss 1.04527736 epoch total loss 1.11585116\n",
      "Trained batch 1171 batch loss 1.13245559 epoch total loss 1.11586535\n",
      "Trained batch 1172 batch loss 1.21543741 epoch total loss 1.11595035\n",
      "Trained batch 1173 batch loss 1.08381546 epoch total loss 1.11592293\n",
      "Trained batch 1174 batch loss 1.22535586 epoch total loss 1.11601615\n",
      "Trained batch 1175 batch loss 1.04313755 epoch total loss 1.11595416\n",
      "Trained batch 1176 batch loss 1.18876791 epoch total loss 1.11601603\n",
      "Trained batch 1177 batch loss 1.11985672 epoch total loss 1.11601925\n",
      "Trained batch 1178 batch loss 1.10485291 epoch total loss 1.11600983\n",
      "Trained batch 1179 batch loss 1.04655087 epoch total loss 1.11595082\n",
      "Trained batch 1180 batch loss 1.1802597 epoch total loss 1.11600542\n",
      "Trained batch 1181 batch loss 1.22023833 epoch total loss 1.11609364\n",
      "Trained batch 1182 batch loss 1.13012636 epoch total loss 1.11610544\n",
      "Trained batch 1183 batch loss 1.32741022 epoch total loss 1.11628413\n",
      "Trained batch 1184 batch loss 1.19694591 epoch total loss 1.1163522\n",
      "Trained batch 1185 batch loss 1.31672025 epoch total loss 1.11652136\n",
      "Trained batch 1186 batch loss 1.23239291 epoch total loss 1.11661899\n",
      "Trained batch 1187 batch loss 1.27612114 epoch total loss 1.11675346\n",
      "Trained batch 1188 batch loss 1.0073384 epoch total loss 1.11666131\n",
      "Trained batch 1189 batch loss 1.07406497 epoch total loss 1.11662555\n",
      "Trained batch 1190 batch loss 1.0585705 epoch total loss 1.11657679\n",
      "Trained batch 1191 batch loss 0.944064677 epoch total loss 1.11643195\n",
      "Trained batch 1192 batch loss 1.04466748 epoch total loss 1.11637175\n",
      "Trained batch 1193 batch loss 1.0321703 epoch total loss 1.11630118\n",
      "Trained batch 1194 batch loss 1.23413348 epoch total loss 1.11639988\n",
      "Trained batch 1195 batch loss 1.2198807 epoch total loss 1.11648643\n",
      "Trained batch 1196 batch loss 1.17742133 epoch total loss 1.11653733\n",
      "Trained batch 1197 batch loss 1.35124898 epoch total loss 1.11673343\n",
      "Trained batch 1198 batch loss 1.28865576 epoch total loss 1.11687696\n",
      "Trained batch 1199 batch loss 1.25605559 epoch total loss 1.11699307\n",
      "Trained batch 1200 batch loss 1.156389 epoch total loss 1.11702585\n",
      "Trained batch 1201 batch loss 1.15786242 epoch total loss 1.11705983\n",
      "Trained batch 1202 batch loss 1.19451439 epoch total loss 1.1171242\n",
      "Trained batch 1203 batch loss 1.11086941 epoch total loss 1.11711895\n",
      "Trained batch 1204 batch loss 1.19597971 epoch total loss 1.1171844\n",
      "Trained batch 1205 batch loss 1.1920706 epoch total loss 1.11724651\n",
      "Trained batch 1206 batch loss 1.0991137 epoch total loss 1.11723149\n",
      "Trained batch 1207 batch loss 1.08025527 epoch total loss 1.11720085\n",
      "Trained batch 1208 batch loss 1.01190197 epoch total loss 1.11711371\n",
      "Trained batch 1209 batch loss 0.91108346 epoch total loss 1.11694336\n",
      "Trained batch 1210 batch loss 0.999909937 epoch total loss 1.11684656\n",
      "Trained batch 1211 batch loss 1.17460513 epoch total loss 1.11689425\n",
      "Trained batch 1212 batch loss 1.18710542 epoch total loss 1.11695218\n",
      "Trained batch 1213 batch loss 1.19673276 epoch total loss 1.11701798\n",
      "Trained batch 1214 batch loss 1.22593081 epoch total loss 1.11710775\n",
      "Trained batch 1215 batch loss 1.23350072 epoch total loss 1.11720359\n",
      "Trained batch 1216 batch loss 1.19416511 epoch total loss 1.11726689\n",
      "Trained batch 1217 batch loss 1.01774657 epoch total loss 1.11718512\n",
      "Trained batch 1218 batch loss 1.08286977 epoch total loss 1.11715698\n",
      "Trained batch 1219 batch loss 1.14681554 epoch total loss 1.1171813\n",
      "Trained batch 1220 batch loss 0.966664076 epoch total loss 1.11705792\n",
      "Trained batch 1221 batch loss 1.06489241 epoch total loss 1.11701524\n",
      "Trained batch 1222 batch loss 1.16311908 epoch total loss 1.11705291\n",
      "Trained batch 1223 batch loss 1.21848655 epoch total loss 1.11713588\n",
      "Trained batch 1224 batch loss 1.12421703 epoch total loss 1.11714172\n",
      "Trained batch 1225 batch loss 1.04952073 epoch total loss 1.11708653\n",
      "Trained batch 1226 batch loss 1.188393 epoch total loss 1.1171447\n",
      "Trained batch 1227 batch loss 1.08128369 epoch total loss 1.1171155\n",
      "Trained batch 1228 batch loss 1.05159104 epoch total loss 1.11706209\n",
      "Trained batch 1229 batch loss 1.06489575 epoch total loss 1.11701977\n",
      "Trained batch 1230 batch loss 1.03345656 epoch total loss 1.11695182\n",
      "Trained batch 1231 batch loss 1.14168191 epoch total loss 1.11697197\n",
      "Trained batch 1232 batch loss 1.17229617 epoch total loss 1.11701679\n",
      "Trained batch 1233 batch loss 1.0643599 epoch total loss 1.116974\n",
      "Trained batch 1234 batch loss 1.00410652 epoch total loss 1.11688256\n",
      "Trained batch 1235 batch loss 1.12519455 epoch total loss 1.11688936\n",
      "Trained batch 1236 batch loss 1.04882407 epoch total loss 1.11683428\n",
      "Trained batch 1237 batch loss 0.97473532 epoch total loss 1.11671948\n",
      "Trained batch 1238 batch loss 1.10438645 epoch total loss 1.11670947\n",
      "Trained batch 1239 batch loss 1.05102718 epoch total loss 1.11665642\n",
      "Trained batch 1240 batch loss 1.21570528 epoch total loss 1.11673629\n",
      "Trained batch 1241 batch loss 1.10576797 epoch total loss 1.11672747\n",
      "Trained batch 1242 batch loss 1.21701443 epoch total loss 1.11680818\n",
      "Trained batch 1243 batch loss 1.08371973 epoch total loss 1.11678159\n",
      "Trained batch 1244 batch loss 1.00465763 epoch total loss 1.11669147\n",
      "Trained batch 1245 batch loss 1.00342071 epoch total loss 1.11660051\n",
      "Trained batch 1246 batch loss 0.961662948 epoch total loss 1.11647618\n",
      "Trained batch 1247 batch loss 1.08073 epoch total loss 1.11644745\n",
      "Trained batch 1248 batch loss 1.13516486 epoch total loss 1.11646247\n",
      "Trained batch 1249 batch loss 1.15206325 epoch total loss 1.11649096\n",
      "Trained batch 1250 batch loss 1.23814619 epoch total loss 1.11658823\n",
      "Trained batch 1251 batch loss 1.12782049 epoch total loss 1.11659729\n",
      "Trained batch 1252 batch loss 1.25175738 epoch total loss 1.11670518\n",
      "Trained batch 1253 batch loss 1.19558549 epoch total loss 1.11676812\n",
      "Trained batch 1254 batch loss 1.3204267 epoch total loss 1.11693048\n",
      "Trained batch 1255 batch loss 1.24058914 epoch total loss 1.11702907\n",
      "Trained batch 1256 batch loss 1.29542124 epoch total loss 1.11717105\n",
      "Trained batch 1257 batch loss 1.17385221 epoch total loss 1.11721611\n",
      "Trained batch 1258 batch loss 1.06487131 epoch total loss 1.11717451\n",
      "Trained batch 1259 batch loss 1.07886863 epoch total loss 1.11714411\n",
      "Trained batch 1260 batch loss 1.00091016 epoch total loss 1.11705172\n",
      "Trained batch 1261 batch loss 0.984218538 epoch total loss 1.11694646\n",
      "Trained batch 1262 batch loss 1.06792355 epoch total loss 1.1169076\n",
      "Trained batch 1263 batch loss 1.12490809 epoch total loss 1.11691391\n",
      "Trained batch 1264 batch loss 1.20038 epoch total loss 1.11698\n",
      "Trained batch 1265 batch loss 1.29651189 epoch total loss 1.11712193\n",
      "Trained batch 1266 batch loss 1.21988392 epoch total loss 1.117203\n",
      "Trained batch 1267 batch loss 1.02431905 epoch total loss 1.11712968\n",
      "Trained batch 1268 batch loss 1.20930648 epoch total loss 1.1172024\n",
      "Trained batch 1269 batch loss 1.19421685 epoch total loss 1.11726308\n",
      "Trained batch 1270 batch loss 1.13041162 epoch total loss 1.11727345\n",
      "Trained batch 1271 batch loss 1.17285311 epoch total loss 1.1173172\n",
      "Trained batch 1272 batch loss 1.17870426 epoch total loss 1.11736548\n",
      "Trained batch 1273 batch loss 1.16736031 epoch total loss 1.1174047\n",
      "Trained batch 1274 batch loss 1.09026337 epoch total loss 1.11738336\n",
      "Trained batch 1275 batch loss 1.14992142 epoch total loss 1.11740887\n",
      "Trained batch 1276 batch loss 1.26383889 epoch total loss 1.11752355\n",
      "Trained batch 1277 batch loss 1.27781785 epoch total loss 1.11764908\n",
      "Trained batch 1278 batch loss 1.11896801 epoch total loss 1.11765015\n",
      "Trained batch 1279 batch loss 1.10163534 epoch total loss 1.11763775\n",
      "Trained batch 1280 batch loss 1.11949408 epoch total loss 1.11763918\n",
      "Trained batch 1281 batch loss 1.1658721 epoch total loss 1.11767685\n",
      "Trained batch 1282 batch loss 1.16426504 epoch total loss 1.11771321\n",
      "Trained batch 1283 batch loss 1.14863145 epoch total loss 1.11773729\n",
      "Trained batch 1284 batch loss 1.17871666 epoch total loss 1.11778486\n",
      "Trained batch 1285 batch loss 1.10608566 epoch total loss 1.11777568\n",
      "Trained batch 1286 batch loss 1.12292194 epoch total loss 1.11777973\n",
      "Trained batch 1287 batch loss 1.00304174 epoch total loss 1.11769056\n",
      "Trained batch 1288 batch loss 1.18618286 epoch total loss 1.11774373\n",
      "Trained batch 1289 batch loss 1.10233855 epoch total loss 1.11773169\n",
      "Trained batch 1290 batch loss 1.03402388 epoch total loss 1.11766684\n",
      "Trained batch 1291 batch loss 1.07397318 epoch total loss 1.11763299\n",
      "Trained batch 1292 batch loss 1.20979893 epoch total loss 1.11770439\n",
      "Trained batch 1293 batch loss 1.20978737 epoch total loss 1.11777568\n",
      "Trained batch 1294 batch loss 1.3169055 epoch total loss 1.11792958\n",
      "Trained batch 1295 batch loss 1.25648844 epoch total loss 1.11803651\n",
      "Trained batch 1296 batch loss 1.22552228 epoch total loss 1.11811936\n",
      "Trained batch 1297 batch loss 1.19036436 epoch total loss 1.11817503\n",
      "Trained batch 1298 batch loss 0.915524125 epoch total loss 1.11801898\n",
      "Trained batch 1299 batch loss 0.96712 epoch total loss 1.11790287\n",
      "Trained batch 1300 batch loss 0.87630713 epoch total loss 1.11771703\n",
      "Trained batch 1301 batch loss 0.966063499 epoch total loss 1.11760044\n",
      "Trained batch 1302 batch loss 1.21989465 epoch total loss 1.117679\n",
      "Trained batch 1303 batch loss 1.20954561 epoch total loss 1.11774945\n",
      "Trained batch 1304 batch loss 1.17906165 epoch total loss 1.11779654\n",
      "Trained batch 1305 batch loss 1.17452431 epoch total loss 1.11784\n",
      "Trained batch 1306 batch loss 1.27820969 epoch total loss 1.11796284\n",
      "Trained batch 1307 batch loss 1.20578372 epoch total loss 1.11803007\n",
      "Trained batch 1308 batch loss 1.19932687 epoch total loss 1.11809218\n",
      "Trained batch 1309 batch loss 0.972921729 epoch total loss 1.11798131\n",
      "Trained batch 1310 batch loss 1.07407939 epoch total loss 1.11794782\n",
      "Trained batch 1311 batch loss 1.16986394 epoch total loss 1.11798739\n",
      "Trained batch 1312 batch loss 1.16806006 epoch total loss 1.11802566\n",
      "Trained batch 1313 batch loss 1.13698602 epoch total loss 1.11804008\n",
      "Trained batch 1314 batch loss 1.18197525 epoch total loss 1.11808872\n",
      "Trained batch 1315 batch loss 1.13576388 epoch total loss 1.11810219\n",
      "Trained batch 1316 batch loss 1.23856401 epoch total loss 1.11819363\n",
      "Trained batch 1317 batch loss 1.1179899 epoch total loss 1.11819351\n",
      "Trained batch 1318 batch loss 1.05237436 epoch total loss 1.11814356\n",
      "Trained batch 1319 batch loss 0.924431741 epoch total loss 1.11799669\n",
      "Trained batch 1320 batch loss 1.03231597 epoch total loss 1.11793184\n",
      "Trained batch 1321 batch loss 1.20235062 epoch total loss 1.11799574\n",
      "Trained batch 1322 batch loss 1.08062887 epoch total loss 1.11796761\n",
      "Trained batch 1323 batch loss 1.07503855 epoch total loss 1.11793518\n",
      "Trained batch 1324 batch loss 1.04612422 epoch total loss 1.11788094\n",
      "Trained batch 1325 batch loss 1.27360833 epoch total loss 1.11799836\n",
      "Trained batch 1326 batch loss 1.05722356 epoch total loss 1.11795259\n",
      "Trained batch 1327 batch loss 1.2165271 epoch total loss 1.11802685\n",
      "Trained batch 1328 batch loss 1.24262559 epoch total loss 1.11812079\n",
      "Trained batch 1329 batch loss 1.10883248 epoch total loss 1.11811388\n",
      "Trained batch 1330 batch loss 1.2341584 epoch total loss 1.11820102\n",
      "Trained batch 1331 batch loss 1.19944394 epoch total loss 1.11826205\n",
      "Trained batch 1332 batch loss 1.31713176 epoch total loss 1.11841142\n",
      "Trained batch 1333 batch loss 1.23278093 epoch total loss 1.11849725\n",
      "Trained batch 1334 batch loss 1.10924602 epoch total loss 1.11849034\n",
      "Trained batch 1335 batch loss 1.25915682 epoch total loss 1.1185956\n",
      "Trained batch 1336 batch loss 1.16513312 epoch total loss 1.11863053\n",
      "Trained batch 1337 batch loss 1.21518183 epoch total loss 1.11870277\n",
      "Trained batch 1338 batch loss 1.1994766 epoch total loss 1.11876309\n",
      "Trained batch 1339 batch loss 1.09445667 epoch total loss 1.11874497\n",
      "Trained batch 1340 batch loss 1.15242267 epoch total loss 1.11877012\n",
      "Trained batch 1341 batch loss 1.02905965 epoch total loss 1.11870325\n",
      "Trained batch 1342 batch loss 1.14527702 epoch total loss 1.11872303\n",
      "Trained batch 1343 batch loss 1.2649188 epoch total loss 1.11883187\n",
      "Trained batch 1344 batch loss 1.11416459 epoch total loss 1.11882842\n",
      "Trained batch 1345 batch loss 1.24658549 epoch total loss 1.11892331\n",
      "Trained batch 1346 batch loss 1.17346323 epoch total loss 1.11896384\n",
      "Trained batch 1347 batch loss 1.26780677 epoch total loss 1.11907434\n",
      "Trained batch 1348 batch loss 1.20668364 epoch total loss 1.11913931\n",
      "Trained batch 1349 batch loss 1.09700334 epoch total loss 1.11912298\n",
      "Trained batch 1350 batch loss 1.10961187 epoch total loss 1.11911595\n",
      "Trained batch 1351 batch loss 1.19383574 epoch total loss 1.11917126\n",
      "Trained batch 1352 batch loss 1.0616039 epoch total loss 1.1191287\n",
      "Trained batch 1353 batch loss 0.985639095 epoch total loss 1.11903\n",
      "Trained batch 1354 batch loss 0.942209542 epoch total loss 1.11889946\n",
      "Trained batch 1355 batch loss 1.10066581 epoch total loss 1.11888599\n",
      "Trained batch 1356 batch loss 1.011935 epoch total loss 1.1188072\n",
      "Trained batch 1357 batch loss 0.992595553 epoch total loss 1.11871409\n",
      "Trained batch 1358 batch loss 1.01351094 epoch total loss 1.11863673\n",
      "Trained batch 1359 batch loss 1.15534163 epoch total loss 1.11866379\n",
      "Trained batch 1360 batch loss 1.13477099 epoch total loss 1.11867559\n",
      "Trained batch 1361 batch loss 1.09755051 epoch total loss 1.11866009\n",
      "Trained batch 1362 batch loss 1.04645479 epoch total loss 1.11860704\n",
      "Trained batch 1363 batch loss 1.09785247 epoch total loss 1.1185919\n",
      "Trained batch 1364 batch loss 1.18556476 epoch total loss 1.11864102\n",
      "Trained batch 1365 batch loss 1.19623017 epoch total loss 1.11869788\n",
      "Trained batch 1366 batch loss 1.12589741 epoch total loss 1.11870313\n",
      "Trained batch 1367 batch loss 1.12246 epoch total loss 1.11870587\n",
      "Trained batch 1368 batch loss 1.13347054 epoch total loss 1.1187166\n",
      "Trained batch 1369 batch loss 1.07496095 epoch total loss 1.11868465\n",
      "Trained batch 1370 batch loss 1.02038419 epoch total loss 1.11861289\n",
      "Trained batch 1371 batch loss 1.0599978 epoch total loss 1.11857021\n",
      "Trained batch 1372 batch loss 1.17239177 epoch total loss 1.11860943\n",
      "Trained batch 1373 batch loss 1.04042876 epoch total loss 1.11855245\n",
      "Trained batch 1374 batch loss 0.970189691 epoch total loss 1.11844444\n",
      "Trained batch 1375 batch loss 1.08205914 epoch total loss 1.11841798\n",
      "Trained batch 1376 batch loss 1.06911206 epoch total loss 1.1183821\n",
      "Trained batch 1377 batch loss 1.07530713 epoch total loss 1.11835086\n",
      "Trained batch 1378 batch loss 1.00230145 epoch total loss 1.1182667\n",
      "Trained batch 1379 batch loss 1.00057578 epoch total loss 1.11818135\n",
      "Trained batch 1380 batch loss 1.09563291 epoch total loss 1.1181649\n",
      "Trained batch 1381 batch loss 1.18625593 epoch total loss 1.11821425\n",
      "Trained batch 1382 batch loss 1.0830307 epoch total loss 1.11818874\n",
      "Trained batch 1383 batch loss 1.21007824 epoch total loss 1.11825526\n",
      "Trained batch 1384 batch loss 1.20429707 epoch total loss 1.11831748\n",
      "Trained batch 1385 batch loss 1.16438007 epoch total loss 1.11835074\n",
      "Trained batch 1386 batch loss 1.27596545 epoch total loss 1.11846447\n",
      "Trained batch 1387 batch loss 1.13531137 epoch total loss 1.11847663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 04:16:31.127352: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "2025-03-19 04:16:31.127401: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "\t [[RemoteCall]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1388 batch loss 1.16864634 epoch total loss 1.11851275\n",
      "Epoch 7 train loss 1.118512749671936\n",
      "Validated batch 1 batch loss 1.04615188\n",
      "Validated batch 2 batch loss 1.1203661\n",
      "Validated batch 3 batch loss 1.15088844\n",
      "Validated batch 4 batch loss 1.0968461\n",
      "Validated batch 5 batch loss 1.23021114\n",
      "Validated batch 6 batch loss 1.25809073\n",
      "Validated batch 7 batch loss 0.984818518\n",
      "Validated batch 8 batch loss 1.13152122\n",
      "Validated batch 9 batch loss 1.11634243\n",
      "Validated batch 10 batch loss 1.22992921\n",
      "Validated batch 11 batch loss 1.12399924\n",
      "Validated batch 12 batch loss 0.979240716\n",
      "Validated batch 13 batch loss 1.04127\n",
      "Validated batch 14 batch loss 1.06887615\n",
      "Validated batch 15 batch loss 1.06654477\n",
      "Validated batch 16 batch loss 1.1565125\n",
      "Validated batch 17 batch loss 1.11119223\n",
      "Validated batch 18 batch loss 1.02309215\n",
      "Validated batch 19 batch loss 1.15665817\n",
      "Validated batch 20 batch loss 1.16221869\n",
      "Validated batch 21 batch loss 1.21675038\n",
      "Validated batch 22 batch loss 1.13687181\n",
      "Validated batch 23 batch loss 1.01250505\n",
      "Validated batch 24 batch loss 1.04010487\n",
      "Validated batch 25 batch loss 1.05228722\n",
      "Validated batch 26 batch loss 1.10556293\n",
      "Validated batch 27 batch loss 1.07562673\n",
      "Validated batch 28 batch loss 1.10837221\n",
      "Validated batch 29 batch loss 1.14921176\n",
      "Validated batch 30 batch loss 1.17837286\n",
      "Validated batch 31 batch loss 1.10257602\n",
      "Validated batch 32 batch loss 1.11292982\n",
      "Validated batch 33 batch loss 1.11288083\n",
      "Validated batch 34 batch loss 1.14997554\n",
      "Validated batch 35 batch loss 1.15589046\n",
      "Validated batch 36 batch loss 1.09317815\n",
      "Validated batch 37 batch loss 1.15130162\n",
      "Validated batch 38 batch loss 1.141433\n",
      "Validated batch 39 batch loss 1.10101104\n",
      "Validated batch 40 batch loss 1.25817657\n",
      "Validated batch 41 batch loss 1.20685148\n",
      "Validated batch 42 batch loss 1.02157843\n",
      "Validated batch 43 batch loss 1.23119211\n",
      "Validated batch 44 batch loss 1.10634053\n",
      "Validated batch 45 batch loss 1.04890811\n",
      "Validated batch 46 batch loss 1.17393625\n",
      "Validated batch 47 batch loss 1.1685065\n",
      "Validated batch 48 batch loss 1.16968513\n",
      "Validated batch 49 batch loss 1.10678101\n",
      "Validated batch 50 batch loss 1.15462947\n",
      "Validated batch 51 batch loss 1.19512904\n",
      "Validated batch 52 batch loss 1.33146107\n",
      "Validated batch 53 batch loss 1.11588836\n",
      "Validated batch 54 batch loss 1.17006683\n",
      "Validated batch 55 batch loss 1.10552907\n",
      "Validated batch 56 batch loss 1.17740202\n",
      "Validated batch 57 batch loss 1.17477942\n",
      "Validated batch 58 batch loss 1.05101728\n",
      "Validated batch 59 batch loss 1.33598065\n",
      "Validated batch 60 batch loss 1.08165884\n",
      "Validated batch 61 batch loss 1.27426696\n",
      "Validated batch 62 batch loss 1.14432335\n",
      "Validated batch 63 batch loss 1.26380312\n",
      "Validated batch 64 batch loss 1.04902768\n",
      "Validated batch 65 batch loss 1.1586771\n",
      "Validated batch 66 batch loss 1.09578681\n",
      "Validated batch 67 batch loss 1.1107403\n",
      "Validated batch 68 batch loss 1.19549608\n",
      "Validated batch 69 batch loss 1.11784828\n",
      "Validated batch 70 batch loss 1.29821694\n",
      "Validated batch 71 batch loss 1.14240265\n",
      "Validated batch 72 batch loss 1.0978272\n",
      "Validated batch 73 batch loss 1.11012673\n",
      "Validated batch 74 batch loss 1.05659163\n",
      "Validated batch 75 batch loss 1.14628232\n",
      "Validated batch 76 batch loss 1.15402889\n",
      "Validated batch 77 batch loss 0.993401825\n",
      "Validated batch 78 batch loss 1.10013068\n",
      "Validated batch 79 batch loss 1.12864292\n",
      "Validated batch 80 batch loss 1.16549671\n",
      "Validated batch 81 batch loss 1.21925879\n",
      "Validated batch 82 batch loss 1.17480075\n",
      "Validated batch 83 batch loss 1.05228519\n",
      "Validated batch 84 batch loss 1.0521822\n",
      "Validated batch 85 batch loss 1.15123391\n",
      "Validated batch 86 batch loss 1.10985816\n",
      "Validated batch 87 batch loss 1.14667225\n",
      "Validated batch 88 batch loss 1.14489841\n",
      "Validated batch 89 batch loss 1.34014225\n",
      "Validated batch 90 batch loss 1.20517766\n",
      "Validated batch 91 batch loss 1.16773164\n",
      "Validated batch 92 batch loss 1.05070508\n",
      "Validated batch 93 batch loss 1.00384533\n",
      "Validated batch 94 batch loss 1.15328658\n",
      "Validated batch 95 batch loss 1.15325773\n",
      "Validated batch 96 batch loss 1.03031778\n",
      "Validated batch 97 batch loss 1.0719533\n",
      "Validated batch 98 batch loss 1.25982237\n",
      "Validated batch 99 batch loss 1.03265679\n",
      "Validated batch 100 batch loss 1.03844094\n",
      "Validated batch 101 batch loss 1.07238245\n",
      "Validated batch 102 batch loss 1.09492242\n",
      "Validated batch 103 batch loss 1.01596773\n",
      "Validated batch 104 batch loss 1.12780762\n",
      "Validated batch 105 batch loss 1.11696196\n",
      "Validated batch 106 batch loss 1.06322432\n",
      "Validated batch 107 batch loss 1.21726072\n",
      "Validated batch 108 batch loss 1.24239647\n",
      "Validated batch 109 batch loss 1.05559015\n",
      "Validated batch 110 batch loss 1.22634125\n",
      "Validated batch 111 batch loss 0.941714466\n",
      "Validated batch 112 batch loss 1.09884298\n",
      "Validated batch 113 batch loss 1.08014464\n",
      "Validated batch 114 batch loss 1.08771873\n",
      "Validated batch 115 batch loss 1.26772714\n",
      "Validated batch 116 batch loss 1.20511687\n",
      "Validated batch 117 batch loss 1.16624546\n",
      "Validated batch 118 batch loss 1.06421149\n",
      "Validated batch 119 batch loss 1.10046089\n",
      "Validated batch 120 batch loss 1.13231063\n",
      "Validated batch 121 batch loss 1.13285971\n",
      "Validated batch 122 batch loss 1.19874358\n",
      "Validated batch 123 batch loss 1.10594106\n",
      "Validated batch 124 batch loss 1.08798504\n",
      "Validated batch 125 batch loss 1.27368069\n",
      "Validated batch 126 batch loss 1.06637561\n",
      "Validated batch 127 batch loss 1.04705811\n",
      "Validated batch 128 batch loss 1.11414337\n",
      "Validated batch 129 batch loss 1.25971198\n",
      "Validated batch 130 batch loss 1.26224279\n",
      "Validated batch 131 batch loss 1.32541454\n",
      "Validated batch 132 batch loss 1.10781991\n",
      "Validated batch 133 batch loss 1.32910061\n",
      "Validated batch 134 batch loss 1.15051341\n",
      "Validated batch 135 batch loss 1.22391939\n",
      "Validated batch 136 batch loss 1.21743906\n",
      "Validated batch 137 batch loss 0.90499711\n",
      "Validated batch 138 batch loss 1.0723629\n",
      "Validated batch 139 batch loss 1.14260435\n",
      "Validated batch 140 batch loss 1.16017795\n",
      "Validated batch 141 batch loss 1.16638458\n",
      "Validated batch 142 batch loss 1.00234604\n",
      "Validated batch 143 batch loss 1.14938378\n",
      "Validated batch 144 batch loss 1.15002942\n",
      "Validated batch 145 batch loss 1.23059082\n",
      "Validated batch 146 batch loss 1.27460217\n",
      "Validated batch 147 batch loss 1.20310402\n",
      "Validated batch 148 batch loss 1.08681083\n",
      "Validated batch 149 batch loss 1.15892196\n",
      "Validated batch 150 batch loss 1.18199933\n",
      "Validated batch 151 batch loss 1.14096808\n",
      "Validated batch 152 batch loss 1.21630442\n",
      "Validated batch 153 batch loss 1.2697196\n",
      "Validated batch 154 batch loss 1.15299\n",
      "Validated batch 155 batch loss 1.26794672\n",
      "Validated batch 156 batch loss 1.12260056\n",
      "Validated batch 157 batch loss 1.10607886\n",
      "Validated batch 158 batch loss 1.11009228\n",
      "Validated batch 159 batch loss 1.04361916\n",
      "Validated batch 160 batch loss 1.25177777\n",
      "Validated batch 161 batch loss 1.10266578\n",
      "Validated batch 162 batch loss 1.13196504\n",
      "Validated batch 163 batch loss 1.03750563\n",
      "Validated batch 164 batch loss 1.13238168\n",
      "Validated batch 165 batch loss 1.10256171\n",
      "Validated batch 166 batch loss 1.09513986\n",
      "Validated batch 167 batch loss 1.17807913\n",
      "Validated batch 168 batch loss 1.16047573\n",
      "Validated batch 169 batch loss 1.21337891\n",
      "Validated batch 170 batch loss 1.21846676\n",
      "Validated batch 171 batch loss 1.17838609\n",
      "Validated batch 172 batch loss 1.1312139\n",
      "Validated batch 173 batch loss 1.33511925\n",
      "Validated batch 174 batch loss 1.19189012\n",
      "Validated batch 175 batch loss 1.26060462\n",
      "Validated batch 176 batch loss 1.21208513\n",
      "Validated batch 177 batch loss 1.28251755\n",
      "Validated batch 178 batch loss 1.23531628\n",
      "Validated batch 179 batch loss 1.09061468\n",
      "Validated batch 180 batch loss 1.1322608\n",
      "Validated batch 181 batch loss 1.23060513\n",
      "Validated batch 182 batch loss 1.27607822\n",
      "Validated batch 183 batch loss 1.05894053\n",
      "Validated batch 184 batch loss 1.19543028\n",
      "Validated batch 185 batch loss 1.09811902\n",
      "Epoch 7 val loss 1.142309546470642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 04:16:47.042937: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "2025-03-19 04:16:47.042984: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "\t [[RemoteCall]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model /home/minho/Desktop/aiffel/pose_estimation/models/model-epoch-7-loss-1.1423.weights.h5 saved.\n",
      "Start epoch 8 with learning rate 0.001\n",
      "Start distributed training...\n",
      "Trained batch 1 batch loss 1.14415693 epoch total loss 1.14415693\n",
      "Trained batch 2 batch loss 1.04603589 epoch total loss 1.09509635\n",
      "Trained batch 3 batch loss 1.20990109 epoch total loss 1.13336456\n",
      "Trained batch 4 batch loss 1.27292573 epoch total loss 1.16825485\n",
      "Trained batch 5 batch loss 1.32292473 epoch total loss 1.19918883\n",
      "Trained batch 6 batch loss 1.24908686 epoch total loss 1.20750511\n",
      "Trained batch 7 batch loss 1.12966335 epoch total loss 1.19638479\n",
      "Trained batch 8 batch loss 1.14244378 epoch total loss 1.18964219\n",
      "Trained batch 9 batch loss 1.10471404 epoch total loss 1.18020582\n",
      "Trained batch 10 batch loss 1.17317 epoch total loss 1.17950225\n",
      "Trained batch 11 batch loss 1.10653389 epoch total loss 1.17286873\n",
      "Trained batch 12 batch loss 1.16370499 epoch total loss 1.17210507\n",
      "Trained batch 13 batch loss 1.27236533 epoch total loss 1.17981744\n",
      "Trained batch 14 batch loss 1.28468978 epoch total loss 1.18730831\n",
      "Trained batch 15 batch loss 1.24913466 epoch total loss 1.19143\n",
      "Trained batch 16 batch loss 1.25562584 epoch total loss 1.19544232\n",
      "Trained batch 17 batch loss 1.31868207 epoch total loss 1.20269167\n",
      "Trained batch 18 batch loss 1.26165104 epoch total loss 1.20596719\n",
      "Trained batch 19 batch loss 1.21995544 epoch total loss 1.20670342\n",
      "Trained batch 20 batch loss 1.30008209 epoch total loss 1.21137226\n",
      "Trained batch 21 batch loss 1.23726511 epoch total loss 1.21260524\n",
      "Trained batch 22 batch loss 1.23345172 epoch total loss 1.21355283\n",
      "Trained batch 23 batch loss 1.1287396 epoch total loss 1.20986533\n",
      "Trained batch 24 batch loss 1.05354571 epoch total loss 1.20335197\n",
      "Trained batch 25 batch loss 1.08037543 epoch total loss 1.19843292\n",
      "Trained batch 26 batch loss 1.08000815 epoch total loss 1.19387805\n",
      "Trained batch 27 batch loss 1.05884993 epoch total loss 1.18887711\n",
      "Trained batch 28 batch loss 1.03341436 epoch total loss 1.18332481\n",
      "Trained batch 29 batch loss 1.0366267 epoch total loss 1.17826617\n",
      "Trained batch 30 batch loss 1.09830761 epoch total loss 1.17560089\n",
      "Trained batch 31 batch loss 1.11230946 epoch total loss 1.17355931\n",
      "Trained batch 32 batch loss 1.1773591 epoch total loss 1.17367804\n",
      "Trained batch 33 batch loss 1.07621956 epoch total loss 1.17072487\n",
      "Trained batch 34 batch loss 0.962871134 epoch total loss 1.16461146\n",
      "Trained batch 35 batch loss 1.0763191 epoch total loss 1.16208887\n",
      "Trained batch 36 batch loss 1.13794291 epoch total loss 1.1614182\n",
      "Trained batch 37 batch loss 1.03432906 epoch total loss 1.1579833\n",
      "Trained batch 38 batch loss 1.0834552 epoch total loss 1.15602207\n",
      "Trained batch 39 batch loss 1.13342977 epoch total loss 1.15544271\n",
      "Trained batch 40 batch loss 0.9668172 epoch total loss 1.15072703\n",
      "Trained batch 41 batch loss 1.01658487 epoch total loss 1.14745533\n",
      "Trained batch 42 batch loss 1.0943495 epoch total loss 1.14619088\n",
      "Trained batch 43 batch loss 1.01589251 epoch total loss 1.1431607\n",
      "Trained batch 44 batch loss 1.1754247 epoch total loss 1.14389408\n",
      "Trained batch 45 batch loss 1.23819685 epoch total loss 1.14598966\n",
      "Trained batch 46 batch loss 1.14996076 epoch total loss 1.14607596\n",
      "Trained batch 47 batch loss 1.09401619 epoch total loss 1.14496827\n",
      "Trained batch 48 batch loss 1.08668768 epoch total loss 1.14375412\n",
      "Trained batch 49 batch loss 1.11748874 epoch total loss 1.14321816\n",
      "Trained batch 50 batch loss 1.14926386 epoch total loss 1.14333904\n",
      "Trained batch 51 batch loss 1.04528642 epoch total loss 1.14141655\n",
      "Trained batch 52 batch loss 1.09815335 epoch total loss 1.14058447\n",
      "Trained batch 53 batch loss 1.2562983 epoch total loss 1.14276779\n",
      "Trained batch 54 batch loss 1.09646225 epoch total loss 1.14191031\n",
      "Trained batch 55 batch loss 1.185431 epoch total loss 1.14270163\n",
      "Trained batch 56 batch loss 1.10362685 epoch total loss 1.14200377\n",
      "Trained batch 57 batch loss 1.04581618 epoch total loss 1.14031637\n",
      "Trained batch 58 batch loss 0.987750769 epoch total loss 1.13768589\n",
      "Trained batch 59 batch loss 1.04496872 epoch total loss 1.13611436\n",
      "Trained batch 60 batch loss 0.978525043 epoch total loss 1.13348782\n",
      "Trained batch 61 batch loss 1.02965832 epoch total loss 1.13178563\n",
      "Trained batch 62 batch loss 0.966112614 epoch total loss 1.12911344\n",
      "Trained batch 63 batch loss 1.05465031 epoch total loss 1.12793148\n",
      "Trained batch 64 batch loss 1.06310463 epoch total loss 1.12691855\n",
      "Trained batch 65 batch loss 1.05965948 epoch total loss 1.12588382\n",
      "Trained batch 66 batch loss 1.14679074 epoch total loss 1.12620056\n",
      "Trained batch 67 batch loss 1.16249526 epoch total loss 1.12674236\n",
      "Trained batch 68 batch loss 1.04974496 epoch total loss 1.12561\n",
      "Trained batch 69 batch loss 0.987099767 epoch total loss 1.12360263\n",
      "Trained batch 70 batch loss 1.00875616 epoch total loss 1.12196195\n",
      "Trained batch 71 batch loss 1.10995376 epoch total loss 1.12179291\n",
      "Trained batch 72 batch loss 1.13013899 epoch total loss 1.12190878\n",
      "Trained batch 73 batch loss 1.18062091 epoch total loss 1.12271309\n",
      "Trained batch 74 batch loss 1.07092106 epoch total loss 1.12201321\n",
      "Trained batch 75 batch loss 1.18174505 epoch total loss 1.12280965\n",
      "Trained batch 76 batch loss 1.15932751 epoch total loss 1.12329006\n",
      "Trained batch 77 batch loss 1.17279804 epoch total loss 1.12393308\n",
      "Trained batch 78 batch loss 1.21360672 epoch total loss 1.12508273\n",
      "Trained batch 79 batch loss 1.29731345 epoch total loss 1.12726283\n",
      "Trained batch 80 batch loss 1.06330538 epoch total loss 1.12646341\n",
      "Trained batch 81 batch loss 1.00176859 epoch total loss 1.12492394\n",
      "Trained batch 82 batch loss 1.07292163 epoch total loss 1.12428987\n",
      "Trained batch 83 batch loss 1.08706975 epoch total loss 1.12384129\n",
      "Trained batch 84 batch loss 0.952721477 epoch total loss 1.12180424\n",
      "Trained batch 85 batch loss 0.999314904 epoch total loss 1.12036312\n",
      "Trained batch 86 batch loss 1.11299753 epoch total loss 1.12027752\n",
      "Trained batch 87 batch loss 1.11606383 epoch total loss 1.12022913\n",
      "Trained batch 88 batch loss 1.10708356 epoch total loss 1.12007976\n",
      "Trained batch 89 batch loss 1.22431147 epoch total loss 1.12125087\n",
      "Trained batch 90 batch loss 1.18182814 epoch total loss 1.12192404\n",
      "Trained batch 91 batch loss 1.13893282 epoch total loss 1.12211084\n",
      "Trained batch 92 batch loss 1.14356589 epoch total loss 1.12234402\n",
      "Trained batch 93 batch loss 1.27752984 epoch total loss 1.12401271\n",
      "Trained batch 94 batch loss 1.12885618 epoch total loss 1.12406421\n",
      "Trained batch 95 batch loss 1.16775036 epoch total loss 1.124524\n",
      "Trained batch 96 batch loss 1.10452986 epoch total loss 1.12431574\n",
      "Trained batch 97 batch loss 1.16805649 epoch total loss 1.12476671\n",
      "Trained batch 98 batch loss 1.16723871 epoch total loss 1.1252\n",
      "Trained batch 99 batch loss 1.22616684 epoch total loss 1.12622\n",
      "Trained batch 100 batch loss 1.14396894 epoch total loss 1.12639737\n",
      "Trained batch 101 batch loss 1.12457132 epoch total loss 1.12637937\n",
      "Trained batch 102 batch loss 1.00210476 epoch total loss 1.12516093\n",
      "Trained batch 103 batch loss 0.964902401 epoch total loss 1.12360513\n",
      "Trained batch 104 batch loss 0.972059965 epoch total loss 1.12214792\n",
      "Trained batch 105 batch loss 0.989811 epoch total loss 1.12088764\n",
      "Trained batch 106 batch loss 1.08190084 epoch total loss 1.12051988\n",
      "Trained batch 107 batch loss 1.14645982 epoch total loss 1.12076223\n",
      "Trained batch 108 batch loss 1.09394646 epoch total loss 1.12051404\n",
      "Trained batch 109 batch loss 1.12338877 epoch total loss 1.12054038\n",
      "Trained batch 110 batch loss 1.04775357 epoch total loss 1.11987865\n",
      "Trained batch 111 batch loss 1.13464975 epoch total loss 1.12001181\n",
      "Trained batch 112 batch loss 1.10092664 epoch total loss 1.11984134\n",
      "Trained batch 113 batch loss 1.10367835 epoch total loss 1.11969829\n",
      "Trained batch 114 batch loss 1.1508286 epoch total loss 1.11997139\n",
      "Trained batch 115 batch loss 1.07700038 epoch total loss 1.11959767\n",
      "Trained batch 116 batch loss 1.06149411 epoch total loss 1.11909688\n",
      "Trained batch 117 batch loss 1.10249043 epoch total loss 1.1189549\n",
      "Trained batch 118 batch loss 1.16485667 epoch total loss 1.11934388\n",
      "Trained batch 119 batch loss 0.992016554 epoch total loss 1.11827397\n",
      "Trained batch 120 batch loss 0.963605106 epoch total loss 1.11698508\n",
      "Trained batch 121 batch loss 1.04352343 epoch total loss 1.11637795\n",
      "Trained batch 122 batch loss 1.11473596 epoch total loss 1.11636436\n",
      "Trained batch 123 batch loss 1.11540151 epoch total loss 1.11635661\n",
      "Trained batch 124 batch loss 1.07744658 epoch total loss 1.11604285\n",
      "Trained batch 125 batch loss 1.01400506 epoch total loss 1.11522651\n",
      "Trained batch 126 batch loss 0.849229753 epoch total loss 1.11311543\n",
      "Trained batch 127 batch loss 0.843319416 epoch total loss 1.11099112\n",
      "Trained batch 128 batch loss 0.838415742 epoch total loss 1.10886157\n",
      "Trained batch 129 batch loss 0.955485463 epoch total loss 1.10767269\n",
      "Trained batch 130 batch loss 0.951440454 epoch total loss 1.10647094\n",
      "Trained batch 131 batch loss 0.888366461 epoch total loss 1.10480595\n",
      "Trained batch 132 batch loss 1.09072828 epoch total loss 1.10469937\n",
      "Trained batch 133 batch loss 1.05238843 epoch total loss 1.10430598\n",
      "Trained batch 134 batch loss 1.07812643 epoch total loss 1.1041106\n",
      "Trained batch 135 batch loss 1.12484407 epoch total loss 1.10426426\n",
      "Trained batch 136 batch loss 1.2093302 epoch total loss 1.10503674\n",
      "Trained batch 137 batch loss 1.09756947 epoch total loss 1.10498226\n",
      "Trained batch 138 batch loss 1.04613042 epoch total loss 1.10455573\n",
      "Trained batch 139 batch loss 1.24126244 epoch total loss 1.1055392\n",
      "Trained batch 140 batch loss 1.15111148 epoch total loss 1.10586476\n",
      "Trained batch 141 batch loss 1.18306935 epoch total loss 1.10641229\n",
      "Trained batch 142 batch loss 1.15310979 epoch total loss 1.10674119\n",
      "Trained batch 143 batch loss 1.08337688 epoch total loss 1.10657775\n",
      "Trained batch 144 batch loss 0.971152782 epoch total loss 1.10563719\n",
      "Trained batch 145 batch loss 1.17955017 epoch total loss 1.10614693\n",
      "Trained batch 146 batch loss 1.07991695 epoch total loss 1.10596728\n",
      "Trained batch 147 batch loss 1.25750518 epoch total loss 1.10699821\n",
      "Trained batch 148 batch loss 1.18470514 epoch total loss 1.1075232\n",
      "Trained batch 149 batch loss 1.25185907 epoch total loss 1.1084919\n",
      "Trained batch 150 batch loss 0.968391776 epoch total loss 1.10755801\n",
      "Trained batch 151 batch loss 1.07424319 epoch total loss 1.10733736\n",
      "Trained batch 152 batch loss 1.13352585 epoch total loss 1.10750973\n",
      "Trained batch 153 batch loss 1.07439041 epoch total loss 1.10729325\n",
      "Trained batch 154 batch loss 1.08170426 epoch total loss 1.10712707\n",
      "Trained batch 155 batch loss 1.03244174 epoch total loss 1.10664523\n",
      "Trained batch 156 batch loss 0.974112868 epoch total loss 1.10579562\n",
      "Trained batch 157 batch loss 1.10294712 epoch total loss 1.1057775\n",
      "Trained batch 158 batch loss 0.994777083 epoch total loss 1.105075\n",
      "Trained batch 159 batch loss 1.20011711 epoch total loss 1.10567272\n",
      "Trained batch 160 batch loss 1.12018061 epoch total loss 1.10576344\n",
      "Trained batch 161 batch loss 1.09180081 epoch total loss 1.10567665\n",
      "Trained batch 162 batch loss 1.00533009 epoch total loss 1.10505724\n",
      "Trained batch 163 batch loss 1.13228261 epoch total loss 1.10522425\n",
      "Trained batch 164 batch loss 1.15005577 epoch total loss 1.1054976\n",
      "Trained batch 165 batch loss 1.14302742 epoch total loss 1.10572505\n",
      "Trained batch 166 batch loss 1.0969094 epoch total loss 1.10567188\n",
      "Trained batch 167 batch loss 1.20730817 epoch total loss 1.10628045\n",
      "Trained batch 168 batch loss 1.04054618 epoch total loss 1.1058892\n",
      "Trained batch 169 batch loss 1.01314032 epoch total loss 1.10534036\n",
      "Trained batch 170 batch loss 1.0060885 epoch total loss 1.10475647\n",
      "Trained batch 171 batch loss 1.14726782 epoch total loss 1.10500515\n",
      "Trained batch 172 batch loss 1.04331684 epoch total loss 1.10464644\n",
      "Trained batch 173 batch loss 1.11181903 epoch total loss 1.10468793\n",
      "Trained batch 174 batch loss 0.926529229 epoch total loss 1.10366404\n",
      "Trained batch 175 batch loss 0.930060506 epoch total loss 1.10267198\n",
      "Trained batch 176 batch loss 1.10790074 epoch total loss 1.10270166\n",
      "Trained batch 177 batch loss 1.18606412 epoch total loss 1.10317266\n",
      "Trained batch 178 batch loss 1.14744234 epoch total loss 1.10342133\n",
      "Trained batch 179 batch loss 1.11401975 epoch total loss 1.10348046\n",
      "Trained batch 180 batch loss 1.21990776 epoch total loss 1.10412729\n",
      "Trained batch 181 batch loss 1.28414667 epoch total loss 1.10512197\n",
      "Trained batch 182 batch loss 1.05100119 epoch total loss 1.10482454\n",
      "Trained batch 183 batch loss 1.17047489 epoch total loss 1.10518324\n",
      "Trained batch 184 batch loss 1.24383736 epoch total loss 1.10593677\n",
      "Trained batch 185 batch loss 1.15897775 epoch total loss 1.10622358\n",
      "Trained batch 186 batch loss 1.05006635 epoch total loss 1.10592163\n",
      "Trained batch 187 batch loss 0.940839469 epoch total loss 1.10503876\n",
      "Trained batch 188 batch loss 0.895511091 epoch total loss 1.10392427\n",
      "Trained batch 189 batch loss 0.951517045 epoch total loss 1.10311794\n",
      "Trained batch 190 batch loss 1.02795899 epoch total loss 1.10272229\n",
      "Trained batch 191 batch loss 1.09545064 epoch total loss 1.10268426\n",
      "Trained batch 192 batch loss 1.15503561 epoch total loss 1.10295689\n",
      "Trained batch 193 batch loss 1.17154253 epoch total loss 1.10331225\n",
      "Trained batch 194 batch loss 1.1856848 epoch total loss 1.10373676\n",
      "Trained batch 195 batch loss 1.04362321 epoch total loss 1.10342848\n",
      "Trained batch 196 batch loss 1.0735023 epoch total loss 1.1032759\n",
      "Trained batch 197 batch loss 1.16028357 epoch total loss 1.10356522\n",
      "Trained batch 198 batch loss 1.16840076 epoch total loss 1.10389268\n",
      "Trained batch 199 batch loss 1.09818745 epoch total loss 1.10386395\n",
      "Trained batch 200 batch loss 1.09597588 epoch total loss 1.1038245\n",
      "Trained batch 201 batch loss 1.17837369 epoch total loss 1.10419548\n",
      "Trained batch 202 batch loss 1.05537271 epoch total loss 1.10395372\n",
      "Trained batch 203 batch loss 0.956461847 epoch total loss 1.10322726\n",
      "Trained batch 204 batch loss 1.06280768 epoch total loss 1.10302901\n",
      "Trained batch 205 batch loss 1.24104476 epoch total loss 1.10370231\n",
      "Trained batch 206 batch loss 1.18002367 epoch total loss 1.10407281\n",
      "Trained batch 207 batch loss 1.04212368 epoch total loss 1.10377359\n",
      "Trained batch 208 batch loss 1.03826392 epoch total loss 1.10345864\n",
      "Trained batch 209 batch loss 1.06965458 epoch total loss 1.10329688\n",
      "Trained batch 210 batch loss 1.09876192 epoch total loss 1.1032753\n",
      "Trained batch 211 batch loss 1.15170741 epoch total loss 1.10350478\n",
      "Trained batch 212 batch loss 1.15333533 epoch total loss 1.10373986\n",
      "Trained batch 213 batch loss 1.2390902 epoch total loss 1.10437524\n",
      "Trained batch 214 batch loss 1.14157093 epoch total loss 1.10454905\n",
      "Trained batch 215 batch loss 0.890651107 epoch total loss 1.10355425\n",
      "Trained batch 216 batch loss 0.957660675 epoch total loss 1.10287881\n",
      "Trained batch 217 batch loss 0.970282733 epoch total loss 1.10226774\n",
      "Trained batch 218 batch loss 1.02974677 epoch total loss 1.10193503\n",
      "Trained batch 219 batch loss 1.01701951 epoch total loss 1.10154724\n",
      "Trained batch 220 batch loss 0.885705233 epoch total loss 1.10056615\n",
      "Trained batch 221 batch loss 1.16893196 epoch total loss 1.1008755\n",
      "Trained batch 222 batch loss 1.10212493 epoch total loss 1.1008811\n",
      "Trained batch 223 batch loss 1.22080612 epoch total loss 1.10141897\n",
      "Trained batch 224 batch loss 1.10666394 epoch total loss 1.10144234\n",
      "Trained batch 225 batch loss 1.13264501 epoch total loss 1.10158098\n",
      "Trained batch 226 batch loss 1.11212325 epoch total loss 1.10162771\n",
      "Trained batch 227 batch loss 1.20462465 epoch total loss 1.10208142\n",
      "Trained batch 228 batch loss 1.08611894 epoch total loss 1.10201132\n",
      "Trained batch 229 batch loss 1.1248914 epoch total loss 1.10211134\n",
      "Trained batch 230 batch loss 1.00437367 epoch total loss 1.10168636\n",
      "Trained batch 231 batch loss 1.06624651 epoch total loss 1.10153294\n",
      "Trained batch 232 batch loss 0.936144 epoch total loss 1.10082006\n",
      "Trained batch 233 batch loss 1.01311934 epoch total loss 1.10044372\n",
      "Trained batch 234 batch loss 1.01975989 epoch total loss 1.10009885\n",
      "Trained batch 235 batch loss 0.991123855 epoch total loss 1.09963512\n",
      "Trained batch 236 batch loss 1.03919113 epoch total loss 1.09937894\n",
      "Trained batch 237 batch loss 0.958557606 epoch total loss 1.0987848\n",
      "Trained batch 238 batch loss 1.09134245 epoch total loss 1.09875345\n",
      "Trained batch 239 batch loss 1.21934795 epoch total loss 1.09925807\n",
      "Trained batch 240 batch loss 1.11697292 epoch total loss 1.09933197\n",
      "Trained batch 241 batch loss 1.11272597 epoch total loss 1.09938753\n",
      "Trained batch 242 batch loss 1.1447 epoch total loss 1.0995748\n",
      "Trained batch 243 batch loss 1.12933576 epoch total loss 1.09969723\n",
      "Trained batch 244 batch loss 1.18297529 epoch total loss 1.10003865\n",
      "Trained batch 245 batch loss 1.10743403 epoch total loss 1.10006881\n",
      "Trained batch 246 batch loss 1.19040728 epoch total loss 1.10043597\n",
      "Trained batch 247 batch loss 1.15172029 epoch total loss 1.10064363\n",
      "Trained batch 248 batch loss 1.24378037 epoch total loss 1.10122073\n",
      "Trained batch 249 batch loss 1.32802677 epoch total loss 1.10213172\n",
      "Trained batch 250 batch loss 1.24475598 epoch total loss 1.10270214\n",
      "Trained batch 251 batch loss 1.09593177 epoch total loss 1.10267508\n",
      "Trained batch 252 batch loss 1.08957887 epoch total loss 1.10262311\n",
      "Trained batch 253 batch loss 1.16855 epoch total loss 1.1028837\n",
      "Trained batch 254 batch loss 1.14176595 epoch total loss 1.10303676\n",
      "Trained batch 255 batch loss 1.18214405 epoch total loss 1.10334682\n",
      "Trained batch 256 batch loss 1.15814924 epoch total loss 1.10356092\n",
      "Trained batch 257 batch loss 1.17154753 epoch total loss 1.10382545\n",
      "Trained batch 258 batch loss 1.14095128 epoch total loss 1.10396934\n",
      "Trained batch 259 batch loss 1.07870603 epoch total loss 1.10387182\n",
      "Trained batch 260 batch loss 1.15143061 epoch total loss 1.10405469\n",
      "Trained batch 261 batch loss 1.26589167 epoch total loss 1.10467482\n",
      "Trained batch 262 batch loss 1.14875102 epoch total loss 1.10484302\n",
      "Trained batch 263 batch loss 1.25511324 epoch total loss 1.10541439\n",
      "Trained batch 264 batch loss 1.09315228 epoch total loss 1.1053679\n",
      "Trained batch 265 batch loss 1.18465281 epoch total loss 1.10566711\n",
      "Trained batch 266 batch loss 1.01274598 epoch total loss 1.10531783\n",
      "Trained batch 267 batch loss 1.15792465 epoch total loss 1.10551488\n",
      "Trained batch 268 batch loss 1.19629312 epoch total loss 1.10585368\n",
      "Trained batch 269 batch loss 1.13932478 epoch total loss 1.10597801\n",
      "Trained batch 270 batch loss 1.19428539 epoch total loss 1.106305\n",
      "Trained batch 271 batch loss 1.09986 epoch total loss 1.10628128\n",
      "Trained batch 272 batch loss 1.18576765 epoch total loss 1.10657346\n",
      "Trained batch 273 batch loss 1.09401155 epoch total loss 1.10652745\n",
      "Trained batch 274 batch loss 1.07355607 epoch total loss 1.10640717\n",
      "Trained batch 275 batch loss 1.02078676 epoch total loss 1.10609579\n",
      "Trained batch 276 batch loss 1.09601974 epoch total loss 1.10605919\n",
      "Trained batch 277 batch loss 1.22181714 epoch total loss 1.10647714\n",
      "Trained batch 278 batch loss 1.09560573 epoch total loss 1.10643804\n",
      "Trained batch 279 batch loss 1.0658114 epoch total loss 1.10629249\n",
      "Trained batch 280 batch loss 1.05586767 epoch total loss 1.10611248\n",
      "Trained batch 281 batch loss 1.19225919 epoch total loss 1.10641909\n",
      "Trained batch 282 batch loss 1.09253538 epoch total loss 1.10636973\n",
      "Trained batch 283 batch loss 1.04214716 epoch total loss 1.10614288\n",
      "Trained batch 284 batch loss 1.01625645 epoch total loss 1.10582638\n",
      "Trained batch 285 batch loss 1.02358603 epoch total loss 1.10553777\n",
      "Trained batch 286 batch loss 1.08741736 epoch total loss 1.10547435\n",
      "Trained batch 287 batch loss 1.14562762 epoch total loss 1.1056143\n",
      "Trained batch 288 batch loss 1.04917336 epoch total loss 1.10541832\n",
      "Trained batch 289 batch loss 1.1719811 epoch total loss 1.10564864\n",
      "Trained batch 290 batch loss 0.945259392 epoch total loss 1.10509551\n",
      "Trained batch 291 batch loss 1.02906477 epoch total loss 1.1048342\n",
      "Trained batch 292 batch loss 0.973277688 epoch total loss 1.10438359\n",
      "Trained batch 293 batch loss 1.06090152 epoch total loss 1.10423529\n",
      "Trained batch 294 batch loss 1.12481356 epoch total loss 1.10430527\n",
      "Trained batch 295 batch loss 1.16350341 epoch total loss 1.1045059\n",
      "Trained batch 296 batch loss 1.15232229 epoch total loss 1.10466743\n",
      "Trained batch 297 batch loss 1.20011592 epoch total loss 1.10498881\n",
      "Trained batch 298 batch loss 1.13333929 epoch total loss 1.10508394\n",
      "Trained batch 299 batch loss 1.07592964 epoch total loss 1.10498643\n",
      "Trained batch 300 batch loss 1.07462645 epoch total loss 1.1048851\n",
      "Trained batch 301 batch loss 1.04114056 epoch total loss 1.10467339\n",
      "Trained batch 302 batch loss 1.14928114 epoch total loss 1.10482109\n",
      "Trained batch 303 batch loss 1.12157416 epoch total loss 1.1048764\n",
      "Trained batch 304 batch loss 1.04954982 epoch total loss 1.10469449\n",
      "Trained batch 305 batch loss 1.02132404 epoch total loss 1.10442114\n",
      "Trained batch 306 batch loss 1.03860092 epoch total loss 1.10420609\n",
      "Trained batch 307 batch loss 1.07569623 epoch total loss 1.1041131\n",
      "Trained batch 308 batch loss 1.10859144 epoch total loss 1.10412765\n",
      "Trained batch 309 batch loss 1.10175979 epoch total loss 1.1041199\n",
      "Trained batch 310 batch loss 1.1108402 epoch total loss 1.10414159\n",
      "Trained batch 311 batch loss 1.21496868 epoch total loss 1.10449803\n",
      "Trained batch 312 batch loss 1.2816391 epoch total loss 1.1050657\n",
      "Trained batch 313 batch loss 1.24161577 epoch total loss 1.10550201\n",
      "Trained batch 314 batch loss 1.26751089 epoch total loss 1.10601795\n",
      "Trained batch 315 batch loss 1.05088794 epoch total loss 1.10584295\n",
      "Trained batch 316 batch loss 1.1785692 epoch total loss 1.10607302\n",
      "Trained batch 317 batch loss 1.22112417 epoch total loss 1.10643601\n",
      "Trained batch 318 batch loss 1.12253368 epoch total loss 1.10648656\n",
      "Trained batch 319 batch loss 1.10814 epoch total loss 1.1064918\n",
      "Trained batch 320 batch loss 1.01589286 epoch total loss 1.10620868\n",
      "Trained batch 321 batch loss 1.12194335 epoch total loss 1.1062578\n",
      "Trained batch 322 batch loss 0.925011933 epoch total loss 1.10569489\n",
      "Trained batch 323 batch loss 1.03167987 epoch total loss 1.10546577\n",
      "Trained batch 324 batch loss 1.09959948 epoch total loss 1.10544765\n",
      "Trained batch 325 batch loss 1.07176077 epoch total loss 1.10534394\n",
      "Trained batch 326 batch loss 1.04262817 epoch total loss 1.10515153\n",
      "Trained batch 327 batch loss 1.17585373 epoch total loss 1.10536778\n",
      "Trained batch 328 batch loss 1.19879925 epoch total loss 1.10565257\n",
      "Trained batch 329 batch loss 1.07501912 epoch total loss 1.10555947\n",
      "Trained batch 330 batch loss 1.15614462 epoch total loss 1.10571277\n",
      "Trained batch 331 batch loss 1.17252946 epoch total loss 1.10591459\n",
      "Trained batch 332 batch loss 1.12157857 epoch total loss 1.1059618\n",
      "Trained batch 333 batch loss 1.07921529 epoch total loss 1.10588157\n",
      "Trained batch 334 batch loss 1.23482943 epoch total loss 1.10626757\n",
      "Trained batch 335 batch loss 1.30431509 epoch total loss 1.10685885\n",
      "Trained batch 336 batch loss 1.13931656 epoch total loss 1.10695541\n",
      "Trained batch 337 batch loss 1.1512754 epoch total loss 1.1070869\n",
      "Trained batch 338 batch loss 1.19094491 epoch total loss 1.10733497\n",
      "Trained batch 339 batch loss 1.10749042 epoch total loss 1.10733545\n",
      "Trained batch 340 batch loss 1.14189136 epoch total loss 1.10743701\n",
      "Trained batch 341 batch loss 1.19514263 epoch total loss 1.10769415\n",
      "Trained batch 342 batch loss 1.15030563 epoch total loss 1.10781872\n",
      "Trained batch 343 batch loss 1.13329601 epoch total loss 1.10789311\n",
      "Trained batch 344 batch loss 1.12326574 epoch total loss 1.10793769\n",
      "Trained batch 345 batch loss 1.11566257 epoch total loss 1.1079601\n",
      "Trained batch 346 batch loss 1.12867141 epoch total loss 1.10802\n",
      "Trained batch 347 batch loss 1.07643592 epoch total loss 1.10792899\n",
      "Trained batch 348 batch loss 1.2314 epoch total loss 1.10828388\n",
      "Trained batch 349 batch loss 1.24197447 epoch total loss 1.1086669\n",
      "Trained batch 350 batch loss 1.04910445 epoch total loss 1.10849667\n",
      "Trained batch 351 batch loss 1.07981837 epoch total loss 1.10841501\n",
      "Trained batch 352 batch loss 1.02768052 epoch total loss 1.10818565\n",
      "Trained batch 353 batch loss 1.15097475 epoch total loss 1.10830677\n",
      "Trained batch 354 batch loss 1.1158855 epoch total loss 1.10832822\n",
      "Trained batch 355 batch loss 1.07288122 epoch total loss 1.10822833\n",
      "Trained batch 356 batch loss 1.16893435 epoch total loss 1.10839891\n",
      "Trained batch 357 batch loss 1.12895441 epoch total loss 1.10845649\n",
      "Trained batch 358 batch loss 1.10966718 epoch total loss 1.10846\n",
      "Trained batch 359 batch loss 1.01873863 epoch total loss 1.10821\n",
      "Trained batch 360 batch loss 1.09304976 epoch total loss 1.10816789\n",
      "Trained batch 361 batch loss 1.06778443 epoch total loss 1.10805595\n",
      "Trained batch 362 batch loss 1.12942612 epoch total loss 1.10811496\n",
      "Trained batch 363 batch loss 1.07557058 epoch total loss 1.10802531\n",
      "Trained batch 364 batch loss 1.09349 epoch total loss 1.10798538\n",
      "Trained batch 365 batch loss 1.05160367 epoch total loss 1.10783088\n",
      "Trained batch 366 batch loss 1.21192026 epoch total loss 1.10811532\n",
      "Trained batch 367 batch loss 1.0085901 epoch total loss 1.107844\n",
      "Trained batch 368 batch loss 0.944189608 epoch total loss 1.10739934\n",
      "Trained batch 369 batch loss 0.996765554 epoch total loss 1.10709953\n",
      "Trained batch 370 batch loss 1.11818421 epoch total loss 1.10712945\n",
      "Trained batch 371 batch loss 1.00391412 epoch total loss 1.10685122\n",
      "Trained batch 372 batch loss 1.05014014 epoch total loss 1.10669875\n",
      "Trained batch 373 batch loss 1.07315552 epoch total loss 1.10660887\n",
      "Trained batch 374 batch loss 1.19639552 epoch total loss 1.10684884\n",
      "Trained batch 375 batch loss 1.1949445 epoch total loss 1.1070838\n",
      "Trained batch 376 batch loss 1.20962524 epoch total loss 1.10735655\n",
      "Trained batch 377 batch loss 1.17292714 epoch total loss 1.10753047\n",
      "Trained batch 378 batch loss 1.08722711 epoch total loss 1.10747671\n",
      "Trained batch 379 batch loss 1.21330249 epoch total loss 1.1077559\n",
      "Trained batch 380 batch loss 1.183231 epoch total loss 1.1079545\n",
      "Trained batch 381 batch loss 1.03732514 epoch total loss 1.10776913\n",
      "Trained batch 382 batch loss 1.12184155 epoch total loss 1.10780597\n",
      "Trained batch 383 batch loss 0.989606917 epoch total loss 1.10749733\n",
      "Trained batch 384 batch loss 1.00059175 epoch total loss 1.10721886\n",
      "Trained batch 385 batch loss 1.11077726 epoch total loss 1.10722816\n",
      "Trained batch 386 batch loss 1.0903573 epoch total loss 1.10718441\n",
      "Trained batch 387 batch loss 1.07903099 epoch total loss 1.10711169\n",
      "Trained batch 388 batch loss 0.965302885 epoch total loss 1.1067462\n",
      "Trained batch 389 batch loss 1.02430272 epoch total loss 1.10653424\n",
      "Trained batch 390 batch loss 1.00281107 epoch total loss 1.10626829\n",
      "Trained batch 391 batch loss 1.21175194 epoch total loss 1.10653806\n",
      "Trained batch 392 batch loss 1.19634736 epoch total loss 1.10676718\n",
      "Trained batch 393 batch loss 1.01277232 epoch total loss 1.10652804\n",
      "Trained batch 394 batch loss 0.993097425 epoch total loss 1.10624015\n",
      "Trained batch 395 batch loss 1.00114846 epoch total loss 1.1059742\n",
      "Trained batch 396 batch loss 0.850125551 epoch total loss 1.10532808\n",
      "Trained batch 397 batch loss 1.05887973 epoch total loss 1.10521114\n",
      "Trained batch 398 batch loss 0.99960345 epoch total loss 1.10494578\n",
      "Trained batch 399 batch loss 1.11877453 epoch total loss 1.10498035\n",
      "Trained batch 400 batch loss 1.0733707 epoch total loss 1.10490131\n",
      "Trained batch 401 batch loss 1.03166354 epoch total loss 1.1047188\n",
      "Trained batch 402 batch loss 1.08874214 epoch total loss 1.10467899\n",
      "Trained batch 403 batch loss 1.13466597 epoch total loss 1.10475349\n",
      "Trained batch 404 batch loss 1.26484227 epoch total loss 1.10514963\n",
      "Trained batch 405 batch loss 1.42150807 epoch total loss 1.10593081\n",
      "Trained batch 406 batch loss 1.33385062 epoch total loss 1.10649216\n",
      "Trained batch 407 batch loss 1.13553226 epoch total loss 1.10656357\n",
      "Trained batch 408 batch loss 0.979878306 epoch total loss 1.10625303\n",
      "Trained batch 409 batch loss 1.13261342 epoch total loss 1.10631752\n",
      "Trained batch 410 batch loss 1.28524041 epoch total loss 1.10675395\n",
      "Trained batch 411 batch loss 1.2261169 epoch total loss 1.10704434\n",
      "Trained batch 412 batch loss 1.2163465 epoch total loss 1.10730958\n",
      "Trained batch 413 batch loss 1.02566195 epoch total loss 1.10711193\n",
      "Trained batch 414 batch loss 1.05445182 epoch total loss 1.10698473\n",
      "Trained batch 415 batch loss 1.01322854 epoch total loss 1.10675871\n",
      "Trained batch 416 batch loss 1.06235385 epoch total loss 1.10665202\n",
      "Trained batch 417 batch loss 1.00467122 epoch total loss 1.1064074\n",
      "Trained batch 418 batch loss 1.04886973 epoch total loss 1.10626972\n",
      "Trained batch 419 batch loss 1.04292345 epoch total loss 1.10611856\n",
      "Trained batch 420 batch loss 1.0319531 epoch total loss 1.10594201\n",
      "Trained batch 421 batch loss 0.99753809 epoch total loss 1.10568452\n",
      "Trained batch 422 batch loss 0.811032712 epoch total loss 1.10498631\n",
      "Trained batch 423 batch loss 1.06602788 epoch total loss 1.10489416\n",
      "Trained batch 424 batch loss 1.03349543 epoch total loss 1.10472584\n",
      "Trained batch 425 batch loss 1.09077346 epoch total loss 1.10469294\n",
      "Trained batch 426 batch loss 1.16253972 epoch total loss 1.10482872\n",
      "Trained batch 427 batch loss 1.09585512 epoch total loss 1.10480773\n",
      "Trained batch 428 batch loss 0.988747597 epoch total loss 1.10453653\n",
      "Trained batch 429 batch loss 0.898556232 epoch total loss 1.10405636\n",
      "Trained batch 430 batch loss 0.928340673 epoch total loss 1.10364783\n",
      "Trained batch 431 batch loss 1.06733668 epoch total loss 1.10356355\n",
      "Trained batch 432 batch loss 1.12073481 epoch total loss 1.10360324\n",
      "Trained batch 433 batch loss 1.15355217 epoch total loss 1.10371864\n",
      "Trained batch 434 batch loss 1.11687183 epoch total loss 1.10374892\n",
      "Trained batch 435 batch loss 1.10368204 epoch total loss 1.1037488\n",
      "Trained batch 436 batch loss 1.09910464 epoch total loss 1.10373807\n",
      "Trained batch 437 batch loss 1.14539492 epoch total loss 1.10383332\n",
      "Trained batch 438 batch loss 1.16863811 epoch total loss 1.10398138\n",
      "Trained batch 439 batch loss 1.05025649 epoch total loss 1.10385895\n",
      "Trained batch 440 batch loss 1.14192593 epoch total loss 1.10394549\n",
      "Trained batch 441 batch loss 1.01188898 epoch total loss 1.10373676\n",
      "Trained batch 442 batch loss 1.05733633 epoch total loss 1.10363185\n",
      "Trained batch 443 batch loss 1.08439684 epoch total loss 1.10358846\n",
      "Trained batch 444 batch loss 1.00244224 epoch total loss 1.10336065\n",
      "Trained batch 445 batch loss 1.08116114 epoch total loss 1.1033107\n",
      "Trained batch 446 batch loss 0.963124752 epoch total loss 1.10299647\n",
      "Trained batch 447 batch loss 0.974622846 epoch total loss 1.10270917\n",
      "Trained batch 448 batch loss 0.999949455 epoch total loss 1.10247982\n",
      "Trained batch 449 batch loss 1.02061379 epoch total loss 1.10229743\n",
      "Trained batch 450 batch loss 1.17945361 epoch total loss 1.10246885\n",
      "Trained batch 451 batch loss 0.993418157 epoch total loss 1.10222709\n",
      "Trained batch 452 batch loss 1.0464586 epoch total loss 1.10210359\n",
      "Trained batch 453 batch loss 1.07018948 epoch total loss 1.10203326\n",
      "Trained batch 454 batch loss 1.05948031 epoch total loss 1.10193944\n",
      "Trained batch 455 batch loss 1.14546633 epoch total loss 1.10203516\n",
      "Trained batch 456 batch loss 1.15722346 epoch total loss 1.10215616\n",
      "Trained batch 457 batch loss 1.05601 epoch total loss 1.10205519\n",
      "Trained batch 458 batch loss 1.03531897 epoch total loss 1.1019094\n",
      "Trained batch 459 batch loss 1.1754005 epoch total loss 1.10206962\n",
      "Trained batch 460 batch loss 1.06676114 epoch total loss 1.10199285\n",
      "Trained batch 461 batch loss 1.03700399 epoch total loss 1.10185194\n",
      "Trained batch 462 batch loss 1.01267707 epoch total loss 1.10165882\n",
      "Trained batch 463 batch loss 1.08039927 epoch total loss 1.10161304\n",
      "Trained batch 464 batch loss 0.971018732 epoch total loss 1.10133147\n",
      "Trained batch 465 batch loss 0.83838886 epoch total loss 1.10076606\n",
      "Trained batch 466 batch loss 0.912559688 epoch total loss 1.10036206\n",
      "Trained batch 467 batch loss 0.999656081 epoch total loss 1.10014641\n",
      "Trained batch 468 batch loss 1.02973175 epoch total loss 1.09999597\n",
      "Trained batch 469 batch loss 0.918040633 epoch total loss 1.09960794\n",
      "Trained batch 470 batch loss 0.926850736 epoch total loss 1.09924042\n",
      "Trained batch 471 batch loss 1.01967216 epoch total loss 1.0990715\n",
      "Trained batch 472 batch loss 0.935861945 epoch total loss 1.09872568\n",
      "Trained batch 473 batch loss 1.14317739 epoch total loss 1.09881961\n",
      "Trained batch 474 batch loss 1.26782656 epoch total loss 1.09917617\n",
      "Trained batch 475 batch loss 1.12011492 epoch total loss 1.09922028\n",
      "Trained batch 476 batch loss 1.1535089 epoch total loss 1.09933436\n",
      "Trained batch 477 batch loss 1.31362104 epoch total loss 1.09978354\n",
      "Trained batch 478 batch loss 1.25748336 epoch total loss 1.10011351\n",
      "Trained batch 479 batch loss 1.21405697 epoch total loss 1.10035133\n",
      "Trained batch 480 batch loss 1.17144847 epoch total loss 1.10049951\n",
      "Trained batch 481 batch loss 1.00190175 epoch total loss 1.10029447\n",
      "Trained batch 482 batch loss 1.21962917 epoch total loss 1.10054195\n",
      "Trained batch 483 batch loss 1.21468139 epoch total loss 1.10077822\n",
      "Trained batch 484 batch loss 1.05158138 epoch total loss 1.10067666\n",
      "Trained batch 485 batch loss 1.04504204 epoch total loss 1.10056186\n",
      "Trained batch 486 batch loss 0.873651505 epoch total loss 1.10009503\n",
      "Trained batch 487 batch loss 0.954312086 epoch total loss 1.09979558\n",
      "Trained batch 488 batch loss 1.02876759 epoch total loss 1.09965\n",
      "Trained batch 489 batch loss 0.960302949 epoch total loss 1.09936512\n",
      "Trained batch 490 batch loss 1.03902936 epoch total loss 1.09924197\n",
      "Trained batch 491 batch loss 0.974093139 epoch total loss 1.0989871\n",
      "Trained batch 492 batch loss 1.12503099 epoch total loss 1.09904\n",
      "Trained batch 493 batch loss 1.1397717 epoch total loss 1.09912276\n",
      "Trained batch 494 batch loss 1.02900553 epoch total loss 1.09898078\n",
      "Trained batch 495 batch loss 1.00870299 epoch total loss 1.09879839\n",
      "Trained batch 496 batch loss 1.1404748 epoch total loss 1.09888244\n",
      "Trained batch 497 batch loss 1.16451359 epoch total loss 1.09901452\n",
      "Trained batch 498 batch loss 1.1054914 epoch total loss 1.09902751\n",
      "Trained batch 499 batch loss 1.10912657 epoch total loss 1.09904766\n",
      "Trained batch 500 batch loss 1.16157675 epoch total loss 1.09917271\n",
      "Trained batch 501 batch loss 1.05373645 epoch total loss 1.09908199\n",
      "Trained batch 502 batch loss 1.10199904 epoch total loss 1.09908783\n",
      "Trained batch 503 batch loss 1.06977904 epoch total loss 1.09902942\n",
      "Trained batch 504 batch loss 1.17881787 epoch total loss 1.09918785\n",
      "Trained batch 505 batch loss 1.08629537 epoch total loss 1.09916234\n",
      "Trained batch 506 batch loss 1.01327574 epoch total loss 1.09899259\n",
      "Trained batch 507 batch loss 1.01328874 epoch total loss 1.09882367\n",
      "Trained batch 508 batch loss 0.959396958 epoch total loss 1.09854925\n",
      "Trained batch 509 batch loss 0.977349877 epoch total loss 1.09831107\n",
      "Trained batch 510 batch loss 1.16497087 epoch total loss 1.09844184\n",
      "Trained batch 511 batch loss 1.14859319 epoch total loss 1.09854\n",
      "Trained batch 512 batch loss 1.09127104 epoch total loss 1.09852576\n",
      "Trained batch 513 batch loss 1.14202976 epoch total loss 1.09861052\n",
      "Trained batch 514 batch loss 1.07801545 epoch total loss 1.09857047\n",
      "Trained batch 515 batch loss 1.09672117 epoch total loss 1.09856689\n",
      "Trained batch 516 batch loss 1.05031657 epoch total loss 1.09847331\n",
      "Trained batch 517 batch loss 1.07967198 epoch total loss 1.09843695\n",
      "Trained batch 518 batch loss 0.990929842 epoch total loss 1.09822941\n",
      "Trained batch 519 batch loss 0.923323393 epoch total loss 1.0978924\n",
      "Trained batch 520 batch loss 0.981954694 epoch total loss 1.09766936\n",
      "Trained batch 521 batch loss 1.12050223 epoch total loss 1.09771323\n",
      "Trained batch 522 batch loss 1.04297364 epoch total loss 1.09760833\n",
      "Trained batch 523 batch loss 1.08495116 epoch total loss 1.09758413\n",
      "Trained batch 524 batch loss 1.08891726 epoch total loss 1.09756756\n",
      "Trained batch 525 batch loss 0.963182449 epoch total loss 1.09731162\n",
      "Trained batch 526 batch loss 1.10366642 epoch total loss 1.09732366\n",
      "Trained batch 527 batch loss 0.944840074 epoch total loss 1.09703434\n",
      "Trained batch 528 batch loss 1.07313848 epoch total loss 1.09698904\n",
      "Trained batch 529 batch loss 0.901554525 epoch total loss 1.09661961\n",
      "Trained batch 530 batch loss 1.06661701 epoch total loss 1.09656286\n",
      "Trained batch 531 batch loss 1.19439816 epoch total loss 1.09674716\n",
      "Trained batch 532 batch loss 0.934532642 epoch total loss 1.09644222\n",
      "Trained batch 533 batch loss 1.08779788 epoch total loss 1.09642589\n",
      "Trained batch 534 batch loss 1.03156757 epoch total loss 1.09630442\n",
      "Trained batch 535 batch loss 1.02944541 epoch total loss 1.09617949\n",
      "Trained batch 536 batch loss 1.17826664 epoch total loss 1.09633255\n",
      "Trained batch 537 batch loss 1.10273802 epoch total loss 1.09634447\n",
      "Trained batch 538 batch loss 1.07629108 epoch total loss 1.09630728\n",
      "Trained batch 539 batch loss 1.14724576 epoch total loss 1.09640169\n",
      "Trained batch 540 batch loss 0.990614176 epoch total loss 1.09620571\n",
      "Trained batch 541 batch loss 1.01948047 epoch total loss 1.09606397\n",
      "Trained batch 542 batch loss 1.05311596 epoch total loss 1.0959847\n",
      "Trained batch 543 batch loss 1.01886058 epoch total loss 1.0958426\n",
      "Trained batch 544 batch loss 1.04504013 epoch total loss 1.09574926\n",
      "Trained batch 545 batch loss 1.01151466 epoch total loss 1.09559476\n",
      "Trained batch 546 batch loss 1.07976639 epoch total loss 1.0955658\n",
      "Trained batch 547 batch loss 1.00273776 epoch total loss 1.09539604\n",
      "Trained batch 548 batch loss 1.11347866 epoch total loss 1.09542906\n",
      "Trained batch 549 batch loss 1.27586102 epoch total loss 1.09575772\n",
      "Trained batch 550 batch loss 1.09939051 epoch total loss 1.09576428\n",
      "Trained batch 551 batch loss 1.21431899 epoch total loss 1.09597933\n",
      "Trained batch 552 batch loss 1.11279547 epoch total loss 1.09600985\n",
      "Trained batch 553 batch loss 1.15229058 epoch total loss 1.09611166\n",
      "Trained batch 554 batch loss 0.994427741 epoch total loss 1.09592807\n",
      "Trained batch 555 batch loss 0.936719358 epoch total loss 1.09564126\n",
      "Trained batch 556 batch loss 1.10423434 epoch total loss 1.09565663\n",
      "Trained batch 557 batch loss 1.19821858 epoch total loss 1.09584081\n",
      "Trained batch 558 batch loss 1.20329452 epoch total loss 1.09603345\n",
      "Trained batch 559 batch loss 1.10020959 epoch total loss 1.09604096\n",
      "Trained batch 560 batch loss 1.13244903 epoch total loss 1.09610593\n",
      "Trained batch 561 batch loss 1.06975532 epoch total loss 1.09605896\n",
      "Trained batch 562 batch loss 1.08305728 epoch total loss 1.09603584\n",
      "Trained batch 563 batch loss 1.00188756 epoch total loss 1.09586871\n",
      "Trained batch 564 batch loss 0.789549232 epoch total loss 1.09532559\n",
      "Trained batch 565 batch loss 0.891560316 epoch total loss 1.09496486\n",
      "Trained batch 566 batch loss 1.1330713 epoch total loss 1.09503222\n",
      "Trained batch 567 batch loss 1.10471523 epoch total loss 1.09504926\n",
      "Trained batch 568 batch loss 1.16137409 epoch total loss 1.09516609\n",
      "Trained batch 569 batch loss 1.14443266 epoch total loss 1.09525263\n",
      "Trained batch 570 batch loss 1.22850418 epoch total loss 1.0954864\n",
      "Trained batch 571 batch loss 1.06156659 epoch total loss 1.09542704\n",
      "Trained batch 572 batch loss 0.925600767 epoch total loss 1.09513009\n",
      "Trained batch 573 batch loss 1.00176191 epoch total loss 1.09496725\n",
      "Trained batch 574 batch loss 1.06359148 epoch total loss 1.09491253\n",
      "Trained batch 575 batch loss 1.04026031 epoch total loss 1.09481752\n",
      "Trained batch 576 batch loss 1.10225463 epoch total loss 1.09483039\n",
      "Trained batch 577 batch loss 1.24667335 epoch total loss 1.09509349\n",
      "Trained batch 578 batch loss 1.18906629 epoch total loss 1.09525609\n",
      "Trained batch 579 batch loss 1.11584067 epoch total loss 1.09529173\n",
      "Trained batch 580 batch loss 1.25651538 epoch total loss 1.09556973\n",
      "Trained batch 581 batch loss 1.18442011 epoch total loss 1.09572268\n",
      "Trained batch 582 batch loss 1.16492295 epoch total loss 1.09584153\n",
      "Trained batch 583 batch loss 1.25426483 epoch total loss 1.09611332\n",
      "Trained batch 584 batch loss 1.22097552 epoch total loss 1.09632707\n",
      "Trained batch 585 batch loss 1.12778282 epoch total loss 1.09638083\n",
      "Trained batch 586 batch loss 1.12849593 epoch total loss 1.09643567\n",
      "Trained batch 587 batch loss 1.04442048 epoch total loss 1.09634709\n",
      "Trained batch 588 batch loss 1.22173166 epoch total loss 1.09656036\n",
      "Trained batch 589 batch loss 1.08543718 epoch total loss 1.0965414\n",
      "Trained batch 590 batch loss 1.07011628 epoch total loss 1.0964967\n",
      "Trained batch 591 batch loss 1.12582242 epoch total loss 1.09654629\n",
      "Trained batch 592 batch loss 1.05989468 epoch total loss 1.0964843\n",
      "Trained batch 593 batch loss 0.960005462 epoch total loss 1.09625423\n",
      "Trained batch 594 batch loss 1.15422153 epoch total loss 1.09635186\n",
      "Trained batch 595 batch loss 1.13708067 epoch total loss 1.09642029\n",
      "Trained batch 596 batch loss 1.22580886 epoch total loss 1.09663737\n",
      "Trained batch 597 batch loss 1.20452213 epoch total loss 1.09681809\n",
      "Trained batch 598 batch loss 1.32254827 epoch total loss 1.09719563\n",
      "Trained batch 599 batch loss 1.15316641 epoch total loss 1.09728897\n",
      "Trained batch 600 batch loss 1.17925203 epoch total loss 1.09742558\n",
      "Trained batch 601 batch loss 1.27340543 epoch total loss 1.09771836\n",
      "Trained batch 602 batch loss 1.08745563 epoch total loss 1.09770131\n",
      "Trained batch 603 batch loss 1.05331826 epoch total loss 1.09762776\n",
      "Trained batch 604 batch loss 1.12529159 epoch total loss 1.09767365\n",
      "Trained batch 605 batch loss 0.987958968 epoch total loss 1.09749234\n",
      "Trained batch 606 batch loss 0.957484484 epoch total loss 1.09726119\n",
      "Trained batch 607 batch loss 1.12775528 epoch total loss 1.0973115\n",
      "Trained batch 608 batch loss 1.04034209 epoch total loss 1.0972178\n",
      "Trained batch 609 batch loss 1.10259271 epoch total loss 1.09722662\n",
      "Trained batch 610 batch loss 1.1558696 epoch total loss 1.0973227\n",
      "Trained batch 611 batch loss 1.14621902 epoch total loss 1.09740281\n",
      "Trained batch 612 batch loss 1.07858 epoch total loss 1.09737206\n",
      "Trained batch 613 batch loss 1.20186353 epoch total loss 1.09754241\n",
      "Trained batch 614 batch loss 1.13421798 epoch total loss 1.09760213\n",
      "Trained batch 615 batch loss 0.978403747 epoch total loss 1.09740829\n",
      "Trained batch 616 batch loss 1.06220293 epoch total loss 1.09735119\n",
      "Trained batch 617 batch loss 1.11079049 epoch total loss 1.09737289\n",
      "Trained batch 618 batch loss 1.10608315 epoch total loss 1.09738708\n",
      "Trained batch 619 batch loss 1.27689147 epoch total loss 1.09767711\n",
      "Trained batch 620 batch loss 1.07872677 epoch total loss 1.09764647\n",
      "Trained batch 621 batch loss 1.07505953 epoch total loss 1.09761012\n",
      "Trained batch 622 batch loss 1.03092599 epoch total loss 1.09750295\n",
      "Trained batch 623 batch loss 1.13471067 epoch total loss 1.09756267\n",
      "Trained batch 624 batch loss 1.0769856 epoch total loss 1.09752965\n",
      "Trained batch 625 batch loss 0.959941745 epoch total loss 1.09730959\n",
      "Trained batch 626 batch loss 1.15067244 epoch total loss 1.09739482\n",
      "Trained batch 627 batch loss 1.16309762 epoch total loss 1.09749961\n",
      "Trained batch 628 batch loss 1.18311191 epoch total loss 1.09763598\n",
      "Trained batch 629 batch loss 1.10579133 epoch total loss 1.09764886\n",
      "Trained batch 630 batch loss 1.01595414 epoch total loss 1.09751916\n",
      "Trained batch 631 batch loss 1.16467893 epoch total loss 1.09762561\n",
      "Trained batch 632 batch loss 1.0455085 epoch total loss 1.09754312\n",
      "Trained batch 633 batch loss 1.07587636 epoch total loss 1.09750891\n",
      "Trained batch 634 batch loss 1.0785203 epoch total loss 1.09747887\n",
      "Trained batch 635 batch loss 1.0820334 epoch total loss 1.09745455\n",
      "Trained batch 636 batch loss 1.08015823 epoch total loss 1.09742737\n",
      "Trained batch 637 batch loss 1.12483776 epoch total loss 1.0974704\n",
      "Trained batch 638 batch loss 1.18858016 epoch total loss 1.09761322\n",
      "Trained batch 639 batch loss 1.17773223 epoch total loss 1.09773862\n",
      "Trained batch 640 batch loss 0.976896703 epoch total loss 1.09754968\n",
      "Trained batch 641 batch loss 1.09723938 epoch total loss 1.0975492\n",
      "Trained batch 642 batch loss 1.03775954 epoch total loss 1.0974561\n",
      "Trained batch 643 batch loss 1.11761153 epoch total loss 1.09748745\n",
      "Trained batch 644 batch loss 1.10858035 epoch total loss 1.09750473\n",
      "Trained batch 645 batch loss 1.16674125 epoch total loss 1.09761202\n",
      "Trained batch 646 batch loss 1.21822238 epoch total loss 1.09779871\n",
      "Trained batch 647 batch loss 1.26769674 epoch total loss 1.09806132\n",
      "Trained batch 648 batch loss 1.13120079 epoch total loss 1.09811246\n",
      "Trained batch 649 batch loss 1.2350682 epoch total loss 1.09832346\n",
      "Trained batch 650 batch loss 1.06077409 epoch total loss 1.09826577\n",
      "Trained batch 651 batch loss 1.04130816 epoch total loss 1.09817827\n",
      "Trained batch 652 batch loss 1.16196966 epoch total loss 1.09827614\n",
      "Trained batch 653 batch loss 1.01750469 epoch total loss 1.09815252\n",
      "Trained batch 654 batch loss 0.901823521 epoch total loss 1.09785223\n",
      "Trained batch 655 batch loss 0.930605412 epoch total loss 1.09759688\n",
      "Trained batch 656 batch loss 0.953128278 epoch total loss 1.0973767\n",
      "Trained batch 657 batch loss 1.35807753 epoch total loss 1.09777343\n",
      "Trained batch 658 batch loss 1.22557187 epoch total loss 1.09796774\n",
      "Trained batch 659 batch loss 1.01113033 epoch total loss 1.0978359\n",
      "Trained batch 660 batch loss 0.999467611 epoch total loss 1.09768689\n",
      "Trained batch 661 batch loss 1.06020939 epoch total loss 1.09763014\n",
      "Trained batch 662 batch loss 1.05409336 epoch total loss 1.09756434\n",
      "Trained batch 663 batch loss 0.976715863 epoch total loss 1.09738207\n",
      "Trained batch 664 batch loss 1.04233825 epoch total loss 1.09729922\n",
      "Trained batch 665 batch loss 1.07590258 epoch total loss 1.09726715\n",
      "Trained batch 666 batch loss 1.01464236 epoch total loss 1.09714305\n",
      "Trained batch 667 batch loss 1.06888545 epoch total loss 1.09710073\n",
      "Trained batch 668 batch loss 1.20015097 epoch total loss 1.09725499\n",
      "Trained batch 669 batch loss 1.12278378 epoch total loss 1.09729314\n",
      "Trained batch 670 batch loss 0.943016291 epoch total loss 1.09706283\n",
      "Trained batch 671 batch loss 1.12712538 epoch total loss 1.09710765\n",
      "Trained batch 672 batch loss 1.09109 epoch total loss 1.09709871\n",
      "Trained batch 673 batch loss 1.06560326 epoch total loss 1.09705186\n",
      "Trained batch 674 batch loss 1.10254717 epoch total loss 1.09706008\n",
      "Trained batch 675 batch loss 1.16416144 epoch total loss 1.0971595\n",
      "Trained batch 676 batch loss 1.19361091 epoch total loss 1.09730208\n",
      "Trained batch 677 batch loss 1.1241492 epoch total loss 1.09734178\n",
      "Trained batch 678 batch loss 1.12300789 epoch total loss 1.09737957\n",
      "Trained batch 679 batch loss 1.05213392 epoch total loss 1.09731293\n",
      "Trained batch 680 batch loss 0.946045041 epoch total loss 1.09709048\n",
      "Trained batch 681 batch loss 1.04054809 epoch total loss 1.09700739\n",
      "Trained batch 682 batch loss 1.13255227 epoch total loss 1.09705961\n",
      "Trained batch 683 batch loss 1.07989049 epoch total loss 1.09703445\n",
      "Trained batch 684 batch loss 1.04791832 epoch total loss 1.09696269\n",
      "Trained batch 685 batch loss 0.965693831 epoch total loss 1.096771\n",
      "Trained batch 686 batch loss 1.01017749 epoch total loss 1.09664476\n",
      "Trained batch 687 batch loss 1.1664474 epoch total loss 1.09674644\n",
      "Trained batch 688 batch loss 1.20152044 epoch total loss 1.09689867\n",
      "Trained batch 689 batch loss 1.05944085 epoch total loss 1.09684432\n",
      "Trained batch 690 batch loss 1.07778037 epoch total loss 1.09681666\n",
      "Trained batch 691 batch loss 0.887078881 epoch total loss 1.09651315\n",
      "Trained batch 692 batch loss 0.787022054 epoch total loss 1.096066\n",
      "Trained batch 693 batch loss 0.775432 epoch total loss 1.09560335\n",
      "Trained batch 694 batch loss 0.950564265 epoch total loss 1.09539437\n",
      "Trained batch 695 batch loss 1.08427763 epoch total loss 1.0953784\n",
      "Trained batch 696 batch loss 1.11288393 epoch total loss 1.09540343\n",
      "Trained batch 697 batch loss 1.07727563 epoch total loss 1.09537745\n",
      "Trained batch 698 batch loss 1.19425 epoch total loss 1.09551919\n",
      "Trained batch 699 batch loss 1.22855926 epoch total loss 1.09570944\n",
      "Trained batch 700 batch loss 1.03082526 epoch total loss 1.09561682\n",
      "Trained batch 701 batch loss 1.15188527 epoch total loss 1.09569705\n",
      "Trained batch 702 batch loss 1.0364362 epoch total loss 1.09561265\n",
      "Trained batch 703 batch loss 1.05137348 epoch total loss 1.0955497\n",
      "Trained batch 704 batch loss 1.06994343 epoch total loss 1.09551334\n",
      "Trained batch 705 batch loss 1.03131878 epoch total loss 1.09542227\n",
      "Trained batch 706 batch loss 1.07462811 epoch total loss 1.09539282\n",
      "Trained batch 707 batch loss 1.17623961 epoch total loss 1.09550726\n",
      "Trained batch 708 batch loss 1.16834807 epoch total loss 1.09561014\n",
      "Trained batch 709 batch loss 1.30927 epoch total loss 1.0959115\n",
      "Trained batch 710 batch loss 1.20875692 epoch total loss 1.09607041\n",
      "Trained batch 711 batch loss 1.19013059 epoch total loss 1.09620261\n",
      "Trained batch 712 batch loss 0.978220582 epoch total loss 1.09603691\n",
      "Trained batch 713 batch loss 0.922941804 epoch total loss 1.09579408\n",
      "Trained batch 714 batch loss 0.920197487 epoch total loss 1.09554827\n",
      "Trained batch 715 batch loss 0.794490278 epoch total loss 1.09512722\n",
      "Trained batch 716 batch loss 0.971510231 epoch total loss 1.09495449\n",
      "Trained batch 717 batch loss 1.06647086 epoch total loss 1.09491479\n",
      "Trained batch 718 batch loss 1.10256279 epoch total loss 1.0949254\n",
      "Trained batch 719 batch loss 1.07394338 epoch total loss 1.0948962\n",
      "Trained batch 720 batch loss 1.17067015 epoch total loss 1.09500134\n",
      "Trained batch 721 batch loss 1.38721776 epoch total loss 1.09540665\n",
      "Trained batch 722 batch loss 1.28280723 epoch total loss 1.09566629\n",
      "Trained batch 723 batch loss 1.15455902 epoch total loss 1.09574771\n",
      "Trained batch 724 batch loss 0.998643637 epoch total loss 1.0956136\n",
      "Trained batch 725 batch loss 0.887804866 epoch total loss 1.09532702\n",
      "Trained batch 726 batch loss 0.94508779 epoch total loss 1.09512\n",
      "Trained batch 727 batch loss 1.05449831 epoch total loss 1.09506416\n",
      "Trained batch 728 batch loss 0.957490921 epoch total loss 1.09487522\n",
      "Trained batch 729 batch loss 0.765615761 epoch total loss 1.09442353\n",
      "Trained batch 730 batch loss 0.822072148 epoch total loss 1.09405053\n",
      "Trained batch 731 batch loss 0.916463137 epoch total loss 1.09380758\n",
      "Trained batch 732 batch loss 0.99600023 epoch total loss 1.09367383\n",
      "Trained batch 733 batch loss 0.927699864 epoch total loss 1.09344745\n",
      "Trained batch 734 batch loss 1.01769388 epoch total loss 1.09334421\n",
      "Trained batch 735 batch loss 1.10930884 epoch total loss 1.09336591\n",
      "Trained batch 736 batch loss 1.12701654 epoch total loss 1.09341168\n",
      "Trained batch 737 batch loss 1.04123163 epoch total loss 1.09334087\n",
      "Trained batch 738 batch loss 0.830320776 epoch total loss 1.09298444\n",
      "Trained batch 739 batch loss 0.765861571 epoch total loss 1.09254181\n",
      "Trained batch 740 batch loss 1.05738008 epoch total loss 1.09249437\n",
      "Trained batch 741 batch loss 1.19717693 epoch total loss 1.09263563\n",
      "Trained batch 742 batch loss 1.32730758 epoch total loss 1.09295189\n",
      "Trained batch 743 batch loss 1.42598689 epoch total loss 1.09340012\n",
      "Trained batch 744 batch loss 1.0591054 epoch total loss 1.09335399\n",
      "Trained batch 745 batch loss 0.981871128 epoch total loss 1.09320438\n",
      "Trained batch 746 batch loss 1.1538105 epoch total loss 1.09328556\n",
      "Trained batch 747 batch loss 1.20430756 epoch total loss 1.09343421\n",
      "Trained batch 748 batch loss 1.11216497 epoch total loss 1.09345925\n",
      "Trained batch 749 batch loss 1.12654543 epoch total loss 1.09350336\n",
      "Trained batch 750 batch loss 1.14059567 epoch total loss 1.0935663\n",
      "Trained batch 751 batch loss 1.23847723 epoch total loss 1.09375918\n",
      "Trained batch 752 batch loss 1.03732181 epoch total loss 1.09368408\n",
      "Trained batch 753 batch loss 1.07969177 epoch total loss 1.09366548\n",
      "Trained batch 754 batch loss 1.01104665 epoch total loss 1.09355593\n",
      "Trained batch 755 batch loss 0.986714959 epoch total loss 1.09341443\n",
      "Trained batch 756 batch loss 1.00904512 epoch total loss 1.09330285\n",
      "Trained batch 757 batch loss 1.1892513 epoch total loss 1.09342957\n",
      "Trained batch 758 batch loss 0.907550454 epoch total loss 1.09318435\n",
      "Trained batch 759 batch loss 0.971719742 epoch total loss 1.09302437\n",
      "Trained batch 760 batch loss 0.998387277 epoch total loss 1.0928998\n",
      "Trained batch 761 batch loss 1.09619606 epoch total loss 1.09290421\n",
      "Trained batch 762 batch loss 1.21312058 epoch total loss 1.09306192\n",
      "Trained batch 763 batch loss 1.13012 epoch total loss 1.09311056\n",
      "Trained batch 764 batch loss 1.01288617 epoch total loss 1.09300554\n",
      "Trained batch 765 batch loss 0.92227596 epoch total loss 1.09278238\n",
      "Trained batch 766 batch loss 1.05010498 epoch total loss 1.09272671\n",
      "Trained batch 767 batch loss 1.19641209 epoch total loss 1.09286189\n",
      "Trained batch 768 batch loss 1.13702595 epoch total loss 1.09291935\n",
      "Trained batch 769 batch loss 1.30002975 epoch total loss 1.09318864\n",
      "Trained batch 770 batch loss 1.30501747 epoch total loss 1.09346378\n",
      "Trained batch 771 batch loss 1.17309678 epoch total loss 1.09356701\n",
      "Trained batch 772 batch loss 1.1420399 epoch total loss 1.09362984\n",
      "Trained batch 773 batch loss 1.13805377 epoch total loss 1.0936873\n",
      "Trained batch 774 batch loss 1.1883055 epoch total loss 1.09380949\n",
      "Trained batch 775 batch loss 1.23895741 epoch total loss 1.09399676\n",
      "Trained batch 776 batch loss 1.16512215 epoch total loss 1.09408844\n",
      "Trained batch 777 batch loss 1.22408926 epoch total loss 1.09425569\n",
      "Trained batch 778 batch loss 1.1872983 epoch total loss 1.09437537\n",
      "Trained batch 779 batch loss 1.17530632 epoch total loss 1.0944792\n",
      "Trained batch 780 batch loss 1.15245676 epoch total loss 1.09455359\n",
      "Trained batch 781 batch loss 1.13512516 epoch total loss 1.09460557\n",
      "Trained batch 782 batch loss 1.03517354 epoch total loss 1.09452951\n",
      "Trained batch 783 batch loss 0.996792 epoch total loss 1.09440458\n",
      "Trained batch 784 batch loss 1.0099349 epoch total loss 1.09429693\n",
      "Trained batch 785 batch loss 0.97798562 epoch total loss 1.09414876\n",
      "Trained batch 786 batch loss 1.00487244 epoch total loss 1.09403515\n",
      "Trained batch 787 batch loss 0.857090175 epoch total loss 1.09373415\n",
      "Trained batch 788 batch loss 0.999901891 epoch total loss 1.09361494\n",
      "Trained batch 789 batch loss 0.943581 epoch total loss 1.09342492\n",
      "Trained batch 790 batch loss 0.978613496 epoch total loss 1.0932796\n",
      "Trained batch 791 batch loss 0.907121778 epoch total loss 1.09304416\n",
      "Trained batch 792 batch loss 1.06046212 epoch total loss 1.09300303\n",
      "Trained batch 793 batch loss 1.07468569 epoch total loss 1.09298\n",
      "Trained batch 794 batch loss 1.04009819 epoch total loss 1.09291339\n",
      "Trained batch 795 batch loss 1.38180494 epoch total loss 1.09327674\n",
      "Trained batch 796 batch loss 1.13302612 epoch total loss 1.09332669\n",
      "Trained batch 797 batch loss 1.06632233 epoch total loss 1.09329283\n",
      "Trained batch 798 batch loss 1.22784805 epoch total loss 1.09346151\n",
      "Trained batch 799 batch loss 1.10444641 epoch total loss 1.09347522\n",
      "Trained batch 800 batch loss 1.1734705 epoch total loss 1.09357524\n",
      "Trained batch 801 batch loss 1.06201696 epoch total loss 1.09353578\n",
      "Trained batch 802 batch loss 0.904075503 epoch total loss 1.09329951\n",
      "Trained batch 803 batch loss 1.10680461 epoch total loss 1.09331632\n",
      "Trained batch 804 batch loss 1.25400734 epoch total loss 1.09351623\n",
      "Trained batch 805 batch loss 1.06560016 epoch total loss 1.09348154\n",
      "Trained batch 806 batch loss 1.12407732 epoch total loss 1.09351957\n",
      "Trained batch 807 batch loss 1.1971879 epoch total loss 1.09364808\n",
      "Trained batch 808 batch loss 0.906775057 epoch total loss 1.09341681\n",
      "Trained batch 809 batch loss 1.13207543 epoch total loss 1.09346461\n",
      "Trained batch 810 batch loss 1.2813642 epoch total loss 1.09369659\n",
      "Trained batch 811 batch loss 1.18720317 epoch total loss 1.09381187\n",
      "Trained batch 812 batch loss 1.03607202 epoch total loss 1.0937407\n",
      "Trained batch 813 batch loss 0.89059335 epoch total loss 1.09349084\n",
      "Trained batch 814 batch loss 0.869379163 epoch total loss 1.09321558\n",
      "Trained batch 815 batch loss 0.95924592 epoch total loss 1.09305108\n",
      "Trained batch 816 batch loss 0.986972928 epoch total loss 1.09292114\n",
      "Trained batch 817 batch loss 0.927379131 epoch total loss 1.09271848\n",
      "Trained batch 818 batch loss 0.928296804 epoch total loss 1.0925175\n",
      "Trained batch 819 batch loss 1.0075531 epoch total loss 1.09241378\n",
      "Trained batch 820 batch loss 0.993383527 epoch total loss 1.09229302\n",
      "Trained batch 821 batch loss 1.03657401 epoch total loss 1.09222519\n",
      "Trained batch 822 batch loss 1.15521252 epoch total loss 1.09230185\n",
      "Trained batch 823 batch loss 1.09779954 epoch total loss 1.0923084\n",
      "Trained batch 824 batch loss 1.01365256 epoch total loss 1.09221303\n",
      "Trained batch 825 batch loss 0.92204237 epoch total loss 1.0920068\n",
      "Trained batch 826 batch loss 0.926081061 epoch total loss 1.09180593\n",
      "Trained batch 827 batch loss 0.942692 epoch total loss 1.09162557\n",
      "Trained batch 828 batch loss 1.02558529 epoch total loss 1.09154582\n",
      "Trained batch 829 batch loss 1.04111671 epoch total loss 1.09148502\n",
      "Trained batch 830 batch loss 0.950631917 epoch total loss 1.09131527\n",
      "Trained batch 831 batch loss 0.95855087 epoch total loss 1.09115553\n",
      "Trained batch 832 batch loss 1.10179067 epoch total loss 1.09116828\n",
      "Trained batch 833 batch loss 1.00008714 epoch total loss 1.09105897\n",
      "Trained batch 834 batch loss 1.09696889 epoch total loss 1.09106612\n",
      "Trained batch 835 batch loss 0.978090525 epoch total loss 1.09093082\n",
      "Trained batch 836 batch loss 1.03206253 epoch total loss 1.09086037\n",
      "Trained batch 837 batch loss 1.00152659 epoch total loss 1.09075356\n",
      "Trained batch 838 batch loss 0.95472455 epoch total loss 1.09059119\n",
      "Trained batch 839 batch loss 0.99628365 epoch total loss 1.09047878\n",
      "Trained batch 840 batch loss 1.19184017 epoch total loss 1.09059954\n",
      "Trained batch 841 batch loss 1.41676128 epoch total loss 1.09098732\n",
      "Trained batch 842 batch loss 1.24704456 epoch total loss 1.0911727\n",
      "Trained batch 843 batch loss 1.18860424 epoch total loss 1.09128821\n",
      "Trained batch 844 batch loss 1.06480193 epoch total loss 1.09125686\n",
      "Trained batch 845 batch loss 0.969125628 epoch total loss 1.09111238\n",
      "Trained batch 846 batch loss 1.02699912 epoch total loss 1.09103656\n",
      "Trained batch 847 batch loss 1.11582947 epoch total loss 1.09106576\n",
      "Trained batch 848 batch loss 1.10634959 epoch total loss 1.09108377\n",
      "Trained batch 849 batch loss 1.25500011 epoch total loss 1.09127688\n",
      "Trained batch 850 batch loss 1.25657415 epoch total loss 1.09147143\n",
      "Trained batch 851 batch loss 1.26379371 epoch total loss 1.09167385\n",
      "Trained batch 852 batch loss 1.17285597 epoch total loss 1.0917691\n",
      "Trained batch 853 batch loss 1.16609824 epoch total loss 1.09185624\n",
      "Trained batch 854 batch loss 1.05249023 epoch total loss 1.09181023\n",
      "Trained batch 855 batch loss 1.02653658 epoch total loss 1.09173381\n",
      "Trained batch 856 batch loss 1.01993537 epoch total loss 1.09165\n",
      "Trained batch 857 batch loss 1.16432548 epoch total loss 1.09173477\n",
      "Trained batch 858 batch loss 1.24640524 epoch total loss 1.09191501\n",
      "Trained batch 859 batch loss 1.07022476 epoch total loss 1.09188986\n",
      "Trained batch 860 batch loss 1.01962686 epoch total loss 1.09180582\n",
      "Trained batch 861 batch loss 0.939711094 epoch total loss 1.09162915\n",
      "Trained batch 862 batch loss 1.05425346 epoch total loss 1.09158576\n",
      "Trained batch 863 batch loss 0.939678907 epoch total loss 1.0914098\n",
      "Trained batch 864 batch loss 1.05530548 epoch total loss 1.09136796\n",
      "Trained batch 865 batch loss 0.984885693 epoch total loss 1.09124494\n",
      "Trained batch 866 batch loss 1.00864768 epoch total loss 1.09114957\n",
      "Trained batch 867 batch loss 1.12757957 epoch total loss 1.09119153\n",
      "Trained batch 868 batch loss 1.1641053 epoch total loss 1.09127557\n",
      "Trained batch 869 batch loss 1.1750071 epoch total loss 1.09137189\n",
      "Trained batch 870 batch loss 1.12087643 epoch total loss 1.09140575\n",
      "Trained batch 871 batch loss 1.23952103 epoch total loss 1.09157574\n",
      "Trained batch 872 batch loss 1.17391634 epoch total loss 1.09167016\n",
      "Trained batch 873 batch loss 1.10960734 epoch total loss 1.09169078\n",
      "Trained batch 874 batch loss 1.13974786 epoch total loss 1.09174573\n",
      "Trained batch 875 batch loss 1.01009548 epoch total loss 1.09165239\n",
      "Trained batch 876 batch loss 1.07448065 epoch total loss 1.09163284\n",
      "Trained batch 877 batch loss 0.899509907 epoch total loss 1.09141374\n",
      "Trained batch 878 batch loss 0.924969673 epoch total loss 1.09122419\n",
      "Trained batch 879 batch loss 0.983385801 epoch total loss 1.09110153\n",
      "Trained batch 880 batch loss 0.967397869 epoch total loss 1.09096098\n",
      "Trained batch 881 batch loss 0.826159596 epoch total loss 1.09066045\n",
      "Trained batch 882 batch loss 0.838000119 epoch total loss 1.09037399\n",
      "Trained batch 883 batch loss 0.765169144 epoch total loss 1.09000576\n",
      "Trained batch 884 batch loss 0.953993201 epoch total loss 1.08985186\n",
      "Trained batch 885 batch loss 1.08102179 epoch total loss 1.08984184\n",
      "Trained batch 886 batch loss 0.988758802 epoch total loss 1.08972776\n",
      "Trained batch 887 batch loss 1.12892342 epoch total loss 1.08977187\n",
      "Trained batch 888 batch loss 1.07540035 epoch total loss 1.08975565\n",
      "Trained batch 889 batch loss 1.13343608 epoch total loss 1.08980477\n",
      "Trained batch 890 batch loss 1.15200269 epoch total loss 1.08987463\n",
      "Trained batch 891 batch loss 1.09487164 epoch total loss 1.08988023\n",
      "Trained batch 892 batch loss 1.05027235 epoch total loss 1.08983588\n",
      "Trained batch 893 batch loss 1.22677124 epoch total loss 1.08998919\n",
      "Trained batch 894 batch loss 1.33933401 epoch total loss 1.09026814\n",
      "Trained batch 895 batch loss 1.277812 epoch total loss 1.09047771\n",
      "Trained batch 896 batch loss 1.41829634 epoch total loss 1.09084356\n",
      "Trained batch 897 batch loss 1.18752503 epoch total loss 1.09095132\n",
      "Trained batch 898 batch loss 0.972948551 epoch total loss 1.09082\n",
      "Trained batch 899 batch loss 1.10865951 epoch total loss 1.09083974\n",
      "Trained batch 900 batch loss 1.04867482 epoch total loss 1.09079289\n",
      "Trained batch 901 batch loss 0.93357569 epoch total loss 1.09061837\n",
      "Trained batch 902 batch loss 0.986541569 epoch total loss 1.09050298\n",
      "Trained batch 903 batch loss 0.979132891 epoch total loss 1.0903796\n",
      "Trained batch 904 batch loss 0.991966367 epoch total loss 1.09027076\n",
      "Trained batch 905 batch loss 0.866905451 epoch total loss 1.09002388\n",
      "Trained batch 906 batch loss 0.938444078 epoch total loss 1.08985651\n",
      "Trained batch 907 batch loss 1.04158175 epoch total loss 1.08980334\n",
      "Trained batch 908 batch loss 1.03636694 epoch total loss 1.08974445\n",
      "Trained batch 909 batch loss 0.909393132 epoch total loss 1.08954608\n",
      "Trained batch 910 batch loss 0.933377743 epoch total loss 1.08937442\n",
      "Trained batch 911 batch loss 0.970352173 epoch total loss 1.08924377\n",
      "Trained batch 912 batch loss 1.14472055 epoch total loss 1.08930457\n",
      "Trained batch 913 batch loss 1.13685989 epoch total loss 1.08935666\n",
      "Trained batch 914 batch loss 1.0061444 epoch total loss 1.08926558\n",
      "Trained batch 915 batch loss 0.875293493 epoch total loss 1.0890317\n",
      "Trained batch 916 batch loss 0.918000877 epoch total loss 1.08884501\n",
      "Trained batch 917 batch loss 0.979245663 epoch total loss 1.08872557\n",
      "Trained batch 918 batch loss 1.0743351 epoch total loss 1.08870983\n",
      "Trained batch 919 batch loss 1.15759194 epoch total loss 1.08878481\n",
      "Trained batch 920 batch loss 1.35142767 epoch total loss 1.08907032\n",
      "Trained batch 921 batch loss 1.3160255 epoch total loss 1.08931673\n",
      "Trained batch 922 batch loss 1.21300125 epoch total loss 1.08945096\n",
      "Trained batch 923 batch loss 1.25904012 epoch total loss 1.08963466\n",
      "Trained batch 924 batch loss 1.06089139 epoch total loss 1.08960354\n",
      "Trained batch 925 batch loss 1.04536462 epoch total loss 1.08955574\n",
      "Trained batch 926 batch loss 1.05940413 epoch total loss 1.0895232\n",
      "Trained batch 927 batch loss 1.07750225 epoch total loss 1.0895102\n",
      "Trained batch 928 batch loss 1.07079315 epoch total loss 1.08949\n",
      "Trained batch 929 batch loss 1.12803686 epoch total loss 1.08953154\n",
      "Trained batch 930 batch loss 1.17493391 epoch total loss 1.08962333\n",
      "Trained batch 931 batch loss 1.25691855 epoch total loss 1.08980298\n",
      "Trained batch 932 batch loss 1.09493947 epoch total loss 1.08980846\n",
      "Trained batch 933 batch loss 1.18642104 epoch total loss 1.08991206\n",
      "Trained batch 934 batch loss 1.16400504 epoch total loss 1.08999133\n",
      "Trained batch 935 batch loss 1.15209746 epoch total loss 1.09005785\n",
      "Trained batch 936 batch loss 1.19208181 epoch total loss 1.09016681\n",
      "Trained batch 937 batch loss 1.18008578 epoch total loss 1.09026277\n",
      "Trained batch 938 batch loss 1.21815276 epoch total loss 1.09039915\n",
      "Trained batch 939 batch loss 1.19098747 epoch total loss 1.0905062\n",
      "Trained batch 940 batch loss 1.14910984 epoch total loss 1.09056866\n",
      "Trained batch 941 batch loss 1.07937455 epoch total loss 1.09055674\n",
      "Trained batch 942 batch loss 1.06475484 epoch total loss 1.0905292\n",
      "Trained batch 943 batch loss 1.06385326 epoch total loss 1.09050095\n",
      "Trained batch 944 batch loss 1.09740317 epoch total loss 1.09050834\n",
      "Trained batch 945 batch loss 1.11661446 epoch total loss 1.09053588\n",
      "Trained batch 946 batch loss 1.01267743 epoch total loss 1.09045362\n",
      "Trained batch 947 batch loss 1.00809097 epoch total loss 1.0903666\n",
      "Trained batch 948 batch loss 0.932665825 epoch total loss 1.09020019\n",
      "Trained batch 949 batch loss 0.95887208 epoch total loss 1.09006178\n",
      "Trained batch 950 batch loss 1.1206367 epoch total loss 1.09009397\n",
      "Trained batch 951 batch loss 1.04795277 epoch total loss 1.09004962\n",
      "Trained batch 952 batch loss 1.12927699 epoch total loss 1.09009087\n",
      "Trained batch 953 batch loss 1.08663678 epoch total loss 1.09008729\n",
      "Trained batch 954 batch loss 1.12992382 epoch total loss 1.09012902\n",
      "Trained batch 955 batch loss 1.15171683 epoch total loss 1.09019351\n",
      "Trained batch 956 batch loss 1.41018379 epoch total loss 1.09052813\n",
      "Trained batch 957 batch loss 1.35350144 epoch total loss 1.09080291\n",
      "Trained batch 958 batch loss 1.24230647 epoch total loss 1.0909611\n",
      "Trained batch 959 batch loss 1.25248575 epoch total loss 1.09112954\n",
      "Trained batch 960 batch loss 1.2347 epoch total loss 1.09127915\n",
      "Trained batch 961 batch loss 1.16622639 epoch total loss 1.09135711\n",
      "Trained batch 962 batch loss 1.10464334 epoch total loss 1.09137094\n",
      "Trained batch 963 batch loss 1.1406312 epoch total loss 1.09142208\n",
      "Trained batch 964 batch loss 1.20196843 epoch total loss 1.09153676\n",
      "Trained batch 965 batch loss 1.11298561 epoch total loss 1.09155905\n",
      "Trained batch 966 batch loss 1.2336657 epoch total loss 1.09170616\n",
      "Trained batch 967 batch loss 1.14370275 epoch total loss 1.09175992\n",
      "Trained batch 968 batch loss 1.05707693 epoch total loss 1.09172416\n",
      "Trained batch 969 batch loss 0.985907376 epoch total loss 1.09161496\n",
      "Trained batch 970 batch loss 0.9858706 epoch total loss 1.09150589\n",
      "Trained batch 971 batch loss 0.968656778 epoch total loss 1.0913794\n",
      "Trained batch 972 batch loss 1.06072676 epoch total loss 1.09134781\n",
      "Trained batch 973 batch loss 1.13651907 epoch total loss 1.09139419\n",
      "Trained batch 974 batch loss 1.20192814 epoch total loss 1.09150767\n",
      "Trained batch 975 batch loss 1.18981409 epoch total loss 1.09160841\n",
      "Trained batch 976 batch loss 1.269449 epoch total loss 1.09179068\n",
      "Trained batch 977 batch loss 1.34297109 epoch total loss 1.09204781\n",
      "Trained batch 978 batch loss 1.14675236 epoch total loss 1.09210372\n",
      "Trained batch 979 batch loss 1.05462027 epoch total loss 1.09206533\n",
      "Trained batch 980 batch loss 1.02143407 epoch total loss 1.09199333\n",
      "Trained batch 981 batch loss 1.22005367 epoch total loss 1.09212387\n",
      "Trained batch 982 batch loss 1.34897017 epoch total loss 1.09238553\n",
      "Trained batch 983 batch loss 1.2161231 epoch total loss 1.0925113\n",
      "Trained batch 984 batch loss 1.20846045 epoch total loss 1.09262919\n",
      "Trained batch 985 batch loss 1.21279871 epoch total loss 1.09275115\n",
      "Trained batch 986 batch loss 1.28022623 epoch total loss 1.09294128\n",
      "Trained batch 987 batch loss 1.1870904 epoch total loss 1.09303677\n",
      "Trained batch 988 batch loss 1.0738194 epoch total loss 1.09301734\n",
      "Trained batch 989 batch loss 1.14220893 epoch total loss 1.09306705\n",
      "Trained batch 990 batch loss 1.13883781 epoch total loss 1.0931133\n",
      "Trained batch 991 batch loss 1.13080812 epoch total loss 1.09315133\n",
      "Trained batch 992 batch loss 1.17450166 epoch total loss 1.09323347\n",
      "Trained batch 993 batch loss 1.1194396 epoch total loss 1.09325981\n",
      "Trained batch 994 batch loss 1.17245626 epoch total loss 1.09333944\n",
      "Trained batch 995 batch loss 1.16710019 epoch total loss 1.09341359\n",
      "Trained batch 996 batch loss 1.20702362 epoch total loss 1.09352767\n",
      "Trained batch 997 batch loss 1.23505747 epoch total loss 1.09366965\n",
      "Trained batch 998 batch loss 1.13267148 epoch total loss 1.09370875\n",
      "Trained batch 999 batch loss 1.20032132 epoch total loss 1.09381545\n",
      "Trained batch 1000 batch loss 1.00208151 epoch total loss 1.09372377\n",
      "Trained batch 1001 batch loss 0.953815341 epoch total loss 1.09358406\n",
      "Trained batch 1002 batch loss 0.950234652 epoch total loss 1.09344089\n",
      "Trained batch 1003 batch loss 1.02076221 epoch total loss 1.09336841\n",
      "Trained batch 1004 batch loss 1.21477652 epoch total loss 1.09348929\n",
      "Trained batch 1005 batch loss 1.16596198 epoch total loss 1.09356153\n",
      "Trained batch 1006 batch loss 1.12142265 epoch total loss 1.09358919\n",
      "Trained batch 1007 batch loss 1.02613795 epoch total loss 1.09352219\n",
      "Trained batch 1008 batch loss 0.99437952 epoch total loss 1.09342384\n",
      "Trained batch 1009 batch loss 1.06628084 epoch total loss 1.09339702\n",
      "Trained batch 1010 batch loss 1.16396856 epoch total loss 1.09346688\n",
      "Trained batch 1011 batch loss 1.13206923 epoch total loss 1.09350502\n",
      "Trained batch 1012 batch loss 1.04859447 epoch total loss 1.09346068\n",
      "Trained batch 1013 batch loss 1.11825204 epoch total loss 1.09348512\n",
      "Trained batch 1014 batch loss 1.27393115 epoch total loss 1.0936631\n",
      "Trained batch 1015 batch loss 1.25087452 epoch total loss 1.09381795\n",
      "Trained batch 1016 batch loss 1.10396218 epoch total loss 1.09382796\n",
      "Trained batch 1017 batch loss 1.12965393 epoch total loss 1.09386313\n",
      "Trained batch 1018 batch loss 1.03687048 epoch total loss 1.09380722\n",
      "Trained batch 1019 batch loss 0.98499769 epoch total loss 1.09370041\n",
      "Trained batch 1020 batch loss 1.02108192 epoch total loss 1.09362924\n",
      "Trained batch 1021 batch loss 1.21154869 epoch total loss 1.09374475\n",
      "Trained batch 1022 batch loss 1.35988355 epoch total loss 1.09400511\n",
      "Trained batch 1023 batch loss 1.05680573 epoch total loss 1.09396875\n",
      "Trained batch 1024 batch loss 1.04348946 epoch total loss 1.0939194\n",
      "Trained batch 1025 batch loss 1.00149298 epoch total loss 1.09382915\n",
      "Trained batch 1026 batch loss 0.929236054 epoch total loss 1.0936687\n",
      "Trained batch 1027 batch loss 1.07604599 epoch total loss 1.09365153\n",
      "Trained batch 1028 batch loss 1.21338797 epoch total loss 1.093768\n",
      "Trained batch 1029 batch loss 1.12030077 epoch total loss 1.09379387\n",
      "Trained batch 1030 batch loss 1.14194477 epoch total loss 1.09384072\n",
      "Trained batch 1031 batch loss 1.16489971 epoch total loss 1.09390962\n",
      "Trained batch 1032 batch loss 1.02246928 epoch total loss 1.09384036\n",
      "Trained batch 1033 batch loss 1.09223461 epoch total loss 1.09383881\n",
      "Trained batch 1034 batch loss 1.1429044 epoch total loss 1.09388638\n",
      "Trained batch 1035 batch loss 1.07138658 epoch total loss 1.09386468\n",
      "Trained batch 1036 batch loss 1.24526536 epoch total loss 1.09401071\n",
      "Trained batch 1037 batch loss 1.02069378 epoch total loss 1.09394014\n",
      "Trained batch 1038 batch loss 0.961705327 epoch total loss 1.0938127\n",
      "Trained batch 1039 batch loss 0.954076886 epoch total loss 1.09367824\n",
      "Trained batch 1040 batch loss 1.05140519 epoch total loss 1.09363759\n",
      "Trained batch 1041 batch loss 1.11204767 epoch total loss 1.09365523\n",
      "Trained batch 1042 batch loss 1.07365799 epoch total loss 1.09363604\n",
      "Trained batch 1043 batch loss 1.04921055 epoch total loss 1.09359336\n",
      "Trained batch 1044 batch loss 0.970518827 epoch total loss 1.09347546\n",
      "Trained batch 1045 batch loss 0.962005913 epoch total loss 1.0933497\n",
      "Trained batch 1046 batch loss 1.01411247 epoch total loss 1.093274\n",
      "Trained batch 1047 batch loss 0.987208605 epoch total loss 1.09317267\n",
      "Trained batch 1048 batch loss 0.951266408 epoch total loss 1.09303725\n",
      "Trained batch 1049 batch loss 0.89412421 epoch total loss 1.0928477\n",
      "Trained batch 1050 batch loss 1.06002569 epoch total loss 1.09281647\n",
      "Trained batch 1051 batch loss 1.05323207 epoch total loss 1.0927788\n",
      "Trained batch 1052 batch loss 0.891709268 epoch total loss 1.09258771\n",
      "Trained batch 1053 batch loss 1.0776 epoch total loss 1.09257352\n",
      "Trained batch 1054 batch loss 0.989085078 epoch total loss 1.0924753\n",
      "Trained batch 1055 batch loss 1.09540224 epoch total loss 1.09247816\n",
      "Trained batch 1056 batch loss 1.16821229 epoch total loss 1.09254992\n",
      "Trained batch 1057 batch loss 1.17719436 epoch total loss 1.09263\n",
      "Trained batch 1058 batch loss 1.17607212 epoch total loss 1.09270883\n",
      "Trained batch 1059 batch loss 1.03716373 epoch total loss 1.09265637\n",
      "Trained batch 1060 batch loss 1.02766323 epoch total loss 1.0925951\n",
      "Trained batch 1061 batch loss 1.0475806 epoch total loss 1.09255266\n",
      "Trained batch 1062 batch loss 1.00604641 epoch total loss 1.09247124\n",
      "Trained batch 1063 batch loss 1.11457741 epoch total loss 1.0924921\n",
      "Trained batch 1064 batch loss 1.07969987 epoch total loss 1.09248006\n",
      "Trained batch 1065 batch loss 1.07617021 epoch total loss 1.0924648\n",
      "Trained batch 1066 batch loss 1.09377849 epoch total loss 1.092466\n",
      "Trained batch 1067 batch loss 0.990818799 epoch total loss 1.09237075\n",
      "Trained batch 1068 batch loss 0.971993089 epoch total loss 1.0922581\n",
      "Trained batch 1069 batch loss 1.13689518 epoch total loss 1.09229982\n",
      "Trained batch 1070 batch loss 1.05040956 epoch total loss 1.0922606\n",
      "Trained batch 1071 batch loss 1.15860271 epoch total loss 1.09232259\n",
      "Trained batch 1072 batch loss 1.06163275 epoch total loss 1.09229398\n",
      "Trained batch 1073 batch loss 0.993000269 epoch total loss 1.09220147\n",
      "Trained batch 1074 batch loss 1.07000601 epoch total loss 1.09218073\n",
      "Trained batch 1075 batch loss 0.927056611 epoch total loss 1.09202707\n",
      "Trained batch 1076 batch loss 0.997304797 epoch total loss 1.09193897\n",
      "Trained batch 1077 batch loss 1.19354558 epoch total loss 1.09203339\n",
      "Trained batch 1078 batch loss 1.1035018 epoch total loss 1.09204412\n",
      "Trained batch 1079 batch loss 1.14086068 epoch total loss 1.0920893\n",
      "Trained batch 1080 batch loss 1.14493251 epoch total loss 1.09213817\n",
      "Trained batch 1081 batch loss 1.25756609 epoch total loss 1.09229124\n",
      "Trained batch 1082 batch loss 1.16144967 epoch total loss 1.09235525\n",
      "Trained batch 1083 batch loss 1.21785367 epoch total loss 1.09247112\n",
      "Trained batch 1084 batch loss 1.11901176 epoch total loss 1.09249568\n",
      "Trained batch 1085 batch loss 1.22104549 epoch total loss 1.09261417\n",
      "Trained batch 1086 batch loss 1.06497097 epoch total loss 1.09258866\n",
      "Trained batch 1087 batch loss 1.08558285 epoch total loss 1.09258223\n",
      "Trained batch 1088 batch loss 1.18052745 epoch total loss 1.09266305\n",
      "Trained batch 1089 batch loss 1.25298166 epoch total loss 1.09281015\n",
      "Trained batch 1090 batch loss 1.12475395 epoch total loss 1.09283948\n",
      "Trained batch 1091 batch loss 0.994722486 epoch total loss 1.0927496\n",
      "Trained batch 1092 batch loss 1.08112562 epoch total loss 1.09273899\n",
      "Trained batch 1093 batch loss 0.914319158 epoch total loss 1.09257579\n",
      "Trained batch 1094 batch loss 0.956343055 epoch total loss 1.09245121\n",
      "Trained batch 1095 batch loss 0.976807594 epoch total loss 1.0923456\n",
      "Trained batch 1096 batch loss 0.999486506 epoch total loss 1.09226084\n",
      "Trained batch 1097 batch loss 1.1250639 epoch total loss 1.09229088\n",
      "Trained batch 1098 batch loss 0.993868947 epoch total loss 1.09220123\n",
      "Trained batch 1099 batch loss 0.928118229 epoch total loss 1.09205186\n",
      "Trained batch 1100 batch loss 1.12706268 epoch total loss 1.09208369\n",
      "Trained batch 1101 batch loss 0.997744381 epoch total loss 1.0919981\n",
      "Trained batch 1102 batch loss 1.02672911 epoch total loss 1.09193885\n",
      "Trained batch 1103 batch loss 1.18702531 epoch total loss 1.09202504\n",
      "Trained batch 1104 batch loss 1.14893103 epoch total loss 1.09207666\n",
      "Trained batch 1105 batch loss 1.12057853 epoch total loss 1.09210241\n",
      "Trained batch 1106 batch loss 1.27540481 epoch total loss 1.09226811\n",
      "Trained batch 1107 batch loss 1.13082087 epoch total loss 1.09230304\n",
      "Trained batch 1108 batch loss 1.14254761 epoch total loss 1.09234834\n",
      "Trained batch 1109 batch loss 1.02983415 epoch total loss 1.09229195\n",
      "Trained batch 1110 batch loss 1.19942868 epoch total loss 1.09238851\n",
      "Trained batch 1111 batch loss 1.02803791 epoch total loss 1.09233069\n",
      "Trained batch 1112 batch loss 1.169554 epoch total loss 1.09240007\n",
      "Trained batch 1113 batch loss 1.11160135 epoch total loss 1.09241736\n",
      "Trained batch 1114 batch loss 1.21579111 epoch total loss 1.0925281\n",
      "Trained batch 1115 batch loss 1.11787462 epoch total loss 1.09255087\n",
      "Trained batch 1116 batch loss 1.02939558 epoch total loss 1.09249425\n",
      "Trained batch 1117 batch loss 1.14497745 epoch total loss 1.09254134\n",
      "Trained batch 1118 batch loss 1.21450198 epoch total loss 1.09265041\n",
      "Trained batch 1119 batch loss 1.18250835 epoch total loss 1.09273064\n",
      "Trained batch 1120 batch loss 1.02991486 epoch total loss 1.09267461\n",
      "Trained batch 1121 batch loss 0.95170927 epoch total loss 1.09254885\n",
      "Trained batch 1122 batch loss 0.886402 epoch total loss 1.09236503\n",
      "Trained batch 1123 batch loss 0.997188091 epoch total loss 1.09228027\n",
      "Trained batch 1124 batch loss 0.916767776 epoch total loss 1.0921241\n",
      "Trained batch 1125 batch loss 1.04345393 epoch total loss 1.09208083\n",
      "Trained batch 1126 batch loss 0.901132941 epoch total loss 1.0919112\n",
      "Trained batch 1127 batch loss 0.976216435 epoch total loss 1.09180856\n",
      "Trained batch 1128 batch loss 1.18713474 epoch total loss 1.09189308\n",
      "Trained batch 1129 batch loss 1.10663652 epoch total loss 1.09190619\n",
      "Trained batch 1130 batch loss 1.13055468 epoch total loss 1.0919404\n",
      "Trained batch 1131 batch loss 1.07881284 epoch total loss 1.09192884\n",
      "Trained batch 1132 batch loss 1.00208616 epoch total loss 1.09184945\n",
      "Trained batch 1133 batch loss 0.872827291 epoch total loss 1.09165621\n",
      "Trained batch 1134 batch loss 0.973935366 epoch total loss 1.09155226\n",
      "Trained batch 1135 batch loss 1.01343179 epoch total loss 1.09148347\n",
      "Trained batch 1136 batch loss 0.994097531 epoch total loss 1.09139776\n",
      "Trained batch 1137 batch loss 0.968299568 epoch total loss 1.09128952\n",
      "Trained batch 1138 batch loss 0.994723439 epoch total loss 1.09120464\n",
      "Trained batch 1139 batch loss 0.988810718 epoch total loss 1.09111476\n",
      "Trained batch 1140 batch loss 0.945192635 epoch total loss 1.09098673\n",
      "Trained batch 1141 batch loss 1.05336976 epoch total loss 1.09095371\n",
      "Trained batch 1142 batch loss 1.13820946 epoch total loss 1.09099507\n",
      "Trained batch 1143 batch loss 0.983113408 epoch total loss 1.09090078\n",
      "Trained batch 1144 batch loss 1.16169441 epoch total loss 1.09096265\n",
      "Trained batch 1145 batch loss 1.11323857 epoch total loss 1.0909822\n",
      "Trained batch 1146 batch loss 1.01204479 epoch total loss 1.0909133\n",
      "Trained batch 1147 batch loss 1.02434492 epoch total loss 1.09085524\n",
      "Trained batch 1148 batch loss 1.08329511 epoch total loss 1.09084857\n",
      "Trained batch 1149 batch loss 1.0287559 epoch total loss 1.09079456\n",
      "Trained batch 1150 batch loss 1.10596871 epoch total loss 1.0908078\n",
      "Trained batch 1151 batch loss 1.16973448 epoch total loss 1.09087634\n",
      "Trained batch 1152 batch loss 1.07060146 epoch total loss 1.0908587\n",
      "Trained batch 1153 batch loss 0.849729776 epoch total loss 1.09064949\n",
      "Trained batch 1154 batch loss 0.90788269 epoch total loss 1.09049118\n",
      "Trained batch 1155 batch loss 0.890883744 epoch total loss 1.09031832\n",
      "Trained batch 1156 batch loss 1.0860908 epoch total loss 1.09031463\n",
      "Trained batch 1157 batch loss 1.06290078 epoch total loss 1.0902909\n",
      "Trained batch 1158 batch loss 0.992658 epoch total loss 1.09020662\n",
      "Trained batch 1159 batch loss 1.09539151 epoch total loss 1.09021103\n",
      "Trained batch 1160 batch loss 1.21928835 epoch total loss 1.09032226\n",
      "Trained batch 1161 batch loss 1.22047651 epoch total loss 1.09043431\n",
      "Trained batch 1162 batch loss 1.28193474 epoch total loss 1.09059918\n",
      "Trained batch 1163 batch loss 1.06968021 epoch total loss 1.09058118\n",
      "Trained batch 1164 batch loss 1.10214913 epoch total loss 1.09059119\n",
      "Trained batch 1165 batch loss 1.02382767 epoch total loss 1.09053385\n",
      "Trained batch 1166 batch loss 1.1104188 epoch total loss 1.0905509\n",
      "Trained batch 1167 batch loss 1.26241648 epoch total loss 1.09069824\n",
      "Trained batch 1168 batch loss 1.22267008 epoch total loss 1.09081125\n",
      "Trained batch 1169 batch loss 1.15163875 epoch total loss 1.09086323\n",
      "Trained batch 1170 batch loss 1.08133924 epoch total loss 1.090855\n",
      "Trained batch 1171 batch loss 1.09752071 epoch total loss 1.09086072\n",
      "Trained batch 1172 batch loss 1.21857011 epoch total loss 1.0909698\n",
      "Trained batch 1173 batch loss 1.13220155 epoch total loss 1.09100497\n",
      "Trained batch 1174 batch loss 1.1907053 epoch total loss 1.09108984\n",
      "Trained batch 1175 batch loss 1.12440252 epoch total loss 1.09111822\n",
      "Trained batch 1176 batch loss 1.0654099 epoch total loss 1.09109628\n",
      "Trained batch 1177 batch loss 1.13703144 epoch total loss 1.09113538\n",
      "Trained batch 1178 batch loss 1.13683546 epoch total loss 1.09117424\n",
      "Trained batch 1179 batch loss 1.21378493 epoch total loss 1.0912782\n",
      "Trained batch 1180 batch loss 1.21103263 epoch total loss 1.09137964\n",
      "Trained batch 1181 batch loss 1.04567182 epoch total loss 1.0913409\n",
      "Trained batch 1182 batch loss 0.886015415 epoch total loss 1.09116721\n",
      "Trained batch 1183 batch loss 1.09630322 epoch total loss 1.0911715\n",
      "Trained batch 1184 batch loss 1.14120293 epoch total loss 1.09121382\n",
      "Trained batch 1185 batch loss 0.996281087 epoch total loss 1.09113383\n",
      "Trained batch 1186 batch loss 1.02216339 epoch total loss 1.09107566\n",
      "Trained batch 1187 batch loss 1.12143123 epoch total loss 1.09110129\n",
      "Trained batch 1188 batch loss 1.15242946 epoch total loss 1.09115291\n",
      "Trained batch 1189 batch loss 1.26951325 epoch total loss 1.09130299\n",
      "Trained batch 1190 batch loss 1.16178 epoch total loss 1.09136212\n",
      "Trained batch 1191 batch loss 1.25559282 epoch total loss 1.0915\n",
      "Trained batch 1192 batch loss 1.32437897 epoch total loss 1.09169543\n",
      "Trained batch 1193 batch loss 1.23198 epoch total loss 1.09181297\n",
      "Trained batch 1194 batch loss 1.18348765 epoch total loss 1.09188974\n",
      "Trained batch 1195 batch loss 1.10728025 epoch total loss 1.09190261\n",
      "Trained batch 1196 batch loss 1.07847512 epoch total loss 1.09189141\n",
      "Trained batch 1197 batch loss 1.15034819 epoch total loss 1.09194028\n",
      "Trained batch 1198 batch loss 1.16411209 epoch total loss 1.09200048\n",
      "Trained batch 1199 batch loss 1.19855928 epoch total loss 1.09208941\n",
      "Trained batch 1200 batch loss 1.30712283 epoch total loss 1.09226859\n",
      "Trained batch 1201 batch loss 1.07791924 epoch total loss 1.09225655\n",
      "Trained batch 1202 batch loss 1.09879363 epoch total loss 1.09226203\n",
      "Trained batch 1203 batch loss 1.08952749 epoch total loss 1.09225965\n",
      "Trained batch 1204 batch loss 1.12587309 epoch total loss 1.09228754\n",
      "Trained batch 1205 batch loss 1.14282477 epoch total loss 1.0923295\n",
      "Trained batch 1206 batch loss 1.12241864 epoch total loss 1.09235454\n",
      "Trained batch 1207 batch loss 1.12000513 epoch total loss 1.09237742\n",
      "Trained batch 1208 batch loss 1.25336599 epoch total loss 1.0925107\n",
      "Trained batch 1209 batch loss 1.24745512 epoch total loss 1.09263885\n",
      "Trained batch 1210 batch loss 1.15112579 epoch total loss 1.09268713\n",
      "Trained batch 1211 batch loss 1.1463033 epoch total loss 1.09273148\n",
      "Trained batch 1212 batch loss 1.23753297 epoch total loss 1.09285104\n",
      "Trained batch 1213 batch loss 1.17044449 epoch total loss 1.09291494\n",
      "Trained batch 1214 batch loss 1.18095756 epoch total loss 1.09298742\n",
      "Trained batch 1215 batch loss 0.988859057 epoch total loss 1.09290171\n",
      "Trained batch 1216 batch loss 1.04980731 epoch total loss 1.0928663\n",
      "Trained batch 1217 batch loss 1.13947177 epoch total loss 1.09290469\n",
      "Trained batch 1218 batch loss 1.13991642 epoch total loss 1.09294319\n",
      "Trained batch 1219 batch loss 1.24081838 epoch total loss 1.09306455\n",
      "Trained batch 1220 batch loss 1.09019828 epoch total loss 1.09306216\n",
      "Trained batch 1221 batch loss 1.1648351 epoch total loss 1.09312093\n",
      "Trained batch 1222 batch loss 1.20294869 epoch total loss 1.09321082\n",
      "Trained batch 1223 batch loss 1.12504506 epoch total loss 1.09323692\n",
      "Trained batch 1224 batch loss 1.23581779 epoch total loss 1.09335339\n",
      "Trained batch 1225 batch loss 1.13454282 epoch total loss 1.09338701\n",
      "Trained batch 1226 batch loss 1.22676218 epoch total loss 1.09349585\n",
      "Trained batch 1227 batch loss 1.16783452 epoch total loss 1.0935564\n",
      "Trained batch 1228 batch loss 1.19872475 epoch total loss 1.093642\n",
      "Trained batch 1229 batch loss 1.17289722 epoch total loss 1.09370649\n",
      "Trained batch 1230 batch loss 1.19662261 epoch total loss 1.09379017\n",
      "Trained batch 1231 batch loss 1.09331656 epoch total loss 1.09378982\n",
      "Trained batch 1232 batch loss 1.09077132 epoch total loss 1.09378731\n",
      "Trained batch 1233 batch loss 1.06036425 epoch total loss 1.09376025\n",
      "Trained batch 1234 batch loss 1.22142315 epoch total loss 1.09386373\n",
      "Trained batch 1235 batch loss 0.968823373 epoch total loss 1.09376252\n",
      "Trained batch 1236 batch loss 1.15373766 epoch total loss 1.09381104\n",
      "Trained batch 1237 batch loss 1.27907038 epoch total loss 1.09396076\n",
      "Trained batch 1238 batch loss 1.15658319 epoch total loss 1.09401143\n",
      "Trained batch 1239 batch loss 1.16793919 epoch total loss 1.09407103\n",
      "Trained batch 1240 batch loss 1.22323489 epoch total loss 1.09417522\n",
      "Trained batch 1241 batch loss 1.21931696 epoch total loss 1.09427619\n",
      "Trained batch 1242 batch loss 1.1339438 epoch total loss 1.09430802\n",
      "Trained batch 1243 batch loss 1.13299787 epoch total loss 1.09433925\n",
      "Trained batch 1244 batch loss 1.0690124 epoch total loss 1.09431887\n",
      "Trained batch 1245 batch loss 1.05441582 epoch total loss 1.0942868\n",
      "Trained batch 1246 batch loss 1.14842474 epoch total loss 1.09433031\n",
      "Trained batch 1247 batch loss 1.16839528 epoch total loss 1.09438968\n",
      "Trained batch 1248 batch loss 1.12284887 epoch total loss 1.09441245\n",
      "Trained batch 1249 batch loss 1.24667883 epoch total loss 1.09453428\n",
      "Trained batch 1250 batch loss 1.1370759 epoch total loss 1.09456837\n",
      "Trained batch 1251 batch loss 1.14435983 epoch total loss 1.09460819\n",
      "Trained batch 1252 batch loss 1.1574111 epoch total loss 1.09465837\n",
      "Trained batch 1253 batch loss 0.9435969 epoch total loss 1.09453785\n",
      "Trained batch 1254 batch loss 0.998153329 epoch total loss 1.09446096\n",
      "Trained batch 1255 batch loss 1.01895475 epoch total loss 1.09440076\n",
      "Trained batch 1256 batch loss 1.23634315 epoch total loss 1.09451377\n",
      "Trained batch 1257 batch loss 1.05771136 epoch total loss 1.09448457\n",
      "Trained batch 1258 batch loss 1.01459861 epoch total loss 1.09442115\n",
      "Trained batch 1259 batch loss 1.01078081 epoch total loss 1.09435463\n",
      "Trained batch 1260 batch loss 1.03586411 epoch total loss 1.09430826\n",
      "Trained batch 1261 batch loss 0.999113321 epoch total loss 1.0942328\n",
      "Trained batch 1262 batch loss 0.950217545 epoch total loss 1.0941186\n",
      "Trained batch 1263 batch loss 1.13795352 epoch total loss 1.09415329\n",
      "Trained batch 1264 batch loss 1.09639812 epoch total loss 1.09415507\n",
      "Trained batch 1265 batch loss 1.15929067 epoch total loss 1.09420657\n",
      "Trained batch 1266 batch loss 1.32721889 epoch total loss 1.09439075\n",
      "Trained batch 1267 batch loss 1.05781436 epoch total loss 1.0943619\n",
      "Trained batch 1268 batch loss 1.05107474 epoch total loss 1.09432769\n",
      "Trained batch 1269 batch loss 1.1508615 epoch total loss 1.09437227\n",
      "Trained batch 1270 batch loss 1.03258896 epoch total loss 1.09432364\n",
      "Trained batch 1271 batch loss 1.11647439 epoch total loss 1.09434104\n",
      "Trained batch 1272 batch loss 1.18507719 epoch total loss 1.09441233\n",
      "Trained batch 1273 batch loss 1.19797754 epoch total loss 1.09449375\n",
      "Trained batch 1274 batch loss 1.08909857 epoch total loss 1.09448946\n",
      "Trained batch 1275 batch loss 1.14327288 epoch total loss 1.09452784\n",
      "Trained batch 1276 batch loss 1.1452632 epoch total loss 1.09456754\n",
      "Trained batch 1277 batch loss 1.33342957 epoch total loss 1.09475458\n",
      "Trained batch 1278 batch loss 1.09228635 epoch total loss 1.09475267\n",
      "Trained batch 1279 batch loss 1.04840577 epoch total loss 1.09471643\n",
      "Trained batch 1280 batch loss 1.11794817 epoch total loss 1.09473455\n",
      "Trained batch 1281 batch loss 1.16576052 epoch total loss 1.09479\n",
      "Trained batch 1282 batch loss 1.21308911 epoch total loss 1.09488237\n",
      "Trained batch 1283 batch loss 1.22078371 epoch total loss 1.09498048\n",
      "Trained batch 1284 batch loss 1.18254423 epoch total loss 1.09504867\n",
      "Trained batch 1285 batch loss 1.33655024 epoch total loss 1.09523654\n",
      "Trained batch 1286 batch loss 1.20292878 epoch total loss 1.09532034\n",
      "Trained batch 1287 batch loss 1.16088021 epoch total loss 1.09537125\n",
      "Trained batch 1288 batch loss 1.22692776 epoch total loss 1.09547341\n",
      "Trained batch 1289 batch loss 1.12715113 epoch total loss 1.09549797\n",
      "Trained batch 1290 batch loss 1.16756105 epoch total loss 1.09555387\n",
      "Trained batch 1291 batch loss 1.11417222 epoch total loss 1.0955683\n",
      "Trained batch 1292 batch loss 1.09483182 epoch total loss 1.0955677\n",
      "Trained batch 1293 batch loss 0.924019873 epoch total loss 1.09543514\n",
      "Trained batch 1294 batch loss 1.12822282 epoch total loss 1.09546041\n",
      "Trained batch 1295 batch loss 1.13687932 epoch total loss 1.09549236\n",
      "Trained batch 1296 batch loss 1.14078951 epoch total loss 1.09552729\n",
      "Trained batch 1297 batch loss 1.04895973 epoch total loss 1.09549141\n",
      "Trained batch 1298 batch loss 1.07986188 epoch total loss 1.09547925\n",
      "Trained batch 1299 batch loss 1.17977548 epoch total loss 1.09554422\n",
      "Trained batch 1300 batch loss 1.09202313 epoch total loss 1.09554148\n",
      "Trained batch 1301 batch loss 1.09713721 epoch total loss 1.09554279\n",
      "Trained batch 1302 batch loss 1.05678272 epoch total loss 1.09551299\n",
      "Trained batch 1303 batch loss 1.11647987 epoch total loss 1.09552908\n",
      "Trained batch 1304 batch loss 1.17570007 epoch total loss 1.09559047\n",
      "Trained batch 1305 batch loss 1.12980521 epoch total loss 1.0956167\n",
      "Trained batch 1306 batch loss 1.21300447 epoch total loss 1.09570658\n",
      "Trained batch 1307 batch loss 1.25902426 epoch total loss 1.09583151\n",
      "Trained batch 1308 batch loss 1.13347614 epoch total loss 1.09586024\n",
      "Trained batch 1309 batch loss 1.16716909 epoch total loss 1.09591472\n",
      "Trained batch 1310 batch loss 1.08283937 epoch total loss 1.09590483\n",
      "Trained batch 1311 batch loss 1.04882121 epoch total loss 1.09586883\n",
      "Trained batch 1312 batch loss 1.18486273 epoch total loss 1.09593666\n",
      "Trained batch 1313 batch loss 1.16083062 epoch total loss 1.09598613\n",
      "Trained batch 1314 batch loss 1.40761137 epoch total loss 1.09622324\n",
      "Trained batch 1315 batch loss 1.30180168 epoch total loss 1.09637952\n",
      "Trained batch 1316 batch loss 1.15012336 epoch total loss 1.09642041\n",
      "Trained batch 1317 batch loss 1.07082546 epoch total loss 1.09640098\n",
      "Trained batch 1318 batch loss 1.18361199 epoch total loss 1.09646714\n",
      "Trained batch 1319 batch loss 1.1505022 epoch total loss 1.09650815\n",
      "Trained batch 1320 batch loss 1.25156093 epoch total loss 1.09662557\n",
      "Trained batch 1321 batch loss 1.16621208 epoch total loss 1.09667826\n",
      "Trained batch 1322 batch loss 1.06075549 epoch total loss 1.0966512\n",
      "Trained batch 1323 batch loss 1.04208171 epoch total loss 1.09661\n",
      "Trained batch 1324 batch loss 1.10156238 epoch total loss 1.09661365\n",
      "Trained batch 1325 batch loss 0.97490567 epoch total loss 1.09652174\n",
      "Trained batch 1326 batch loss 0.965832472 epoch total loss 1.09642327\n",
      "Trained batch 1327 batch loss 1.00537527 epoch total loss 1.0963546\n",
      "Trained batch 1328 batch loss 1.0844214 epoch total loss 1.09634566\n",
      "Trained batch 1329 batch loss 1.08082652 epoch total loss 1.09633398\n",
      "Trained batch 1330 batch loss 1.12690282 epoch total loss 1.09635699\n",
      "Trained batch 1331 batch loss 1.21035576 epoch total loss 1.09644258\n",
      "Trained batch 1332 batch loss 1.18045366 epoch total loss 1.09650564\n",
      "Trained batch 1333 batch loss 1.19089842 epoch total loss 1.09657645\n",
      "Trained batch 1334 batch loss 1.10723758 epoch total loss 1.09658444\n",
      "Trained batch 1335 batch loss 1.08125281 epoch total loss 1.096573\n",
      "Trained batch 1336 batch loss 1.11683762 epoch total loss 1.09658813\n",
      "Trained batch 1337 batch loss 1.13238502 epoch total loss 1.09661484\n",
      "Trained batch 1338 batch loss 0.937303662 epoch total loss 1.09649575\n",
      "Trained batch 1339 batch loss 0.871345043 epoch total loss 1.09632766\n",
      "Trained batch 1340 batch loss 1.00548172 epoch total loss 1.09625983\n",
      "Trained batch 1341 batch loss 1.12463474 epoch total loss 1.09628093\n",
      "Trained batch 1342 batch loss 1.13252175 epoch total loss 1.09630799\n",
      "Trained batch 1343 batch loss 1.16792202 epoch total loss 1.0963614\n",
      "Trained batch 1344 batch loss 1.1980195 epoch total loss 1.09643698\n",
      "Trained batch 1345 batch loss 1.16529953 epoch total loss 1.09648824\n",
      "Trained batch 1346 batch loss 1.16647339 epoch total loss 1.09654021\n",
      "Trained batch 1347 batch loss 1.29664803 epoch total loss 1.09668875\n",
      "Trained batch 1348 batch loss 1.19616187 epoch total loss 1.09676254\n",
      "Trained batch 1349 batch loss 1.18164837 epoch total loss 1.09682548\n",
      "Trained batch 1350 batch loss 1.20501935 epoch total loss 1.09690571\n",
      "Trained batch 1351 batch loss 1.09439087 epoch total loss 1.0969038\n",
      "Trained batch 1352 batch loss 1.14881563 epoch total loss 1.09694219\n",
      "Trained batch 1353 batch loss 1.00444531 epoch total loss 1.09687376\n",
      "Trained batch 1354 batch loss 1.12228882 epoch total loss 1.0968926\n",
      "Trained batch 1355 batch loss 1.13111699 epoch total loss 1.09691775\n",
      "Trained batch 1356 batch loss 0.970760345 epoch total loss 1.09682477\n",
      "Trained batch 1357 batch loss 0.915528059 epoch total loss 1.09669113\n",
      "Trained batch 1358 batch loss 0.955167532 epoch total loss 1.09658694\n",
      "Trained batch 1359 batch loss 0.91066283 epoch total loss 1.09645009\n",
      "Trained batch 1360 batch loss 1.0684855 epoch total loss 1.09642959\n",
      "Trained batch 1361 batch loss 1.42636728 epoch total loss 1.09667194\n",
      "Trained batch 1362 batch loss 1.35850859 epoch total loss 1.09686422\n",
      "Trained batch 1363 batch loss 1.31198287 epoch total loss 1.09702206\n",
      "Trained batch 1364 batch loss 1.20309234 epoch total loss 1.0970999\n",
      "Trained batch 1365 batch loss 1.26141012 epoch total loss 1.09722018\n",
      "Trained batch 1366 batch loss 1.33579159 epoch total loss 1.09739482\n",
      "Trained batch 1367 batch loss 1.45525 epoch total loss 1.09765661\n",
      "Trained batch 1368 batch loss 1.3619411 epoch total loss 1.09784985\n",
      "Trained batch 1369 batch loss 1.25529575 epoch total loss 1.09796476\n",
      "Trained batch 1370 batch loss 1.08446515 epoch total loss 1.09795487\n",
      "Trained batch 1371 batch loss 1.0021472 epoch total loss 1.09788513\n",
      "Trained batch 1372 batch loss 1.18730402 epoch total loss 1.09795022\n",
      "Trained batch 1373 batch loss 1.14169645 epoch total loss 1.09798205\n",
      "Trained batch 1374 batch loss 1.20572889 epoch total loss 1.09806049\n",
      "Trained batch 1375 batch loss 1.19129324 epoch total loss 1.09812832\n",
      "Trained batch 1376 batch loss 1.25472915 epoch total loss 1.09824216\n",
      "Trained batch 1377 batch loss 1.26619422 epoch total loss 1.09836411\n",
      "Trained batch 1378 batch loss 1.31584096 epoch total loss 1.09852195\n",
      "Trained batch 1379 batch loss 1.1726253 epoch total loss 1.09857559\n",
      "Trained batch 1380 batch loss 1.21818054 epoch total loss 1.09866226\n",
      "Trained batch 1381 batch loss 1.10673332 epoch total loss 1.0986681\n",
      "Trained batch 1382 batch loss 1.06967652 epoch total loss 1.09864712\n",
      "Trained batch 1383 batch loss 1.16282976 epoch total loss 1.09869349\n",
      "Trained batch 1384 batch loss 1.17434013 epoch total loss 1.09874821\n",
      "Trained batch 1385 batch loss 1.12678969 epoch total loss 1.09876847\n",
      "Trained batch 1386 batch loss 1.06444061 epoch total loss 1.09874368\n",
      "Trained batch 1387 batch loss 1.07922602 epoch total loss 1.09872961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 04:25:02.897269: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "2025-03-19 04:25:02.897313: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "\t [[RemoteCall]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1388 batch loss 1.12387776 epoch total loss 1.09874773\n",
      "Epoch 8 train loss 1.098747730255127\n",
      "Validated batch 1 batch loss 1.16782296\n",
      "Validated batch 2 batch loss 1.0805254\n",
      "Validated batch 3 batch loss 1.11279559\n",
      "Validated batch 4 batch loss 1.11008358\n",
      "Validated batch 5 batch loss 1.24438846\n",
      "Validated batch 6 batch loss 1.25266111\n",
      "Validated batch 7 batch loss 1.0703752\n",
      "Validated batch 8 batch loss 1.13254809\n",
      "Validated batch 9 batch loss 1.11235213\n",
      "Validated batch 10 batch loss 1.11811471\n",
      "Validated batch 11 batch loss 1.18727112\n",
      "Validated batch 12 batch loss 1.06372952\n",
      "Validated batch 13 batch loss 1.30313373\n",
      "Validated batch 14 batch loss 1.04968596\n",
      "Validated batch 15 batch loss 1.19591808\n",
      "Validated batch 16 batch loss 1.19207215\n",
      "Validated batch 17 batch loss 1.24787903\n",
      "Validated batch 18 batch loss 0.974120259\n",
      "Validated batch 19 batch loss 1.23613167\n",
      "Validated batch 20 batch loss 1.08246601\n",
      "Validated batch 21 batch loss 1.1966027\n",
      "Validated batch 22 batch loss 1.10827291\n",
      "Validated batch 23 batch loss 1.12822151\n",
      "Validated batch 24 batch loss 1.20233154\n",
      "Validated batch 25 batch loss 1.1099174\n",
      "Validated batch 26 batch loss 1.09192944\n",
      "Validated batch 27 batch loss 1.09050143\n",
      "Validated batch 28 batch loss 1.10218179\n",
      "Validated batch 29 batch loss 1.18657577\n",
      "Validated batch 30 batch loss 1.13046265\n",
      "Validated batch 31 batch loss 1.13488901\n",
      "Validated batch 32 batch loss 1.194291\n",
      "Validated batch 33 batch loss 1.21288443\n",
      "Validated batch 34 batch loss 1.05491161\n",
      "Validated batch 35 batch loss 1.03022671\n",
      "Validated batch 36 batch loss 1.17815554\n",
      "Validated batch 37 batch loss 1.15474439\n",
      "Validated batch 38 batch loss 1.27061963\n",
      "Validated batch 39 batch loss 1.28841293\n",
      "Validated batch 40 batch loss 1.14360118\n",
      "Validated batch 41 batch loss 1.30209315\n",
      "Validated batch 42 batch loss 1.13965762\n",
      "Validated batch 43 batch loss 1.20288348\n",
      "Validated batch 44 batch loss 1.18970358\n",
      "Validated batch 45 batch loss 0.901561737\n",
      "Validated batch 46 batch loss 1.14270616\n",
      "Validated batch 47 batch loss 1.07989931\n",
      "Validated batch 48 batch loss 1.03275704\n",
      "Validated batch 49 batch loss 1.12085438\n",
      "Validated batch 50 batch loss 1.07588112\n",
      "Validated batch 51 batch loss 1.13514769\n",
      "Validated batch 52 batch loss 1.13383639\n",
      "Validated batch 53 batch loss 1.17599785\n",
      "Validated batch 54 batch loss 1.09916222\n",
      "Validated batch 55 batch loss 1.18897378\n",
      "Validated batch 56 batch loss 1.08842123\n",
      "Validated batch 57 batch loss 1.12477863\n",
      "Validated batch 58 batch loss 1.21166134\n",
      "Validated batch 59 batch loss 1.09291697\n",
      "Validated batch 60 batch loss 1.11692595\n",
      "Validated batch 61 batch loss 1.17416155\n",
      "Validated batch 62 batch loss 1.09774029\n",
      "Validated batch 63 batch loss 1.3022933\n",
      "Validated batch 64 batch loss 1.17413068\n",
      "Validated batch 65 batch loss 1.02796721\n",
      "Validated batch 66 batch loss 1.24349606\n",
      "Validated batch 67 batch loss 1.14095688\n",
      "Validated batch 68 batch loss 1.03600454\n",
      "Validated batch 69 batch loss 1.14189816\n",
      "Validated batch 70 batch loss 1.11688685\n",
      "Validated batch 71 batch loss 1.08346355\n",
      "Validated batch 72 batch loss 1.14443874\n",
      "Validated batch 73 batch loss 1.08713341\n",
      "Validated batch 74 batch loss 1.12151718\n",
      "Validated batch 75 batch loss 1.22391224\n",
      "Validated batch 76 batch loss 1.17238557\n",
      "Validated batch 77 batch loss 1.24043524\n",
      "Validated batch 78 batch loss 1.18029857\n",
      "Validated batch 79 batch loss 1.11341441\n",
      "Validated batch 80 batch loss 1.19568264\n",
      "Validated batch 81 batch loss 1.30384088\n",
      "Validated batch 82 batch loss 1.23523366\n",
      "Validated batch 83 batch loss 1.28221655\n",
      "Validated batch 84 batch loss 1.22243953\n",
      "Validated batch 85 batch loss 1.26047015\n",
      "Validated batch 86 batch loss 1.25822973\n",
      "Validated batch 87 batch loss 1.1083591\n",
      "Validated batch 88 batch loss 1.17248929\n",
      "Validated batch 89 batch loss 1.25384784\n",
      "Validated batch 90 batch loss 1.1827594\n",
      "Validated batch 91 batch loss 1.10103726\n",
      "Validated batch 92 batch loss 1.1456852\n",
      "Validated batch 93 batch loss 1.07383239\n",
      "Validated batch 94 batch loss 1.13066328\n",
      "Validated batch 95 batch loss 1.11873245\n",
      "Validated batch 96 batch loss 1.06479073\n",
      "Validated batch 97 batch loss 1.06307\n",
      "Validated batch 98 batch loss 1.23448396\n",
      "Validated batch 99 batch loss 1.05682504\n",
      "Validated batch 100 batch loss 1.03677821\n",
      "Validated batch 101 batch loss 1.05657375\n",
      "Validated batch 102 batch loss 1.07523453\n",
      "Validated batch 103 batch loss 0.997790456\n",
      "Validated batch 104 batch loss 1.13330865\n",
      "Validated batch 105 batch loss 1.09121013\n",
      "Validated batch 106 batch loss 1.03677988\n",
      "Validated batch 107 batch loss 1.22440863\n",
      "Validated batch 108 batch loss 1.22145224\n",
      "Validated batch 109 batch loss 1.0424881\n",
      "Validated batch 110 batch loss 1.24409187\n",
      "Validated batch 111 batch loss 0.942864895\n",
      "Validated batch 112 batch loss 1.05835021\n",
      "Validated batch 113 batch loss 1.04728806\n",
      "Validated batch 114 batch loss 1.07917047\n",
      "Validated batch 115 batch loss 1.28523219\n",
      "Validated batch 116 batch loss 1.13015699\n",
      "Validated batch 117 batch loss 1.18135798\n",
      "Validated batch 118 batch loss 1.11911869\n",
      "Validated batch 119 batch loss 1.08869624\n",
      "Validated batch 120 batch loss 1.16391468\n",
      "Validated batch 121 batch loss 1.198241\n",
      "Validated batch 122 batch loss 1.19452929\n",
      "Validated batch 123 batch loss 1.24381351\n",
      "Validated batch 124 batch loss 1.21898019\n",
      "Validated batch 125 batch loss 1.09156394\n",
      "Validated batch 126 batch loss 1.15673852\n",
      "Validated batch 127 batch loss 1.15466857\n",
      "Validated batch 128 batch loss 1.14753902\n",
      "Validated batch 129 batch loss 1.27078986\n",
      "Validated batch 130 batch loss 1.2335937\n",
      "Validated batch 131 batch loss 1.21848655\n",
      "Validated batch 132 batch loss 1.25527191\n",
      "Validated batch 133 batch loss 1.11331975\n",
      "Validated batch 134 batch loss 1.102391\n",
      "Validated batch 135 batch loss 1.08213329\n",
      "Validated batch 136 batch loss 1.03610611\n",
      "Validated batch 137 batch loss 1.22671604\n",
      "Validated batch 138 batch loss 1.05441535\n",
      "Validated batch 139 batch loss 1.23442495\n",
      "Validated batch 140 batch loss 1.13766754\n",
      "Validated batch 141 batch loss 1.13641858\n",
      "Validated batch 142 batch loss 1.13039112\n",
      "Validated batch 143 batch loss 1.04323697\n",
      "Validated batch 144 batch loss 1.16979909\n",
      "Validated batch 145 batch loss 1.18982196\n",
      "Validated batch 146 batch loss 1.09341717\n",
      "Validated batch 147 batch loss 1.13686979\n",
      "Validated batch 148 batch loss 1.17169261\n",
      "Validated batch 149 batch loss 1.15239215\n",
      "Validated batch 150 batch loss 1.22449481\n",
      "Validated batch 151 batch loss 1.19722509\n",
      "Validated batch 152 batch loss 1.05680454\n",
      "Validated batch 153 batch loss 1.08487034\n",
      "Validated batch 154 batch loss 1.15797973\n",
      "Validated batch 155 batch loss 1.09450507\n",
      "Validated batch 156 batch loss 1.1592052\n",
      "Validated batch 157 batch loss 1.16264796\n",
      "Validated batch 158 batch loss 1.25577402\n",
      "Validated batch 159 batch loss 1.25353515\n",
      "Validated batch 160 batch loss 1.13879395\n",
      "Validated batch 161 batch loss 1.0794487\n",
      "Validated batch 162 batch loss 1.0345149\n",
      "Validated batch 163 batch loss 1.05849373\n",
      "Validated batch 164 batch loss 1.21281767\n",
      "Validated batch 165 batch loss 1.08503139\n",
      "Validated batch 166 batch loss 1.13068318\n",
      "Validated batch 167 batch loss 1.32935655\n",
      "Validated batch 168 batch loss 0.999144673\n",
      "Validated batch 169 batch loss 1.12164378\n",
      "Validated batch 170 batch loss 1.12722135\n",
      "Validated batch 171 batch loss 1.18810248\n",
      "Validated batch 172 batch loss 1.20069134\n",
      "Validated batch 173 batch loss 1.10037863\n",
      "Validated batch 174 batch loss 0.891540468\n",
      "Validated batch 175 batch loss 1.18782783\n",
      "Validated batch 176 batch loss 1.05039096\n",
      "Validated batch 177 batch loss 1.10545623\n",
      "Validated batch 178 batch loss 1.22996378\n",
      "Validated batch 179 batch loss 0.994801402\n",
      "Validated batch 180 batch loss 1.18409479\n",
      "Validated batch 181 batch loss 1.21770489\n",
      "Validated batch 182 batch loss 1.16748333\n",
      "Validated batch 183 batch loss 1.13622427\n",
      "Validated batch 184 batch loss 1.02205539\n",
      "Validated batch 185 batch loss 1.11454034\n",
      "Epoch 8 val loss 1.1426701545715332\n",
      "Start epoch 9 with learning rate 0.001\n",
      "Start distributed training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 04:25:18.819842: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "2025-03-19 04:25:18.819880: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "\t [[RemoteCall]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1 batch loss 1.0610404 epoch total loss 1.0610404\n",
      "Trained batch 2 batch loss 1.14588654 epoch total loss 1.10346341\n",
      "Trained batch 3 batch loss 1.25606608 epoch total loss 1.15433097\n",
      "Trained batch 4 batch loss 1.14598823 epoch total loss 1.15224528\n",
      "Trained batch 5 batch loss 1.25707793 epoch total loss 1.17321181\n",
      "Trained batch 6 batch loss 1.37676764 epoch total loss 1.20713782\n",
      "Trained batch 7 batch loss 1.1367811 epoch total loss 1.19708693\n",
      "Trained batch 8 batch loss 1.07702065 epoch total loss 1.1820786\n",
      "Trained batch 9 batch loss 1.14620364 epoch total loss 1.17809248\n",
      "Trained batch 10 batch loss 1.09632564 epoch total loss 1.16991591\n",
      "Trained batch 11 batch loss 1.16332698 epoch total loss 1.16931689\n",
      "Trained batch 12 batch loss 1.1661458 epoch total loss 1.1690526\n",
      "Trained batch 13 batch loss 1.20572329 epoch total loss 1.17187333\n",
      "Trained batch 14 batch loss 1.23786378 epoch total loss 1.17658699\n",
      "Trained batch 15 batch loss 1.32143247 epoch total loss 1.1862433\n",
      "Trained batch 16 batch loss 1.31000018 epoch total loss 1.19397807\n",
      "Trained batch 17 batch loss 1.24277329 epoch total loss 1.19684839\n",
      "Trained batch 18 batch loss 1.2454139 epoch total loss 1.19954646\n",
      "Trained batch 19 batch loss 1.21554661 epoch total loss 1.20038867\n",
      "Trained batch 20 batch loss 1.24038243 epoch total loss 1.20238841\n",
      "Trained batch 21 batch loss 1.2390846 epoch total loss 1.20413578\n",
      "Trained batch 22 batch loss 1.12210131 epoch total loss 1.20040691\n",
      "Trained batch 23 batch loss 1.23132145 epoch total loss 1.20175099\n",
      "Trained batch 24 batch loss 1.03887212 epoch total loss 1.19496441\n",
      "Trained batch 25 batch loss 1.1303215 epoch total loss 1.19237864\n",
      "Trained batch 26 batch loss 1.09265912 epoch total loss 1.18854332\n",
      "Trained batch 27 batch loss 1.09165955 epoch total loss 1.184955\n",
      "Trained batch 28 batch loss 1.04566801 epoch total loss 1.17998052\n",
      "Trained batch 29 batch loss 1.02769709 epoch total loss 1.17472947\n",
      "Trained batch 30 batch loss 1.02434111 epoch total loss 1.16971648\n",
      "Trained batch 31 batch loss 1.01611578 epoch total loss 1.16476166\n",
      "Trained batch 32 batch loss 0.98686403 epoch total loss 1.15920234\n",
      "Trained batch 33 batch loss 1.09737182 epoch total loss 1.15732861\n",
      "Trained batch 34 batch loss 1.12299085 epoch total loss 1.15631866\n",
      "Trained batch 35 batch loss 1.09533978 epoch total loss 1.15457642\n",
      "Trained batch 36 batch loss 1.077039 epoch total loss 1.15242255\n",
      "Trained batch 37 batch loss 1.06541681 epoch total loss 1.15007114\n",
      "Trained batch 38 batch loss 1.12791646 epoch total loss 1.14948809\n",
      "Trained batch 39 batch loss 0.998150468 epoch total loss 1.14560771\n",
      "Trained batch 40 batch loss 0.944526434 epoch total loss 1.14058065\n",
      "Trained batch 41 batch loss 1.1291213 epoch total loss 1.14030111\n",
      "Trained batch 42 batch loss 1.01532447 epoch total loss 1.13732553\n",
      "Trained batch 43 batch loss 1.05691934 epoch total loss 1.13545561\n",
      "Trained batch 44 batch loss 1.15991175 epoch total loss 1.13601136\n",
      "Trained batch 45 batch loss 1.17414069 epoch total loss 1.1368587\n",
      "Trained batch 46 batch loss 1.084234 epoch total loss 1.13571465\n",
      "Trained batch 47 batch loss 1.11826491 epoch total loss 1.13534331\n",
      "Trained batch 48 batch loss 1.09805381 epoch total loss 1.13456643\n",
      "Trained batch 49 batch loss 1.11147475 epoch total loss 1.13409519\n",
      "Trained batch 50 batch loss 1.05492592 epoch total loss 1.13251173\n",
      "Trained batch 51 batch loss 1.10819364 epoch total loss 1.1320349\n",
      "Trained batch 52 batch loss 1.00566256 epoch total loss 1.12960458\n",
      "Trained batch 53 batch loss 0.968488276 epoch total loss 1.12656462\n",
      "Trained batch 54 batch loss 1.13929713 epoch total loss 1.12680042\n",
      "Trained batch 55 batch loss 1.24838173 epoch total loss 1.12901103\n",
      "Trained batch 56 batch loss 1.13764822 epoch total loss 1.12916529\n",
      "Trained batch 57 batch loss 1.10011876 epoch total loss 1.12865567\n",
      "Trained batch 58 batch loss 1.03809488 epoch total loss 1.12709427\n",
      "Trained batch 59 batch loss 1.04481971 epoch total loss 1.12569988\n",
      "Trained batch 60 batch loss 1.1139307 epoch total loss 1.12550366\n",
      "Trained batch 61 batch loss 0.944351912 epoch total loss 1.12253392\n",
      "Trained batch 62 batch loss 1.01008105 epoch total loss 1.12072015\n",
      "Trained batch 63 batch loss 1.13225198 epoch total loss 1.12090325\n",
      "Trained batch 64 batch loss 1.20270586 epoch total loss 1.12218142\n",
      "Trained batch 65 batch loss 1.01043034 epoch total loss 1.12046218\n",
      "Trained batch 66 batch loss 0.929063499 epoch total loss 1.11756217\n",
      "Trained batch 67 batch loss 0.907438 epoch total loss 1.11442602\n",
      "Trained batch 68 batch loss 1.26023579 epoch total loss 1.11657035\n",
      "Trained batch 69 batch loss 1.1086297 epoch total loss 1.1164552\n",
      "Trained batch 70 batch loss 1.18252838 epoch total loss 1.1173991\n",
      "Trained batch 71 batch loss 1.19296646 epoch total loss 1.1184634\n",
      "Trained batch 72 batch loss 1.31861353 epoch total loss 1.12124324\n",
      "Trained batch 73 batch loss 1.13174677 epoch total loss 1.12138712\n",
      "Trained batch 74 batch loss 1.16368484 epoch total loss 1.12195873\n",
      "Trained batch 75 batch loss 1.19136381 epoch total loss 1.12288415\n",
      "Trained batch 76 batch loss 1.16200244 epoch total loss 1.12339878\n",
      "Trained batch 77 batch loss 1.17330372 epoch total loss 1.12404692\n",
      "Trained batch 78 batch loss 0.875683427 epoch total loss 1.12086284\n",
      "Trained batch 79 batch loss 0.944792688 epoch total loss 1.1186341\n",
      "Trained batch 80 batch loss 0.918692946 epoch total loss 1.11613488\n",
      "Trained batch 81 batch loss 0.957205296 epoch total loss 1.11417282\n",
      "Trained batch 82 batch loss 1.07338262 epoch total loss 1.11367524\n",
      "Trained batch 83 batch loss 1.07066941 epoch total loss 1.11315715\n",
      "Trained batch 84 batch loss 1.19670463 epoch total loss 1.11415172\n",
      "Trained batch 85 batch loss 1.16611838 epoch total loss 1.11476302\n",
      "Trained batch 86 batch loss 1.09181285 epoch total loss 1.11449623\n",
      "Trained batch 87 batch loss 0.945701838 epoch total loss 1.11255598\n",
      "Trained batch 88 batch loss 1.01156712 epoch total loss 1.11140835\n",
      "Trained batch 89 batch loss 0.88909018 epoch total loss 1.10891044\n",
      "Trained batch 90 batch loss 0.927318513 epoch total loss 1.1068927\n",
      "Trained batch 91 batch loss 0.913085818 epoch total loss 1.10476303\n",
      "Trained batch 92 batch loss 0.955416203 epoch total loss 1.10313964\n",
      "Trained batch 93 batch loss 0.886452556 epoch total loss 1.10080969\n",
      "Trained batch 94 batch loss 1.00478017 epoch total loss 1.09978807\n",
      "Trained batch 95 batch loss 0.81259948 epoch total loss 1.09676504\n",
      "Trained batch 96 batch loss 0.919715822 epoch total loss 1.09492075\n",
      "Trained batch 97 batch loss 0.94485724 epoch total loss 1.09337378\n",
      "Trained batch 98 batch loss 0.959077716 epoch total loss 1.09200335\n",
      "Trained batch 99 batch loss 0.98415339 epoch total loss 1.09091389\n",
      "Trained batch 100 batch loss 1.03648245 epoch total loss 1.0903697\n",
      "Trained batch 101 batch loss 1.29680264 epoch total loss 1.09241354\n",
      "Trained batch 102 batch loss 1.05574834 epoch total loss 1.09205413\n",
      "Trained batch 103 batch loss 1.05920815 epoch total loss 1.09173524\n",
      "Trained batch 104 batch loss 1.18797374 epoch total loss 1.09266067\n",
      "Trained batch 105 batch loss 1.03405237 epoch total loss 1.09210241\n",
      "Trained batch 106 batch loss 1.06688547 epoch total loss 1.09186459\n",
      "Trained batch 107 batch loss 1.04921532 epoch total loss 1.09146595\n",
      "Trained batch 108 batch loss 0.973008633 epoch total loss 1.09036911\n",
      "Trained batch 109 batch loss 1.08022499 epoch total loss 1.090276\n",
      "Trained batch 110 batch loss 1.07025242 epoch total loss 1.09009397\n",
      "Trained batch 111 batch loss 1.16089928 epoch total loss 1.09073186\n",
      "Trained batch 112 batch loss 1.06930435 epoch total loss 1.09054053\n",
      "Trained batch 113 batch loss 1.20389748 epoch total loss 1.09154367\n",
      "Trained batch 114 batch loss 1.06712568 epoch total loss 1.09132946\n",
      "Trained batch 115 batch loss 0.982662082 epoch total loss 1.09038448\n",
      "Trained batch 116 batch loss 0.998951197 epoch total loss 1.08959627\n",
      "Trained batch 117 batch loss 0.945045233 epoch total loss 1.08836079\n",
      "Trained batch 118 batch loss 1.04188526 epoch total loss 1.08796692\n",
      "Trained batch 119 batch loss 1.11342585 epoch total loss 1.08818078\n",
      "Trained batch 120 batch loss 1.06556273 epoch total loss 1.08799231\n",
      "Trained batch 121 batch loss 0.873903513 epoch total loss 1.08622301\n",
      "Trained batch 122 batch loss 0.981409729 epoch total loss 1.08536386\n",
      "Trained batch 123 batch loss 1.12745047 epoch total loss 1.08570611\n",
      "Trained batch 124 batch loss 1.15058589 epoch total loss 1.08622944\n",
      "Trained batch 125 batch loss 1.00250936 epoch total loss 1.08555961\n",
      "Trained batch 126 batch loss 1.13503873 epoch total loss 1.08595228\n",
      "Trained batch 127 batch loss 0.779027343 epoch total loss 1.08353555\n",
      "Trained batch 128 batch loss 0.742915392 epoch total loss 1.08087444\n",
      "Trained batch 129 batch loss 0.65765 epoch total loss 1.07759368\n",
      "Trained batch 130 batch loss 0.863156259 epoch total loss 1.07594419\n",
      "Trained batch 131 batch loss 1.15383029 epoch total loss 1.07653868\n",
      "Trained batch 132 batch loss 1.02130985 epoch total loss 1.07612038\n",
      "Trained batch 133 batch loss 1.22098958 epoch total loss 1.07720959\n",
      "Trained batch 134 batch loss 1.17586577 epoch total loss 1.07794583\n",
      "Trained batch 135 batch loss 1.12450218 epoch total loss 1.0782907\n",
      "Trained batch 136 batch loss 1.15497184 epoch total loss 1.07885456\n",
      "Trained batch 137 batch loss 1.145275 epoch total loss 1.07933939\n",
      "Trained batch 138 batch loss 1.24382114 epoch total loss 1.08053124\n",
      "Trained batch 139 batch loss 1.14972639 epoch total loss 1.08102906\n",
      "Trained batch 140 batch loss 1.1200062 epoch total loss 1.08130741\n",
      "Trained batch 141 batch loss 1.13423491 epoch total loss 1.0816828\n",
      "Trained batch 142 batch loss 1.2933116 epoch total loss 1.08317304\n",
      "Trained batch 143 batch loss 1.26840043 epoch total loss 1.08446836\n",
      "Trained batch 144 batch loss 1.10532439 epoch total loss 1.08461332\n",
      "Trained batch 145 batch loss 1.14375377 epoch total loss 1.08502114\n",
      "Trained batch 146 batch loss 0.898431957 epoch total loss 1.08374321\n",
      "Trained batch 147 batch loss 0.918039083 epoch total loss 1.08261597\n",
      "Trained batch 148 batch loss 0.960029244 epoch total loss 1.08178759\n",
      "Trained batch 149 batch loss 0.999835312 epoch total loss 1.08123755\n",
      "Trained batch 150 batch loss 1.17105174 epoch total loss 1.08183634\n",
      "Trained batch 151 batch loss 1.19370914 epoch total loss 1.08257723\n",
      "Trained batch 152 batch loss 1.28400064 epoch total loss 1.08390236\n",
      "Trained batch 153 batch loss 1.3156631 epoch total loss 1.08541715\n",
      "Trained batch 154 batch loss 1.13454235 epoch total loss 1.08573604\n",
      "Trained batch 155 batch loss 0.991303325 epoch total loss 1.08512676\n",
      "Trained batch 156 batch loss 0.944752812 epoch total loss 1.08422697\n",
      "Trained batch 157 batch loss 1.18727338 epoch total loss 1.08488333\n",
      "Trained batch 158 batch loss 1.2434901 epoch total loss 1.08588707\n",
      "Trained batch 159 batch loss 1.24341357 epoch total loss 1.08687782\n",
      "Trained batch 160 batch loss 1.23436141 epoch total loss 1.08779955\n",
      "Trained batch 161 batch loss 1.13333249 epoch total loss 1.08808231\n",
      "Trained batch 162 batch loss 1.15560758 epoch total loss 1.08849919\n",
      "Trained batch 163 batch loss 1.18583655 epoch total loss 1.08909631\n",
      "Trained batch 164 batch loss 1.13451707 epoch total loss 1.08937335\n",
      "Trained batch 165 batch loss 1.03729105 epoch total loss 1.08905768\n",
      "Trained batch 166 batch loss 1.07198942 epoch total loss 1.08895493\n",
      "Trained batch 167 batch loss 1.06418693 epoch total loss 1.08880663\n",
      "Trained batch 168 batch loss 1.20740557 epoch total loss 1.08951259\n",
      "Trained batch 169 batch loss 1.11890745 epoch total loss 1.08968651\n",
      "Trained batch 170 batch loss 1.12740803 epoch total loss 1.08990848\n",
      "Trained batch 171 batch loss 1.14719117 epoch total loss 1.09024346\n",
      "Trained batch 172 batch loss 1.17442226 epoch total loss 1.09073281\n",
      "Trained batch 173 batch loss 1.13722444 epoch total loss 1.09100151\n",
      "Trained batch 174 batch loss 1.10049248 epoch total loss 1.09105611\n",
      "Trained batch 175 batch loss 1.05970967 epoch total loss 1.09087694\n",
      "Trained batch 176 batch loss 1.00150406 epoch total loss 1.09036922\n",
      "Trained batch 177 batch loss 1.08742166 epoch total loss 1.09035254\n",
      "Trained batch 178 batch loss 1.06032133 epoch total loss 1.09018385\n",
      "Trained batch 179 batch loss 1.08860278 epoch total loss 1.09017503\n",
      "Trained batch 180 batch loss 1.11286581 epoch total loss 1.09030104\n",
      "Trained batch 181 batch loss 1.20744145 epoch total loss 1.09094834\n",
      "Trained batch 182 batch loss 1.2728368 epoch total loss 1.09194767\n",
      "Trained batch 183 batch loss 1.154706 epoch total loss 1.09229064\n",
      "Trained batch 184 batch loss 1.01429701 epoch total loss 1.09186673\n",
      "Trained batch 185 batch loss 1.07785511 epoch total loss 1.09179103\n",
      "Trained batch 186 batch loss 0.899289489 epoch total loss 1.09075606\n",
      "Trained batch 187 batch loss 1.11245418 epoch total loss 1.09087217\n",
      "Trained batch 188 batch loss 1.03920841 epoch total loss 1.09059739\n",
      "Trained batch 189 batch loss 1.14531529 epoch total loss 1.09088683\n",
      "Trained batch 190 batch loss 1.13280511 epoch total loss 1.09110749\n",
      "Trained batch 191 batch loss 1.09987092 epoch total loss 1.09115338\n",
      "Trained batch 192 batch loss 0.997328401 epoch total loss 1.09066474\n",
      "Trained batch 193 batch loss 1.06586289 epoch total loss 1.09053612\n",
      "Trained batch 194 batch loss 1.0055306 epoch total loss 1.0900979\n",
      "Trained batch 195 batch loss 1.03279006 epoch total loss 1.08980405\n",
      "Trained batch 196 batch loss 1.03567064 epoch total loss 1.08952796\n",
      "Trained batch 197 batch loss 1.03053534 epoch total loss 1.08922839\n",
      "Trained batch 198 batch loss 1.18166637 epoch total loss 1.08969533\n",
      "Trained batch 199 batch loss 0.963446379 epoch total loss 1.0890609\n",
      "Trained batch 200 batch loss 1.07513356 epoch total loss 1.08899128\n",
      "Trained batch 201 batch loss 1.12934935 epoch total loss 1.08919203\n",
      "Trained batch 202 batch loss 0.934218049 epoch total loss 1.0884248\n",
      "Trained batch 203 batch loss 1.0808419 epoch total loss 1.08838749\n",
      "Trained batch 204 batch loss 1.18536294 epoch total loss 1.0888629\n",
      "Trained batch 205 batch loss 1.05798769 epoch total loss 1.08871222\n",
      "Trained batch 206 batch loss 1.21185529 epoch total loss 1.08930993\n",
      "Trained batch 207 batch loss 1.15512526 epoch total loss 1.08962786\n",
      "Trained batch 208 batch loss 1.11250615 epoch total loss 1.08973789\n",
      "Trained batch 209 batch loss 1.30223751 epoch total loss 1.09075463\n",
      "Trained batch 210 batch loss 1.23072135 epoch total loss 1.09142113\n",
      "Trained batch 211 batch loss 1.07419682 epoch total loss 1.09133959\n",
      "Trained batch 212 batch loss 1.13365245 epoch total loss 1.09153914\n",
      "Trained batch 213 batch loss 1.00393844 epoch total loss 1.09112787\n",
      "Trained batch 214 batch loss 0.983759284 epoch total loss 1.09062612\n",
      "Trained batch 215 batch loss 1.01182544 epoch total loss 1.09025967\n",
      "Trained batch 216 batch loss 0.990015388 epoch total loss 1.08979559\n",
      "Trained batch 217 batch loss 1.09331894 epoch total loss 1.0898118\n",
      "Trained batch 218 batch loss 1.21912622 epoch total loss 1.09040499\n",
      "Trained batch 219 batch loss 1.07926834 epoch total loss 1.0903542\n",
      "Trained batch 220 batch loss 1.25717545 epoch total loss 1.09111249\n",
      "Trained batch 221 batch loss 1.11455977 epoch total loss 1.09121859\n",
      "Trained batch 222 batch loss 1.04409838 epoch total loss 1.09100628\n",
      "Trained batch 223 batch loss 1.14640498 epoch total loss 1.09125471\n",
      "Trained batch 224 batch loss 1.04686725 epoch total loss 1.09105659\n",
      "Trained batch 225 batch loss 0.931389689 epoch total loss 1.09034693\n",
      "Trained batch 226 batch loss 0.915270448 epoch total loss 1.08957231\n",
      "Trained batch 227 batch loss 0.972873092 epoch total loss 1.08905816\n",
      "Trained batch 228 batch loss 1.06423426 epoch total loss 1.08894932\n",
      "Trained batch 229 batch loss 0.921804905 epoch total loss 1.0882194\n",
      "Trained batch 230 batch loss 0.978439689 epoch total loss 1.08774209\n",
      "Trained batch 231 batch loss 1.05276263 epoch total loss 1.08759069\n",
      "Trained batch 232 batch loss 1.12252975 epoch total loss 1.08774126\n",
      "Trained batch 233 batch loss 1.0809716 epoch total loss 1.08771229\n",
      "Trained batch 234 batch loss 1.04158711 epoch total loss 1.08751512\n",
      "Trained batch 235 batch loss 1.12776041 epoch total loss 1.0876863\n",
      "Trained batch 236 batch loss 1.14455199 epoch total loss 1.08792734\n",
      "Trained batch 237 batch loss 1.13960826 epoch total loss 1.08814549\n",
      "Trained batch 238 batch loss 1.08725023 epoch total loss 1.08814168\n",
      "Trained batch 239 batch loss 1.08107 epoch total loss 1.088112\n",
      "Trained batch 240 batch loss 1.12141812 epoch total loss 1.08825088\n",
      "Trained batch 241 batch loss 1.14105642 epoch total loss 1.08847\n",
      "Trained batch 242 batch loss 1.1301477 epoch total loss 1.08864224\n",
      "Trained batch 243 batch loss 1.1926614 epoch total loss 1.08907032\n",
      "Trained batch 244 batch loss 1.12715864 epoch total loss 1.08922637\n",
      "Trained batch 245 batch loss 1.09243929 epoch total loss 1.08923948\n",
      "Trained batch 246 batch loss 1.06451249 epoch total loss 1.08913898\n",
      "Trained batch 247 batch loss 1.04288197 epoch total loss 1.08895171\n",
      "Trained batch 248 batch loss 1.12309027 epoch total loss 1.08908927\n",
      "Trained batch 249 batch loss 1.14011681 epoch total loss 1.0892942\n",
      "Trained batch 250 batch loss 1.18188655 epoch total loss 1.08966458\n",
      "Trained batch 251 batch loss 1.01752973 epoch total loss 1.08937716\n",
      "Trained batch 252 batch loss 1.16084278 epoch total loss 1.08966064\n",
      "Trained batch 253 batch loss 1.10397696 epoch total loss 1.08971727\n",
      "Trained batch 254 batch loss 1.1558609 epoch total loss 1.08997762\n",
      "Trained batch 255 batch loss 1.09045792 epoch total loss 1.08997941\n",
      "Trained batch 256 batch loss 1.2142036 epoch total loss 1.09046471\n",
      "Trained batch 257 batch loss 1.2122438 epoch total loss 1.09093857\n",
      "Trained batch 258 batch loss 1.01673782 epoch total loss 1.09065092\n",
      "Trained batch 259 batch loss 0.939410806 epoch total loss 1.09006703\n",
      "Trained batch 260 batch loss 0.841572881 epoch total loss 1.08911133\n",
      "Trained batch 261 batch loss 0.930770755 epoch total loss 1.08850455\n",
      "Trained batch 262 batch loss 1.10136533 epoch total loss 1.08855379\n",
      "Trained batch 263 batch loss 1.02569079 epoch total loss 1.08831477\n",
      "Trained batch 264 batch loss 1.07491732 epoch total loss 1.08826399\n",
      "Trained batch 265 batch loss 1.23218775 epoch total loss 1.08880711\n",
      "Trained batch 266 batch loss 1.25682688 epoch total loss 1.0894388\n",
      "Trained batch 267 batch loss 1.06693 epoch total loss 1.0893544\n",
      "Trained batch 268 batch loss 1.14135408 epoch total loss 1.08954847\n",
      "Trained batch 269 batch loss 0.992584 epoch total loss 1.08918798\n",
      "Trained batch 270 batch loss 0.932465672 epoch total loss 1.08860755\n",
      "Trained batch 271 batch loss 0.868436217 epoch total loss 1.08779514\n",
      "Trained batch 272 batch loss 0.975302815 epoch total loss 1.0873816\n",
      "Trained batch 273 batch loss 0.830438256 epoch total loss 1.08644044\n",
      "Trained batch 274 batch loss 0.801366806 epoch total loss 1.0854\n",
      "Trained batch 275 batch loss 0.832141399 epoch total loss 1.08447909\n",
      "Trained batch 276 batch loss 0.871988177 epoch total loss 1.08370912\n",
      "Trained batch 277 batch loss 0.976641834 epoch total loss 1.08332264\n",
      "Trained batch 278 batch loss 1.00643408 epoch total loss 1.08304608\n",
      "Trained batch 279 batch loss 1.05087793 epoch total loss 1.0829308\n",
      "Trained batch 280 batch loss 0.96456337 epoch total loss 1.08250809\n",
      "Trained batch 281 batch loss 1.11888361 epoch total loss 1.08263755\n",
      "Trained batch 282 batch loss 1.10618663 epoch total loss 1.08272111\n",
      "Trained batch 283 batch loss 1.15591061 epoch total loss 1.0829798\n",
      "Trained batch 284 batch loss 1.10746884 epoch total loss 1.08306611\n",
      "Trained batch 285 batch loss 1.08688259 epoch total loss 1.08307946\n",
      "Trained batch 286 batch loss 1.21681893 epoch total loss 1.08354712\n",
      "Trained batch 287 batch loss 1.16052139 epoch total loss 1.08381534\n",
      "Trained batch 288 batch loss 1.17113769 epoch total loss 1.08411849\n",
      "Trained batch 289 batch loss 0.975076139 epoch total loss 1.08374119\n",
      "Trained batch 290 batch loss 1.17712116 epoch total loss 1.08406317\n",
      "Trained batch 291 batch loss 1.04812193 epoch total loss 1.08393967\n",
      "Trained batch 292 batch loss 1.05995274 epoch total loss 1.08385766\n",
      "Trained batch 293 batch loss 1.09260726 epoch total loss 1.08388746\n",
      "Trained batch 294 batch loss 1.07385111 epoch total loss 1.08385336\n",
      "Trained batch 295 batch loss 1.12153268 epoch total loss 1.08398104\n",
      "Trained batch 296 batch loss 1.09410036 epoch total loss 1.08401525\n",
      "Trained batch 297 batch loss 0.924114 epoch total loss 1.08347678\n",
      "Trained batch 298 batch loss 0.922457695 epoch total loss 1.08293641\n",
      "Trained batch 299 batch loss 0.894859254 epoch total loss 1.08230746\n",
      "Trained batch 300 batch loss 0.950654149 epoch total loss 1.08186865\n",
      "Trained batch 301 batch loss 1.02313721 epoch total loss 1.0816735\n",
      "Trained batch 302 batch loss 0.859095573 epoch total loss 1.08093643\n",
      "Trained batch 303 batch loss 0.987856805 epoch total loss 1.08062923\n",
      "Trained batch 304 batch loss 1.07355392 epoch total loss 1.08060598\n",
      "Trained batch 305 batch loss 1.13103962 epoch total loss 1.08077133\n",
      "Trained batch 306 batch loss 1.17908728 epoch total loss 1.0810926\n",
      "Trained batch 307 batch loss 1.1406635 epoch total loss 1.08128655\n",
      "Trained batch 308 batch loss 1.09163129 epoch total loss 1.08132029\n",
      "Trained batch 309 batch loss 1.21464658 epoch total loss 1.0817517\n",
      "Trained batch 310 batch loss 1.09533918 epoch total loss 1.08179557\n",
      "Trained batch 311 batch loss 1.13262463 epoch total loss 1.08195901\n",
      "Trained batch 312 batch loss 1.28141236 epoch total loss 1.08259833\n",
      "Trained batch 313 batch loss 0.991741836 epoch total loss 1.08230793\n",
      "Trained batch 314 batch loss 0.845809698 epoch total loss 1.08155477\n",
      "Trained batch 315 batch loss 1.03390789 epoch total loss 1.08140349\n",
      "Trained batch 316 batch loss 1.05297744 epoch total loss 1.08131349\n",
      "Trained batch 317 batch loss 1.05424654 epoch total loss 1.08122814\n",
      "Trained batch 318 batch loss 1.03851533 epoch total loss 1.08109379\n",
      "Trained batch 319 batch loss 1.11251247 epoch total loss 1.08119237\n",
      "Trained batch 320 batch loss 1.10558045 epoch total loss 1.08126855\n",
      "Trained batch 321 batch loss 1.07509208 epoch total loss 1.08124936\n",
      "Trained batch 322 batch loss 1.15679622 epoch total loss 1.08148396\n",
      "Trained batch 323 batch loss 1.18075705 epoch total loss 1.0817914\n",
      "Trained batch 324 batch loss 1.18141031 epoch total loss 1.08209884\n",
      "Trained batch 325 batch loss 1.17225993 epoch total loss 1.08237624\n",
      "Trained batch 326 batch loss 1.03478324 epoch total loss 1.08223021\n",
      "Trained batch 327 batch loss 1.15874743 epoch total loss 1.08246434\n",
      "Trained batch 328 batch loss 1.26175165 epoch total loss 1.08301091\n",
      "Trained batch 329 batch loss 1.25711393 epoch total loss 1.08354008\n",
      "Trained batch 330 batch loss 1.18416154 epoch total loss 1.08384502\n",
      "Trained batch 331 batch loss 1.13015342 epoch total loss 1.08398497\n",
      "Trained batch 332 batch loss 1.22401547 epoch total loss 1.08440673\n",
      "Trained batch 333 batch loss 1.12702537 epoch total loss 1.08453465\n",
      "Trained batch 334 batch loss 1.02226818 epoch total loss 1.08434832\n",
      "Trained batch 335 batch loss 0.951987922 epoch total loss 1.08395326\n",
      "Trained batch 336 batch loss 1.09261608 epoch total loss 1.08397901\n",
      "Trained batch 337 batch loss 1.19633198 epoch total loss 1.08431232\n",
      "Trained batch 338 batch loss 1.13366282 epoch total loss 1.08445835\n",
      "Trained batch 339 batch loss 1.08539701 epoch total loss 1.08446109\n",
      "Trained batch 340 batch loss 0.985780478 epoch total loss 1.08417094\n",
      "Trained batch 341 batch loss 0.921835721 epoch total loss 1.08369482\n",
      "Trained batch 342 batch loss 0.916151106 epoch total loss 1.08320498\n",
      "Trained batch 343 batch loss 0.950263143 epoch total loss 1.08281732\n",
      "Trained batch 344 batch loss 0.978209615 epoch total loss 1.08251321\n",
      "Trained batch 345 batch loss 0.956938148 epoch total loss 1.08214927\n",
      "Trained batch 346 batch loss 0.999689817 epoch total loss 1.08191097\n",
      "Trained batch 347 batch loss 1.08951688 epoch total loss 1.08193278\n",
      "Trained batch 348 batch loss 1.06762552 epoch total loss 1.08189178\n",
      "Trained batch 349 batch loss 1.1543299 epoch total loss 1.08209932\n",
      "Trained batch 350 batch loss 0.967755258 epoch total loss 1.08177257\n",
      "Trained batch 351 batch loss 0.958601356 epoch total loss 1.08142161\n",
      "Trained batch 352 batch loss 1.07013726 epoch total loss 1.08138955\n",
      "Trained batch 353 batch loss 1.1430397 epoch total loss 1.08156419\n",
      "Trained batch 354 batch loss 1.07333291 epoch total loss 1.08154094\n",
      "Trained batch 355 batch loss 1.03353977 epoch total loss 1.08140564\n",
      "Trained batch 356 batch loss 1.23914707 epoch total loss 1.08184874\n",
      "Trained batch 357 batch loss 1.17353868 epoch total loss 1.08210564\n",
      "Trained batch 358 batch loss 1.24804258 epoch total loss 1.08256912\n",
      "Trained batch 359 batch loss 1.06361556 epoch total loss 1.08251643\n",
      "Trained batch 360 batch loss 1.29108787 epoch total loss 1.08309567\n",
      "Trained batch 361 batch loss 1.23792744 epoch total loss 1.08352458\n",
      "Trained batch 362 batch loss 1.08040714 epoch total loss 1.083516\n",
      "Trained batch 363 batch loss 1.10678673 epoch total loss 1.08358014\n",
      "Trained batch 364 batch loss 0.978995562 epoch total loss 1.08329284\n",
      "Trained batch 365 batch loss 1.0034976 epoch total loss 1.08307421\n",
      "Trained batch 366 batch loss 1.05756772 epoch total loss 1.08300447\n",
      "Trained batch 367 batch loss 0.970016062 epoch total loss 1.08269656\n",
      "Trained batch 368 batch loss 1.09309959 epoch total loss 1.08272481\n",
      "Trained batch 369 batch loss 0.966159 epoch total loss 1.08240891\n",
      "Trained batch 370 batch loss 1.01528597 epoch total loss 1.08222759\n",
      "Trained batch 371 batch loss 1.03284144 epoch total loss 1.08209443\n",
      "Trained batch 372 batch loss 1.0863651 epoch total loss 1.08210588\n",
      "Trained batch 373 batch loss 1.13575411 epoch total loss 1.08224964\n",
      "Trained batch 374 batch loss 1.07070279 epoch total loss 1.08221889\n",
      "Trained batch 375 batch loss 0.996490836 epoch total loss 1.08199024\n",
      "Trained batch 376 batch loss 1.03135824 epoch total loss 1.08185565\n",
      "Trained batch 377 batch loss 1.09184158 epoch total loss 1.08188212\n",
      "Trained batch 378 batch loss 1.20501208 epoch total loss 1.0822078\n",
      "Trained batch 379 batch loss 1.12023807 epoch total loss 1.08230817\n",
      "Trained batch 380 batch loss 1.0205816 epoch total loss 1.08214569\n",
      "Trained batch 381 batch loss 1.00897658 epoch total loss 1.08195364\n",
      "Trained batch 382 batch loss 0.979102612 epoch total loss 1.08168435\n",
      "Trained batch 383 batch loss 1.10452259 epoch total loss 1.08174396\n",
      "Trained batch 384 batch loss 1.08881712 epoch total loss 1.08176243\n",
      "Trained batch 385 batch loss 1.14428186 epoch total loss 1.0819248\n",
      "Trained batch 386 batch loss 1.27081561 epoch total loss 1.08241415\n",
      "Trained batch 387 batch loss 1.23288131 epoch total loss 1.08280289\n",
      "Trained batch 388 batch loss 1.1943779 epoch total loss 1.08309042\n",
      "Trained batch 389 batch loss 1.09171629 epoch total loss 1.0831126\n",
      "Trained batch 390 batch loss 1.00769258 epoch total loss 1.08291924\n",
      "Trained batch 391 batch loss 0.878223181 epoch total loss 1.08239579\n",
      "Trained batch 392 batch loss 1.04080129 epoch total loss 1.0822897\n",
      "Trained batch 393 batch loss 1.05037451 epoch total loss 1.0822084\n",
      "Trained batch 394 batch loss 1.07491016 epoch total loss 1.08218992\n",
      "Trained batch 395 batch loss 0.969076037 epoch total loss 1.08190358\n",
      "Trained batch 396 batch loss 0.962320924 epoch total loss 1.08160162\n",
      "Trained batch 397 batch loss 0.984304845 epoch total loss 1.08135653\n",
      "Trained batch 398 batch loss 1.01538956 epoch total loss 1.08119082\n",
      "Trained batch 399 batch loss 1.09919059 epoch total loss 1.08123589\n",
      "Trained batch 400 batch loss 0.937783122 epoch total loss 1.08087718\n",
      "Trained batch 401 batch loss 1.21010149 epoch total loss 1.08119953\n",
      "Trained batch 402 batch loss 0.975065589 epoch total loss 1.08093548\n",
      "Trained batch 403 batch loss 1.00789225 epoch total loss 1.08075428\n",
      "Trained batch 404 batch loss 1.00572824 epoch total loss 1.08056855\n",
      "Trained batch 405 batch loss 1.11409521 epoch total loss 1.0806514\n",
      "Trained batch 406 batch loss 1.08177114 epoch total loss 1.08065414\n",
      "Trained batch 407 batch loss 1.0928508 epoch total loss 1.08068419\n",
      "Trained batch 408 batch loss 1.12098312 epoch total loss 1.08078289\n",
      "Trained batch 409 batch loss 1.04723835 epoch total loss 1.08070087\n",
      "Trained batch 410 batch loss 1.01318979 epoch total loss 1.08053613\n",
      "Trained batch 411 batch loss 1.01283836 epoch total loss 1.0803715\n",
      "Trained batch 412 batch loss 1.01561975 epoch total loss 1.08021438\n",
      "Trained batch 413 batch loss 1.07587886 epoch total loss 1.08020377\n",
      "Trained batch 414 batch loss 1.03823936 epoch total loss 1.08010244\n",
      "Trained batch 415 batch loss 0.967919409 epoch total loss 1.0798322\n",
      "Trained batch 416 batch loss 0.994919062 epoch total loss 1.07962811\n",
      "Trained batch 417 batch loss 1.10056615 epoch total loss 1.0796783\n",
      "Trained batch 418 batch loss 1.19272685 epoch total loss 1.07994866\n",
      "Trained batch 419 batch loss 1.21072876 epoch total loss 1.08026075\n",
      "Trained batch 420 batch loss 1.13671529 epoch total loss 1.08039522\n",
      "Trained batch 421 batch loss 1.0632205 epoch total loss 1.08035445\n",
      "Trained batch 422 batch loss 1.1872648 epoch total loss 1.08060777\n",
      "Trained batch 423 batch loss 1.15852141 epoch total loss 1.08079195\n",
      "Trained batch 424 batch loss 1.10869622 epoch total loss 1.08085775\n",
      "Trained batch 425 batch loss 1.06705165 epoch total loss 1.08082521\n",
      "Trained batch 426 batch loss 1.06304526 epoch total loss 1.08078349\n",
      "Trained batch 427 batch loss 0.987182558 epoch total loss 1.08056438\n",
      "Trained batch 428 batch loss 1.0359143 epoch total loss 1.08046007\n",
      "Trained batch 429 batch loss 1.10061312 epoch total loss 1.08050704\n",
      "Trained batch 430 batch loss 1.03389823 epoch total loss 1.08039868\n",
      "Trained batch 431 batch loss 0.989716649 epoch total loss 1.08018827\n",
      "Trained batch 432 batch loss 1.04155302 epoch total loss 1.08009887\n",
      "Trained batch 433 batch loss 0.964025 epoch total loss 1.07983077\n",
      "Trained batch 434 batch loss 0.915005565 epoch total loss 1.07945096\n",
      "Trained batch 435 batch loss 0.944361687 epoch total loss 1.07914042\n",
      "Trained batch 436 batch loss 0.871111274 epoch total loss 1.07866335\n",
      "Trained batch 437 batch loss 0.940167606 epoch total loss 1.07834637\n",
      "Trained batch 438 batch loss 1.05778193 epoch total loss 1.0782994\n",
      "Trained batch 439 batch loss 1.09142864 epoch total loss 1.07832932\n",
      "Trained batch 440 batch loss 1.12309527 epoch total loss 1.07843113\n",
      "Trained batch 441 batch loss 1.11296082 epoch total loss 1.07850933\n",
      "Trained batch 442 batch loss 0.987542272 epoch total loss 1.07830358\n",
      "Trained batch 443 batch loss 0.92947793 epoch total loss 1.07796764\n",
      "Trained batch 444 batch loss 0.995182633 epoch total loss 1.07778108\n",
      "Trained batch 445 batch loss 1.08038878 epoch total loss 1.07778692\n",
      "Trained batch 446 batch loss 1.17786372 epoch total loss 1.07801139\n",
      "Trained batch 447 batch loss 1.12334287 epoch total loss 1.07811272\n",
      "Trained batch 448 batch loss 1.11053348 epoch total loss 1.0781852\n",
      "Trained batch 449 batch loss 1.14035273 epoch total loss 1.0783236\n",
      "Trained batch 450 batch loss 1.15502536 epoch total loss 1.07849407\n",
      "Trained batch 451 batch loss 1.04814 epoch total loss 1.07842672\n",
      "Trained batch 452 batch loss 1.09660554 epoch total loss 1.07846701\n",
      "Trained batch 453 batch loss 1.02501857 epoch total loss 1.07834899\n",
      "Trained batch 454 batch loss 1.09050035 epoch total loss 1.07837582\n",
      "Trained batch 455 batch loss 1.12522531 epoch total loss 1.07847869\n",
      "Trained batch 456 batch loss 0.971856892 epoch total loss 1.07824492\n",
      "Trained batch 457 batch loss 1.16070783 epoch total loss 1.07842541\n",
      "Trained batch 458 batch loss 1.28587127 epoch total loss 1.07887828\n",
      "Trained batch 459 batch loss 1.08680582 epoch total loss 1.07889557\n",
      "Trained batch 460 batch loss 1.00880933 epoch total loss 1.07874322\n",
      "Trained batch 461 batch loss 0.832924843 epoch total loss 1.07820988\n",
      "Trained batch 462 batch loss 0.936521471 epoch total loss 1.07790327\n",
      "Trained batch 463 batch loss 1.00569689 epoch total loss 1.07774734\n",
      "Trained batch 464 batch loss 1.07421267 epoch total loss 1.07773972\n",
      "Trained batch 465 batch loss 1.14554548 epoch total loss 1.07788551\n",
      "Trained batch 466 batch loss 1.16734266 epoch total loss 1.07807744\n",
      "Trained batch 467 batch loss 1.02339566 epoch total loss 1.07796037\n",
      "Trained batch 468 batch loss 0.963798285 epoch total loss 1.07771647\n",
      "Trained batch 469 batch loss 1.00595772 epoch total loss 1.07756341\n",
      "Trained batch 470 batch loss 1.06260753 epoch total loss 1.0775317\n",
      "Trained batch 471 batch loss 1.1436528 epoch total loss 1.077672\n",
      "Trained batch 472 batch loss 1.09690213 epoch total loss 1.07771277\n",
      "Trained batch 473 batch loss 1.11845922 epoch total loss 1.07779896\n",
      "Trained batch 474 batch loss 0.902919114 epoch total loss 1.07743\n",
      "Trained batch 475 batch loss 0.948444605 epoch total loss 1.07715845\n",
      "Trained batch 476 batch loss 0.95928812 epoch total loss 1.07691085\n",
      "Trained batch 477 batch loss 1.12784636 epoch total loss 1.07701766\n",
      "Trained batch 478 batch loss 1.11080503 epoch total loss 1.07708824\n",
      "Trained batch 479 batch loss 1.24747705 epoch total loss 1.07744408\n",
      "Trained batch 480 batch loss 1.10674596 epoch total loss 1.07750511\n",
      "Trained batch 481 batch loss 1.09209335 epoch total loss 1.07753551\n",
      "Trained batch 482 batch loss 1.23493385 epoch total loss 1.07786202\n",
      "Trained batch 483 batch loss 1.03525448 epoch total loss 1.07777381\n",
      "Trained batch 484 batch loss 1.09310389 epoch total loss 1.0778054\n",
      "Trained batch 485 batch loss 0.99932754 epoch total loss 1.07764363\n",
      "Trained batch 486 batch loss 1.08797264 epoch total loss 1.07766485\n",
      "Trained batch 487 batch loss 0.993327677 epoch total loss 1.07749176\n",
      "Trained batch 488 batch loss 1.00096679 epoch total loss 1.07733488\n",
      "Trained batch 489 batch loss 1.03629553 epoch total loss 1.07725108\n",
      "Trained batch 490 batch loss 1.19405532 epoch total loss 1.07748938\n",
      "Trained batch 491 batch loss 1.28801978 epoch total loss 1.07791817\n",
      "Trained batch 492 batch loss 1.21784914 epoch total loss 1.07820249\n",
      "Trained batch 493 batch loss 1.10886586 epoch total loss 1.07826483\n",
      "Trained batch 494 batch loss 1.17071593 epoch total loss 1.07845187\n",
      "Trained batch 495 batch loss 1.04718757 epoch total loss 1.07838869\n",
      "Trained batch 496 batch loss 0.97252506 epoch total loss 1.07817531\n",
      "Trained batch 497 batch loss 0.873137653 epoch total loss 1.07776272\n",
      "Trained batch 498 batch loss 0.847568631 epoch total loss 1.07730055\n",
      "Trained batch 499 batch loss 0.893237948 epoch total loss 1.07693172\n",
      "Trained batch 500 batch loss 0.960739315 epoch total loss 1.07669938\n",
      "Trained batch 501 batch loss 1.1336813 epoch total loss 1.0768131\n",
      "Trained batch 502 batch loss 1.12198412 epoch total loss 1.0769031\n",
      "Trained batch 503 batch loss 1.17840159 epoch total loss 1.07710493\n",
      "Trained batch 504 batch loss 1.25385129 epoch total loss 1.07745552\n",
      "Trained batch 505 batch loss 1.30124259 epoch total loss 1.07789874\n",
      "Trained batch 506 batch loss 1.29237485 epoch total loss 1.07832253\n",
      "Trained batch 507 batch loss 1.09258842 epoch total loss 1.07835066\n",
      "Trained batch 508 batch loss 1.03990626 epoch total loss 1.07827508\n",
      "Trained batch 509 batch loss 1.02265382 epoch total loss 1.07816577\n",
      "Trained batch 510 batch loss 0.978037119 epoch total loss 1.07796943\n",
      "Trained batch 511 batch loss 0.897595048 epoch total loss 1.07761645\n",
      "Trained batch 512 batch loss 0.97922045 epoch total loss 1.07742429\n",
      "Trained batch 513 batch loss 0.924112678 epoch total loss 1.07712543\n",
      "Trained batch 514 batch loss 0.955521524 epoch total loss 1.07688892\n",
      "Trained batch 515 batch loss 0.83769381 epoch total loss 1.07642448\n",
      "Trained batch 516 batch loss 0.872290552 epoch total loss 1.07602882\n",
      "Trained batch 517 batch loss 1.07438219 epoch total loss 1.07602572\n",
      "Trained batch 518 batch loss 0.978837907 epoch total loss 1.07583809\n",
      "Trained batch 519 batch loss 1.02099574 epoch total loss 1.07573235\n",
      "Trained batch 520 batch loss 0.967377901 epoch total loss 1.07552409\n",
      "Trained batch 521 batch loss 1.04709578 epoch total loss 1.07546961\n",
      "Trained batch 522 batch loss 1.18093824 epoch total loss 1.07567155\n",
      "Trained batch 523 batch loss 1.26425886 epoch total loss 1.07603216\n",
      "Trained batch 524 batch loss 1.04375768 epoch total loss 1.07597065\n",
      "Trained batch 525 batch loss 0.996908903 epoch total loss 1.07582\n",
      "Trained batch 526 batch loss 1.10257816 epoch total loss 1.07587087\n",
      "Trained batch 527 batch loss 1.07807827 epoch total loss 1.07587504\n",
      "Trained batch 528 batch loss 1.16585875 epoch total loss 1.07604539\n",
      "Trained batch 529 batch loss 1.0156498 epoch total loss 1.07593119\n",
      "Trained batch 530 batch loss 1.04293418 epoch total loss 1.07586884\n",
      "Trained batch 531 batch loss 1.07259321 epoch total loss 1.07586265\n",
      "Trained batch 532 batch loss 1.10589814 epoch total loss 1.07591915\n",
      "Trained batch 533 batch loss 1.01165104 epoch total loss 1.07579851\n",
      "Trained batch 534 batch loss 1.11705768 epoch total loss 1.07587588\n",
      "Trained batch 535 batch loss 1.10511589 epoch total loss 1.07593048\n",
      "Trained batch 536 batch loss 0.962943256 epoch total loss 1.07571971\n",
      "Trained batch 537 batch loss 1.02764225 epoch total loss 1.07563019\n",
      "Trained batch 538 batch loss 1.07683706 epoch total loss 1.07563245\n",
      "Trained batch 539 batch loss 1.11707246 epoch total loss 1.07570934\n",
      "Trained batch 540 batch loss 1.0459584 epoch total loss 1.07565415\n",
      "Trained batch 541 batch loss 0.952833056 epoch total loss 1.07542717\n",
      "Trained batch 542 batch loss 0.986748099 epoch total loss 1.0752635\n",
      "Trained batch 543 batch loss 1.15134871 epoch total loss 1.07540369\n",
      "Trained batch 544 batch loss 1.04481828 epoch total loss 1.07534742\n",
      "Trained batch 545 batch loss 0.852857828 epoch total loss 1.07493913\n",
      "Trained batch 546 batch loss 0.880695105 epoch total loss 1.07458341\n",
      "Trained batch 547 batch loss 0.882778585 epoch total loss 1.0742327\n",
      "Trained batch 548 batch loss 1.12884903 epoch total loss 1.07433236\n",
      "Trained batch 549 batch loss 1.15477133 epoch total loss 1.07447886\n",
      "Trained batch 550 batch loss 1.13900828 epoch total loss 1.07459629\n",
      "Trained batch 551 batch loss 1.13428164 epoch total loss 1.07470465\n",
      "Trained batch 552 batch loss 1.00735641 epoch total loss 1.0745827\n",
      "Trained batch 553 batch loss 0.886882663 epoch total loss 1.07424331\n",
      "Trained batch 554 batch loss 1.02682471 epoch total loss 1.0741576\n",
      "Trained batch 555 batch loss 1.06221235 epoch total loss 1.07413602\n",
      "Trained batch 556 batch loss 1.0848819 epoch total loss 1.07415545\n",
      "Trained batch 557 batch loss 1.05979466 epoch total loss 1.0741297\n",
      "Trained batch 558 batch loss 1.12177646 epoch total loss 1.07421505\n",
      "Trained batch 559 batch loss 1.25096655 epoch total loss 1.0745312\n",
      "Trained batch 560 batch loss 1.11289489 epoch total loss 1.07459974\n",
      "Trained batch 561 batch loss 1.14287388 epoch total loss 1.07472146\n",
      "Trained batch 562 batch loss 1.06104493 epoch total loss 1.07469714\n",
      "Trained batch 563 batch loss 0.984211206 epoch total loss 1.07453644\n",
      "Trained batch 564 batch loss 1.09350359 epoch total loss 1.07457006\n",
      "Trained batch 565 batch loss 1.00550294 epoch total loss 1.07444775\n",
      "Trained batch 566 batch loss 1.12080407 epoch total loss 1.07452965\n",
      "Trained batch 567 batch loss 1.14248562 epoch total loss 1.07464945\n",
      "Trained batch 568 batch loss 1.12571931 epoch total loss 1.07473934\n",
      "Trained batch 569 batch loss 1.19227505 epoch total loss 1.07494593\n",
      "Trained batch 570 batch loss 0.99242425 epoch total loss 1.07480121\n",
      "Trained batch 571 batch loss 0.974868655 epoch total loss 1.07462609\n",
      "Trained batch 572 batch loss 0.990514398 epoch total loss 1.0744791\n",
      "Trained batch 573 batch loss 0.958490491 epoch total loss 1.07427669\n",
      "Trained batch 574 batch loss 0.872720838 epoch total loss 1.07392561\n",
      "Trained batch 575 batch loss 1.02542901 epoch total loss 1.07384133\n",
      "Trained batch 576 batch loss 0.967105269 epoch total loss 1.07365596\n",
      "Trained batch 577 batch loss 1.05451202 epoch total loss 1.07362282\n",
      "Trained batch 578 batch loss 0.979166567 epoch total loss 1.07345939\n",
      "Trained batch 579 batch loss 0.948367715 epoch total loss 1.07324338\n",
      "Trained batch 580 batch loss 0.933557093 epoch total loss 1.07300246\n",
      "Trained batch 581 batch loss 0.895776868 epoch total loss 1.0726974\n",
      "Trained batch 582 batch loss 1.21250749 epoch total loss 1.07293761\n",
      "Trained batch 583 batch loss 1.05962503 epoch total loss 1.07291484\n",
      "Trained batch 584 batch loss 1.1373837 epoch total loss 1.07302523\n",
      "Trained batch 585 batch loss 1.11763597 epoch total loss 1.0731014\n",
      "Trained batch 586 batch loss 1.18200874 epoch total loss 1.07328725\n",
      "Trained batch 587 batch loss 1.21646357 epoch total loss 1.07353127\n",
      "Trained batch 588 batch loss 1.19510877 epoch total loss 1.0737381\n",
      "Trained batch 589 batch loss 1.1430254 epoch total loss 1.07385564\n",
      "Trained batch 590 batch loss 1.06346321 epoch total loss 1.07383811\n",
      "Trained batch 591 batch loss 1.11265898 epoch total loss 1.0739038\n",
      "Trained batch 592 batch loss 1.06429672 epoch total loss 1.07388747\n",
      "Trained batch 593 batch loss 1.06760919 epoch total loss 1.07387698\n",
      "Trained batch 594 batch loss 1.0030961 epoch total loss 1.07375777\n",
      "Trained batch 595 batch loss 1.16264713 epoch total loss 1.07390726\n",
      "Trained batch 596 batch loss 1.15447688 epoch total loss 1.07404244\n",
      "Trained batch 597 batch loss 1.04596186 epoch total loss 1.07399535\n",
      "Trained batch 598 batch loss 1.15836954 epoch total loss 1.0741365\n",
      "Trained batch 599 batch loss 1.1138438 epoch total loss 1.07420278\n",
      "Trained batch 600 batch loss 0.910403609 epoch total loss 1.07392979\n",
      "Trained batch 601 batch loss 1.07205582 epoch total loss 1.07392669\n",
      "Trained batch 602 batch loss 1.05442786 epoch total loss 1.07389426\n",
      "Trained batch 603 batch loss 0.979630947 epoch total loss 1.07373798\n",
      "Trained batch 604 batch loss 1.02165604 epoch total loss 1.07365179\n",
      "Trained batch 605 batch loss 0.953212142 epoch total loss 1.07345259\n",
      "Trained batch 606 batch loss 1.11157751 epoch total loss 1.07351553\n",
      "Trained batch 607 batch loss 1.13905859 epoch total loss 1.07362354\n",
      "Trained batch 608 batch loss 1.14747906 epoch total loss 1.07374489\n",
      "Trained batch 609 batch loss 1.0999912 epoch total loss 1.07378805\n",
      "Trained batch 610 batch loss 1.08948445 epoch total loss 1.07381368\n",
      "Trained batch 611 batch loss 0.966368318 epoch total loss 1.07363784\n",
      "Trained batch 612 batch loss 1.06483328 epoch total loss 1.07362342\n",
      "Trained batch 613 batch loss 1.00660777 epoch total loss 1.0735141\n",
      "Trained batch 614 batch loss 0.97275269 epoch total loss 1.07335007\n",
      "Trained batch 615 batch loss 0.876029074 epoch total loss 1.07302916\n",
      "Trained batch 616 batch loss 0.975933373 epoch total loss 1.07287157\n",
      "Trained batch 617 batch loss 0.966419578 epoch total loss 1.07269907\n",
      "Trained batch 618 batch loss 0.934680164 epoch total loss 1.07247579\n",
      "Trained batch 619 batch loss 0.896017194 epoch total loss 1.07219064\n",
      "Trained batch 620 batch loss 1.03636658 epoch total loss 1.07213295\n",
      "Trained batch 621 batch loss 1.03763628 epoch total loss 1.07207739\n",
      "Trained batch 622 batch loss 1.20331848 epoch total loss 1.07228839\n",
      "Trained batch 623 batch loss 1.02940261 epoch total loss 1.07221961\n",
      "Trained batch 624 batch loss 1.08988929 epoch total loss 1.07224786\n",
      "Trained batch 625 batch loss 0.919022322 epoch total loss 1.07200277\n",
      "Trained batch 626 batch loss 1.08824301 epoch total loss 1.07202876\n",
      "Trained batch 627 batch loss 1.01818383 epoch total loss 1.07194281\n",
      "Trained batch 628 batch loss 1.08678937 epoch total loss 1.07196653\n",
      "Trained batch 629 batch loss 0.912744343 epoch total loss 1.07171333\n",
      "Trained batch 630 batch loss 0.881375849 epoch total loss 1.07141113\n",
      "Trained batch 631 batch loss 1.13519108 epoch total loss 1.07151222\n",
      "Trained batch 632 batch loss 1.0990994 epoch total loss 1.07155585\n",
      "Trained batch 633 batch loss 1.1425786 epoch total loss 1.07166815\n",
      "Trained batch 634 batch loss 1.08558631 epoch total loss 1.07169008\n",
      "Trained batch 635 batch loss 1.18551517 epoch total loss 1.07186925\n",
      "Trained batch 636 batch loss 1.06402969 epoch total loss 1.07185686\n",
      "Trained batch 637 batch loss 0.972367764 epoch total loss 1.07170069\n",
      "Trained batch 638 batch loss 0.952685833 epoch total loss 1.07151413\n",
      "Trained batch 639 batch loss 1.06148493 epoch total loss 1.07149839\n",
      "Trained batch 640 batch loss 1.13258803 epoch total loss 1.07159388\n",
      "Trained batch 641 batch loss 1.10145092 epoch total loss 1.07164037\n",
      "Trained batch 642 batch loss 1.26153779 epoch total loss 1.07193625\n",
      "Trained batch 643 batch loss 1.11919355 epoch total loss 1.07200968\n",
      "Trained batch 644 batch loss 1.07317233 epoch total loss 1.07201159\n",
      "Trained batch 645 batch loss 1.19327497 epoch total loss 1.07219958\n",
      "Trained batch 646 batch loss 1.19641531 epoch total loss 1.07239187\n",
      "Trained batch 647 batch loss 1.19192469 epoch total loss 1.07257652\n",
      "Trained batch 648 batch loss 1.20358455 epoch total loss 1.07277882\n",
      "Trained batch 649 batch loss 1.22011936 epoch total loss 1.0730058\n",
      "Trained batch 650 batch loss 1.09749496 epoch total loss 1.07304335\n",
      "Trained batch 651 batch loss 1.08577716 epoch total loss 1.0730629\n",
      "Trained batch 652 batch loss 0.958524287 epoch total loss 1.07288718\n",
      "Trained batch 653 batch loss 0.992400765 epoch total loss 1.07276392\n",
      "Trained batch 654 batch loss 1.05387068 epoch total loss 1.07273507\n",
      "Trained batch 655 batch loss 1.02523863 epoch total loss 1.07266259\n",
      "Trained batch 656 batch loss 1.09285522 epoch total loss 1.07269335\n",
      "Trained batch 657 batch loss 1.18622589 epoch total loss 1.07286608\n",
      "Trained batch 658 batch loss 1.20246947 epoch total loss 1.07306302\n",
      "Trained batch 659 batch loss 1.29603851 epoch total loss 1.07340145\n",
      "Trained batch 660 batch loss 1.29814649 epoch total loss 1.07374191\n",
      "Trained batch 661 batch loss 1.2022481 epoch total loss 1.07393634\n",
      "Trained batch 662 batch loss 1.04173231 epoch total loss 1.07388771\n",
      "Trained batch 663 batch loss 1.05410099 epoch total loss 1.0738579\n",
      "Trained batch 664 batch loss 1.28190398 epoch total loss 1.07417119\n",
      "Trained batch 665 batch loss 1.16749191 epoch total loss 1.07431149\n",
      "Trained batch 666 batch loss 1.00123823 epoch total loss 1.07420182\n",
      "Trained batch 667 batch loss 0.998782516 epoch total loss 1.07408869\n",
      "Trained batch 668 batch loss 1.02472341 epoch total loss 1.07401478\n",
      "Trained batch 669 batch loss 1.01288569 epoch total loss 1.07392347\n",
      "Trained batch 670 batch loss 1.02793717 epoch total loss 1.0738548\n",
      "Trained batch 671 batch loss 1.08510399 epoch total loss 1.07387149\n",
      "Trained batch 672 batch loss 1.06172347 epoch total loss 1.07385349\n",
      "Trained batch 673 batch loss 1.01253867 epoch total loss 1.0737623\n",
      "Trained batch 674 batch loss 1.08219814 epoch total loss 1.07377481\n",
      "Trained batch 675 batch loss 1.02007616 epoch total loss 1.0736953\n",
      "Trained batch 676 batch loss 1.129848 epoch total loss 1.07377827\n",
      "Trained batch 677 batch loss 1.00610495 epoch total loss 1.07367837\n",
      "Trained batch 678 batch loss 1.0755161 epoch total loss 1.073681\n",
      "Trained batch 679 batch loss 1.02544951 epoch total loss 1.07361007\n",
      "Trained batch 680 batch loss 1.13212073 epoch total loss 1.07369614\n",
      "Trained batch 681 batch loss 1.05844676 epoch total loss 1.07367373\n",
      "Trained batch 682 batch loss 1.13217652 epoch total loss 1.07375956\n",
      "Trained batch 683 batch loss 1.09032965 epoch total loss 1.07378387\n",
      "Trained batch 684 batch loss 0.948671937 epoch total loss 1.07360089\n",
      "Trained batch 685 batch loss 0.957802057 epoch total loss 1.07343185\n",
      "Trained batch 686 batch loss 1.09367847 epoch total loss 1.07346141\n",
      "Trained batch 687 batch loss 1.01303697 epoch total loss 1.07337356\n",
      "Trained batch 688 batch loss 1.10946512 epoch total loss 1.07342589\n",
      "Trained batch 689 batch loss 1.06556952 epoch total loss 1.07341444\n",
      "Trained batch 690 batch loss 1.07899356 epoch total loss 1.07342255\n",
      "Trained batch 691 batch loss 0.915718615 epoch total loss 1.07319427\n",
      "Trained batch 692 batch loss 0.935146213 epoch total loss 1.07299483\n",
      "Trained batch 693 batch loss 1.07083607 epoch total loss 1.07299173\n",
      "Trained batch 694 batch loss 0.921386659 epoch total loss 1.07277322\n",
      "Trained batch 695 batch loss 1.16099882 epoch total loss 1.07290018\n",
      "Trained batch 696 batch loss 1.12767494 epoch total loss 1.07297897\n",
      "Trained batch 697 batch loss 0.96398747 epoch total loss 1.07282257\n",
      "Trained batch 698 batch loss 0.944130123 epoch total loss 1.07263827\n",
      "Trained batch 699 batch loss 0.916668534 epoch total loss 1.07241511\n",
      "Trained batch 700 batch loss 0.949846148 epoch total loss 1.07224\n",
      "Trained batch 701 batch loss 0.852749 epoch total loss 1.07192683\n",
      "Trained batch 702 batch loss 0.907163 epoch total loss 1.07169211\n",
      "Trained batch 703 batch loss 1.01853251 epoch total loss 1.07161653\n",
      "Trained batch 704 batch loss 1.05833197 epoch total loss 1.0715977\n",
      "Trained batch 705 batch loss 0.899036169 epoch total loss 1.07135296\n",
      "Trained batch 706 batch loss 0.931862235 epoch total loss 1.07115543\n",
      "Trained batch 707 batch loss 1.02085459 epoch total loss 1.07108426\n",
      "Trained batch 708 batch loss 1.0817976 epoch total loss 1.0710994\n",
      "Trained batch 709 batch loss 1.06351066 epoch total loss 1.07108879\n",
      "Trained batch 710 batch loss 1.22727096 epoch total loss 1.07130873\n",
      "Trained batch 711 batch loss 1.03404474 epoch total loss 1.0712564\n",
      "Trained batch 712 batch loss 1.12791705 epoch total loss 1.07133591\n",
      "Trained batch 713 batch loss 0.959614873 epoch total loss 1.07117927\n",
      "Trained batch 714 batch loss 1.02539301 epoch total loss 1.07111514\n",
      "Trained batch 715 batch loss 0.956865668 epoch total loss 1.07095528\n",
      "Trained batch 716 batch loss 1.01180577 epoch total loss 1.07087266\n",
      "Trained batch 717 batch loss 0.873920083 epoch total loss 1.07059789\n",
      "Trained batch 718 batch loss 1.02096426 epoch total loss 1.07052875\n",
      "Trained batch 719 batch loss 1.20059896 epoch total loss 1.07070971\n",
      "Trained batch 720 batch loss 1.04811358 epoch total loss 1.07067823\n",
      "Trained batch 721 batch loss 1.10226679 epoch total loss 1.0707221\n",
      "Trained batch 722 batch loss 1.0829047 epoch total loss 1.07073903\n",
      "Trained batch 723 batch loss 0.90306741 epoch total loss 1.07050705\n",
      "Trained batch 724 batch loss 1.05573046 epoch total loss 1.07048666\n",
      "Trained batch 725 batch loss 1.20524788 epoch total loss 1.07067251\n",
      "Trained batch 726 batch loss 1.00021231 epoch total loss 1.07057548\n",
      "Trained batch 727 batch loss 0.973773718 epoch total loss 1.07044232\n",
      "Trained batch 728 batch loss 0.929347515 epoch total loss 1.07024848\n",
      "Trained batch 729 batch loss 1.01284146 epoch total loss 1.07016969\n",
      "Trained batch 730 batch loss 1.14658546 epoch total loss 1.07027435\n",
      "Trained batch 731 batch loss 1.05557036 epoch total loss 1.07025421\n",
      "Trained batch 732 batch loss 1.01879358 epoch total loss 1.07018387\n",
      "Trained batch 733 batch loss 0.940003157 epoch total loss 1.07000637\n",
      "Trained batch 734 batch loss 0.862587214 epoch total loss 1.06972373\n",
      "Trained batch 735 batch loss 0.899527371 epoch total loss 1.06949222\n",
      "Trained batch 736 batch loss 0.902450562 epoch total loss 1.06926525\n",
      "Trained batch 737 batch loss 0.851355374 epoch total loss 1.06896961\n",
      "Trained batch 738 batch loss 1.18459642 epoch total loss 1.06912625\n",
      "Trained batch 739 batch loss 1.0947752 epoch total loss 1.06916106\n",
      "Trained batch 740 batch loss 1.07869649 epoch total loss 1.06917381\n",
      "Trained batch 741 batch loss 0.886951685 epoch total loss 1.068928\n",
      "Trained batch 742 batch loss 0.927232087 epoch total loss 1.06873703\n",
      "Trained batch 743 batch loss 0.991729319 epoch total loss 1.06863332\n",
      "Trained batch 744 batch loss 0.918572426 epoch total loss 1.06843162\n",
      "Trained batch 745 batch loss 0.989735246 epoch total loss 1.068326\n",
      "Trained batch 746 batch loss 1.0468837 epoch total loss 1.06829727\n",
      "Trained batch 747 batch loss 1.09417427 epoch total loss 1.06833196\n",
      "Trained batch 748 batch loss 1.0152384 epoch total loss 1.06826103\n",
      "Trained batch 749 batch loss 1.06997991 epoch total loss 1.06826329\n",
      "Trained batch 750 batch loss 1.12933671 epoch total loss 1.06834471\n",
      "Trained batch 751 batch loss 1.2970196 epoch total loss 1.06864917\n",
      "Trained batch 752 batch loss 1.32371902 epoch total loss 1.06898844\n",
      "Trained batch 753 batch loss 1.26431775 epoch total loss 1.06924784\n",
      "Trained batch 754 batch loss 1.11552095 epoch total loss 1.06930923\n",
      "Trained batch 755 batch loss 0.939906418 epoch total loss 1.06913781\n",
      "Trained batch 756 batch loss 1.15465403 epoch total loss 1.06925094\n",
      "Trained batch 757 batch loss 1.18254757 epoch total loss 1.06940055\n",
      "Trained batch 758 batch loss 1.12419593 epoch total loss 1.06947291\n",
      "Trained batch 759 batch loss 1.23360765 epoch total loss 1.06968915\n",
      "Trained batch 760 batch loss 1.21366358 epoch total loss 1.06987858\n",
      "Trained batch 761 batch loss 1.33955407 epoch total loss 1.07023299\n",
      "Trained batch 762 batch loss 1.27362299 epoch total loss 1.0704999\n",
      "Trained batch 763 batch loss 1.39264035 epoch total loss 1.07092202\n",
      "Trained batch 764 batch loss 1.28906751 epoch total loss 1.07120752\n",
      "Trained batch 765 batch loss 1.14840221 epoch total loss 1.07130849\n",
      "Trained batch 766 batch loss 1.14275241 epoch total loss 1.07140172\n",
      "Trained batch 767 batch loss 1.16186714 epoch total loss 1.07151973\n",
      "Trained batch 768 batch loss 1.22598362 epoch total loss 1.07172084\n",
      "Trained batch 769 batch loss 1.19311547 epoch total loss 1.07187867\n",
      "Trained batch 770 batch loss 1.16103244 epoch total loss 1.07199442\n",
      "Trained batch 771 batch loss 1.08352876 epoch total loss 1.07200944\n",
      "Trained batch 772 batch loss 1.09999597 epoch total loss 1.07204568\n",
      "Trained batch 773 batch loss 1.04700947 epoch total loss 1.07201326\n",
      "Trained batch 774 batch loss 0.974828959 epoch total loss 1.07188773\n",
      "Trained batch 775 batch loss 0.968593597 epoch total loss 1.07175446\n",
      "Trained batch 776 batch loss 0.89378047 epoch total loss 1.0715251\n",
      "Trained batch 777 batch loss 1.0702157 epoch total loss 1.07152343\n",
      "Trained batch 778 batch loss 1.15257776 epoch total loss 1.07162762\n",
      "Trained batch 779 batch loss 1.25583577 epoch total loss 1.07186413\n",
      "Trained batch 780 batch loss 1.19109869 epoch total loss 1.07201695\n",
      "Trained batch 781 batch loss 1.05236268 epoch total loss 1.0719918\n",
      "Trained batch 782 batch loss 1.18661356 epoch total loss 1.07213831\n",
      "Trained batch 783 batch loss 1.15513718 epoch total loss 1.07224429\n",
      "Trained batch 784 batch loss 1.17628312 epoch total loss 1.07237709\n",
      "Trained batch 785 batch loss 1.1833694 epoch total loss 1.07251835\n",
      "Trained batch 786 batch loss 1.1367296 epoch total loss 1.07260013\n",
      "Trained batch 787 batch loss 1.21489882 epoch total loss 1.07278085\n",
      "Trained batch 788 batch loss 1.1445117 epoch total loss 1.07287192\n",
      "Trained batch 789 batch loss 1.26735473 epoch total loss 1.07311845\n",
      "Trained batch 790 batch loss 1.29914105 epoch total loss 1.07340455\n",
      "Trained batch 791 batch loss 1.22027409 epoch total loss 1.07359016\n",
      "Trained batch 792 batch loss 1.07339454 epoch total loss 1.07358992\n",
      "Trained batch 793 batch loss 1.14017141 epoch total loss 1.07367384\n",
      "Trained batch 794 batch loss 1.15079379 epoch total loss 1.073771\n",
      "Trained batch 795 batch loss 1.11269224 epoch total loss 1.07382\n",
      "Trained batch 796 batch loss 1.11851454 epoch total loss 1.07387614\n",
      "Trained batch 797 batch loss 1.12270617 epoch total loss 1.07393742\n",
      "Trained batch 798 batch loss 1.13037968 epoch total loss 1.07400811\n",
      "Trained batch 799 batch loss 1.13553035 epoch total loss 1.07408512\n",
      "Trained batch 800 batch loss 1.21457076 epoch total loss 1.07426083\n",
      "Trained batch 801 batch loss 1.04538751 epoch total loss 1.07422471\n",
      "Trained batch 802 batch loss 1.1227107 epoch total loss 1.07428515\n",
      "Trained batch 803 batch loss 1.06540465 epoch total loss 1.07427418\n",
      "Trained batch 804 batch loss 1.1331358 epoch total loss 1.07434738\n",
      "Trained batch 805 batch loss 1.25190759 epoch total loss 1.07456791\n",
      "Trained batch 806 batch loss 1.23633385 epoch total loss 1.07476854\n",
      "Trained batch 807 batch loss 1.16648388 epoch total loss 1.07488227\n",
      "Trained batch 808 batch loss 1.14142489 epoch total loss 1.07496464\n",
      "Trained batch 809 batch loss 1.07832801 epoch total loss 1.0749687\n",
      "Trained batch 810 batch loss 1.07736146 epoch total loss 1.07497168\n",
      "Trained batch 811 batch loss 0.979992747 epoch total loss 1.07485449\n",
      "Trained batch 812 batch loss 1.05116773 epoch total loss 1.07482529\n",
      "Trained batch 813 batch loss 1.13668108 epoch total loss 1.07490134\n",
      "Trained batch 814 batch loss 0.976295471 epoch total loss 1.07478034\n",
      "Trained batch 815 batch loss 0.959456444 epoch total loss 1.07463884\n",
      "Trained batch 816 batch loss 0.903452158 epoch total loss 1.07442904\n",
      "Trained batch 817 batch loss 0.857983 epoch total loss 1.07416403\n",
      "Trained batch 818 batch loss 1.10068583 epoch total loss 1.07419646\n",
      "Trained batch 819 batch loss 1.30345464 epoch total loss 1.07447648\n",
      "Trained batch 820 batch loss 1.26626539 epoch total loss 1.07471025\n",
      "Trained batch 821 batch loss 1.28130698 epoch total loss 1.0749619\n",
      "Trained batch 822 batch loss 1.15412831 epoch total loss 1.07505822\n",
      "Trained batch 823 batch loss 1.29865599 epoch total loss 1.0753299\n",
      "Trained batch 824 batch loss 1.08897793 epoch total loss 1.07534647\n",
      "Trained batch 825 batch loss 1.26727843 epoch total loss 1.07557917\n",
      "Trained batch 826 batch loss 1.13475394 epoch total loss 1.07565081\n",
      "Trained batch 827 batch loss 1.13825619 epoch total loss 1.07572651\n",
      "Trained batch 828 batch loss 1.21200669 epoch total loss 1.07589114\n",
      "Trained batch 829 batch loss 1.02664888 epoch total loss 1.07583177\n",
      "Trained batch 830 batch loss 0.963833451 epoch total loss 1.07569671\n",
      "Trained batch 831 batch loss 0.976556838 epoch total loss 1.0755775\n",
      "Trained batch 832 batch loss 1.06966245 epoch total loss 1.07557034\n",
      "Trained batch 833 batch loss 1.16951632 epoch total loss 1.07568312\n",
      "Trained batch 834 batch loss 1.24942565 epoch total loss 1.07589149\n",
      "Trained batch 835 batch loss 1.11144161 epoch total loss 1.07593405\n",
      "Trained batch 836 batch loss 1.25759411 epoch total loss 1.07615125\n",
      "Trained batch 837 batch loss 1.11717725 epoch total loss 1.07620037\n",
      "Trained batch 838 batch loss 1.1510241 epoch total loss 1.07628953\n",
      "Trained batch 839 batch loss 1.1714108 epoch total loss 1.0764029\n",
      "Trained batch 840 batch loss 1.16450787 epoch total loss 1.07650781\n",
      "Trained batch 841 batch loss 1.18834448 epoch total loss 1.07664073\n",
      "Trained batch 842 batch loss 1.18123853 epoch total loss 1.07676494\n",
      "Trained batch 843 batch loss 1.1894809 epoch total loss 1.07689869\n",
      "Trained batch 844 batch loss 1.16565144 epoch total loss 1.07700384\n",
      "Trained batch 845 batch loss 1.2011776 epoch total loss 1.0771507\n",
      "Trained batch 846 batch loss 1.05378008 epoch total loss 1.07712317\n",
      "Trained batch 847 batch loss 1.0047096 epoch total loss 1.07703757\n",
      "Trained batch 848 batch loss 1.14081538 epoch total loss 1.07711279\n",
      "Trained batch 849 batch loss 1.15271163 epoch total loss 1.07720184\n",
      "Trained batch 850 batch loss 1.1062113 epoch total loss 1.07723594\n",
      "Trained batch 851 batch loss 1.21262085 epoch total loss 1.07739508\n",
      "Trained batch 852 batch loss 1.06998551 epoch total loss 1.07738638\n",
      "Trained batch 853 batch loss 1.11323667 epoch total loss 1.07742846\n",
      "Trained batch 854 batch loss 1.14210415 epoch total loss 1.07750416\n",
      "Trained batch 855 batch loss 1.18117452 epoch total loss 1.07762539\n",
      "Trained batch 856 batch loss 1.1587131 epoch total loss 1.07772\n",
      "Trained batch 857 batch loss 1.15129399 epoch total loss 1.077806\n",
      "Trained batch 858 batch loss 1.10506773 epoch total loss 1.07783771\n",
      "Trained batch 859 batch loss 1.14749956 epoch total loss 1.07791877\n",
      "Trained batch 860 batch loss 1.13906384 epoch total loss 1.07798982\n",
      "Trained batch 861 batch loss 1.08256388 epoch total loss 1.07799518\n",
      "Trained batch 862 batch loss 1.17020154 epoch total loss 1.07810223\n",
      "Trained batch 863 batch loss 1.07465446 epoch total loss 1.07809818\n",
      "Trained batch 864 batch loss 1.11415923 epoch total loss 1.0781399\n",
      "Trained batch 865 batch loss 1.02201033 epoch total loss 1.07807505\n",
      "Trained batch 866 batch loss 1.0416218 epoch total loss 1.07803297\n",
      "Trained batch 867 batch loss 0.950446606 epoch total loss 1.07788575\n",
      "Trained batch 868 batch loss 1.08117115 epoch total loss 1.07788956\n",
      "Trained batch 869 batch loss 1.05796552 epoch total loss 1.07786667\n",
      "Trained batch 870 batch loss 0.893590093 epoch total loss 1.07765484\n",
      "Trained batch 871 batch loss 0.961017787 epoch total loss 1.07752097\n",
      "Trained batch 872 batch loss 1.14284599 epoch total loss 1.07759583\n",
      "Trained batch 873 batch loss 1.13118291 epoch total loss 1.07765722\n",
      "Trained batch 874 batch loss 1.06155443 epoch total loss 1.07763886\n",
      "Trained batch 875 batch loss 1.06624091 epoch total loss 1.07762575\n",
      "Trained batch 876 batch loss 1.08366549 epoch total loss 1.07763267\n",
      "Trained batch 877 batch loss 1.17135644 epoch total loss 1.0777396\n",
      "Trained batch 878 batch loss 1.18313146 epoch total loss 1.07785964\n",
      "Trained batch 879 batch loss 0.994654179 epoch total loss 1.07776487\n",
      "Trained batch 880 batch loss 1.08835971 epoch total loss 1.07777691\n",
      "Trained batch 881 batch loss 0.926547527 epoch total loss 1.07760537\n",
      "Trained batch 882 batch loss 1.0495708 epoch total loss 1.07757354\n",
      "Trained batch 883 batch loss 0.890255868 epoch total loss 1.07736135\n",
      "Trained batch 884 batch loss 0.954491436 epoch total loss 1.07722235\n",
      "Trained batch 885 batch loss 1.03184676 epoch total loss 1.07717109\n",
      "Trained batch 886 batch loss 1.05261219 epoch total loss 1.07714343\n",
      "Trained batch 887 batch loss 0.92563808 epoch total loss 1.0769726\n",
      "Trained batch 888 batch loss 0.932346463 epoch total loss 1.07680976\n",
      "Trained batch 889 batch loss 0.98573482 epoch total loss 1.07670736\n",
      "Trained batch 890 batch loss 1.06546783 epoch total loss 1.07669473\n",
      "Trained batch 891 batch loss 1.06349099 epoch total loss 1.07667983\n",
      "Trained batch 892 batch loss 1.0310421 epoch total loss 1.0766288\n",
      "Trained batch 893 batch loss 0.892046571 epoch total loss 1.07642198\n",
      "Trained batch 894 batch loss 0.872316182 epoch total loss 1.07619369\n",
      "Trained batch 895 batch loss 0.982150316 epoch total loss 1.07608867\n",
      "Trained batch 896 batch loss 1.01034594 epoch total loss 1.07601535\n",
      "Trained batch 897 batch loss 1.18876934 epoch total loss 1.076141\n",
      "Trained batch 898 batch loss 1.21539569 epoch total loss 1.07629609\n",
      "Trained batch 899 batch loss 1.33935034 epoch total loss 1.07658875\n",
      "Trained batch 900 batch loss 1.18839478 epoch total loss 1.07671297\n",
      "Trained batch 901 batch loss 1.19997358 epoch total loss 1.07684982\n",
      "Trained batch 902 batch loss 1.1782279 epoch total loss 1.07696211\n",
      "Trained batch 903 batch loss 1.03579247 epoch total loss 1.07691658\n",
      "Trained batch 904 batch loss 0.998832464 epoch total loss 1.07683015\n",
      "Trained batch 905 batch loss 1.00573754 epoch total loss 1.07675159\n",
      "Trained batch 906 batch loss 1.08451986 epoch total loss 1.07676017\n",
      "Trained batch 907 batch loss 1.07281816 epoch total loss 1.07675588\n",
      "Trained batch 908 batch loss 1.10501623 epoch total loss 1.07678699\n",
      "Trained batch 909 batch loss 1.14121759 epoch total loss 1.07685792\n",
      "Trained batch 910 batch loss 1.13953304 epoch total loss 1.07692671\n",
      "Trained batch 911 batch loss 1.16730142 epoch total loss 1.07702601\n",
      "Trained batch 912 batch loss 1.1851809 epoch total loss 1.0771445\n",
      "Trained batch 913 batch loss 1.07425034 epoch total loss 1.0771414\n",
      "Trained batch 914 batch loss 1.10491967 epoch total loss 1.0771718\n",
      "Trained batch 915 batch loss 1.20994925 epoch total loss 1.07731688\n",
      "Trained batch 916 batch loss 0.983961 epoch total loss 1.07721496\n",
      "Trained batch 917 batch loss 0.955942631 epoch total loss 1.07708275\n",
      "Trained batch 918 batch loss 1.08170927 epoch total loss 1.07708776\n",
      "Trained batch 919 batch loss 1.18256783 epoch total loss 1.07720256\n",
      "Trained batch 920 batch loss 1.02232838 epoch total loss 1.07714295\n",
      "Trained batch 921 batch loss 1.09939265 epoch total loss 1.07716703\n",
      "Trained batch 922 batch loss 1.21766293 epoch total loss 1.07731938\n",
      "Trained batch 923 batch loss 1.11087835 epoch total loss 1.07735586\n",
      "Trained batch 924 batch loss 1.011078 epoch total loss 1.0772841\n",
      "Trained batch 925 batch loss 1.04271913 epoch total loss 1.07724679\n",
      "Trained batch 926 batch loss 1.10583937 epoch total loss 1.07727766\n",
      "Trained batch 927 batch loss 0.9628 epoch total loss 1.07715416\n",
      "Trained batch 928 batch loss 1.13347769 epoch total loss 1.07721484\n",
      "Trained batch 929 batch loss 1.15175426 epoch total loss 1.07729506\n",
      "Trained batch 930 batch loss 0.936712325 epoch total loss 1.07714391\n",
      "Trained batch 931 batch loss 1.03098309 epoch total loss 1.07709432\n",
      "Trained batch 932 batch loss 1.24397278 epoch total loss 1.07727337\n",
      "Trained batch 933 batch loss 1.24610806 epoch total loss 1.07745433\n",
      "Trained batch 934 batch loss 1.13018465 epoch total loss 1.07751083\n",
      "Trained batch 935 batch loss 1.17846155 epoch total loss 1.07761872\n",
      "Trained batch 936 batch loss 1.13277245 epoch total loss 1.07767773\n",
      "Trained batch 937 batch loss 1.11467648 epoch total loss 1.07771719\n",
      "Trained batch 938 batch loss 1.23682344 epoch total loss 1.07788682\n",
      "Trained batch 939 batch loss 1.03165102 epoch total loss 1.07783759\n",
      "Trained batch 940 batch loss 1.0863843 epoch total loss 1.07784665\n",
      "Trained batch 941 batch loss 0.954743505 epoch total loss 1.07771587\n",
      "Trained batch 942 batch loss 0.945390224 epoch total loss 1.07757533\n",
      "Trained batch 943 batch loss 0.901075244 epoch total loss 1.07738817\n",
      "Trained batch 944 batch loss 1.01119828 epoch total loss 1.07731807\n",
      "Trained batch 945 batch loss 0.940954 epoch total loss 1.07717371\n",
      "Trained batch 946 batch loss 0.793161809 epoch total loss 1.07687354\n",
      "Trained batch 947 batch loss 0.87488395 epoch total loss 1.07666028\n",
      "Trained batch 948 batch loss 0.780786633 epoch total loss 1.07634807\n",
      "Trained batch 949 batch loss 0.930247664 epoch total loss 1.07619417\n",
      "Trained batch 950 batch loss 0.979556441 epoch total loss 1.07609236\n",
      "Trained batch 951 batch loss 1.01208472 epoch total loss 1.07602513\n",
      "Trained batch 952 batch loss 0.981734872 epoch total loss 1.07592607\n",
      "Trained batch 953 batch loss 0.999182522 epoch total loss 1.07584548\n",
      "Trained batch 954 batch loss 1.1374954 epoch total loss 1.07591009\n",
      "Trained batch 955 batch loss 1.10683584 epoch total loss 1.0759424\n",
      "Trained batch 956 batch loss 1.0317 epoch total loss 1.07589614\n",
      "Trained batch 957 batch loss 1.02785957 epoch total loss 1.07584596\n",
      "Trained batch 958 batch loss 1.218804 epoch total loss 1.07599509\n",
      "Trained batch 959 batch loss 1.14406538 epoch total loss 1.07606614\n",
      "Trained batch 960 batch loss 1.2632587 epoch total loss 1.07626116\n",
      "Trained batch 961 batch loss 1.21991479 epoch total loss 1.07641065\n",
      "Trained batch 962 batch loss 1.07312334 epoch total loss 1.07640731\n",
      "Trained batch 963 batch loss 1.01670694 epoch total loss 1.07634532\n",
      "Trained batch 964 batch loss 1.15246701 epoch total loss 1.07642424\n",
      "Trained batch 965 batch loss 1.0420959 epoch total loss 1.07638872\n",
      "Trained batch 966 batch loss 1.16656554 epoch total loss 1.07648206\n",
      "Trained batch 967 batch loss 1.30865431 epoch total loss 1.07672215\n",
      "Trained batch 968 batch loss 1.29466784 epoch total loss 1.07694733\n",
      "Trained batch 969 batch loss 1.11885464 epoch total loss 1.0769906\n",
      "Trained batch 970 batch loss 1.1501112 epoch total loss 1.07706606\n",
      "Trained batch 971 batch loss 0.953271747 epoch total loss 1.07693851\n",
      "Trained batch 972 batch loss 1.16585195 epoch total loss 1.07703006\n",
      "Trained batch 973 batch loss 1.24679637 epoch total loss 1.07720447\n",
      "Trained batch 974 batch loss 1.10838437 epoch total loss 1.07723653\n",
      "Trained batch 975 batch loss 1.00460875 epoch total loss 1.07716203\n",
      "Trained batch 976 batch loss 0.935194969 epoch total loss 1.07701659\n",
      "Trained batch 977 batch loss 0.931228817 epoch total loss 1.07686746\n",
      "Trained batch 978 batch loss 1.11120844 epoch total loss 1.07690251\n",
      "Trained batch 979 batch loss 0.919240236 epoch total loss 1.07674146\n",
      "Trained batch 980 batch loss 0.815627515 epoch total loss 1.07647502\n",
      "Trained batch 981 batch loss 0.90039432 epoch total loss 1.07629561\n",
      "Trained batch 982 batch loss 0.967498779 epoch total loss 1.07618475\n",
      "Trained batch 983 batch loss 1.08100581 epoch total loss 1.07618976\n",
      "Trained batch 984 batch loss 1.02962 epoch total loss 1.07614243\n",
      "Trained batch 985 batch loss 1.07221496 epoch total loss 1.0761385\n",
      "Trained batch 986 batch loss 1.17135561 epoch total loss 1.07623518\n",
      "Trained batch 987 batch loss 1.19704199 epoch total loss 1.07635748\n",
      "Trained batch 988 batch loss 1.24167395 epoch total loss 1.07652485\n",
      "Trained batch 989 batch loss 1.17572403 epoch total loss 1.07662523\n",
      "Trained batch 990 batch loss 1.02434802 epoch total loss 1.07657242\n",
      "Trained batch 991 batch loss 1.09804702 epoch total loss 1.076594\n",
      "Trained batch 992 batch loss 1.04892683 epoch total loss 1.0765661\n",
      "Trained batch 993 batch loss 1.24294209 epoch total loss 1.07673371\n",
      "Trained batch 994 batch loss 1.12234437 epoch total loss 1.07677948\n",
      "Trained batch 995 batch loss 1.22151864 epoch total loss 1.07692504\n",
      "Trained batch 996 batch loss 1.15603089 epoch total loss 1.07700443\n",
      "Trained batch 997 batch loss 1.04490221 epoch total loss 1.07697225\n",
      "Trained batch 998 batch loss 1.12022984 epoch total loss 1.07701564\n",
      "Trained batch 999 batch loss 1.05472684 epoch total loss 1.07699323\n",
      "Trained batch 1000 batch loss 1.09404492 epoch total loss 1.07701027\n",
      "Trained batch 1001 batch loss 1.11645412 epoch total loss 1.07704961\n",
      "Trained batch 1002 batch loss 1.0647887 epoch total loss 1.07703745\n",
      "Trained batch 1003 batch loss 1.04280901 epoch total loss 1.07700336\n",
      "Trained batch 1004 batch loss 1.03477168 epoch total loss 1.07696128\n",
      "Trained batch 1005 batch loss 1.01201046 epoch total loss 1.07689667\n",
      "Trained batch 1006 batch loss 0.994558811 epoch total loss 1.07681477\n",
      "Trained batch 1007 batch loss 0.852111 epoch total loss 1.07659149\n",
      "Trained batch 1008 batch loss 0.927262068 epoch total loss 1.07644343\n",
      "Trained batch 1009 batch loss 0.902771711 epoch total loss 1.0762713\n",
      "Trained batch 1010 batch loss 1.06308842 epoch total loss 1.0762583\n",
      "Trained batch 1011 batch loss 0.942675889 epoch total loss 1.0761261\n",
      "Trained batch 1012 batch loss 0.892592728 epoch total loss 1.07594478\n",
      "Trained batch 1013 batch loss 1.00534666 epoch total loss 1.07587504\n",
      "Trained batch 1014 batch loss 1.00948679 epoch total loss 1.0758096\n",
      "Trained batch 1015 batch loss 1.09829628 epoch total loss 1.07583177\n",
      "Trained batch 1016 batch loss 1.14010906 epoch total loss 1.07589507\n",
      "Trained batch 1017 batch loss 1.08208013 epoch total loss 1.07590103\n",
      "Trained batch 1018 batch loss 1.26119816 epoch total loss 1.07608318\n",
      "Trained batch 1019 batch loss 1.12150669 epoch total loss 1.07612765\n",
      "Trained batch 1020 batch loss 1.01511598 epoch total loss 1.07606792\n",
      "Trained batch 1021 batch loss 1.00002694 epoch total loss 1.07599342\n",
      "Trained batch 1022 batch loss 1.00008082 epoch total loss 1.07591915\n",
      "Trained batch 1023 batch loss 1.01852953 epoch total loss 1.075863\n",
      "Trained batch 1024 batch loss 1.08663368 epoch total loss 1.07587361\n",
      "Trained batch 1025 batch loss 1.09750414 epoch total loss 1.07589471\n",
      "Trained batch 1026 batch loss 1.1157217 epoch total loss 1.07593358\n",
      "Trained batch 1027 batch loss 1.15861785 epoch total loss 1.07601404\n",
      "Trained batch 1028 batch loss 1.19771111 epoch total loss 1.07613242\n",
      "Trained batch 1029 batch loss 1.01281059 epoch total loss 1.0760709\n",
      "Trained batch 1030 batch loss 1.22442 epoch total loss 1.07621491\n",
      "Trained batch 1031 batch loss 1.17398417 epoch total loss 1.07630968\n",
      "Trained batch 1032 batch loss 0.954818308 epoch total loss 1.07619202\n",
      "Trained batch 1033 batch loss 1.07396328 epoch total loss 1.07618988\n",
      "Trained batch 1034 batch loss 1.10527682 epoch total loss 1.07621789\n",
      "Trained batch 1035 batch loss 1.07804847 epoch total loss 1.07621968\n",
      "Trained batch 1036 batch loss 1.24908113 epoch total loss 1.07638645\n",
      "Trained batch 1037 batch loss 1.2550869 epoch total loss 1.07655883\n",
      "Trained batch 1038 batch loss 1.09982264 epoch total loss 1.07658124\n",
      "Trained batch 1039 batch loss 0.989187837 epoch total loss 1.07649708\n",
      "Trained batch 1040 batch loss 1.08631587 epoch total loss 1.0765065\n",
      "Trained batch 1041 batch loss 1.14008105 epoch total loss 1.07656765\n",
      "Trained batch 1042 batch loss 1.11147022 epoch total loss 1.07660115\n",
      "Trained batch 1043 batch loss 1.1488477 epoch total loss 1.07667029\n",
      "Trained batch 1044 batch loss 1.07065725 epoch total loss 1.07666457\n",
      "Trained batch 1045 batch loss 1.1819725 epoch total loss 1.07676542\n",
      "Trained batch 1046 batch loss 1.09253228 epoch total loss 1.07678044\n",
      "Trained batch 1047 batch loss 1.18317938 epoch total loss 1.07688212\n",
      "Trained batch 1048 batch loss 1.30454123 epoch total loss 1.07709944\n",
      "Trained batch 1049 batch loss 1.10902333 epoch total loss 1.07712984\n",
      "Trained batch 1050 batch loss 1.05095196 epoch total loss 1.07710481\n",
      "Trained batch 1051 batch loss 1.16707253 epoch total loss 1.07719052\n",
      "Trained batch 1052 batch loss 1.06999063 epoch total loss 1.0771836\n",
      "Trained batch 1053 batch loss 1.18546247 epoch total loss 1.07728636\n",
      "Trained batch 1054 batch loss 1.14015448 epoch total loss 1.07734597\n",
      "Trained batch 1055 batch loss 1.06577742 epoch total loss 1.07733512\n",
      "Trained batch 1056 batch loss 1.13778138 epoch total loss 1.07739234\n",
      "Trained batch 1057 batch loss 1.19670546 epoch total loss 1.07750523\n",
      "Trained batch 1058 batch loss 1.1868161 epoch total loss 1.07760847\n",
      "Trained batch 1059 batch loss 1.25077367 epoch total loss 1.0777719\n",
      "Trained batch 1060 batch loss 1.05180788 epoch total loss 1.07774734\n",
      "Trained batch 1061 batch loss 1.21107793 epoch total loss 1.07787299\n",
      "Trained batch 1062 batch loss 1.17184234 epoch total loss 1.07796156\n",
      "Trained batch 1063 batch loss 1.12124753 epoch total loss 1.07800221\n",
      "Trained batch 1064 batch loss 1.04951453 epoch total loss 1.07797551\n",
      "Trained batch 1065 batch loss 1.05067837 epoch total loss 1.07794988\n",
      "Trained batch 1066 batch loss 1.00681 epoch total loss 1.07788312\n",
      "Trained batch 1067 batch loss 1.13383389 epoch total loss 1.07793558\n",
      "Trained batch 1068 batch loss 0.986546576 epoch total loss 1.07785\n",
      "Trained batch 1069 batch loss 1.0966177 epoch total loss 1.07786751\n",
      "Trained batch 1070 batch loss 1.09072161 epoch total loss 1.07787955\n",
      "Trained batch 1071 batch loss 1.02428377 epoch total loss 1.07782948\n",
      "Trained batch 1072 batch loss 0.886011362 epoch total loss 1.07765055\n",
      "Trained batch 1073 batch loss 0.785497427 epoch total loss 1.07737827\n",
      "Trained batch 1074 batch loss 1.05152416 epoch total loss 1.07735419\n",
      "Trained batch 1075 batch loss 1.12478185 epoch total loss 1.0773983\n",
      "Trained batch 1076 batch loss 1.00456703 epoch total loss 1.07733047\n",
      "Trained batch 1077 batch loss 1.07304406 epoch total loss 1.07732654\n",
      "Trained batch 1078 batch loss 1.04138267 epoch total loss 1.07729316\n",
      "Trained batch 1079 batch loss 1.01579261 epoch total loss 1.07723606\n",
      "Trained batch 1080 batch loss 0.936385453 epoch total loss 1.07710576\n",
      "Trained batch 1081 batch loss 0.965919733 epoch total loss 1.07700288\n",
      "Trained batch 1082 batch loss 1.02440226 epoch total loss 1.07695425\n",
      "Trained batch 1083 batch loss 1.17655981 epoch total loss 1.07704616\n",
      "Trained batch 1084 batch loss 1.05314541 epoch total loss 1.0770241\n",
      "Trained batch 1085 batch loss 1.07425022 epoch total loss 1.07702148\n",
      "Trained batch 1086 batch loss 1.18656576 epoch total loss 1.07712233\n",
      "Trained batch 1087 batch loss 1.14494395 epoch total loss 1.07718468\n",
      "Trained batch 1088 batch loss 1.15917432 epoch total loss 1.07726\n",
      "Trained batch 1089 batch loss 1.09222412 epoch total loss 1.07727373\n",
      "Trained batch 1090 batch loss 1.21320486 epoch total loss 1.07739854\n",
      "Trained batch 1091 batch loss 1.1060605 epoch total loss 1.07742476\n",
      "Trained batch 1092 batch loss 1.13845098 epoch total loss 1.07748067\n",
      "Trained batch 1093 batch loss 1.0247879 epoch total loss 1.07743239\n",
      "Trained batch 1094 batch loss 1.06916773 epoch total loss 1.07742488\n",
      "Trained batch 1095 batch loss 0.943138957 epoch total loss 1.07730234\n",
      "Trained batch 1096 batch loss 0.99730587 epoch total loss 1.07722926\n",
      "Trained batch 1097 batch loss 1.11394143 epoch total loss 1.07726276\n",
      "Trained batch 1098 batch loss 1.01506102 epoch total loss 1.07720602\n",
      "Trained batch 1099 batch loss 0.942696571 epoch total loss 1.07708371\n",
      "Trained batch 1100 batch loss 0.951475322 epoch total loss 1.07696939\n",
      "Trained batch 1101 batch loss 0.975168765 epoch total loss 1.076877\n",
      "Trained batch 1102 batch loss 1.00652599 epoch total loss 1.0768131\n",
      "Trained batch 1103 batch loss 1.07053566 epoch total loss 1.0768075\n",
      "Trained batch 1104 batch loss 1.11474049 epoch total loss 1.07684183\n",
      "Trained batch 1105 batch loss 1.07784057 epoch total loss 1.07684278\n",
      "Trained batch 1106 batch loss 1.06949878 epoch total loss 1.07683611\n",
      "Trained batch 1107 batch loss 1.1092447 epoch total loss 1.07686532\n",
      "Trained batch 1108 batch loss 1.03546524 epoch total loss 1.076828\n",
      "Trained batch 1109 batch loss 1.02049923 epoch total loss 1.07677722\n",
      "Trained batch 1110 batch loss 1.0197258 epoch total loss 1.07672596\n",
      "Trained batch 1111 batch loss 0.950768828 epoch total loss 1.07661259\n",
      "Trained batch 1112 batch loss 1.16174352 epoch total loss 1.07668912\n",
      "Trained batch 1113 batch loss 1.10211802 epoch total loss 1.07671201\n",
      "Trained batch 1114 batch loss 1.03796434 epoch total loss 1.0766772\n",
      "Trained batch 1115 batch loss 1.29014802 epoch total loss 1.07686877\n",
      "Trained batch 1116 batch loss 1.2401495 epoch total loss 1.07701504\n",
      "Trained batch 1117 batch loss 1.08955097 epoch total loss 1.07702625\n",
      "Trained batch 1118 batch loss 1.09794474 epoch total loss 1.07704496\n",
      "Trained batch 1119 batch loss 0.970382929 epoch total loss 1.0769496\n",
      "Trained batch 1120 batch loss 1.15144 epoch total loss 1.07701612\n",
      "Trained batch 1121 batch loss 0.960075319 epoch total loss 1.07691181\n",
      "Trained batch 1122 batch loss 0.886307418 epoch total loss 1.07674193\n",
      "Trained batch 1123 batch loss 0.88443166 epoch total loss 1.07657075\n",
      "Trained batch 1124 batch loss 0.986107826 epoch total loss 1.07649016\n",
      "Trained batch 1125 batch loss 1.16372609 epoch total loss 1.07656765\n",
      "Trained batch 1126 batch loss 1.25108433 epoch total loss 1.07672274\n",
      "Trained batch 1127 batch loss 1.22988653 epoch total loss 1.07685864\n",
      "Trained batch 1128 batch loss 1.3169651 epoch total loss 1.07707155\n",
      "Trained batch 1129 batch loss 1.19659638 epoch total loss 1.07717741\n",
      "Trained batch 1130 batch loss 1.12944674 epoch total loss 1.07722366\n",
      "Trained batch 1131 batch loss 1.18749571 epoch total loss 1.07732117\n",
      "Trained batch 1132 batch loss 1.18191385 epoch total loss 1.07741344\n",
      "Trained batch 1133 batch loss 1.22487795 epoch total loss 1.07754362\n",
      "Trained batch 1134 batch loss 1.11118281 epoch total loss 1.0775733\n",
      "Trained batch 1135 batch loss 1.05157602 epoch total loss 1.07755041\n",
      "Trained batch 1136 batch loss 1.11756372 epoch total loss 1.0775857\n",
      "Trained batch 1137 batch loss 1.02610016 epoch total loss 1.0775404\n",
      "Trained batch 1138 batch loss 1.09753549 epoch total loss 1.07755804\n",
      "Trained batch 1139 batch loss 1.21409523 epoch total loss 1.07767785\n",
      "Trained batch 1140 batch loss 1.12749517 epoch total loss 1.07772148\n",
      "Trained batch 1141 batch loss 1.12325776 epoch total loss 1.07776141\n",
      "Trained batch 1142 batch loss 1.12054396 epoch total loss 1.07779884\n",
      "Trained batch 1143 batch loss 1.16874504 epoch total loss 1.07787836\n",
      "Trained batch 1144 batch loss 0.994892478 epoch total loss 1.07780588\n",
      "Trained batch 1145 batch loss 1.03792918 epoch total loss 1.07777107\n",
      "Trained batch 1146 batch loss 0.899216294 epoch total loss 1.07761526\n",
      "Trained batch 1147 batch loss 1.13389242 epoch total loss 1.07766426\n",
      "Trained batch 1148 batch loss 1.11687303 epoch total loss 1.07769835\n",
      "Trained batch 1149 batch loss 0.994554162 epoch total loss 1.07762599\n",
      "Trained batch 1150 batch loss 1.23036349 epoch total loss 1.07775879\n",
      "Trained batch 1151 batch loss 1.08248329 epoch total loss 1.07776296\n",
      "Trained batch 1152 batch loss 1.15389526 epoch total loss 1.077829\n",
      "Trained batch 1153 batch loss 1.13992333 epoch total loss 1.07788289\n",
      "Trained batch 1154 batch loss 1.05904341 epoch total loss 1.07786655\n",
      "Trained batch 1155 batch loss 1.1060605 epoch total loss 1.07789099\n",
      "Trained batch 1156 batch loss 1.12751353 epoch total loss 1.07793403\n",
      "Trained batch 1157 batch loss 1.10484934 epoch total loss 1.07795727\n",
      "Trained batch 1158 batch loss 1.13651192 epoch total loss 1.07800782\n",
      "Trained batch 1159 batch loss 1.2366339 epoch total loss 1.07814467\n",
      "Trained batch 1160 batch loss 1.20378625 epoch total loss 1.07825291\n",
      "Trained batch 1161 batch loss 1.17860842 epoch total loss 1.07833934\n",
      "Trained batch 1162 batch loss 1.08108282 epoch total loss 1.07834172\n",
      "Trained batch 1163 batch loss 1.06401753 epoch total loss 1.07832932\n",
      "Trained batch 1164 batch loss 1.20796919 epoch total loss 1.07844079\n",
      "Trained batch 1165 batch loss 1.06496692 epoch total loss 1.07842922\n",
      "Trained batch 1166 batch loss 1.13936627 epoch total loss 1.07848144\n",
      "Trained batch 1167 batch loss 1.17188883 epoch total loss 1.07856154\n",
      "Trained batch 1168 batch loss 1.1573112 epoch total loss 1.07862902\n",
      "Trained batch 1169 batch loss 1.11965156 epoch total loss 1.07866406\n",
      "Trained batch 1170 batch loss 1.10954368 epoch total loss 1.07869041\n",
      "Trained batch 1171 batch loss 1.13428605 epoch total loss 1.07873785\n",
      "Trained batch 1172 batch loss 0.870546 epoch total loss 1.07856023\n",
      "Trained batch 1173 batch loss 0.778000295 epoch total loss 1.07830405\n",
      "Trained batch 1174 batch loss 1.03258967 epoch total loss 1.07826507\n",
      "Trained batch 1175 batch loss 1.14724922 epoch total loss 1.07832372\n",
      "Trained batch 1176 batch loss 1.34349477 epoch total loss 1.07854927\n",
      "Trained batch 1177 batch loss 1.28626776 epoch total loss 1.0787257\n",
      "Trained batch 1178 batch loss 1.10994625 epoch total loss 1.07875228\n",
      "Trained batch 1179 batch loss 0.972636342 epoch total loss 1.07866228\n",
      "Trained batch 1180 batch loss 1.09783936 epoch total loss 1.07867849\n",
      "Trained batch 1181 batch loss 1.10140967 epoch total loss 1.07869768\n",
      "Trained batch 1182 batch loss 1.09669232 epoch total loss 1.07871294\n",
      "Trained batch 1183 batch loss 1.15516782 epoch total loss 1.07877755\n",
      "Trained batch 1184 batch loss 1.13135076 epoch total loss 1.0788219\n",
      "Trained batch 1185 batch loss 1.0969975 epoch total loss 1.07883728\n",
      "Trained batch 1186 batch loss 1.11871123 epoch total loss 1.07887089\n",
      "Trained batch 1187 batch loss 1.02545273 epoch total loss 1.07882595\n",
      "Trained batch 1188 batch loss 0.932261884 epoch total loss 1.07870257\n",
      "Trained batch 1189 batch loss 1.04514933 epoch total loss 1.07867432\n",
      "Trained batch 1190 batch loss 1.10056329 epoch total loss 1.07869279\n",
      "Trained batch 1191 batch loss 0.942485213 epoch total loss 1.07857847\n",
      "Trained batch 1192 batch loss 0.934684217 epoch total loss 1.07845771\n",
      "Trained batch 1193 batch loss 0.99728024 epoch total loss 1.07838976\n",
      "Trained batch 1194 batch loss 1.07026863 epoch total loss 1.07838297\n",
      "Trained batch 1195 batch loss 1.14174187 epoch total loss 1.0784359\n",
      "Trained batch 1196 batch loss 1.13282537 epoch total loss 1.07848144\n",
      "Trained batch 1197 batch loss 1.15452 epoch total loss 1.07854497\n",
      "Trained batch 1198 batch loss 1.23708344 epoch total loss 1.0786773\n",
      "Trained batch 1199 batch loss 1.19420147 epoch total loss 1.07877362\n",
      "Trained batch 1200 batch loss 1.06487966 epoch total loss 1.07876205\n",
      "Trained batch 1201 batch loss 1.27678812 epoch total loss 1.0789268\n",
      "Trained batch 1202 batch loss 1.19606435 epoch total loss 1.07902431\n",
      "Trained batch 1203 batch loss 1.15274906 epoch total loss 1.07908559\n",
      "Trained batch 1204 batch loss 1.0743947 epoch total loss 1.07908154\n",
      "Trained batch 1205 batch loss 1.11778593 epoch total loss 1.07911372\n",
      "Trained batch 1206 batch loss 0.968649685 epoch total loss 1.07902205\n",
      "Trained batch 1207 batch loss 1.08142173 epoch total loss 1.07902408\n",
      "Trained batch 1208 batch loss 1.09356618 epoch total loss 1.07903612\n",
      "Trained batch 1209 batch loss 1.12949777 epoch total loss 1.07907784\n",
      "Trained batch 1210 batch loss 1.04807353 epoch total loss 1.07905221\n",
      "Trained batch 1211 batch loss 1.04284096 epoch total loss 1.07902229\n",
      "Trained batch 1212 batch loss 1.11897814 epoch total loss 1.07905531\n",
      "Trained batch 1213 batch loss 1.07818627 epoch total loss 1.07905471\n",
      "Trained batch 1214 batch loss 1.0669291 epoch total loss 1.0790447\n",
      "Trained batch 1215 batch loss 0.992112339 epoch total loss 1.07897305\n",
      "Trained batch 1216 batch loss 0.969543 epoch total loss 1.07888305\n",
      "Trained batch 1217 batch loss 1.01423347 epoch total loss 1.07882988\n",
      "Trained batch 1218 batch loss 1.00307941 epoch total loss 1.07876778\n",
      "Trained batch 1219 batch loss 1.08878314 epoch total loss 1.07877588\n",
      "Trained batch 1220 batch loss 1.06527984 epoch total loss 1.07876492\n",
      "Trained batch 1221 batch loss 1.21050072 epoch total loss 1.07887268\n",
      "Trained batch 1222 batch loss 1.2273041 epoch total loss 1.07899415\n",
      "Trained batch 1223 batch loss 1.13987422 epoch total loss 1.07904398\n",
      "Trained batch 1224 batch loss 1.06284738 epoch total loss 1.07903075\n",
      "Trained batch 1225 batch loss 1.06877184 epoch total loss 1.07902229\n",
      "Trained batch 1226 batch loss 1.04240513 epoch total loss 1.07899249\n",
      "Trained batch 1227 batch loss 1.10346 epoch total loss 1.07901239\n",
      "Trained batch 1228 batch loss 1.14824331 epoch total loss 1.07906878\n",
      "Trained batch 1229 batch loss 1.18358123 epoch total loss 1.07915378\n",
      "Trained batch 1230 batch loss 1.06791639 epoch total loss 1.0791446\n",
      "Trained batch 1231 batch loss 1.13485146 epoch total loss 1.0791899\n",
      "Trained batch 1232 batch loss 1.20683277 epoch total loss 1.07929349\n",
      "Trained batch 1233 batch loss 1.23197734 epoch total loss 1.07941723\n",
      "Trained batch 1234 batch loss 1.11605573 epoch total loss 1.07944703\n",
      "Trained batch 1235 batch loss 1.00147378 epoch total loss 1.07938385\n",
      "Trained batch 1236 batch loss 1.1375643 epoch total loss 1.07943094\n",
      "Trained batch 1237 batch loss 1.13164079 epoch total loss 1.07947314\n",
      "Trained batch 1238 batch loss 1.09625721 epoch total loss 1.07948673\n",
      "Trained batch 1239 batch loss 1.12064981 epoch total loss 1.07951987\n",
      "Trained batch 1240 batch loss 1.0457294 epoch total loss 1.07949269\n",
      "Trained batch 1241 batch loss 0.945298553 epoch total loss 1.07938457\n",
      "Trained batch 1242 batch loss 1.09766221 epoch total loss 1.07939923\n",
      "Trained batch 1243 batch loss 0.979537964 epoch total loss 1.07931888\n",
      "Trained batch 1244 batch loss 1.07555771 epoch total loss 1.0793159\n",
      "Trained batch 1245 batch loss 1.0563767 epoch total loss 1.07929742\n",
      "Trained batch 1246 batch loss 0.987971842 epoch total loss 1.07922411\n",
      "Trained batch 1247 batch loss 1.07157731 epoch total loss 1.07921791\n",
      "Trained batch 1248 batch loss 0.991605759 epoch total loss 1.0791477\n",
      "Trained batch 1249 batch loss 1.01393425 epoch total loss 1.07909548\n",
      "Trained batch 1250 batch loss 1.01358831 epoch total loss 1.07904303\n",
      "Trained batch 1251 batch loss 1.02479506 epoch total loss 1.07899964\n",
      "Trained batch 1252 batch loss 1.05957317 epoch total loss 1.07898414\n",
      "Trained batch 1253 batch loss 0.996256828 epoch total loss 1.0789181\n",
      "Trained batch 1254 batch loss 1.05636907 epoch total loss 1.0789001\n",
      "Trained batch 1255 batch loss 1.01901805 epoch total loss 1.07885242\n",
      "Trained batch 1256 batch loss 1.08080888 epoch total loss 1.07885396\n",
      "Trained batch 1257 batch loss 1.15724373 epoch total loss 1.07891631\n",
      "Trained batch 1258 batch loss 1.10327399 epoch total loss 1.07893574\n",
      "Trained batch 1259 batch loss 0.997146368 epoch total loss 1.07887077\n",
      "Trained batch 1260 batch loss 1.06491649 epoch total loss 1.07885981\n",
      "Trained batch 1261 batch loss 1.04188085 epoch total loss 1.07883036\n",
      "Trained batch 1262 batch loss 1.00097263 epoch total loss 1.07876873\n",
      "Trained batch 1263 batch loss 1.00581253 epoch total loss 1.07871103\n",
      "Trained batch 1264 batch loss 1.10895288 epoch total loss 1.07873499\n",
      "Trained batch 1265 batch loss 0.936326504 epoch total loss 1.07862234\n",
      "Trained batch 1266 batch loss 0.972190201 epoch total loss 1.0785383\n",
      "Trained batch 1267 batch loss 1.03308964 epoch total loss 1.07850242\n",
      "Trained batch 1268 batch loss 0.966091275 epoch total loss 1.07841372\n",
      "Trained batch 1269 batch loss 1.00698566 epoch total loss 1.07835746\n",
      "Trained batch 1270 batch loss 0.979001105 epoch total loss 1.07827914\n",
      "Trained batch 1271 batch loss 1.13749051 epoch total loss 1.07832575\n",
      "Trained batch 1272 batch loss 1.05004883 epoch total loss 1.07830346\n",
      "Trained batch 1273 batch loss 1.08733797 epoch total loss 1.07831061\n",
      "Trained batch 1274 batch loss 0.971301675 epoch total loss 1.07822657\n",
      "Trained batch 1275 batch loss 1.15250635 epoch total loss 1.07828474\n",
      "Trained batch 1276 batch loss 1.11865473 epoch total loss 1.07831645\n",
      "Trained batch 1277 batch loss 1.07036376 epoch total loss 1.07831013\n",
      "Trained batch 1278 batch loss 1.18404937 epoch total loss 1.07839298\n",
      "Trained batch 1279 batch loss 1.12926638 epoch total loss 1.07843268\n",
      "Trained batch 1280 batch loss 1.28872299 epoch total loss 1.07859695\n",
      "Trained batch 1281 batch loss 1.41351628 epoch total loss 1.07885849\n",
      "Trained batch 1282 batch loss 1.28493607 epoch total loss 1.07901919\n",
      "Trained batch 1283 batch loss 1.13177848 epoch total loss 1.07906032\n",
      "Trained batch 1284 batch loss 0.981806159 epoch total loss 1.07898462\n",
      "Trained batch 1285 batch loss 1.0874294 epoch total loss 1.07899117\n",
      "Trained batch 1286 batch loss 1.16595793 epoch total loss 1.07905889\n",
      "Trained batch 1287 batch loss 1.10900497 epoch total loss 1.07908213\n",
      "Trained batch 1288 batch loss 1.13468719 epoch total loss 1.07912529\n",
      "Trained batch 1289 batch loss 1.20478976 epoch total loss 1.0792228\n",
      "Trained batch 1290 batch loss 1.14310622 epoch total loss 1.07927227\n",
      "Trained batch 1291 batch loss 1.35208559 epoch total loss 1.07948351\n",
      "Trained batch 1292 batch loss 1.29457855 epoch total loss 1.07965\n",
      "Trained batch 1293 batch loss 1.12873 epoch total loss 1.07968807\n",
      "Trained batch 1294 batch loss 1.17599416 epoch total loss 1.07976246\n",
      "Trained batch 1295 batch loss 1.04503155 epoch total loss 1.07973564\n",
      "Trained batch 1296 batch loss 1.17128396 epoch total loss 1.07980633\n",
      "Trained batch 1297 batch loss 1.08264327 epoch total loss 1.07980847\n",
      "Trained batch 1298 batch loss 1.02113986 epoch total loss 1.07976329\n",
      "Trained batch 1299 batch loss 1.0777005 epoch total loss 1.07976174\n",
      "Trained batch 1300 batch loss 1.07535625 epoch total loss 1.07975829\n",
      "Trained batch 1301 batch loss 1.03134131 epoch total loss 1.07972109\n",
      "Trained batch 1302 batch loss 1.19457483 epoch total loss 1.07980931\n",
      "Trained batch 1303 batch loss 0.976505518 epoch total loss 1.07973\n",
      "Trained batch 1304 batch loss 1.09738624 epoch total loss 1.07974362\n",
      "Trained batch 1305 batch loss 1.18465471 epoch total loss 1.07982409\n",
      "Trained batch 1306 batch loss 1.10777092 epoch total loss 1.07984543\n",
      "Trained batch 1307 batch loss 0.998446941 epoch total loss 1.0797832\n",
      "Trained batch 1308 batch loss 1.12457395 epoch total loss 1.07981741\n",
      "Trained batch 1309 batch loss 1.27889061 epoch total loss 1.07996953\n",
      "Trained batch 1310 batch loss 1.26538467 epoch total loss 1.08011115\n",
      "Trained batch 1311 batch loss 0.900533617 epoch total loss 1.07997417\n",
      "Trained batch 1312 batch loss 0.911337137 epoch total loss 1.07984567\n",
      "Trained batch 1313 batch loss 0.972697794 epoch total loss 1.07976401\n",
      "Trained batch 1314 batch loss 0.858272731 epoch total loss 1.07959545\n",
      "Trained batch 1315 batch loss 0.929149091 epoch total loss 1.07948101\n",
      "Trained batch 1316 batch loss 0.977627575 epoch total loss 1.07940364\n",
      "Trained batch 1317 batch loss 0.991860271 epoch total loss 1.07933712\n",
      "Trained batch 1318 batch loss 0.9925161 epoch total loss 1.07927132\n",
      "Trained batch 1319 batch loss 0.989134431 epoch total loss 1.07920301\n",
      "Trained batch 1320 batch loss 1.04385257 epoch total loss 1.07917619\n",
      "Trained batch 1321 batch loss 1.11240733 epoch total loss 1.07920134\n",
      "Trained batch 1322 batch loss 1.103544 epoch total loss 1.0792197\n",
      "Trained batch 1323 batch loss 0.919086754 epoch total loss 1.0790987\n",
      "Trained batch 1324 batch loss 0.928027809 epoch total loss 1.07898462\n",
      "Trained batch 1325 batch loss 0.828348517 epoch total loss 1.07879543\n",
      "Trained batch 1326 batch loss 0.966418326 epoch total loss 1.07871068\n",
      "Trained batch 1327 batch loss 1.0554409 epoch total loss 1.07869315\n",
      "Trained batch 1328 batch loss 1.00276911 epoch total loss 1.07863593\n",
      "Trained batch 1329 batch loss 0.869172096 epoch total loss 1.07847834\n",
      "Trained batch 1330 batch loss 0.920634449 epoch total loss 1.07835972\n",
      "Trained batch 1331 batch loss 1.13561833 epoch total loss 1.07840276\n",
      "Trained batch 1332 batch loss 1.04712987 epoch total loss 1.07837927\n",
      "Trained batch 1333 batch loss 0.920516491 epoch total loss 1.07826078\n",
      "Trained batch 1334 batch loss 1.10079837 epoch total loss 1.07827771\n",
      "Trained batch 1335 batch loss 0.891079128 epoch total loss 1.07813752\n",
      "Trained batch 1336 batch loss 0.944809079 epoch total loss 1.07803774\n",
      "Trained batch 1337 batch loss 1.09257364 epoch total loss 1.07804859\n",
      "Trained batch 1338 batch loss 1.06786633 epoch total loss 1.07804096\n",
      "Trained batch 1339 batch loss 1.12458479 epoch total loss 1.07807577\n",
      "Trained batch 1340 batch loss 1.16519904 epoch total loss 1.07814074\n",
      "Trained batch 1341 batch loss 1.30689895 epoch total loss 1.07831132\n",
      "Trained batch 1342 batch loss 1.08307838 epoch total loss 1.0783149\n",
      "Trained batch 1343 batch loss 1.06677735 epoch total loss 1.07830632\n",
      "Trained batch 1344 batch loss 1.11647642 epoch total loss 1.07833469\n",
      "Trained batch 1345 batch loss 1.03108454 epoch total loss 1.07829964\n",
      "Trained batch 1346 batch loss 1.06264687 epoch total loss 1.07828796\n",
      "Trained batch 1347 batch loss 1.19412243 epoch total loss 1.07837391\n",
      "Trained batch 1348 batch loss 1.01086175 epoch total loss 1.07832384\n",
      "Trained batch 1349 batch loss 0.950169146 epoch total loss 1.07822883\n",
      "Trained batch 1350 batch loss 1.07344258 epoch total loss 1.07822537\n",
      "Trained batch 1351 batch loss 1.12441695 epoch total loss 1.07825959\n",
      "Trained batch 1352 batch loss 1.19377971 epoch total loss 1.07834494\n",
      "Trained batch 1353 batch loss 1.1147511 epoch total loss 1.07837188\n",
      "Trained batch 1354 batch loss 1.04143095 epoch total loss 1.07834458\n",
      "Trained batch 1355 batch loss 1.01976705 epoch total loss 1.07830131\n",
      "Trained batch 1356 batch loss 1.10251701 epoch total loss 1.07831919\n",
      "Trained batch 1357 batch loss 1.19459772 epoch total loss 1.0784049\n",
      "Trained batch 1358 batch loss 1.20272505 epoch total loss 1.07849646\n",
      "Trained batch 1359 batch loss 1.10925746 epoch total loss 1.07851911\n",
      "Trained batch 1360 batch loss 1.15899527 epoch total loss 1.07857823\n",
      "Trained batch 1361 batch loss 1.10014391 epoch total loss 1.07859397\n",
      "Trained batch 1362 batch loss 1.01542664 epoch total loss 1.0785476\n",
      "Trained batch 1363 batch loss 0.966807663 epoch total loss 1.07846558\n",
      "Trained batch 1364 batch loss 1.17459548 epoch total loss 1.07853603\n",
      "Trained batch 1365 batch loss 1.11919951 epoch total loss 1.07856584\n",
      "Trained batch 1366 batch loss 1.16885448 epoch total loss 1.07863188\n",
      "Trained batch 1367 batch loss 1.15445316 epoch total loss 1.07868731\n",
      "Trained batch 1368 batch loss 1.030509 epoch total loss 1.07865214\n",
      "Trained batch 1369 batch loss 1.06521797 epoch total loss 1.07864225\n",
      "Trained batch 1370 batch loss 0.88830781 epoch total loss 1.07850337\n",
      "Trained batch 1371 batch loss 0.970614314 epoch total loss 1.07842457\n",
      "Trained batch 1372 batch loss 1.03198886 epoch total loss 1.07839072\n",
      "Trained batch 1373 batch loss 1.29093683 epoch total loss 1.07854557\n",
      "Trained batch 1374 batch loss 1.07121229 epoch total loss 1.07854021\n",
      "Trained batch 1375 batch loss 1.09836555 epoch total loss 1.07855463\n",
      "Trained batch 1376 batch loss 1.00082731 epoch total loss 1.07849813\n",
      "Trained batch 1377 batch loss 1.04497099 epoch total loss 1.07847381\n",
      "Trained batch 1378 batch loss 1.08203 epoch total loss 1.07847631\n",
      "Trained batch 1379 batch loss 0.974153578 epoch total loss 1.07840061\n",
      "Trained batch 1380 batch loss 0.977272034 epoch total loss 1.07832742\n",
      "Trained batch 1381 batch loss 1.04201198 epoch total loss 1.07830107\n",
      "Trained batch 1382 batch loss 1.18456721 epoch total loss 1.07837796\n",
      "Trained batch 1383 batch loss 1.11121547 epoch total loss 1.07840168\n",
      "Trained batch 1384 batch loss 1.03255463 epoch total loss 1.07836866\n",
      "Trained batch 1385 batch loss 1.05655527 epoch total loss 1.07835281\n",
      "Trained batch 1386 batch loss 1.07437193 epoch total loss 1.07835\n",
      "Trained batch 1387 batch loss 1.08896136 epoch total loss 1.07835758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 04:33:34.858441: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "2025-03-19 04:33:34.858496: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "\t [[RemoteCall]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1388 batch loss 1.04018533 epoch total loss 1.07833\n",
      "Epoch 9 train loss 1.0783300399780273\n",
      "Validated batch 1 batch loss 1.12294376\n",
      "Validated batch 2 batch loss 1.08580923\n",
      "Validated batch 3 batch loss 1.03557944\n",
      "Validated batch 4 batch loss 1.10466051\n",
      "Validated batch 5 batch loss 1.0886929\n",
      "Validated batch 6 batch loss 1.15687644\n",
      "Validated batch 7 batch loss 1.19389415\n",
      "Validated batch 8 batch loss 1.17893481\n",
      "Validated batch 9 batch loss 1.14929378\n",
      "Validated batch 10 batch loss 1.06181228\n",
      "Validated batch 11 batch loss 1.15932584\n",
      "Validated batch 12 batch loss 1.10165691\n",
      "Validated batch 13 batch loss 1.13084245\n",
      "Validated batch 14 batch loss 1.25417125\n",
      "Validated batch 15 batch loss 1.2229948\n",
      "Validated batch 16 batch loss 1.13242972\n",
      "Validated batch 17 batch loss 1.29281068\n",
      "Validated batch 18 batch loss 1.03705561\n",
      "Validated batch 19 batch loss 1.21278524\n",
      "Validated batch 20 batch loss 0.877263427\n",
      "Validated batch 21 batch loss 1.11743855\n",
      "Validated batch 22 batch loss 1.17185771\n",
      "Validated batch 23 batch loss 0.99912262\n",
      "Validated batch 24 batch loss 1.05950975\n",
      "Validated batch 25 batch loss 1.00099504\n",
      "Validated batch 26 batch loss 1.08703792\n",
      "Validated batch 27 batch loss 1.0627327\n",
      "Validated batch 28 batch loss 1.0886445\n",
      "Validated batch 29 batch loss 1.15419197\n",
      "Validated batch 30 batch loss 1.16507912\n",
      "Validated batch 31 batch loss 1.06813765\n",
      "Validated batch 32 batch loss 1.14901304\n",
      "Validated batch 33 batch loss 1.1057179\n",
      "Validated batch 34 batch loss 1.11369836\n",
      "Validated batch 35 batch loss 1.13050258\n",
      "Validated batch 36 batch loss 1.06802058\n",
      "Validated batch 37 batch loss 1.09668553\n",
      "Validated batch 38 batch loss 1.10327172\n",
      "Validated batch 39 batch loss 1.0785532\n",
      "Validated batch 40 batch loss 1.25463593\n",
      "Validated batch 41 batch loss 1.19803143\n",
      "Validated batch 42 batch loss 0.979155779\n",
      "Validated batch 43 batch loss 1.21563983\n",
      "Validated batch 44 batch loss 1.06405222\n",
      "Validated batch 45 batch loss 0.978334486\n",
      "Validated batch 46 batch loss 1.1147052\n",
      "Validated batch 47 batch loss 1.00833988\n",
      "Validated batch 48 batch loss 1.09147787\n",
      "Validated batch 49 batch loss 1.15281868\n",
      "Validated batch 50 batch loss 1.06675184\n",
      "Validated batch 51 batch loss 1.20597816\n",
      "Validated batch 52 batch loss 1.28759468\n",
      "Validated batch 53 batch loss 0.96237421\n",
      "Validated batch 54 batch loss 1.11230469\n",
      "Validated batch 55 batch loss 1.12992883\n",
      "Validated batch 56 batch loss 1.19651675\n",
      "Validated batch 57 batch loss 1.17297554\n",
      "Validated batch 58 batch loss 0.990833759\n",
      "Validated batch 59 batch loss 0.973493755\n",
      "Validated batch 60 batch loss 1.0993433\n",
      "Validated batch 61 batch loss 1.06752181\n",
      "Validated batch 62 batch loss 1.09293365\n",
      "Validated batch 63 batch loss 1.16135645\n",
      "Validated batch 64 batch loss 1.00294757\n",
      "Validated batch 65 batch loss 1.16067231\n",
      "Validated batch 66 batch loss 1.1549747\n",
      "Validated batch 67 batch loss 1.18831372\n",
      "Validated batch 68 batch loss 1.12878883\n",
      "Validated batch 69 batch loss 0.999206245\n",
      "Validated batch 70 batch loss 1.04539835\n",
      "Validated batch 71 batch loss 1.12745559\n",
      "Validated batch 72 batch loss 1.10573518\n",
      "Validated batch 73 batch loss 1.06858253\n",
      "Validated batch 74 batch loss 1.09857047\n",
      "Validated batch 75 batch loss 1.19417548\n",
      "Validated batch 76 batch loss 0.994690955\n",
      "Validated batch 77 batch loss 0.947852731\n",
      "Validated batch 78 batch loss 1.02945983\n",
      "Validated batch 79 batch loss 1.06344903\n",
      "Validated batch 80 batch loss 0.968964458\n",
      "Validated batch 81 batch loss 1.11199212\n",
      "Validated batch 82 batch loss 1.08757389\n",
      "Validated batch 83 batch loss 1.02529025\n",
      "Validated batch 84 batch loss 1.21590567\n",
      "Validated batch 85 batch loss 1.16170573\n",
      "Validated batch 86 batch loss 1.0325948\n",
      "Validated batch 87 batch loss 1.20397055\n",
      "Validated batch 88 batch loss 0.899353087\n",
      "Validated batch 89 batch loss 1.04812598\n",
      "Validated batch 90 batch loss 1.03101671\n",
      "Validated batch 91 batch loss 1.08516312\n",
      "Validated batch 92 batch loss 1.30175614\n",
      "Validated batch 93 batch loss 1.2197938\n",
      "Validated batch 94 batch loss 1.13527226\n",
      "Validated batch 95 batch loss 1.03763795\n",
      "Validated batch 96 batch loss 1.12521076\n",
      "Validated batch 97 batch loss 1.00669241\n",
      "Validated batch 98 batch loss 1.10550702\n",
      "Validated batch 99 batch loss 1.14329755\n",
      "Validated batch 100 batch loss 1.06399417\n",
      "Validated batch 101 batch loss 1.10877478\n",
      "Validated batch 102 batch loss 1.11570823\n",
      "Validated batch 103 batch loss 1.1776495\n",
      "Validated batch 104 batch loss 1.22680438\n",
      "Validated batch 105 batch loss 1.11867166\n",
      "Validated batch 106 batch loss 1.07018661\n",
      "Validated batch 107 batch loss 1.04362345\n",
      "Validated batch 108 batch loss 1.16309655\n",
      "Validated batch 109 batch loss 1.07658339\n",
      "Validated batch 110 batch loss 1.17471695\n",
      "Validated batch 111 batch loss 1.14826655\n",
      "Validated batch 112 batch loss 1.29806566\n",
      "Validated batch 113 batch loss 1.21435237\n",
      "Validated batch 114 batch loss 1.13395214\n",
      "Validated batch 115 batch loss 1.0447886\n",
      "Validated batch 116 batch loss 1.05745828\n",
      "Validated batch 117 batch loss 1.08117664\n",
      "Validated batch 118 batch loss 1.07892048\n",
      "Validated batch 119 batch loss 1.07154179\n",
      "Validated batch 120 batch loss 1.09141457\n",
      "Validated batch 121 batch loss 1.12317479\n",
      "Validated batch 122 batch loss 1.20530534\n",
      "Validated batch 123 batch loss 1.09059179\n",
      "Validated batch 124 batch loss 1.06641948\n",
      "Validated batch 125 batch loss 1.26932859\n",
      "Validated batch 126 batch loss 1.03086734\n",
      "Validated batch 127 batch loss 1.02020109\n",
      "Validated batch 128 batch loss 1.12353599\n",
      "Validated batch 129 batch loss 1.23984909\n",
      "Validated batch 130 batch loss 1.21708345\n",
      "Validated batch 131 batch loss 1.24237633\n",
      "Validated batch 132 batch loss 1.10118043\n",
      "Validated batch 133 batch loss 1.28001332\n",
      "Validated batch 134 batch loss 1.14197159\n",
      "Validated batch 135 batch loss 1.22866631\n",
      "Validated batch 136 batch loss 1.18497193\n",
      "Validated batch 137 batch loss 0.866691172\n",
      "Validated batch 138 batch loss 1.01425767\n",
      "Validated batch 139 batch loss 1.0885942\n",
      "Validated batch 140 batch loss 1.10658526\n",
      "Validated batch 141 batch loss 1.06367648\n",
      "Validated batch 142 batch loss 1.09064853\n",
      "Validated batch 143 batch loss 1.03641772\n",
      "Validated batch 144 batch loss 1.21462226\n",
      "Validated batch 145 batch loss 1.13035583\n",
      "Validated batch 146 batch loss 1.20205581\n",
      "Validated batch 147 batch loss 1.17065752\n",
      "Validated batch 148 batch loss 1.14827764\n",
      "Validated batch 149 batch loss 1.10185349\n",
      "Validated batch 150 batch loss 1.30945015\n",
      "Validated batch 151 batch loss 1.16461492\n",
      "Validated batch 152 batch loss 1.22653115\n",
      "Validated batch 153 batch loss 1.20129168\n",
      "Validated batch 154 batch loss 1.31010067\n",
      "Validated batch 155 batch loss 1.22367144\n",
      "Validated batch 156 batch loss 1.11009026\n",
      "Validated batch 157 batch loss 1.15100253\n",
      "Validated batch 158 batch loss 1.24017644\n",
      "Validated batch 159 batch loss 1.20740473\n",
      "Validated batch 160 batch loss 1.09078634\n",
      "Validated batch 161 batch loss 1.14137399\n",
      "Validated batch 162 batch loss 1.11779332\n",
      "Validated batch 163 batch loss 1.11283946\n",
      "Validated batch 164 batch loss 1.12718904\n",
      "Validated batch 165 batch loss 1.10483778\n",
      "Validated batch 166 batch loss 1.22303212\n",
      "Validated batch 167 batch loss 1.35406303\n",
      "Validated batch 168 batch loss 1.06518865\n",
      "Validated batch 169 batch loss 1.15983582\n",
      "Validated batch 170 batch loss 1.02852869\n",
      "Validated batch 171 batch loss 1.20830369\n",
      "Validated batch 172 batch loss 1.16810918\n",
      "Validated batch 173 batch loss 1.16646397\n",
      "Validated batch 174 batch loss 1.16861403\n",
      "Validated batch 175 batch loss 1.20847988\n",
      "Validated batch 176 batch loss 1.15479493\n",
      "Validated batch 177 batch loss 1.20936501\n",
      "Validated batch 178 batch loss 1.23107672\n",
      "Validated batch 179 batch loss 1.1314826\n",
      "Validated batch 180 batch loss 1.08107114\n",
      "Validated batch 181 batch loss 1.2088728\n",
      "Validated batch 182 batch loss 1.09722018\n",
      "Validated batch 183 batch loss 1.11831605\n",
      "Validated batch 184 batch loss 1.11687279\n",
      "Validated batch 185 batch loss 1.25354326\n",
      "Epoch 9 val loss 1.1232788562774658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 04:33:50.793555: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "2025-03-19 04:33:50.793594: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "\t [[RemoteCall]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model /home/minho/Desktop/aiffel/pose_estimation/models/model-epoch-9-loss-1.1233.weights.h5 saved.\n",
      "Start epoch 10 with learning rate 0.001\n",
      "Start distributed training...\n",
      "Trained batch 1 batch loss 1.36785746 epoch total loss 1.36785746\n",
      "Trained batch 2 batch loss 1.17255259 epoch total loss 1.27020502\n",
      "Trained batch 3 batch loss 0.985364556 epoch total loss 1.17525816\n",
      "Trained batch 4 batch loss 1.01287079 epoch total loss 1.13466132\n",
      "Trained batch 5 batch loss 1.13659453 epoch total loss 1.13504791\n",
      "Trained batch 6 batch loss 1.19417191 epoch total loss 1.14490187\n",
      "Trained batch 7 batch loss 1.19865716 epoch total loss 1.15258121\n",
      "Trained batch 8 batch loss 1.18063414 epoch total loss 1.15608788\n",
      "Trained batch 9 batch loss 1.17877483 epoch total loss 1.15860868\n",
      "Trained batch 10 batch loss 1.19036579 epoch total loss 1.16178441\n",
      "Trained batch 11 batch loss 1.00641572 epoch total loss 1.1476599\n",
      "Trained batch 12 batch loss 1.07254696 epoch total loss 1.14140046\n",
      "Trained batch 13 batch loss 1.19442487 epoch total loss 1.14547932\n",
      "Trained batch 14 batch loss 1.10248911 epoch total loss 1.14240861\n",
      "Trained batch 15 batch loss 1.04490376 epoch total loss 1.13590825\n",
      "Trained batch 16 batch loss 1.13298094 epoch total loss 1.13572526\n",
      "Trained batch 17 batch loss 1.07645106 epoch total loss 1.13223851\n",
      "Trained batch 18 batch loss 1.10139918 epoch total loss 1.13052511\n",
      "Trained batch 19 batch loss 1.11473942 epoch total loss 1.12969434\n",
      "Trained batch 20 batch loss 1.21595955 epoch total loss 1.13400757\n",
      "Trained batch 21 batch loss 1.11026824 epoch total loss 1.13287711\n",
      "Trained batch 22 batch loss 1.19798446 epoch total loss 1.13583648\n",
      "Trained batch 23 batch loss 1.10975909 epoch total loss 1.13470268\n",
      "Trained batch 24 batch loss 1.20889068 epoch total loss 1.1377939\n",
      "Trained batch 25 batch loss 1.17052805 epoch total loss 1.13910329\n",
      "Trained batch 26 batch loss 1.05220723 epoch total loss 1.13576114\n",
      "Trained batch 27 batch loss 1.18736732 epoch total loss 1.13767242\n",
      "Trained batch 28 batch loss 1.12679815 epoch total loss 1.13728404\n",
      "Trained batch 29 batch loss 1.08082533 epoch total loss 1.13533711\n",
      "Trained batch 30 batch loss 0.975876 epoch total loss 1.13002181\n",
      "Trained batch 31 batch loss 1.13590622 epoch total loss 1.13021159\n",
      "Trained batch 32 batch loss 1.18645597 epoch total loss 1.13196921\n",
      "Trained batch 33 batch loss 1.142308 epoch total loss 1.1322825\n",
      "Trained batch 34 batch loss 1.06068444 epoch total loss 1.13017666\n",
      "Trained batch 35 batch loss 1.13692939 epoch total loss 1.13036954\n",
      "Trained batch 36 batch loss 1.00802362 epoch total loss 1.12697101\n",
      "Trained batch 37 batch loss 0.975160241 epoch total loss 1.12286794\n",
      "Trained batch 38 batch loss 1.1277194 epoch total loss 1.12299573\n",
      "Trained batch 39 batch loss 1.19944 epoch total loss 1.12495577\n",
      "Trained batch 40 batch loss 1.16561866 epoch total loss 1.12597239\n",
      "Trained batch 41 batch loss 1.06035423 epoch total loss 1.12437201\n",
      "Trained batch 42 batch loss 0.987349212 epoch total loss 1.1211096\n",
      "Trained batch 43 batch loss 0.902446926 epoch total loss 1.11602437\n",
      "Trained batch 44 batch loss 1.07215345 epoch total loss 1.11502731\n",
      "Trained batch 45 batch loss 1.11185598 epoch total loss 1.11495686\n",
      "Trained batch 46 batch loss 0.99063015 epoch total loss 1.11225414\n",
      "Trained batch 47 batch loss 0.856707752 epoch total loss 1.10681701\n",
      "Trained batch 48 batch loss 0.995572925 epoch total loss 1.10449934\n",
      "Trained batch 49 batch loss 0.829862356 epoch total loss 1.09889448\n",
      "Trained batch 50 batch loss 0.894783795 epoch total loss 1.09481227\n",
      "Trained batch 51 batch loss 0.923127174 epoch total loss 1.0914458\n",
      "Trained batch 52 batch loss 1.07739913 epoch total loss 1.09117579\n",
      "Trained batch 53 batch loss 0.969809413 epoch total loss 1.08888578\n",
      "Trained batch 54 batch loss 1.05861151 epoch total loss 1.08832526\n",
      "Trained batch 55 batch loss 1.06835055 epoch total loss 1.08796203\n",
      "Trained batch 56 batch loss 1.08370721 epoch total loss 1.0878861\n",
      "Trained batch 57 batch loss 1.32022691 epoch total loss 1.09196222\n",
      "Trained batch 58 batch loss 1.27505767 epoch total loss 1.09511912\n",
      "Trained batch 59 batch loss 1.22553813 epoch total loss 1.09732962\n",
      "Trained batch 60 batch loss 1.14646864 epoch total loss 1.09814858\n",
      "Trained batch 61 batch loss 0.972501218 epoch total loss 1.09608889\n",
      "Trained batch 62 batch loss 1.00229442 epoch total loss 1.09457612\n",
      "Trained batch 63 batch loss 1.23221207 epoch total loss 1.09676087\n",
      "Trained batch 64 batch loss 1.15485013 epoch total loss 1.09766853\n",
      "Trained batch 65 batch loss 1.1046288 epoch total loss 1.09777558\n",
      "Trained batch 66 batch loss 1.27965486 epoch total loss 1.10053134\n",
      "Trained batch 67 batch loss 1.07833695 epoch total loss 1.10020018\n",
      "Trained batch 68 batch loss 1.15041184 epoch total loss 1.10093856\n",
      "Trained batch 69 batch loss 1.14755726 epoch total loss 1.10161424\n",
      "Trained batch 70 batch loss 1.12894666 epoch total loss 1.10200465\n",
      "Trained batch 71 batch loss 1.07687759 epoch total loss 1.10165083\n",
      "Trained batch 72 batch loss 1.17155421 epoch total loss 1.10262167\n",
      "Trained batch 73 batch loss 1.0621562 epoch total loss 1.10206735\n",
      "Trained batch 74 batch loss 1.21674013 epoch total loss 1.10361707\n",
      "Trained batch 75 batch loss 1.20381117 epoch total loss 1.10495305\n",
      "Trained batch 76 batch loss 1.1966083 epoch total loss 1.10615897\n",
      "Trained batch 77 batch loss 1.09063125 epoch total loss 1.10595727\n",
      "Trained batch 78 batch loss 1.01729262 epoch total loss 1.10482061\n",
      "Trained batch 79 batch loss 1.13605118 epoch total loss 1.10521591\n",
      "Trained batch 80 batch loss 1.06318855 epoch total loss 1.10469055\n",
      "Trained batch 81 batch loss 1.08868527 epoch total loss 1.1044929\n",
      "Trained batch 82 batch loss 1.08403313 epoch total loss 1.1042434\n",
      "Trained batch 83 batch loss 1.11114097 epoch total loss 1.10432649\n",
      "Trained batch 84 batch loss 1.12062681 epoch total loss 1.10452056\n",
      "Trained batch 85 batch loss 1.07636857 epoch total loss 1.1041894\n",
      "Trained batch 86 batch loss 1.08695769 epoch total loss 1.10398901\n",
      "Trained batch 87 batch loss 1.10822 epoch total loss 1.10403764\n",
      "Trained batch 88 batch loss 1.09236813 epoch total loss 1.10390508\n",
      "Trained batch 89 batch loss 1.20924056 epoch total loss 1.10508871\n",
      "Trained batch 90 batch loss 1.09909153 epoch total loss 1.10502195\n",
      "Trained batch 91 batch loss 0.918131471 epoch total loss 1.10296822\n",
      "Trained batch 92 batch loss 1.12088203 epoch total loss 1.10316288\n",
      "Trained batch 93 batch loss 1.0876044 epoch total loss 1.10299563\n",
      "Trained batch 94 batch loss 0.945555568 epoch total loss 1.10132074\n",
      "Trained batch 95 batch loss 0.969672382 epoch total loss 1.09993494\n",
      "Trained batch 96 batch loss 1.01364303 epoch total loss 1.0990361\n",
      "Trained batch 97 batch loss 1.00615871 epoch total loss 1.09807849\n",
      "Trained batch 98 batch loss 1.0144614 epoch total loss 1.09722531\n",
      "Trained batch 99 batch loss 1.0676167 epoch total loss 1.09692621\n",
      "Trained batch 100 batch loss 0.984206915 epoch total loss 1.09579897\n",
      "Trained batch 101 batch loss 1.0292511 epoch total loss 1.0951401\n",
      "Trained batch 102 batch loss 0.943448901 epoch total loss 1.09365296\n",
      "Trained batch 103 batch loss 0.879219234 epoch total loss 1.09157109\n",
      "Trained batch 104 batch loss 1.02178466 epoch total loss 1.09090006\n",
      "Trained batch 105 batch loss 1.02994812 epoch total loss 1.09031951\n",
      "Trained batch 106 batch loss 1.00005889 epoch total loss 1.089468\n",
      "Trained batch 107 batch loss 1.0995326 epoch total loss 1.08956206\n",
      "Trained batch 108 batch loss 1.07123923 epoch total loss 1.08939242\n",
      "Trained batch 109 batch loss 1.04965687 epoch total loss 1.08902788\n",
      "Trained batch 110 batch loss 1.03034687 epoch total loss 1.08849442\n",
      "Trained batch 111 batch loss 1.04376173 epoch total loss 1.08809149\n",
      "Trained batch 112 batch loss 1.11469138 epoch total loss 1.08832896\n",
      "Trained batch 113 batch loss 1.00789738 epoch total loss 1.08761716\n",
      "Trained batch 114 batch loss 0.93391335 epoch total loss 1.0862689\n",
      "Trained batch 115 batch loss 0.876262128 epoch total loss 1.08444273\n",
      "Trained batch 116 batch loss 1.02892041 epoch total loss 1.08396411\n",
      "Trained batch 117 batch loss 1.19281507 epoch total loss 1.08489454\n",
      "Trained batch 118 batch loss 1.16243553 epoch total loss 1.08555162\n",
      "Trained batch 119 batch loss 1.17450738 epoch total loss 1.08629918\n",
      "Trained batch 120 batch loss 1.18351495 epoch total loss 1.08710933\n",
      "Trained batch 121 batch loss 1.06475687 epoch total loss 1.08692467\n",
      "Trained batch 122 batch loss 1.15375888 epoch total loss 1.08747256\n",
      "Trained batch 123 batch loss 1.09230673 epoch total loss 1.08751178\n",
      "Trained batch 124 batch loss 1.10501742 epoch total loss 1.08765292\n",
      "Trained batch 125 batch loss 1.09055126 epoch total loss 1.08767605\n",
      "Trained batch 126 batch loss 1.18605733 epoch total loss 1.08845675\n",
      "Trained batch 127 batch loss 1.20258284 epoch total loss 1.08935535\n",
      "Trained batch 128 batch loss 1.1697619 epoch total loss 1.08998358\n",
      "Trained batch 129 batch loss 1.13211238 epoch total loss 1.0903101\n",
      "Trained batch 130 batch loss 1.03931713 epoch total loss 1.0899179\n",
      "Trained batch 131 batch loss 0.821370065 epoch total loss 1.08786786\n",
      "Trained batch 132 batch loss 0.946475625 epoch total loss 1.08679676\n",
      "Trained batch 133 batch loss 1.16933203 epoch total loss 1.08741724\n",
      "Trained batch 134 batch loss 1.09299183 epoch total loss 1.08745885\n",
      "Trained batch 135 batch loss 1.06877589 epoch total loss 1.08732045\n",
      "Trained batch 136 batch loss 1.12509513 epoch total loss 1.08759809\n",
      "Trained batch 137 batch loss 1.01427007 epoch total loss 1.08706284\n",
      "Trained batch 138 batch loss 0.942248702 epoch total loss 1.08601344\n",
      "Trained batch 139 batch loss 0.89612031 epoch total loss 1.0846473\n",
      "Trained batch 140 batch loss 0.939734876 epoch total loss 1.0836122\n",
      "Trained batch 141 batch loss 1.02983141 epoch total loss 1.08323073\n",
      "Trained batch 142 batch loss 1.1002177 epoch total loss 1.08335042\n",
      "Trained batch 143 batch loss 1.1091876 epoch total loss 1.08353114\n",
      "Trained batch 144 batch loss 1.22872233 epoch total loss 1.08453941\n",
      "Trained batch 145 batch loss 1.0857724 epoch total loss 1.08454788\n",
      "Trained batch 146 batch loss 1.16358757 epoch total loss 1.08508933\n",
      "Trained batch 147 batch loss 1.08377635 epoch total loss 1.08508027\n",
      "Trained batch 148 batch loss 1.15999639 epoch total loss 1.08558655\n",
      "Trained batch 149 batch loss 1.07997918 epoch total loss 1.085549\n",
      "Trained batch 150 batch loss 1.27285719 epoch total loss 1.08679771\n",
      "Trained batch 151 batch loss 1.06962311 epoch total loss 1.08668399\n",
      "Trained batch 152 batch loss 1.1710422 epoch total loss 1.08723891\n",
      "Trained batch 153 batch loss 1.13192487 epoch total loss 1.08753097\n",
      "Trained batch 154 batch loss 0.997697651 epoch total loss 1.08694768\n",
      "Trained batch 155 batch loss 1.0684613 epoch total loss 1.08682847\n",
      "Trained batch 156 batch loss 1.05212128 epoch total loss 1.08660591\n",
      "Trained batch 157 batch loss 0.950718403 epoch total loss 1.08574045\n",
      "Trained batch 158 batch loss 1.12731862 epoch total loss 1.08600354\n",
      "Trained batch 159 batch loss 1.09105265 epoch total loss 1.08603525\n",
      "Trained batch 160 batch loss 1.25648773 epoch total loss 1.08710063\n",
      "Trained batch 161 batch loss 0.916591287 epoch total loss 1.08604157\n",
      "Trained batch 162 batch loss 0.852490664 epoch total loss 1.08459985\n",
      "Trained batch 163 batch loss 0.891675711 epoch total loss 1.08341634\n",
      "Trained batch 164 batch loss 0.910912037 epoch total loss 1.08236456\n",
      "Trained batch 165 batch loss 0.832572699 epoch total loss 1.0808506\n",
      "Trained batch 166 batch loss 0.867967367 epoch total loss 1.07956815\n",
      "Trained batch 167 batch loss 0.916124105 epoch total loss 1.07858944\n",
      "Trained batch 168 batch loss 0.927878559 epoch total loss 1.07769227\n",
      "Trained batch 169 batch loss 0.959177554 epoch total loss 1.07699108\n",
      "Trained batch 170 batch loss 1.00409102 epoch total loss 1.07656229\n",
      "Trained batch 171 batch loss 1.09252357 epoch total loss 1.07665563\n",
      "Trained batch 172 batch loss 1.12964249 epoch total loss 1.07696366\n",
      "Trained batch 173 batch loss 0.968883455 epoch total loss 1.07633889\n",
      "Trained batch 174 batch loss 1.04421258 epoch total loss 1.07615435\n",
      "Trained batch 175 batch loss 0.949397922 epoch total loss 1.07543\n",
      "Trained batch 176 batch loss 1.13853407 epoch total loss 1.07578862\n",
      "Trained batch 177 batch loss 1.1281569 epoch total loss 1.07608449\n",
      "Trained batch 178 batch loss 1.27504992 epoch total loss 1.07720232\n",
      "Trained batch 179 batch loss 1.31382751 epoch total loss 1.07852423\n",
      "Trained batch 180 batch loss 1.28078353 epoch total loss 1.07964778\n",
      "Trained batch 181 batch loss 1.26819134 epoch total loss 1.08068943\n",
      "Trained batch 182 batch loss 0.947520137 epoch total loss 1.07995784\n",
      "Trained batch 183 batch loss 0.916827083 epoch total loss 1.0790664\n",
      "Trained batch 184 batch loss 1.06643891 epoch total loss 1.07899773\n",
      "Trained batch 185 batch loss 0.949731827 epoch total loss 1.07829905\n",
      "Trained batch 186 batch loss 0.888501465 epoch total loss 1.07727861\n",
      "Trained batch 187 batch loss 0.940668225 epoch total loss 1.0765481\n",
      "Trained batch 188 batch loss 0.882513285 epoch total loss 1.07551599\n",
      "Trained batch 189 batch loss 0.894448876 epoch total loss 1.07455802\n",
      "Trained batch 190 batch loss 0.785546899 epoch total loss 1.07303691\n",
      "Trained batch 191 batch loss 0.89406389 epoch total loss 1.0720998\n",
      "Trained batch 192 batch loss 1.04711437 epoch total loss 1.07196975\n",
      "Trained batch 193 batch loss 0.936650395 epoch total loss 1.07126856\n",
      "Trained batch 194 batch loss 0.920229793 epoch total loss 1.07049\n",
      "Trained batch 195 batch loss 0.751285672 epoch total loss 1.06885302\n",
      "Trained batch 196 batch loss 1.00152063 epoch total loss 1.06850958\n",
      "Trained batch 197 batch loss 1.10790896 epoch total loss 1.06870961\n",
      "Trained batch 198 batch loss 1.18919635 epoch total loss 1.06931806\n",
      "Trained batch 199 batch loss 1.32562602 epoch total loss 1.07060599\n",
      "Trained batch 200 batch loss 1.22191644 epoch total loss 1.07136261\n",
      "Trained batch 201 batch loss 1.15316641 epoch total loss 1.0717696\n",
      "Trained batch 202 batch loss 1.18042994 epoch total loss 1.07230759\n",
      "Trained batch 203 batch loss 1.07515299 epoch total loss 1.07232153\n",
      "Trained batch 204 batch loss 0.972289622 epoch total loss 1.07183123\n",
      "Trained batch 205 batch loss 1.01596212 epoch total loss 1.07155871\n",
      "Trained batch 206 batch loss 1.1027683 epoch total loss 1.07171011\n",
      "Trained batch 207 batch loss 1.13566661 epoch total loss 1.0720191\n",
      "Trained batch 208 batch loss 1.04256558 epoch total loss 1.0718776\n",
      "Trained batch 209 batch loss 1.08498859 epoch total loss 1.0719403\n",
      "Trained batch 210 batch loss 1.03955901 epoch total loss 1.07178617\n",
      "Trained batch 211 batch loss 0.96206224 epoch total loss 1.07126617\n",
      "Trained batch 212 batch loss 0.861610889 epoch total loss 1.07027721\n",
      "Trained batch 213 batch loss 0.987220883 epoch total loss 1.06988728\n",
      "Trained batch 214 batch loss 1.03846133 epoch total loss 1.06974053\n",
      "Trained batch 215 batch loss 0.927904069 epoch total loss 1.06908083\n",
      "Trained batch 216 batch loss 0.989084125 epoch total loss 1.06871045\n",
      "Trained batch 217 batch loss 1.13964117 epoch total loss 1.06903732\n",
      "Trained batch 218 batch loss 1.05210435 epoch total loss 1.06895971\n",
      "Trained batch 219 batch loss 1.01621544 epoch total loss 1.06871891\n",
      "Trained batch 220 batch loss 1.11527681 epoch total loss 1.06893051\n",
      "Trained batch 221 batch loss 1.10351038 epoch total loss 1.06908703\n",
      "Trained batch 222 batch loss 1.13180685 epoch total loss 1.06936955\n",
      "Trained batch 223 batch loss 1.1107198 epoch total loss 1.06955492\n",
      "Trained batch 224 batch loss 0.937553287 epoch total loss 1.06896567\n",
      "Trained batch 225 batch loss 0.953372538 epoch total loss 1.06845188\n",
      "Trained batch 226 batch loss 0.902741313 epoch total loss 1.06771863\n",
      "Trained batch 227 batch loss 0.90583545 epoch total loss 1.06700552\n",
      "Trained batch 228 batch loss 0.971435189 epoch total loss 1.06658638\n",
      "Trained batch 229 batch loss 0.92408812 epoch total loss 1.0659641\n",
      "Trained batch 230 batch loss 0.987203956 epoch total loss 1.06562161\n",
      "Trained batch 231 batch loss 0.969948173 epoch total loss 1.06520748\n",
      "Trained batch 232 batch loss 0.948141336 epoch total loss 1.06470287\n",
      "Trained batch 233 batch loss 1.0468936 epoch total loss 1.06462646\n",
      "Trained batch 234 batch loss 0.901893497 epoch total loss 1.06393099\n",
      "Trained batch 235 batch loss 1.09164596 epoch total loss 1.06404889\n",
      "Trained batch 236 batch loss 1.05169272 epoch total loss 1.06399655\n",
      "Trained batch 237 batch loss 1.06293559 epoch total loss 1.06399202\n",
      "Trained batch 238 batch loss 1.09779334 epoch total loss 1.06413412\n",
      "Trained batch 239 batch loss 1.12860942 epoch total loss 1.06440389\n",
      "Trained batch 240 batch loss 1.07971191 epoch total loss 1.06446767\n",
      "Trained batch 241 batch loss 1.0444814 epoch total loss 1.0643847\n",
      "Trained batch 242 batch loss 1.08787668 epoch total loss 1.06448185\n",
      "Trained batch 243 batch loss 1.20190501 epoch total loss 1.06504738\n",
      "Trained batch 244 batch loss 1.26119363 epoch total loss 1.06585133\n",
      "Trained batch 245 batch loss 1.09650528 epoch total loss 1.06597638\n",
      "Trained batch 246 batch loss 1.06823015 epoch total loss 1.06598556\n",
      "Trained batch 247 batch loss 1.22433591 epoch total loss 1.06662667\n",
      "Trained batch 248 batch loss 1.16215754 epoch total loss 1.06701195\n",
      "Trained batch 249 batch loss 1.05529821 epoch total loss 1.06696486\n",
      "Trained batch 250 batch loss 1.11366844 epoch total loss 1.06715178\n",
      "Trained batch 251 batch loss 0.989529133 epoch total loss 1.06684244\n",
      "Trained batch 252 batch loss 1.05706346 epoch total loss 1.06680369\n",
      "Trained batch 253 batch loss 1.08336568 epoch total loss 1.06686926\n",
      "Trained batch 254 batch loss 1.00451541 epoch total loss 1.06662369\n",
      "Trained batch 255 batch loss 1.19004989 epoch total loss 1.0671078\n",
      "Trained batch 256 batch loss 1.03097701 epoch total loss 1.06696665\n",
      "Trained batch 257 batch loss 0.969753325 epoch total loss 1.0665884\n",
      "Trained batch 258 batch loss 1.04792535 epoch total loss 1.06651604\n",
      "Trained batch 259 batch loss 1.11157906 epoch total loss 1.06669\n",
      "Trained batch 260 batch loss 1.10688186 epoch total loss 1.06684458\n",
      "Trained batch 261 batch loss 1.18507266 epoch total loss 1.06729746\n",
      "Trained batch 262 batch loss 1.0729382 epoch total loss 1.06731904\n",
      "Trained batch 263 batch loss 0.95839256 epoch total loss 1.0669049\n",
      "Trained batch 264 batch loss 0.971705675 epoch total loss 1.06654429\n",
      "Trained batch 265 batch loss 1.06761527 epoch total loss 1.06654835\n",
      "Trained batch 266 batch loss 0.983474791 epoch total loss 1.06623614\n",
      "Trained batch 267 batch loss 0.966820955 epoch total loss 1.06586385\n",
      "Trained batch 268 batch loss 1.17148888 epoch total loss 1.06625783\n",
      "Trained batch 269 batch loss 1.0313518 epoch total loss 1.06612802\n",
      "Trained batch 270 batch loss 0.998867333 epoch total loss 1.06587899\n",
      "Trained batch 271 batch loss 1.11768508 epoch total loss 1.06607008\n",
      "Trained batch 272 batch loss 1.17646456 epoch total loss 1.06647587\n",
      "Trained batch 273 batch loss 0.997419834 epoch total loss 1.06622291\n",
      "Trained batch 274 batch loss 0.95966661 epoch total loss 1.06583405\n",
      "Trained batch 275 batch loss 1.08358932 epoch total loss 1.06589854\n",
      "Trained batch 276 batch loss 0.975951672 epoch total loss 1.06557262\n",
      "Trained batch 277 batch loss 1.11863303 epoch total loss 1.06576419\n",
      "Trained batch 278 batch loss 1.14388382 epoch total loss 1.06604517\n",
      "Trained batch 279 batch loss 0.90874505 epoch total loss 1.06548142\n",
      "Trained batch 280 batch loss 0.947295129 epoch total loss 1.0650593\n",
      "Trained batch 281 batch loss 0.942523 epoch total loss 1.06462336\n",
      "Trained batch 282 batch loss 1.10574269 epoch total loss 1.06476915\n",
      "Trained batch 283 batch loss 1.04448402 epoch total loss 1.0646975\n",
      "Trained batch 284 batch loss 1.01608622 epoch total loss 1.06452632\n",
      "Trained batch 285 batch loss 1.03216338 epoch total loss 1.06441271\n",
      "Trained batch 286 batch loss 1.01317692 epoch total loss 1.06423366\n",
      "Trained batch 287 batch loss 1.05866575 epoch total loss 1.06421423\n",
      "Trained batch 288 batch loss 1.05060267 epoch total loss 1.0641669\n",
      "Trained batch 289 batch loss 1.15402126 epoch total loss 1.0644778\n",
      "Trained batch 290 batch loss 1.28909349 epoch total loss 1.06525242\n",
      "Trained batch 291 batch loss 1.09064269 epoch total loss 1.06533957\n",
      "Trained batch 292 batch loss 1.07181048 epoch total loss 1.06536174\n",
      "Trained batch 293 batch loss 1.08180737 epoch total loss 1.06541789\n",
      "Trained batch 294 batch loss 0.911524951 epoch total loss 1.06489444\n",
      "Trained batch 295 batch loss 0.911784768 epoch total loss 1.0643754\n",
      "Trained batch 296 batch loss 1.19793928 epoch total loss 1.06482661\n",
      "Trained batch 297 batch loss 1.14843452 epoch total loss 1.06510818\n",
      "Trained batch 298 batch loss 1.1721015 epoch total loss 1.06546712\n",
      "Trained batch 299 batch loss 1.07481945 epoch total loss 1.06549847\n",
      "Trained batch 300 batch loss 1.02244484 epoch total loss 1.06535494\n",
      "Trained batch 301 batch loss 1.01562536 epoch total loss 1.06518972\n",
      "Trained batch 302 batch loss 1.02393627 epoch total loss 1.06505311\n",
      "Trained batch 303 batch loss 0.945129395 epoch total loss 1.06465733\n",
      "Trained batch 304 batch loss 1.01420808 epoch total loss 1.06449139\n",
      "Trained batch 305 batch loss 1.01079082 epoch total loss 1.06431532\n",
      "Trained batch 306 batch loss 1.03106165 epoch total loss 1.06420672\n",
      "Trained batch 307 batch loss 0.99440521 epoch total loss 1.06397939\n",
      "Trained batch 308 batch loss 1.01400399 epoch total loss 1.06381714\n",
      "Trained batch 309 batch loss 1.05270374 epoch total loss 1.06378114\n",
      "Trained batch 310 batch loss 1.20948935 epoch total loss 1.06425118\n",
      "Trained batch 311 batch loss 1.14860082 epoch total loss 1.06452239\n",
      "Trained batch 312 batch loss 1.18615127 epoch total loss 1.0649122\n",
      "Trained batch 313 batch loss 1.11140633 epoch total loss 1.06506085\n",
      "Trained batch 314 batch loss 1.02892482 epoch total loss 1.06494582\n",
      "Trained batch 315 batch loss 1.1187408 epoch total loss 1.06511652\n",
      "Trained batch 316 batch loss 1.13181257 epoch total loss 1.06532764\n",
      "Trained batch 317 batch loss 1.0442301 epoch total loss 1.06526101\n",
      "Trained batch 318 batch loss 1.02126181 epoch total loss 1.06512272\n",
      "Trained batch 319 batch loss 0.933311939 epoch total loss 1.06470954\n",
      "Trained batch 320 batch loss 1.07951593 epoch total loss 1.0647558\n",
      "Trained batch 321 batch loss 1.03058255 epoch total loss 1.06464934\n",
      "Trained batch 322 batch loss 1.02805078 epoch total loss 1.06453562\n",
      "Trained batch 323 batch loss 1.03530252 epoch total loss 1.06444514\n",
      "Trained batch 324 batch loss 0.917849183 epoch total loss 1.06399274\n",
      "Trained batch 325 batch loss 1.03208184 epoch total loss 1.06389451\n",
      "Trained batch 326 batch loss 0.94851476 epoch total loss 1.06354058\n",
      "Trained batch 327 batch loss 1.17197406 epoch total loss 1.06387222\n",
      "Trained batch 328 batch loss 1.20554483 epoch total loss 1.06430411\n",
      "Trained batch 329 batch loss 1.15523458 epoch total loss 1.06458044\n",
      "Trained batch 330 batch loss 0.9357813 epoch total loss 1.06419027\n",
      "Trained batch 331 batch loss 0.930848479 epoch total loss 1.06378734\n",
      "Trained batch 332 batch loss 0.959009528 epoch total loss 1.06347179\n",
      "Trained batch 333 batch loss 1.08091664 epoch total loss 1.06352413\n",
      "Trained batch 334 batch loss 1.07203388 epoch total loss 1.06354952\n",
      "Trained batch 335 batch loss 1.12816811 epoch total loss 1.06374252\n",
      "Trained batch 336 batch loss 1.06025481 epoch total loss 1.06373203\n",
      "Trained batch 337 batch loss 1.079054 epoch total loss 1.06377745\n",
      "Trained batch 338 batch loss 0.970059395 epoch total loss 1.06350017\n",
      "Trained batch 339 batch loss 1.06758404 epoch total loss 1.06351233\n",
      "Trained batch 340 batch loss 1.10145831 epoch total loss 1.06362391\n",
      "Trained batch 341 batch loss 1.10667634 epoch total loss 1.06375027\n",
      "Trained batch 342 batch loss 1.08455729 epoch total loss 1.06381106\n",
      "Trained batch 343 batch loss 1.02511644 epoch total loss 1.06369829\n",
      "Trained batch 344 batch loss 0.97943 epoch total loss 1.06345332\n",
      "Trained batch 345 batch loss 0.833021 epoch total loss 1.06278539\n",
      "Trained batch 346 batch loss 0.984396577 epoch total loss 1.06255889\n",
      "Trained batch 347 batch loss 1.11368561 epoch total loss 1.06270611\n",
      "Trained batch 348 batch loss 1.04702556 epoch total loss 1.06266105\n",
      "Trained batch 349 batch loss 1.01073837 epoch total loss 1.06251228\n",
      "Trained batch 350 batch loss 1.08627045 epoch total loss 1.06258023\n",
      "Trained batch 351 batch loss 1.08621764 epoch total loss 1.06264758\n",
      "Trained batch 352 batch loss 1.0346657 epoch total loss 1.06256807\n",
      "Trained batch 353 batch loss 1.10259962 epoch total loss 1.06268144\n",
      "Trained batch 354 batch loss 1.00997603 epoch total loss 1.06253254\n",
      "Trained batch 355 batch loss 1.06148231 epoch total loss 1.06252968\n",
      "Trained batch 356 batch loss 0.851395071 epoch total loss 1.06193662\n",
      "Trained batch 357 batch loss 0.999464512 epoch total loss 1.06176162\n",
      "Trained batch 358 batch loss 1.06892228 epoch total loss 1.06178153\n",
      "Trained batch 359 batch loss 1.14444745 epoch total loss 1.06201184\n",
      "Trained batch 360 batch loss 1.08943641 epoch total loss 1.06208801\n",
      "Trained batch 361 batch loss 0.960722148 epoch total loss 1.06180727\n",
      "Trained batch 362 batch loss 1.08841968 epoch total loss 1.06188071\n",
      "Trained batch 363 batch loss 1.07902586 epoch total loss 1.06192803\n",
      "Trained batch 364 batch loss 0.967034757 epoch total loss 1.06166732\n",
      "Trained batch 365 batch loss 0.941700578 epoch total loss 1.06133866\n",
      "Trained batch 366 batch loss 0.922460794 epoch total loss 1.06095922\n",
      "Trained batch 367 batch loss 1.02264142 epoch total loss 1.06085479\n",
      "Trained batch 368 batch loss 0.996874452 epoch total loss 1.06068099\n",
      "Trained batch 369 batch loss 1.08572805 epoch total loss 1.06074882\n",
      "Trained batch 370 batch loss 1.16892433 epoch total loss 1.06104124\n",
      "Trained batch 371 batch loss 1.10387635 epoch total loss 1.06115663\n",
      "Trained batch 372 batch loss 1.06210446 epoch total loss 1.06115925\n",
      "Trained batch 373 batch loss 1.17683554 epoch total loss 1.06146932\n",
      "Trained batch 374 batch loss 1.07524228 epoch total loss 1.06150627\n",
      "Trained batch 375 batch loss 1.09931684 epoch total loss 1.061607\n",
      "Trained batch 376 batch loss 1.02138627 epoch total loss 1.06150007\n",
      "Trained batch 377 batch loss 1.18295598 epoch total loss 1.06182218\n",
      "Trained batch 378 batch loss 1.09650278 epoch total loss 1.06191397\n",
      "Trained batch 379 batch loss 1.04780495 epoch total loss 1.06187665\n",
      "Trained batch 380 batch loss 1.05450749 epoch total loss 1.06185734\n",
      "Trained batch 381 batch loss 1.13698494 epoch total loss 1.06205451\n",
      "Trained batch 382 batch loss 1.07803679 epoch total loss 1.06209636\n",
      "Trained batch 383 batch loss 0.972157478 epoch total loss 1.06186152\n",
      "Trained batch 384 batch loss 0.854718566 epoch total loss 1.06132209\n",
      "Trained batch 385 batch loss 0.962393582 epoch total loss 1.06106508\n",
      "Trained batch 386 batch loss 0.869241536 epoch total loss 1.06056809\n",
      "Trained batch 387 batch loss 1.00056446 epoch total loss 1.060413\n",
      "Trained batch 388 batch loss 1.02700543 epoch total loss 1.06032693\n",
      "Trained batch 389 batch loss 0.875695 epoch total loss 1.05985236\n",
      "Trained batch 390 batch loss 0.894672155 epoch total loss 1.05942881\n",
      "Trained batch 391 batch loss 0.944004536 epoch total loss 1.05913365\n",
      "Trained batch 392 batch loss 1.03559422 epoch total loss 1.05907357\n",
      "Trained batch 393 batch loss 0.921711683 epoch total loss 1.05872405\n",
      "Trained batch 394 batch loss 1.00713181 epoch total loss 1.05859315\n",
      "Trained batch 395 batch loss 1.10559726 epoch total loss 1.05871212\n",
      "Trained batch 396 batch loss 1.20866704 epoch total loss 1.05909085\n",
      "Trained batch 397 batch loss 1.11452186 epoch total loss 1.05923045\n",
      "Trained batch 398 batch loss 1.0782125 epoch total loss 1.05927813\n",
      "Trained batch 399 batch loss 1.03244734 epoch total loss 1.0592109\n",
      "Trained batch 400 batch loss 1.09168649 epoch total loss 1.05929208\n",
      "Trained batch 401 batch loss 1.09730768 epoch total loss 1.05938685\n",
      "Trained batch 402 batch loss 0.997307658 epoch total loss 1.05923247\n",
      "Trained batch 403 batch loss 1.02279019 epoch total loss 1.05914211\n",
      "Trained batch 404 batch loss 0.969208419 epoch total loss 1.05891943\n",
      "Trained batch 405 batch loss 1.02016711 epoch total loss 1.05882382\n",
      "Trained batch 406 batch loss 0.897606492 epoch total loss 1.05842674\n",
      "Trained batch 407 batch loss 0.986143291 epoch total loss 1.05824912\n",
      "Trained batch 408 batch loss 0.89512527 epoch total loss 1.05784929\n",
      "Trained batch 409 batch loss 0.91090554 epoch total loss 1.05749\n",
      "Trained batch 410 batch loss 0.854939818 epoch total loss 1.05699599\n",
      "Trained batch 411 batch loss 0.979107261 epoch total loss 1.05680645\n",
      "Trained batch 412 batch loss 1.03545046 epoch total loss 1.05675471\n",
      "Trained batch 413 batch loss 1.09271312 epoch total loss 1.05684173\n",
      "Trained batch 414 batch loss 1.11347699 epoch total loss 1.05697858\n",
      "Trained batch 415 batch loss 1.19693065 epoch total loss 1.05731571\n",
      "Trained batch 416 batch loss 1.09018385 epoch total loss 1.05739474\n",
      "Trained batch 417 batch loss 1.17970693 epoch total loss 1.05768812\n",
      "Trained batch 418 batch loss 1.15039361 epoch total loss 1.05790985\n",
      "Trained batch 419 batch loss 0.987261653 epoch total loss 1.05774128\n",
      "Trained batch 420 batch loss 1.14924932 epoch total loss 1.0579592\n",
      "Trained batch 421 batch loss 1.17122734 epoch total loss 1.05822825\n",
      "Trained batch 422 batch loss 0.946647644 epoch total loss 1.05796385\n",
      "Trained batch 423 batch loss 0.888851523 epoch total loss 1.05756414\n",
      "Trained batch 424 batch loss 0.952847302 epoch total loss 1.05731714\n",
      "Trained batch 425 batch loss 1.14564419 epoch total loss 1.05752492\n",
      "Trained batch 426 batch loss 1.0769552 epoch total loss 1.05757058\n",
      "Trained batch 427 batch loss 1.09379935 epoch total loss 1.05765545\n",
      "Trained batch 428 batch loss 1.06745434 epoch total loss 1.05767834\n",
      "Trained batch 429 batch loss 1.16733432 epoch total loss 1.05793393\n",
      "Trained batch 430 batch loss 1.07980418 epoch total loss 1.05798471\n",
      "Trained batch 431 batch loss 1.10211062 epoch total loss 1.05808711\n",
      "Trained batch 432 batch loss 1.1903795 epoch total loss 1.05839336\n",
      "Trained batch 433 batch loss 1.28860235 epoch total loss 1.05892503\n",
      "Trained batch 434 batch loss 1.1618197 epoch total loss 1.05916214\n",
      "Trained batch 435 batch loss 1.10763037 epoch total loss 1.0592736\n",
      "Trained batch 436 batch loss 0.955156744 epoch total loss 1.05903482\n",
      "Trained batch 437 batch loss 1.09972918 epoch total loss 1.05912793\n",
      "Trained batch 438 batch loss 0.920716226 epoch total loss 1.0588119\n",
      "Trained batch 439 batch loss 1.01646638 epoch total loss 1.05871546\n",
      "Trained batch 440 batch loss 0.976879716 epoch total loss 1.0585295\n",
      "Trained batch 441 batch loss 1.05268347 epoch total loss 1.05851614\n",
      "Trained batch 442 batch loss 0.911457241 epoch total loss 1.05818343\n",
      "Trained batch 443 batch loss 1.00224924 epoch total loss 1.05805719\n",
      "Trained batch 444 batch loss 1.01564908 epoch total loss 1.0579617\n",
      "Trained batch 445 batch loss 1.00004172 epoch total loss 1.05783153\n",
      "Trained batch 446 batch loss 0.96679318 epoch total loss 1.05762744\n",
      "Trained batch 447 batch loss 1.07622659 epoch total loss 1.05766904\n",
      "Trained batch 448 batch loss 1.02434206 epoch total loss 1.05759466\n",
      "Trained batch 449 batch loss 1.04335546 epoch total loss 1.05756307\n",
      "Trained batch 450 batch loss 1.13391304 epoch total loss 1.0577327\n",
      "Trained batch 451 batch loss 1.10316348 epoch total loss 1.05783343\n",
      "Trained batch 452 batch loss 1.1108135 epoch total loss 1.05795062\n",
      "Trained batch 453 batch loss 0.875651479 epoch total loss 1.05754817\n",
      "Trained batch 454 batch loss 1.02897644 epoch total loss 1.05748522\n",
      "Trained batch 455 batch loss 0.974658966 epoch total loss 1.05730319\n",
      "Trained batch 456 batch loss 1.09506202 epoch total loss 1.05738592\n",
      "Trained batch 457 batch loss 1.08048201 epoch total loss 1.05743647\n",
      "Trained batch 458 batch loss 1.21782243 epoch total loss 1.0577867\n",
      "Trained batch 459 batch loss 1.19084549 epoch total loss 1.05807662\n",
      "Trained batch 460 batch loss 1.14579391 epoch total loss 1.05826724\n",
      "Trained batch 461 batch loss 1.26049089 epoch total loss 1.05870593\n",
      "Trained batch 462 batch loss 1.20897388 epoch total loss 1.05903125\n",
      "Trained batch 463 batch loss 1.15860736 epoch total loss 1.0592463\n",
      "Trained batch 464 batch loss 1.14965641 epoch total loss 1.05944109\n",
      "Trained batch 465 batch loss 1.17319894 epoch total loss 1.05968571\n",
      "Trained batch 466 batch loss 1.14549673 epoch total loss 1.05986989\n",
      "Trained batch 467 batch loss 1.18380415 epoch total loss 1.06013536\n",
      "Trained batch 468 batch loss 1.18274403 epoch total loss 1.06039727\n",
      "Trained batch 469 batch loss 1.16788602 epoch total loss 1.06062651\n",
      "Trained batch 470 batch loss 1.09699321 epoch total loss 1.06070375\n",
      "Trained batch 471 batch loss 0.995579362 epoch total loss 1.06056559\n",
      "Trained batch 472 batch loss 0.854830623 epoch total loss 1.06012964\n",
      "Trained batch 473 batch loss 1.09527183 epoch total loss 1.06020391\n",
      "Trained batch 474 batch loss 0.922442913 epoch total loss 1.0599134\n",
      "Trained batch 475 batch loss 1.04975414 epoch total loss 1.05989194\n",
      "Trained batch 476 batch loss 1.24123073 epoch total loss 1.06027293\n",
      "Trained batch 477 batch loss 1.08736575 epoch total loss 1.06032968\n",
      "Trained batch 478 batch loss 0.944313288 epoch total loss 1.06008697\n",
      "Trained batch 479 batch loss 1.02805412 epoch total loss 1.06002009\n",
      "Trained batch 480 batch loss 1.16836905 epoch total loss 1.06024587\n",
      "Trained batch 481 batch loss 1.14112389 epoch total loss 1.06041396\n",
      "Trained batch 482 batch loss 1.11247063 epoch total loss 1.06052196\n",
      "Trained batch 483 batch loss 1.11122847 epoch total loss 1.06062686\n",
      "Trained batch 484 batch loss 1.33641636 epoch total loss 1.06119668\n",
      "Trained batch 485 batch loss 1.21907175 epoch total loss 1.06152213\n",
      "Trained batch 486 batch loss 1.10668778 epoch total loss 1.06161511\n",
      "Trained batch 487 batch loss 1.10394168 epoch total loss 1.06170201\n",
      "Trained batch 488 batch loss 1.26483405 epoch total loss 1.06211829\n",
      "Trained batch 489 batch loss 1.09549797 epoch total loss 1.0621866\n",
      "Trained batch 490 batch loss 0.9527511 epoch total loss 1.06196332\n",
      "Trained batch 491 batch loss 0.941445649 epoch total loss 1.06171787\n",
      "Trained batch 492 batch loss 0.843014419 epoch total loss 1.06127334\n",
      "Trained batch 493 batch loss 0.949617565 epoch total loss 1.06104696\n",
      "Trained batch 494 batch loss 1.02561331 epoch total loss 1.06097519\n",
      "Trained batch 495 batch loss 1.13249946 epoch total loss 1.06111979\n",
      "Trained batch 496 batch loss 1.0860157 epoch total loss 1.06116986\n",
      "Trained batch 497 batch loss 1.2185421 epoch total loss 1.0614866\n",
      "Trained batch 498 batch loss 1.08195961 epoch total loss 1.06152773\n",
      "Trained batch 499 batch loss 1.03125989 epoch total loss 1.06146705\n",
      "Trained batch 500 batch loss 1.23829615 epoch total loss 1.06182063\n",
      "Trained batch 501 batch loss 1.31370378 epoch total loss 1.06232345\n",
      "Trained batch 502 batch loss 1.17737961 epoch total loss 1.06255269\n",
      "Trained batch 503 batch loss 1.35400128 epoch total loss 1.06313205\n",
      "Trained batch 504 batch loss 1.25166392 epoch total loss 1.06350613\n",
      "Trained batch 505 batch loss 1.18124318 epoch total loss 1.06373918\n",
      "Trained batch 506 batch loss 1.13799679 epoch total loss 1.06388593\n",
      "Trained batch 507 batch loss 1.10471046 epoch total loss 1.06396651\n",
      "Trained batch 508 batch loss 1.14274478 epoch total loss 1.0641216\n",
      "Trained batch 509 batch loss 1.09521151 epoch total loss 1.06418276\n",
      "Trained batch 510 batch loss 1.13166547 epoch total loss 1.06431496\n",
      "Trained batch 511 batch loss 1.12407875 epoch total loss 1.06443202\n",
      "Trained batch 512 batch loss 1.07949758 epoch total loss 1.06446135\n",
      "Trained batch 513 batch loss 0.97236073 epoch total loss 1.06428182\n",
      "Trained batch 514 batch loss 1.03994298 epoch total loss 1.06423438\n",
      "Trained batch 515 batch loss 0.929005384 epoch total loss 1.06397188\n",
      "Trained batch 516 batch loss 0.933296204 epoch total loss 1.06371856\n",
      "Trained batch 517 batch loss 0.98208642 epoch total loss 1.06356072\n",
      "Trained batch 518 batch loss 1.16737497 epoch total loss 1.06376112\n",
      "Trained batch 519 batch loss 1.2779727 epoch total loss 1.06417382\n",
      "Trained batch 520 batch loss 1.1816324 epoch total loss 1.06439972\n",
      "Trained batch 521 batch loss 1.09904456 epoch total loss 1.06446624\n",
      "Trained batch 522 batch loss 1.17872715 epoch total loss 1.06468511\n",
      "Trained batch 523 batch loss 1.0019393 epoch total loss 1.06456518\n",
      "Trained batch 524 batch loss 1.08266771 epoch total loss 1.06459963\n",
      "Trained batch 525 batch loss 1.04384613 epoch total loss 1.06456\n",
      "Trained batch 526 batch loss 1.12528813 epoch total loss 1.06467557\n",
      "Trained batch 527 batch loss 1.18228412 epoch total loss 1.06489873\n",
      "Trained batch 528 batch loss 1.21519494 epoch total loss 1.06518352\n",
      "Trained batch 529 batch loss 1.07897139 epoch total loss 1.06520951\n",
      "Trained batch 530 batch loss 1.06611598 epoch total loss 1.06521118\n",
      "Trained batch 531 batch loss 1.11542678 epoch total loss 1.06530583\n",
      "Trained batch 532 batch loss 1.04283214 epoch total loss 1.06526351\n",
      "Trained batch 533 batch loss 1.13201022 epoch total loss 1.0653888\n",
      "Trained batch 534 batch loss 1.14838362 epoch total loss 1.06554425\n",
      "Trained batch 535 batch loss 1.07216716 epoch total loss 1.06555653\n",
      "Trained batch 536 batch loss 1.1945492 epoch total loss 1.06579721\n",
      "Trained batch 537 batch loss 1.13059068 epoch total loss 1.06591785\n",
      "Trained batch 538 batch loss 1.1236434 epoch total loss 1.06602514\n",
      "Trained batch 539 batch loss 1.15045464 epoch total loss 1.06618178\n",
      "Trained batch 540 batch loss 1.20016503 epoch total loss 1.06643\n",
      "Trained batch 541 batch loss 1.136729 epoch total loss 1.06655991\n",
      "Trained batch 542 batch loss 1.13457644 epoch total loss 1.06668544\n",
      "Trained batch 543 batch loss 1.10356045 epoch total loss 1.06675339\n",
      "Trained batch 544 batch loss 1.13309383 epoch total loss 1.06687534\n",
      "Trained batch 545 batch loss 1.22678196 epoch total loss 1.06716883\n",
      "Trained batch 546 batch loss 1.04750061 epoch total loss 1.06713271\n",
      "Trained batch 547 batch loss 1.05735505 epoch total loss 1.06711495\n",
      "Trained batch 548 batch loss 1.10173452 epoch total loss 1.06717813\n",
      "Trained batch 549 batch loss 1.08717239 epoch total loss 1.06721449\n",
      "Trained batch 550 batch loss 1.05035233 epoch total loss 1.06718385\n",
      "Trained batch 551 batch loss 1.14826488 epoch total loss 1.06733096\n",
      "Trained batch 552 batch loss 1.12878716 epoch total loss 1.0674423\n",
      "Trained batch 553 batch loss 1.09433866 epoch total loss 1.06749094\n",
      "Trained batch 554 batch loss 1.12364399 epoch total loss 1.06759238\n",
      "Trained batch 555 batch loss 1.02834511 epoch total loss 1.06752157\n",
      "Trained batch 556 batch loss 1.00901389 epoch total loss 1.06741643\n",
      "Trained batch 557 batch loss 0.955305815 epoch total loss 1.0672152\n",
      "Trained batch 558 batch loss 0.970140517 epoch total loss 1.06704116\n",
      "Trained batch 559 batch loss 1.00802028 epoch total loss 1.06693554\n",
      "Trained batch 560 batch loss 0.971586227 epoch total loss 1.06676531\n",
      "Trained batch 561 batch loss 0.930525601 epoch total loss 1.06652248\n",
      "Trained batch 562 batch loss 1.03477502 epoch total loss 1.06646597\n",
      "Trained batch 563 batch loss 1.06118679 epoch total loss 1.06645656\n",
      "Trained batch 564 batch loss 0.961263597 epoch total loss 1.06627\n",
      "Trained batch 565 batch loss 1.02313805 epoch total loss 1.0661937\n",
      "Trained batch 566 batch loss 1.02094066 epoch total loss 1.06611371\n",
      "Trained batch 567 batch loss 1.04118776 epoch total loss 1.06606972\n",
      "Trained batch 568 batch loss 1.03454304 epoch total loss 1.06601429\n",
      "Trained batch 569 batch loss 1.13278127 epoch total loss 1.06613159\n",
      "Trained batch 570 batch loss 1.11888206 epoch total loss 1.0662241\n",
      "Trained batch 571 batch loss 0.905472159 epoch total loss 1.06594253\n",
      "Trained batch 572 batch loss 0.987695932 epoch total loss 1.06580567\n",
      "Trained batch 573 batch loss 1.08961606 epoch total loss 1.06584728\n",
      "Trained batch 574 batch loss 1.00995088 epoch total loss 1.06574988\n",
      "Trained batch 575 batch loss 0.891897678 epoch total loss 1.06544757\n",
      "Trained batch 576 batch loss 0.939547539 epoch total loss 1.06522894\n",
      "Trained batch 577 batch loss 1.00047159 epoch total loss 1.06511676\n",
      "Trained batch 578 batch loss 1.14737308 epoch total loss 1.0652591\n",
      "Trained batch 579 batch loss 1.0264473 epoch total loss 1.0651921\n",
      "Trained batch 580 batch loss 1.09056079 epoch total loss 1.06523585\n",
      "Trained batch 581 batch loss 0.895378709 epoch total loss 1.06494355\n",
      "Trained batch 582 batch loss 0.862324715 epoch total loss 1.06459534\n",
      "Trained batch 583 batch loss 0.848659635 epoch total loss 1.06422484\n",
      "Trained batch 584 batch loss 0.824754477 epoch total loss 1.06381488\n",
      "Trained batch 585 batch loss 0.930745959 epoch total loss 1.06358731\n",
      "Trained batch 586 batch loss 0.887389421 epoch total loss 1.06328666\n",
      "Trained batch 587 batch loss 1.19920802 epoch total loss 1.06351829\n",
      "Trained batch 588 batch loss 1.1781404 epoch total loss 1.06371319\n",
      "Trained batch 589 batch loss 1.20942771 epoch total loss 1.06396055\n",
      "Trained batch 590 batch loss 1.1811794 epoch total loss 1.06415927\n",
      "Trained batch 591 batch loss 1.13294625 epoch total loss 1.06427562\n",
      "Trained batch 592 batch loss 0.972427845 epoch total loss 1.06412041\n",
      "Trained batch 593 batch loss 1.10086775 epoch total loss 1.0641824\n",
      "Trained batch 594 batch loss 0.944861233 epoch total loss 1.06398153\n",
      "Trained batch 595 batch loss 1.10382426 epoch total loss 1.06404853\n",
      "Trained batch 596 batch loss 1.05620694 epoch total loss 1.06403542\n",
      "Trained batch 597 batch loss 0.884364247 epoch total loss 1.06373441\n",
      "Trained batch 598 batch loss 0.894547224 epoch total loss 1.06345141\n",
      "Trained batch 599 batch loss 0.874195158 epoch total loss 1.0631355\n",
      "Trained batch 600 batch loss 0.96795094 epoch total loss 1.06297684\n",
      "Trained batch 601 batch loss 1.02826023 epoch total loss 1.06291914\n",
      "Trained batch 602 batch loss 1.16558206 epoch total loss 1.06308961\n",
      "Trained batch 603 batch loss 1.29496765 epoch total loss 1.06347418\n",
      "Trained batch 604 batch loss 1.32389021 epoch total loss 1.06390548\n",
      "Trained batch 605 batch loss 1.12168264 epoch total loss 1.06400096\n",
      "Trained batch 606 batch loss 1.22188711 epoch total loss 1.06426144\n",
      "Trained batch 607 batch loss 1.17597795 epoch total loss 1.0644455\n",
      "Trained batch 608 batch loss 1.33055735 epoch total loss 1.06488323\n",
      "Trained batch 609 batch loss 1.22352338 epoch total loss 1.0651437\n",
      "Trained batch 610 batch loss 1.16126895 epoch total loss 1.06530118\n",
      "Trained batch 611 batch loss 1.15254402 epoch total loss 1.06544399\n",
      "Trained batch 612 batch loss 1.23579335 epoch total loss 1.06572235\n",
      "Trained batch 613 batch loss 1.17451835 epoch total loss 1.06589973\n",
      "Trained batch 614 batch loss 1.09023213 epoch total loss 1.06593931\n",
      "Trained batch 615 batch loss 1.02849758 epoch total loss 1.06587851\n",
      "Trained batch 616 batch loss 1.0832572 epoch total loss 1.06590664\n",
      "Trained batch 617 batch loss 1.09281635 epoch total loss 1.06595027\n",
      "Trained batch 618 batch loss 1.06786156 epoch total loss 1.06595337\n",
      "Trained batch 619 batch loss 1.0843755 epoch total loss 1.06598318\n",
      "Trained batch 620 batch loss 1.14319873 epoch total loss 1.06610763\n",
      "Trained batch 621 batch loss 1.08706379 epoch total loss 1.06614137\n",
      "Trained batch 622 batch loss 1.14274275 epoch total loss 1.06626451\n",
      "Trained batch 623 batch loss 0.982714951 epoch total loss 1.06613052\n",
      "Trained batch 624 batch loss 1.04084182 epoch total loss 1.06608987\n",
      "Trained batch 625 batch loss 0.885091245 epoch total loss 1.06580031\n",
      "Trained batch 626 batch loss 0.924530268 epoch total loss 1.06557465\n",
      "Trained batch 627 batch loss 1.14249945 epoch total loss 1.06569743\n",
      "Trained batch 628 batch loss 1.16132498 epoch total loss 1.06584966\n",
      "Trained batch 629 batch loss 1.11096299 epoch total loss 1.06592131\n",
      "Trained batch 630 batch loss 1.02054131 epoch total loss 1.06584942\n",
      "Trained batch 631 batch loss 0.954879522 epoch total loss 1.06567359\n",
      "Trained batch 632 batch loss 0.954082906 epoch total loss 1.06549704\n",
      "Trained batch 633 batch loss 1.0891912 epoch total loss 1.06553435\n",
      "Trained batch 634 batch loss 1.0679872 epoch total loss 1.06553829\n",
      "Trained batch 635 batch loss 1.03165793 epoch total loss 1.065485\n",
      "Trained batch 636 batch loss 1.03672135 epoch total loss 1.06543982\n",
      "Trained batch 637 batch loss 1.08060598 epoch total loss 1.06546366\n",
      "Trained batch 638 batch loss 1.04121292 epoch total loss 1.06542552\n",
      "Trained batch 639 batch loss 1.14875007 epoch total loss 1.06555593\n",
      "Trained batch 640 batch loss 1.08141017 epoch total loss 1.06558073\n",
      "Trained batch 641 batch loss 0.897756815 epoch total loss 1.06531894\n",
      "Trained batch 642 batch loss 1.05632639 epoch total loss 1.06530499\n",
      "Trained batch 643 batch loss 1.06117153 epoch total loss 1.06529856\n",
      "Trained batch 644 batch loss 1.03870153 epoch total loss 1.06525719\n",
      "Trained batch 645 batch loss 1.10353148 epoch total loss 1.06531656\n",
      "Trained batch 646 batch loss 1.03827238 epoch total loss 1.0652746\n",
      "Trained batch 647 batch loss 0.992967606 epoch total loss 1.0651629\n",
      "Trained batch 648 batch loss 0.918920577 epoch total loss 1.06493723\n",
      "Trained batch 649 batch loss 0.950097919 epoch total loss 1.06476033\n",
      "Trained batch 650 batch loss 1.00893497 epoch total loss 1.06467438\n",
      "Trained batch 651 batch loss 1.07992494 epoch total loss 1.06469774\n",
      "Trained batch 652 batch loss 1.03647697 epoch total loss 1.06465447\n",
      "Trained batch 653 batch loss 1.09130788 epoch total loss 1.06469536\n",
      "Trained batch 654 batch loss 1.13510311 epoch total loss 1.064803\n",
      "Trained batch 655 batch loss 0.95926559 epoch total loss 1.06464195\n",
      "Trained batch 656 batch loss 0.874554515 epoch total loss 1.06435215\n",
      "Trained batch 657 batch loss 1.00683796 epoch total loss 1.06426466\n",
      "Trained batch 658 batch loss 1.04318058 epoch total loss 1.06423259\n",
      "Trained batch 659 batch loss 0.897387803 epoch total loss 1.06397939\n",
      "Trained batch 660 batch loss 0.979157031 epoch total loss 1.06385088\n",
      "Trained batch 661 batch loss 0.882682145 epoch total loss 1.06357682\n",
      "Trained batch 662 batch loss 0.877053678 epoch total loss 1.06329513\n",
      "Trained batch 663 batch loss 0.952741385 epoch total loss 1.06312835\n",
      "Trained batch 664 batch loss 1.05786479 epoch total loss 1.06312048\n",
      "Trained batch 665 batch loss 1.25519 epoch total loss 1.06340933\n",
      "Trained batch 666 batch loss 0.985106289 epoch total loss 1.06329167\n",
      "Trained batch 667 batch loss 1.07489991 epoch total loss 1.06330907\n",
      "Trained batch 668 batch loss 0.989355922 epoch total loss 1.06319845\n",
      "Trained batch 669 batch loss 1.0093329 epoch total loss 1.06311798\n",
      "Trained batch 670 batch loss 0.986716211 epoch total loss 1.0630039\n",
      "Trained batch 671 batch loss 1.00216162 epoch total loss 1.06291318\n",
      "Trained batch 672 batch loss 1.07588959 epoch total loss 1.06293237\n",
      "Trained batch 673 batch loss 1.03416204 epoch total loss 1.0628897\n",
      "Trained batch 674 batch loss 1.07176328 epoch total loss 1.06290293\n",
      "Trained batch 675 batch loss 1.12860489 epoch total loss 1.0630002\n",
      "Trained batch 676 batch loss 1.09537709 epoch total loss 1.06304812\n",
      "Trained batch 677 batch loss 1.0284977 epoch total loss 1.0629971\n",
      "Trained batch 678 batch loss 1.0171926 epoch total loss 1.06292963\n",
      "Trained batch 679 batch loss 0.966604114 epoch total loss 1.06278777\n",
      "Trained batch 680 batch loss 1.01148427 epoch total loss 1.06271231\n",
      "Trained batch 681 batch loss 0.887655318 epoch total loss 1.06245518\n",
      "Trained batch 682 batch loss 0.821071506 epoch total loss 1.06210124\n",
      "Trained batch 683 batch loss 0.922119319 epoch total loss 1.0618962\n",
      "Trained batch 684 batch loss 0.862253129 epoch total loss 1.06160438\n",
      "Trained batch 685 batch loss 0.960763395 epoch total loss 1.06145716\n",
      "Trained batch 686 batch loss 0.930487633 epoch total loss 1.06126618\n",
      "Trained batch 687 batch loss 1.00088978 epoch total loss 1.06117833\n",
      "Trained batch 688 batch loss 1.00903964 epoch total loss 1.06110263\n",
      "Trained batch 689 batch loss 1.02772665 epoch total loss 1.06105411\n",
      "Trained batch 690 batch loss 1.07634401 epoch total loss 1.06107628\n",
      "Trained batch 691 batch loss 1.05415332 epoch total loss 1.06106627\n",
      "Trained batch 692 batch loss 1.19745672 epoch total loss 1.06126332\n",
      "Trained batch 693 batch loss 1.18407178 epoch total loss 1.06144059\n",
      "Trained batch 694 batch loss 1.03721619 epoch total loss 1.06140566\n",
      "Trained batch 695 batch loss 1.00396991 epoch total loss 1.06132305\n",
      "Trained batch 696 batch loss 1.01024199 epoch total loss 1.06124961\n",
      "Trained batch 697 batch loss 1.12007022 epoch total loss 1.06133401\n",
      "Trained batch 698 batch loss 1.1087265 epoch total loss 1.06140184\n",
      "Trained batch 699 batch loss 1.20700741 epoch total loss 1.06161022\n",
      "Trained batch 700 batch loss 1.1738348 epoch total loss 1.06177056\n",
      "Trained batch 701 batch loss 1.15975475 epoch total loss 1.06191027\n",
      "Trained batch 702 batch loss 1.17608547 epoch total loss 1.06207287\n",
      "Trained batch 703 batch loss 1.11578774 epoch total loss 1.06214929\n",
      "Trained batch 704 batch loss 1.19458854 epoch total loss 1.0623374\n",
      "Trained batch 705 batch loss 1.18844652 epoch total loss 1.06251633\n",
      "Trained batch 706 batch loss 1.10467958 epoch total loss 1.06257606\n",
      "Trained batch 707 batch loss 1.1850667 epoch total loss 1.06274927\n",
      "Trained batch 708 batch loss 1.09740591 epoch total loss 1.06279826\n",
      "Trained batch 709 batch loss 1.09527183 epoch total loss 1.06284404\n",
      "Trained batch 710 batch loss 1.06649327 epoch total loss 1.06284916\n",
      "Trained batch 711 batch loss 1.09182715 epoch total loss 1.06288993\n",
      "Trained batch 712 batch loss 1.07031322 epoch total loss 1.0629003\n",
      "Trained batch 713 batch loss 1.05705 epoch total loss 1.0628922\n",
      "Trained batch 714 batch loss 1.0152843 epoch total loss 1.06282544\n",
      "Trained batch 715 batch loss 1.01040864 epoch total loss 1.06275213\n",
      "Trained batch 716 batch loss 0.882987738 epoch total loss 1.06250107\n",
      "Trained batch 717 batch loss 0.898856759 epoch total loss 1.06227291\n",
      "Trained batch 718 batch loss 0.872536361 epoch total loss 1.06200862\n",
      "Trained batch 719 batch loss 1.05730557 epoch total loss 1.06200218\n",
      "Trained batch 720 batch loss 1.18098438 epoch total loss 1.06216741\n",
      "Trained batch 721 batch loss 1.35920811 epoch total loss 1.06257927\n",
      "Trained batch 722 batch loss 1.14476609 epoch total loss 1.06269312\n",
      "Trained batch 723 batch loss 1.03940701 epoch total loss 1.06266093\n",
      "Trained batch 724 batch loss 1.02184117 epoch total loss 1.06260467\n",
      "Trained batch 725 batch loss 1.11723685 epoch total loss 1.06268\n",
      "Trained batch 726 batch loss 1.10271263 epoch total loss 1.0627352\n",
      "Trained batch 727 batch loss 1.17196691 epoch total loss 1.0628854\n",
      "Trained batch 728 batch loss 1.09540176 epoch total loss 1.06293011\n",
      "Trained batch 729 batch loss 1.117998 epoch total loss 1.06300557\n",
      "Trained batch 730 batch loss 1.07367384 epoch total loss 1.06302023\n",
      "Trained batch 731 batch loss 0.97796905 epoch total loss 1.06290388\n",
      "Trained batch 732 batch loss 0.960294843 epoch total loss 1.06276369\n",
      "Trained batch 733 batch loss 0.920183659 epoch total loss 1.06256914\n",
      "Trained batch 734 batch loss 1.04043889 epoch total loss 1.06253898\n",
      "Trained batch 735 batch loss 1.07118559 epoch total loss 1.06255078\n",
      "Trained batch 736 batch loss 1.01729298 epoch total loss 1.06248927\n",
      "Trained batch 737 batch loss 0.839945 epoch total loss 1.06218731\n",
      "Trained batch 738 batch loss 1.09498024 epoch total loss 1.06223166\n",
      "Trained batch 739 batch loss 0.965942204 epoch total loss 1.06210136\n",
      "Trained batch 740 batch loss 0.947508037 epoch total loss 1.06194651\n",
      "Trained batch 741 batch loss 0.788389564 epoch total loss 1.06157744\n",
      "Trained batch 742 batch loss 0.859579 epoch total loss 1.06130517\n",
      "Trained batch 743 batch loss 1.15158439 epoch total loss 1.06142664\n",
      "Trained batch 744 batch loss 1.12102604 epoch total loss 1.06150675\n",
      "Trained batch 745 batch loss 1.21316075 epoch total loss 1.06171036\n",
      "Trained batch 746 batch loss 1.11542058 epoch total loss 1.06178236\n",
      "Trained batch 747 batch loss 0.930425942 epoch total loss 1.06160641\n",
      "Trained batch 748 batch loss 0.913258314 epoch total loss 1.06140816\n",
      "Trained batch 749 batch loss 1.02619648 epoch total loss 1.06136107\n",
      "Trained batch 750 batch loss 1.01031101 epoch total loss 1.06129301\n",
      "Trained batch 751 batch loss 1.03536844 epoch total loss 1.06125844\n",
      "Trained batch 752 batch loss 1.01976943 epoch total loss 1.06120336\n",
      "Trained batch 753 batch loss 1.08297515 epoch total loss 1.06123221\n",
      "Trained batch 754 batch loss 1.24386263 epoch total loss 1.06147432\n",
      "Trained batch 755 batch loss 1.17488968 epoch total loss 1.06162453\n",
      "Trained batch 756 batch loss 1.05808139 epoch total loss 1.06161988\n",
      "Trained batch 757 batch loss 1.11226356 epoch total loss 1.06168675\n",
      "Trained batch 758 batch loss 0.890921175 epoch total loss 1.06146157\n",
      "Trained batch 759 batch loss 1.00689316 epoch total loss 1.06138968\n",
      "Trained batch 760 batch loss 1.14685166 epoch total loss 1.0615021\n",
      "Trained batch 761 batch loss 1.10805726 epoch total loss 1.06156325\n",
      "Trained batch 762 batch loss 1.19871163 epoch total loss 1.06174326\n",
      "Trained batch 763 batch loss 1.06729031 epoch total loss 1.06175041\n",
      "Trained batch 764 batch loss 0.999312401 epoch total loss 1.06166875\n",
      "Trained batch 765 batch loss 1.11200798 epoch total loss 1.06173456\n",
      "Trained batch 766 batch loss 1.0605104 epoch total loss 1.06173289\n",
      "Trained batch 767 batch loss 1.02475941 epoch total loss 1.06168473\n",
      "Trained batch 768 batch loss 0.947557211 epoch total loss 1.06153619\n",
      "Trained batch 769 batch loss 1.04084349 epoch total loss 1.06150925\n",
      "Trained batch 770 batch loss 1.08806348 epoch total loss 1.0615437\n",
      "Trained batch 771 batch loss 1.16059923 epoch total loss 1.06167221\n",
      "Trained batch 772 batch loss 1.09876704 epoch total loss 1.06172025\n",
      "Trained batch 773 batch loss 1.1062324 epoch total loss 1.06177783\n",
      "Trained batch 774 batch loss 0.988888502 epoch total loss 1.06168365\n",
      "Trained batch 775 batch loss 1.06220114 epoch total loss 1.06168437\n",
      "Trained batch 776 batch loss 1.06062317 epoch total loss 1.06168294\n",
      "Trained batch 777 batch loss 0.865379274 epoch total loss 1.06143022\n",
      "Trained batch 778 batch loss 1.2014637 epoch total loss 1.06161034\n",
      "Trained batch 779 batch loss 1.09289408 epoch total loss 1.0616504\n",
      "Trained batch 780 batch loss 1.06096339 epoch total loss 1.06164956\n",
      "Trained batch 781 batch loss 1.08512437 epoch total loss 1.0616796\n",
      "Trained batch 782 batch loss 0.960640192 epoch total loss 1.0615505\n",
      "Trained batch 783 batch loss 0.945859194 epoch total loss 1.06140268\n",
      "Trained batch 784 batch loss 1.06010771 epoch total loss 1.06140101\n",
      "Trained batch 785 batch loss 1.05964708 epoch total loss 1.06139874\n",
      "Trained batch 786 batch loss 1.08458102 epoch total loss 1.06142831\n",
      "Trained batch 787 batch loss 1.1469593 epoch total loss 1.06153703\n",
      "Trained batch 788 batch loss 1.15350938 epoch total loss 1.06165373\n",
      "Trained batch 789 batch loss 1.29125774 epoch total loss 1.06194472\n",
      "Trained batch 790 batch loss 1.22909188 epoch total loss 1.06215632\n",
      "Trained batch 791 batch loss 1.17353177 epoch total loss 1.06229711\n",
      "Trained batch 792 batch loss 1.07649159 epoch total loss 1.06231499\n",
      "Trained batch 793 batch loss 1.0625999 epoch total loss 1.06231534\n",
      "Trained batch 794 batch loss 1.20957947 epoch total loss 1.06250083\n",
      "Trained batch 795 batch loss 1.14988065 epoch total loss 1.06261075\n",
      "Trained batch 796 batch loss 1.09316587 epoch total loss 1.06264913\n",
      "Trained batch 797 batch loss 0.942388713 epoch total loss 1.06249821\n",
      "Trained batch 798 batch loss 1.06693721 epoch total loss 1.06250381\n",
      "Trained batch 799 batch loss 0.872675657 epoch total loss 1.06226623\n",
      "Trained batch 800 batch loss 1.00072861 epoch total loss 1.06218934\n",
      "Trained batch 801 batch loss 1.03218281 epoch total loss 1.06215179\n",
      "Trained batch 802 batch loss 0.952034593 epoch total loss 1.06201458\n",
      "Trained batch 803 batch loss 0.974892616 epoch total loss 1.0619061\n",
      "Trained batch 804 batch loss 0.931682944 epoch total loss 1.06174409\n",
      "Trained batch 805 batch loss 0.915732265 epoch total loss 1.06156266\n",
      "Trained batch 806 batch loss 0.975938857 epoch total loss 1.06145644\n",
      "Trained batch 807 batch loss 1.08452988 epoch total loss 1.06148505\n",
      "Trained batch 808 batch loss 1.09652662 epoch total loss 1.06152844\n",
      "Trained batch 809 batch loss 1.05426466 epoch total loss 1.06151938\n",
      "Trained batch 810 batch loss 1.12828445 epoch total loss 1.06160188\n",
      "Trained batch 811 batch loss 0.938469172 epoch total loss 1.06145\n",
      "Trained batch 812 batch loss 0.973207533 epoch total loss 1.0613414\n",
      "Trained batch 813 batch loss 1.05475199 epoch total loss 1.0613333\n",
      "Trained batch 814 batch loss 1.02684879 epoch total loss 1.06129086\n",
      "Trained batch 815 batch loss 1.06100714 epoch total loss 1.06129062\n",
      "Trained batch 816 batch loss 1.10989666 epoch total loss 1.06135023\n",
      "Trained batch 817 batch loss 1.08310938 epoch total loss 1.06137681\n",
      "Trained batch 818 batch loss 1.04763842 epoch total loss 1.06136012\n",
      "Trained batch 819 batch loss 1.0663681 epoch total loss 1.0613662\n",
      "Trained batch 820 batch loss 1.05145729 epoch total loss 1.06135416\n",
      "Trained batch 821 batch loss 1.07500231 epoch total loss 1.06137073\n",
      "Trained batch 822 batch loss 0.998654723 epoch total loss 1.06129444\n",
      "Trained batch 823 batch loss 1.11098886 epoch total loss 1.06135476\n",
      "Trained batch 824 batch loss 1.11421514 epoch total loss 1.06141889\n",
      "Trained batch 825 batch loss 1.21019399 epoch total loss 1.06159925\n",
      "Trained batch 826 batch loss 0.981552482 epoch total loss 1.06150234\n",
      "Trained batch 827 batch loss 0.964216292 epoch total loss 1.0613848\n",
      "Trained batch 828 batch loss 0.913464487 epoch total loss 1.0612061\n",
      "Trained batch 829 batch loss 0.857822359 epoch total loss 1.06096077\n",
      "Trained batch 830 batch loss 1.18622684 epoch total loss 1.06111169\n",
      "Trained batch 831 batch loss 1.29056621 epoch total loss 1.0613879\n",
      "Trained batch 832 batch loss 1.16812813 epoch total loss 1.06151617\n",
      "Trained batch 833 batch loss 0.955392301 epoch total loss 1.06138873\n",
      "Trained batch 834 batch loss 0.975755215 epoch total loss 1.06128609\n",
      "Trained batch 835 batch loss 1.025365 epoch total loss 1.06124306\n",
      "Trained batch 836 batch loss 0.999011338 epoch total loss 1.06116867\n",
      "Trained batch 837 batch loss 0.950037777 epoch total loss 1.06103587\n",
      "Trained batch 838 batch loss 0.997916102 epoch total loss 1.06096053\n",
      "Trained batch 839 batch loss 1.07043195 epoch total loss 1.06097186\n",
      "Trained batch 840 batch loss 1.12547648 epoch total loss 1.06104863\n",
      "Trained batch 841 batch loss 1.05721903 epoch total loss 1.0610441\n",
      "Trained batch 842 batch loss 1.08840668 epoch total loss 1.06107652\n",
      "Trained batch 843 batch loss 0.994950175 epoch total loss 1.06099808\n",
      "Trained batch 844 batch loss 0.94011575 epoch total loss 1.06085491\n",
      "Trained batch 845 batch loss 1.09443045 epoch total loss 1.06089461\n",
      "Trained batch 846 batch loss 1.04589558 epoch total loss 1.06087685\n",
      "Trained batch 847 batch loss 1.02840471 epoch total loss 1.06083846\n",
      "Trained batch 848 batch loss 1.07478821 epoch total loss 1.06085491\n",
      "Trained batch 849 batch loss 0.860219479 epoch total loss 1.06061864\n",
      "Trained batch 850 batch loss 0.911956549 epoch total loss 1.06044364\n",
      "Trained batch 851 batch loss 0.944631338 epoch total loss 1.06030762\n",
      "Trained batch 852 batch loss 0.94810307 epoch total loss 1.0601759\n",
      "Trained batch 853 batch loss 0.982822657 epoch total loss 1.0600853\n",
      "Trained batch 854 batch loss 1.01829696 epoch total loss 1.06003642\n",
      "Trained batch 855 batch loss 1.06866848 epoch total loss 1.06004643\n",
      "Trained batch 856 batch loss 1.24200225 epoch total loss 1.06025898\n",
      "Trained batch 857 batch loss 1.19959307 epoch total loss 1.06042159\n",
      "Trained batch 858 batch loss 1.30846953 epoch total loss 1.06071067\n",
      "Trained batch 859 batch loss 1.08029044 epoch total loss 1.06073344\n",
      "Trained batch 860 batch loss 0.942566752 epoch total loss 1.06059611\n",
      "Trained batch 861 batch loss 1.00921428 epoch total loss 1.06053638\n",
      "Trained batch 862 batch loss 1.06240964 epoch total loss 1.06053865\n",
      "Trained batch 863 batch loss 1.0678364 epoch total loss 1.06054699\n",
      "Trained batch 864 batch loss 1.18014503 epoch total loss 1.0606854\n",
      "Trained batch 865 batch loss 1.18772018 epoch total loss 1.06083226\n",
      "Trained batch 866 batch loss 1.08048058 epoch total loss 1.06085503\n",
      "Trained batch 867 batch loss 0.944560349 epoch total loss 1.06072092\n",
      "Trained batch 868 batch loss 1.04016066 epoch total loss 1.0606972\n",
      "Trained batch 869 batch loss 1.09026194 epoch total loss 1.06073129\n",
      "Trained batch 870 batch loss 1.21268094 epoch total loss 1.06090593\n",
      "Trained batch 871 batch loss 1.09086597 epoch total loss 1.06094038\n",
      "Trained batch 872 batch loss 1.11468196 epoch total loss 1.06100202\n",
      "Trained batch 873 batch loss 1.08725441 epoch total loss 1.06103206\n",
      "Trained batch 874 batch loss 1.13232458 epoch total loss 1.0611136\n",
      "Trained batch 875 batch loss 1.12946761 epoch total loss 1.0611918\n",
      "Trained batch 876 batch loss 1.17353797 epoch total loss 1.06132\n",
      "Trained batch 877 batch loss 1.23592615 epoch total loss 1.06151903\n",
      "Trained batch 878 batch loss 1.08563471 epoch total loss 1.06154656\n",
      "Trained batch 879 batch loss 1.10580754 epoch total loss 1.06159687\n",
      "Trained batch 880 batch loss 1.27255392 epoch total loss 1.0618366\n",
      "Trained batch 881 batch loss 1.09414876 epoch total loss 1.06187332\n",
      "Trained batch 882 batch loss 1.10235059 epoch total loss 1.06191921\n",
      "Trained batch 883 batch loss 1.09725964 epoch total loss 1.06195927\n",
      "Trained batch 884 batch loss 1.10950673 epoch total loss 1.06201315\n",
      "Trained batch 885 batch loss 1.0858885 epoch total loss 1.06204009\n",
      "Trained batch 886 batch loss 1.00266862 epoch total loss 1.06197309\n",
      "Trained batch 887 batch loss 1.20375872 epoch total loss 1.06213284\n",
      "Trained batch 888 batch loss 1.07706904 epoch total loss 1.06214976\n",
      "Trained batch 889 batch loss 1.03240573 epoch total loss 1.06211627\n",
      "Trained batch 890 batch loss 1.03522182 epoch total loss 1.06208611\n",
      "Trained batch 891 batch loss 1.08800018 epoch total loss 1.06211519\n",
      "Trained batch 892 batch loss 1.1261375 epoch total loss 1.06218696\n",
      "Trained batch 893 batch loss 1.05550873 epoch total loss 1.06217945\n",
      "Trained batch 894 batch loss 1.08224034 epoch total loss 1.06220186\n",
      "Trained batch 895 batch loss 1.07169235 epoch total loss 1.06221247\n",
      "Trained batch 896 batch loss 1.02047205 epoch total loss 1.06216586\n",
      "Trained batch 897 batch loss 1.10499835 epoch total loss 1.06221366\n",
      "Trained batch 898 batch loss 1.11413252 epoch total loss 1.06227148\n",
      "Trained batch 899 batch loss 1.11397386 epoch total loss 1.06232893\n",
      "Trained batch 900 batch loss 1.13316679 epoch total loss 1.06240761\n",
      "Trained batch 901 batch loss 1.02677488 epoch total loss 1.06236815\n",
      "Trained batch 902 batch loss 0.966959715 epoch total loss 1.0622623\n",
      "Trained batch 903 batch loss 1.04750204 epoch total loss 1.06224597\n",
      "Trained batch 904 batch loss 1.12198615 epoch total loss 1.06231213\n",
      "Trained batch 905 batch loss 1.16857982 epoch total loss 1.06242955\n",
      "Trained batch 906 batch loss 1.15454602 epoch total loss 1.06253123\n",
      "Trained batch 907 batch loss 1.1665144 epoch total loss 1.06264579\n",
      "Trained batch 908 batch loss 1.19697821 epoch total loss 1.06279373\n",
      "Trained batch 909 batch loss 1.26944375 epoch total loss 1.06302106\n",
      "Trained batch 910 batch loss 1.04481125 epoch total loss 1.06300104\n",
      "Trained batch 911 batch loss 1.00855684 epoch total loss 1.06294131\n",
      "Trained batch 912 batch loss 1.14904976 epoch total loss 1.06303573\n",
      "Trained batch 913 batch loss 1.07726383 epoch total loss 1.06305134\n",
      "Trained batch 914 batch loss 1.01269257 epoch total loss 1.06299627\n",
      "Trained batch 915 batch loss 1.07690132 epoch total loss 1.06301141\n",
      "Trained batch 916 batch loss 1.08493328 epoch total loss 1.06303537\n",
      "Trained batch 917 batch loss 1.08370984 epoch total loss 1.06305802\n",
      "Trained batch 918 batch loss 1.01309192 epoch total loss 1.06300354\n",
      "Trained batch 919 batch loss 1.01542068 epoch total loss 1.0629518\n",
      "Trained batch 920 batch loss 1.15250909 epoch total loss 1.06304908\n",
      "Trained batch 921 batch loss 1.00397086 epoch total loss 1.06298494\n",
      "Trained batch 922 batch loss 1.02421236 epoch total loss 1.06294298\n",
      "Trained batch 923 batch loss 1.11243391 epoch total loss 1.06299651\n",
      "Trained batch 924 batch loss 1.1704284 epoch total loss 1.06311285\n",
      "Trained batch 925 batch loss 1.21051788 epoch total loss 1.06327212\n",
      "Trained batch 926 batch loss 1.18059921 epoch total loss 1.06339884\n",
      "Trained batch 927 batch loss 1.17181933 epoch total loss 1.06351578\n",
      "Trained batch 928 batch loss 1.10229993 epoch total loss 1.06355762\n",
      "Trained batch 929 batch loss 0.992289841 epoch total loss 1.06348085\n",
      "Trained batch 930 batch loss 0.861402631 epoch total loss 1.06326365\n",
      "Trained batch 931 batch loss 0.810289562 epoch total loss 1.06299186\n",
      "Trained batch 932 batch loss 0.806357801 epoch total loss 1.06271648\n",
      "Trained batch 933 batch loss 0.970455885 epoch total loss 1.06261766\n",
      "Trained batch 934 batch loss 0.94754678 epoch total loss 1.0624944\n",
      "Trained batch 935 batch loss 0.925917745 epoch total loss 1.06234837\n",
      "Trained batch 936 batch loss 0.93737787 epoch total loss 1.06221485\n",
      "Trained batch 937 batch loss 0.969303727 epoch total loss 1.06211567\n",
      "Trained batch 938 batch loss 1.14619839 epoch total loss 1.06220531\n",
      "Trained batch 939 batch loss 1.02333558 epoch total loss 1.06216383\n",
      "Trained batch 940 batch loss 1.11541688 epoch total loss 1.06222057\n",
      "Trained batch 941 batch loss 1.15385747 epoch total loss 1.06231797\n",
      "Trained batch 942 batch loss 1.1929965 epoch total loss 1.06245661\n",
      "Trained batch 943 batch loss 1.11816907 epoch total loss 1.06251574\n",
      "Trained batch 944 batch loss 0.955301702 epoch total loss 1.06240213\n",
      "Trained batch 945 batch loss 1.04780304 epoch total loss 1.06238675\n",
      "Trained batch 946 batch loss 1.0979805 epoch total loss 1.0624243\n",
      "Trained batch 947 batch loss 1.06460571 epoch total loss 1.06242669\n",
      "Trained batch 948 batch loss 0.809520721 epoch total loss 1.0621599\n",
      "Trained batch 949 batch loss 0.80406 epoch total loss 1.06188786\n",
      "Trained batch 950 batch loss 0.944262862 epoch total loss 1.06176412\n",
      "Trained batch 951 batch loss 1.15408111 epoch total loss 1.06186116\n",
      "Trained batch 952 batch loss 1.14451849 epoch total loss 1.06194794\n",
      "Trained batch 953 batch loss 1.23531592 epoch total loss 1.06212986\n",
      "Trained batch 954 batch loss 1.09991574 epoch total loss 1.06216943\n",
      "Trained batch 955 batch loss 1.13733888 epoch total loss 1.06224823\n",
      "Trained batch 956 batch loss 1.04799247 epoch total loss 1.06223321\n",
      "Trained batch 957 batch loss 1.13000023 epoch total loss 1.06230402\n",
      "Trained batch 958 batch loss 0.997622967 epoch total loss 1.06223655\n",
      "Trained batch 959 batch loss 0.97838378 epoch total loss 1.06214917\n",
      "Trained batch 960 batch loss 1.03496552 epoch total loss 1.0621208\n",
      "Trained batch 961 batch loss 1.03756762 epoch total loss 1.06209528\n",
      "Trained batch 962 batch loss 1.17739677 epoch total loss 1.06221509\n",
      "Trained batch 963 batch loss 1.11943626 epoch total loss 1.06227458\n",
      "Trained batch 964 batch loss 0.996195674 epoch total loss 1.06220603\n",
      "Trained batch 965 batch loss 1.02900493 epoch total loss 1.06217158\n",
      "Trained batch 966 batch loss 1.07616425 epoch total loss 1.06218612\n",
      "Trained batch 967 batch loss 1.18678916 epoch total loss 1.06231499\n",
      "Trained batch 968 batch loss 1.07012928 epoch total loss 1.06232297\n",
      "Trained batch 969 batch loss 1.06930423 epoch total loss 1.06233013\n",
      "Trained batch 970 batch loss 1.08643103 epoch total loss 1.06235504\n",
      "Trained batch 971 batch loss 1.07997894 epoch total loss 1.06237316\n",
      "Trained batch 972 batch loss 0.937529802 epoch total loss 1.06224465\n",
      "Trained batch 973 batch loss 1.08535874 epoch total loss 1.06226838\n",
      "Trained batch 974 batch loss 1.07627189 epoch total loss 1.0622828\n",
      "Trained batch 975 batch loss 1.09297085 epoch total loss 1.06231427\n",
      "Trained batch 976 batch loss 1.07056785 epoch total loss 1.06232274\n",
      "Trained batch 977 batch loss 1.09276271 epoch total loss 1.06235397\n",
      "Trained batch 978 batch loss 1.05315435 epoch total loss 1.06234443\n",
      "Trained batch 979 batch loss 1.00568342 epoch total loss 1.06228662\n",
      "Trained batch 980 batch loss 0.902676284 epoch total loss 1.06212378\n",
      "Trained batch 981 batch loss 1.08813477 epoch total loss 1.06215036\n",
      "Trained batch 982 batch loss 0.963578 epoch total loss 1.06205\n",
      "Trained batch 983 batch loss 0.954387546 epoch total loss 1.06194043\n",
      "Trained batch 984 batch loss 0.959612608 epoch total loss 1.06183648\n",
      "Trained batch 985 batch loss 1.00063372 epoch total loss 1.06177425\n",
      "Trained batch 986 batch loss 1.09994185 epoch total loss 1.061813\n",
      "Trained batch 987 batch loss 0.954754353 epoch total loss 1.06170452\n",
      "Trained batch 988 batch loss 0.923134089 epoch total loss 1.06156421\n",
      "Trained batch 989 batch loss 1.01950467 epoch total loss 1.06152177\n",
      "Trained batch 990 batch loss 1.09556162 epoch total loss 1.0615561\n",
      "Trained batch 991 batch loss 1.10585642 epoch total loss 1.0616008\n",
      "Trained batch 992 batch loss 1.07547235 epoch total loss 1.06161475\n",
      "Trained batch 993 batch loss 1.15656972 epoch total loss 1.06171036\n",
      "Trained batch 994 batch loss 1.11805308 epoch total loss 1.0617671\n",
      "Trained batch 995 batch loss 1.1853683 epoch total loss 1.06189132\n",
      "Trained batch 996 batch loss 1.1168642 epoch total loss 1.06194651\n",
      "Trained batch 997 batch loss 1.21317959 epoch total loss 1.06209815\n",
      "Trained batch 998 batch loss 1.01371455 epoch total loss 1.06204963\n",
      "Trained batch 999 batch loss 0.942608833 epoch total loss 1.06193006\n",
      "Trained batch 1000 batch loss 1.06900823 epoch total loss 1.06193709\n",
      "Trained batch 1001 batch loss 0.86033988 epoch total loss 1.06173575\n",
      "Trained batch 1002 batch loss 1.15967417 epoch total loss 1.0618335\n",
      "Trained batch 1003 batch loss 1.05786681 epoch total loss 1.06182957\n",
      "Trained batch 1004 batch loss 1.1018033 epoch total loss 1.06186938\n",
      "Trained batch 1005 batch loss 1.10127556 epoch total loss 1.0619086\n",
      "Trained batch 1006 batch loss 1.22469831 epoch total loss 1.06207049\n",
      "Trained batch 1007 batch loss 1.02261877 epoch total loss 1.06203127\n",
      "Trained batch 1008 batch loss 1.15898609 epoch total loss 1.06212735\n",
      "Trained batch 1009 batch loss 1.16581893 epoch total loss 1.06223011\n",
      "Trained batch 1010 batch loss 1.02944779 epoch total loss 1.06219757\n",
      "Trained batch 1011 batch loss 0.938199282 epoch total loss 1.06207502\n",
      "Trained batch 1012 batch loss 1.04938388 epoch total loss 1.0620625\n",
      "Trained batch 1013 batch loss 1.2383734 epoch total loss 1.06223655\n",
      "Trained batch 1014 batch loss 1.08462691 epoch total loss 1.0622586\n",
      "Trained batch 1015 batch loss 1.09915674 epoch total loss 1.06229496\n",
      "Trained batch 1016 batch loss 1.21328211 epoch total loss 1.06244349\n",
      "Trained batch 1017 batch loss 1.06782258 epoch total loss 1.06244886\n",
      "Trained batch 1018 batch loss 1.02704191 epoch total loss 1.06241417\n",
      "Trained batch 1019 batch loss 1.07034636 epoch total loss 1.06242192\n",
      "Trained batch 1020 batch loss 1.06914043 epoch total loss 1.06242847\n",
      "Trained batch 1021 batch loss 1.16504264 epoch total loss 1.06252897\n",
      "Trained batch 1022 batch loss 1.03248262 epoch total loss 1.06249952\n",
      "Trained batch 1023 batch loss 1.06457877 epoch total loss 1.06250155\n",
      "Trained batch 1024 batch loss 1.18674576 epoch total loss 1.0626229\n",
      "Trained batch 1025 batch loss 1.23691893 epoch total loss 1.06279302\n",
      "Trained batch 1026 batch loss 1.1511066 epoch total loss 1.06287909\n",
      "Trained batch 1027 batch loss 1.02994263 epoch total loss 1.0628469\n",
      "Trained batch 1028 batch loss 1.00635219 epoch total loss 1.06279194\n",
      "Trained batch 1029 batch loss 1.04039526 epoch total loss 1.06277025\n",
      "Trained batch 1030 batch loss 1.15367949 epoch total loss 1.06285846\n",
      "Trained batch 1031 batch loss 1.12524819 epoch total loss 1.06291902\n",
      "Trained batch 1032 batch loss 1.25702906 epoch total loss 1.06310713\n",
      "Trained batch 1033 batch loss 1.24590027 epoch total loss 1.06328404\n",
      "Trained batch 1034 batch loss 1.10704267 epoch total loss 1.06332636\n",
      "Trained batch 1035 batch loss 0.962506413 epoch total loss 1.06322896\n",
      "Trained batch 1036 batch loss 1.11639762 epoch total loss 1.06328034\n",
      "Trained batch 1037 batch loss 1.17208147 epoch total loss 1.06338537\n",
      "Trained batch 1038 batch loss 1.24544513 epoch total loss 1.06356072\n",
      "Trained batch 1039 batch loss 1.10334539 epoch total loss 1.06359911\n",
      "Trained batch 1040 batch loss 0.891846299 epoch total loss 1.063434\n",
      "Trained batch 1041 batch loss 1.1249907 epoch total loss 1.06349313\n",
      "Trained batch 1042 batch loss 1.25957739 epoch total loss 1.06368124\n",
      "Trained batch 1043 batch loss 1.22780311 epoch total loss 1.0638386\n",
      "Trained batch 1044 batch loss 1.1890862 epoch total loss 1.06395853\n",
      "Trained batch 1045 batch loss 0.986185253 epoch total loss 1.06388414\n",
      "Trained batch 1046 batch loss 0.994793832 epoch total loss 1.06381798\n",
      "Trained batch 1047 batch loss 1.1907202 epoch total loss 1.06393921\n",
      "Trained batch 1048 batch loss 1.12244689 epoch total loss 1.063995\n",
      "Trained batch 1049 batch loss 1.14766073 epoch total loss 1.06407475\n",
      "Trained batch 1050 batch loss 1.08659351 epoch total loss 1.06409621\n",
      "Trained batch 1051 batch loss 1.25825965 epoch total loss 1.06428099\n",
      "Trained batch 1052 batch loss 1.30947125 epoch total loss 1.06451404\n",
      "Trained batch 1053 batch loss 1.27939427 epoch total loss 1.06471813\n",
      "Trained batch 1054 batch loss 1.20341325 epoch total loss 1.06484973\n",
      "Trained batch 1055 batch loss 1.014902 epoch total loss 1.06480229\n",
      "Trained batch 1056 batch loss 1.16066396 epoch total loss 1.06489313\n",
      "Trained batch 1057 batch loss 1.08312035 epoch total loss 1.06491029\n",
      "Trained batch 1058 batch loss 1.13368726 epoch total loss 1.06497538\n",
      "Trained batch 1059 batch loss 1.11503983 epoch total loss 1.06502259\n",
      "Trained batch 1060 batch loss 1.09258747 epoch total loss 1.06504846\n",
      "Trained batch 1061 batch loss 0.995392084 epoch total loss 1.06498277\n",
      "Trained batch 1062 batch loss 1.04223919 epoch total loss 1.06496143\n",
      "Trained batch 1063 batch loss 1.05898035 epoch total loss 1.06495571\n",
      "Trained batch 1064 batch loss 1.12900269 epoch total loss 1.06501603\n",
      "Trained batch 1065 batch loss 1.04616058 epoch total loss 1.06499827\n",
      "Trained batch 1066 batch loss 1.09657979 epoch total loss 1.06502783\n",
      "Trained batch 1067 batch loss 1.03468394 epoch total loss 1.06499946\n",
      "Trained batch 1068 batch loss 0.876621604 epoch total loss 1.06482303\n",
      "Trained batch 1069 batch loss 1.02916765 epoch total loss 1.06478965\n",
      "Trained batch 1070 batch loss 0.896333575 epoch total loss 1.0646323\n",
      "Trained batch 1071 batch loss 1.02513266 epoch total loss 1.06459534\n",
      "Trained batch 1072 batch loss 0.999316931 epoch total loss 1.06453443\n",
      "Trained batch 1073 batch loss 1.01694727 epoch total loss 1.06449008\n",
      "Trained batch 1074 batch loss 0.922316 epoch total loss 1.06435776\n",
      "Trained batch 1075 batch loss 1.01400399 epoch total loss 1.06431091\n",
      "Trained batch 1076 batch loss 0.993069768 epoch total loss 1.06424475\n",
      "Trained batch 1077 batch loss 1.08062065 epoch total loss 1.06425989\n",
      "Trained batch 1078 batch loss 1.12259769 epoch total loss 1.06431401\n",
      "Trained batch 1079 batch loss 1.17055178 epoch total loss 1.06441236\n",
      "Trained batch 1080 batch loss 1.00452948 epoch total loss 1.06435692\n",
      "Trained batch 1081 batch loss 1.02662933 epoch total loss 1.06432199\n",
      "Trained batch 1082 batch loss 1.07070506 epoch total loss 1.06432796\n",
      "Trained batch 1083 batch loss 0.954392731 epoch total loss 1.06422639\n",
      "Trained batch 1084 batch loss 1.01127255 epoch total loss 1.06417739\n",
      "Trained batch 1085 batch loss 1.08241558 epoch total loss 1.0641942\n",
      "Trained batch 1086 batch loss 1.09339988 epoch total loss 1.06422114\n",
      "Trained batch 1087 batch loss 1.23729253 epoch total loss 1.06438041\n",
      "Trained batch 1088 batch loss 0.976235032 epoch total loss 1.06429935\n",
      "Trained batch 1089 batch loss 1.09877944 epoch total loss 1.06433094\n",
      "Trained batch 1090 batch loss 1.02162266 epoch total loss 1.06429172\n",
      "Trained batch 1091 batch loss 1.04830885 epoch total loss 1.06427717\n",
      "Trained batch 1092 batch loss 1.0197084 epoch total loss 1.06423628\n",
      "Trained batch 1093 batch loss 1.0423491 epoch total loss 1.06421626\n",
      "Trained batch 1094 batch loss 1.32804155 epoch total loss 1.06445742\n",
      "Trained batch 1095 batch loss 1.10400248 epoch total loss 1.06449354\n",
      "Trained batch 1096 batch loss 1.15344238 epoch total loss 1.0645746\n",
      "Trained batch 1097 batch loss 1.14709401 epoch total loss 1.06464982\n",
      "Trained batch 1098 batch loss 1.19965887 epoch total loss 1.06477284\n",
      "Trained batch 1099 batch loss 1.22373259 epoch total loss 1.06491756\n",
      "Trained batch 1100 batch loss 1.07503438 epoch total loss 1.06492674\n",
      "Trained batch 1101 batch loss 1.15498185 epoch total loss 1.06500864\n",
      "Trained batch 1102 batch loss 1.02022016 epoch total loss 1.06496799\n",
      "Trained batch 1103 batch loss 1.09177589 epoch total loss 1.06499231\n",
      "Trained batch 1104 batch loss 0.920362234 epoch total loss 1.0648613\n",
      "Trained batch 1105 batch loss 0.953071952 epoch total loss 1.06476021\n",
      "Trained batch 1106 batch loss 1.09865785 epoch total loss 1.06479084\n",
      "Trained batch 1107 batch loss 1.15282714 epoch total loss 1.06487036\n",
      "Trained batch 1108 batch loss 1.07089162 epoch total loss 1.06487584\n",
      "Trained batch 1109 batch loss 1.2035718 epoch total loss 1.06500101\n",
      "Trained batch 1110 batch loss 1.06233847 epoch total loss 1.06499863\n",
      "Trained batch 1111 batch loss 1.01775801 epoch total loss 1.06495607\n",
      "Trained batch 1112 batch loss 0.962453485 epoch total loss 1.0648638\n",
      "Trained batch 1113 batch loss 0.951200604 epoch total loss 1.06476164\n",
      "Trained batch 1114 batch loss 1.07302153 epoch total loss 1.06476903\n",
      "Trained batch 1115 batch loss 1.03479862 epoch total loss 1.06474221\n",
      "Trained batch 1116 batch loss 1.10555887 epoch total loss 1.0647788\n",
      "Trained batch 1117 batch loss 1.10949934 epoch total loss 1.06481874\n",
      "Trained batch 1118 batch loss 1.0251627 epoch total loss 1.06478333\n",
      "Trained batch 1119 batch loss 1.07325506 epoch total loss 1.06479084\n",
      "Trained batch 1120 batch loss 1.05003405 epoch total loss 1.06477773\n",
      "Trained batch 1121 batch loss 0.976365805 epoch total loss 1.06469882\n",
      "Trained batch 1122 batch loss 1.09820104 epoch total loss 1.06472862\n",
      "Trained batch 1123 batch loss 1.05628538 epoch total loss 1.06472111\n",
      "Trained batch 1124 batch loss 1.02604985 epoch total loss 1.06468666\n",
      "Trained batch 1125 batch loss 1.04949737 epoch total loss 1.06467307\n",
      "Trained batch 1126 batch loss 0.987924397 epoch total loss 1.06460488\n",
      "Trained batch 1127 batch loss 0.92382735 epoch total loss 1.06448\n",
      "Trained batch 1128 batch loss 0.898196518 epoch total loss 1.0643326\n",
      "Trained batch 1129 batch loss 1.00712585 epoch total loss 1.06428182\n",
      "Trained batch 1130 batch loss 0.964520216 epoch total loss 1.06419349\n",
      "Trained batch 1131 batch loss 0.91234833 epoch total loss 1.06405926\n",
      "Trained batch 1132 batch loss 0.963689148 epoch total loss 1.06397069\n",
      "Trained batch 1133 batch loss 0.9552266 epoch total loss 1.06387472\n",
      "Trained batch 1134 batch loss 0.867877483 epoch total loss 1.06370187\n",
      "Trained batch 1135 batch loss 1.03879654 epoch total loss 1.06367993\n",
      "Trained batch 1136 batch loss 0.985717773 epoch total loss 1.06361127\n",
      "Trained batch 1137 batch loss 0.828505874 epoch total loss 1.06340456\n",
      "Trained batch 1138 batch loss 0.901074588 epoch total loss 1.06326187\n",
      "Trained batch 1139 batch loss 0.997450829 epoch total loss 1.06320417\n",
      "Trained batch 1140 batch loss 0.89644593 epoch total loss 1.0630579\n",
      "Trained batch 1141 batch loss 1.167557 epoch total loss 1.06314945\n",
      "Trained batch 1142 batch loss 1.17544258 epoch total loss 1.0632478\n",
      "Trained batch 1143 batch loss 1.17246962 epoch total loss 1.06334341\n",
      "Trained batch 1144 batch loss 0.966116309 epoch total loss 1.06325841\n",
      "Trained batch 1145 batch loss 1.03483784 epoch total loss 1.06323349\n",
      "Trained batch 1146 batch loss 1.14068294 epoch total loss 1.06330097\n",
      "Trained batch 1147 batch loss 1.12908471 epoch total loss 1.06335831\n",
      "Trained batch 1148 batch loss 0.9635548 epoch total loss 1.06327128\n",
      "Trained batch 1149 batch loss 0.993704 epoch total loss 1.06321073\n",
      "Trained batch 1150 batch loss 0.908605158 epoch total loss 1.06307626\n",
      "Trained batch 1151 batch loss 1.19568813 epoch total loss 1.06319153\n",
      "Trained batch 1152 batch loss 1.12061071 epoch total loss 1.06324136\n",
      "Trained batch 1153 batch loss 1.05066848 epoch total loss 1.0632304\n",
      "Trained batch 1154 batch loss 0.895959616 epoch total loss 1.06308544\n",
      "Trained batch 1155 batch loss 1.0462575 epoch total loss 1.06307089\n",
      "Trained batch 1156 batch loss 1.15583789 epoch total loss 1.06315124\n",
      "Trained batch 1157 batch loss 1.11997914 epoch total loss 1.06320035\n",
      "Trained batch 1158 batch loss 1.08748388 epoch total loss 1.06322134\n",
      "Trained batch 1159 batch loss 1.14903641 epoch total loss 1.06329536\n",
      "Trained batch 1160 batch loss 1.3027097 epoch total loss 1.06350183\n",
      "Trained batch 1161 batch loss 1.11670887 epoch total loss 1.06354761\n",
      "Trained batch 1162 batch loss 1.1578964 epoch total loss 1.06362879\n",
      "Trained batch 1163 batch loss 1.23907304 epoch total loss 1.06377959\n",
      "Trained batch 1164 batch loss 1.20018864 epoch total loss 1.06389678\n",
      "Trained batch 1165 batch loss 1.07120061 epoch total loss 1.06390297\n",
      "Trained batch 1166 batch loss 0.989657164 epoch total loss 1.06383932\n",
      "Trained batch 1167 batch loss 0.945189953 epoch total loss 1.06373763\n",
      "Trained batch 1168 batch loss 0.997759521 epoch total loss 1.06368124\n",
      "Trained batch 1169 batch loss 1.04315901 epoch total loss 1.06366372\n",
      "Trained batch 1170 batch loss 1.0120883 epoch total loss 1.06361961\n",
      "Trained batch 1171 batch loss 0.91948688 epoch total loss 1.06349647\n",
      "Trained batch 1172 batch loss 0.936840594 epoch total loss 1.06338847\n",
      "Trained batch 1173 batch loss 0.910358846 epoch total loss 1.06325805\n",
      "Trained batch 1174 batch loss 0.930746257 epoch total loss 1.06314516\n",
      "Trained batch 1175 batch loss 0.993264794 epoch total loss 1.06308568\n",
      "Trained batch 1176 batch loss 1.00296748 epoch total loss 1.06303453\n",
      "Trained batch 1177 batch loss 0.91544652 epoch total loss 1.06290913\n",
      "Trained batch 1178 batch loss 0.969823837 epoch total loss 1.06283009\n",
      "Trained batch 1179 batch loss 1.06647563 epoch total loss 1.06283331\n",
      "Trained batch 1180 batch loss 0.956304967 epoch total loss 1.06274295\n",
      "Trained batch 1181 batch loss 1.06077456 epoch total loss 1.0627414\n",
      "Trained batch 1182 batch loss 0.946128428 epoch total loss 1.06264269\n",
      "Trained batch 1183 batch loss 0.880880654 epoch total loss 1.06248903\n",
      "Trained batch 1184 batch loss 0.922385812 epoch total loss 1.06237066\n",
      "Trained batch 1185 batch loss 0.964770198 epoch total loss 1.06228828\n",
      "Trained batch 1186 batch loss 1.08340096 epoch total loss 1.06230605\n",
      "Trained batch 1187 batch loss 1.0711931 epoch total loss 1.06231356\n",
      "Trained batch 1188 batch loss 1.31447601 epoch total loss 1.06252575\n",
      "Trained batch 1189 batch loss 1.18856049 epoch total loss 1.06263185\n",
      "Trained batch 1190 batch loss 1.02228022 epoch total loss 1.06259799\n",
      "Trained batch 1191 batch loss 1.13051021 epoch total loss 1.06265497\n",
      "Trained batch 1192 batch loss 1.05911803 epoch total loss 1.06265199\n",
      "Trained batch 1193 batch loss 0.969732285 epoch total loss 1.06257403\n",
      "Trained batch 1194 batch loss 1.00084615 epoch total loss 1.06252241\n",
      "Trained batch 1195 batch loss 1.11648047 epoch total loss 1.06256747\n",
      "Trained batch 1196 batch loss 0.946617186 epoch total loss 1.06247056\n",
      "Trained batch 1197 batch loss 0.942367315 epoch total loss 1.0623703\n",
      "Trained batch 1198 batch loss 0.739571 epoch total loss 1.06210089\n",
      "Trained batch 1199 batch loss 1.04057145 epoch total loss 1.06208289\n",
      "Trained batch 1200 batch loss 0.996419907 epoch total loss 1.06202817\n",
      "Trained batch 1201 batch loss 1.18995953 epoch total loss 1.06213474\n",
      "Trained batch 1202 batch loss 1.19464195 epoch total loss 1.06224501\n",
      "Trained batch 1203 batch loss 1.23401165 epoch total loss 1.06238782\n",
      "Trained batch 1204 batch loss 1.21982777 epoch total loss 1.0625186\n",
      "Trained batch 1205 batch loss 1.18121111 epoch total loss 1.06261706\n",
      "Trained batch 1206 batch loss 1.11971152 epoch total loss 1.06266439\n",
      "Trained batch 1207 batch loss 0.984199405 epoch total loss 1.06259942\n",
      "Trained batch 1208 batch loss 1.00839865 epoch total loss 1.0625546\n",
      "Trained batch 1209 batch loss 1.04846358 epoch total loss 1.06254292\n",
      "Trained batch 1210 batch loss 0.914563656 epoch total loss 1.06242061\n",
      "Trained batch 1211 batch loss 1.14917719 epoch total loss 1.06249225\n",
      "Trained batch 1212 batch loss 1.0870626 epoch total loss 1.06251252\n",
      "Trained batch 1213 batch loss 1.14696205 epoch total loss 1.06258214\n",
      "Trained batch 1214 batch loss 1.11071038 epoch total loss 1.06262171\n",
      "Trained batch 1215 batch loss 1.32847428 epoch total loss 1.06284058\n",
      "Trained batch 1216 batch loss 1.19002271 epoch total loss 1.06294525\n",
      "Trained batch 1217 batch loss 1.21708298 epoch total loss 1.06307185\n",
      "Trained batch 1218 batch loss 1.12498283 epoch total loss 1.06312263\n",
      "Trained batch 1219 batch loss 0.983132362 epoch total loss 1.06305707\n",
      "Trained batch 1220 batch loss 1.12953091 epoch total loss 1.06311154\n",
      "Trained batch 1221 batch loss 1.09877801 epoch total loss 1.06314075\n",
      "Trained batch 1222 batch loss 0.990488291 epoch total loss 1.06308126\n",
      "Trained batch 1223 batch loss 0.925761461 epoch total loss 1.06296897\n",
      "Trained batch 1224 batch loss 0.913428545 epoch total loss 1.0628469\n",
      "Trained batch 1225 batch loss 1.02913475 epoch total loss 1.06281936\n",
      "Trained batch 1226 batch loss 0.970165372 epoch total loss 1.06274378\n",
      "Trained batch 1227 batch loss 0.863270879 epoch total loss 1.0625813\n",
      "Trained batch 1228 batch loss 1.03983927 epoch total loss 1.0625627\n",
      "Trained batch 1229 batch loss 1.08848834 epoch total loss 1.0625838\n",
      "Trained batch 1230 batch loss 1.10322285 epoch total loss 1.06261694\n",
      "Trained batch 1231 batch loss 1.04442632 epoch total loss 1.06260216\n",
      "Trained batch 1232 batch loss 1.02343547 epoch total loss 1.06257033\n",
      "Trained batch 1233 batch loss 1.13195491 epoch total loss 1.0626266\n",
      "Trained batch 1234 batch loss 1.14776683 epoch total loss 1.06269562\n",
      "Trained batch 1235 batch loss 1.07539582 epoch total loss 1.06270599\n",
      "Trained batch 1236 batch loss 1.14346886 epoch total loss 1.06277132\n",
      "Trained batch 1237 batch loss 1.03120375 epoch total loss 1.06274581\n",
      "Trained batch 1238 batch loss 1.08466327 epoch total loss 1.06276357\n",
      "Trained batch 1239 batch loss 1.07869422 epoch total loss 1.06277645\n",
      "Trained batch 1240 batch loss 1.18271172 epoch total loss 1.06287324\n",
      "Trained batch 1241 batch loss 1.28687537 epoch total loss 1.06305373\n",
      "Trained batch 1242 batch loss 1.33314729 epoch total loss 1.06327116\n",
      "Trained batch 1243 batch loss 1.12743151 epoch total loss 1.06332278\n",
      "Trained batch 1244 batch loss 1.07827258 epoch total loss 1.06333482\n",
      "Trained batch 1245 batch loss 1.11828756 epoch total loss 1.06337893\n",
      "Trained batch 1246 batch loss 1.12401104 epoch total loss 1.06342757\n",
      "Trained batch 1247 batch loss 1.06041551 epoch total loss 1.06342518\n",
      "Trained batch 1248 batch loss 1.1727736 epoch total loss 1.0635128\n",
      "Trained batch 1249 batch loss 1.18934703 epoch total loss 1.06361353\n",
      "Trained batch 1250 batch loss 1.27174807 epoch total loss 1.06378\n",
      "Trained batch 1251 batch loss 1.23283863 epoch total loss 1.06391513\n",
      "Trained batch 1252 batch loss 1.27198052 epoch total loss 1.06408131\n",
      "Trained batch 1253 batch loss 1.27955377 epoch total loss 1.06425321\n",
      "Trained batch 1254 batch loss 1.29007804 epoch total loss 1.06443322\n",
      "Trained batch 1255 batch loss 1.19597638 epoch total loss 1.064538\n",
      "Trained batch 1256 batch loss 1.13637185 epoch total loss 1.06459522\n",
      "Trained batch 1257 batch loss 1.19947243 epoch total loss 1.06470251\n",
      "Trained batch 1258 batch loss 1.17836523 epoch total loss 1.06479287\n",
      "Trained batch 1259 batch loss 1.06225204 epoch total loss 1.06479084\n",
      "Trained batch 1260 batch loss 1.09697056 epoch total loss 1.06481636\n",
      "Trained batch 1261 batch loss 1.18650651 epoch total loss 1.0649128\n",
      "Trained batch 1262 batch loss 1.14651966 epoch total loss 1.06497753\n",
      "Trained batch 1263 batch loss 1.05804801 epoch total loss 1.06497204\n",
      "Trained batch 1264 batch loss 1.080621 epoch total loss 1.06498444\n",
      "Trained batch 1265 batch loss 0.958868206 epoch total loss 1.06490052\n",
      "Trained batch 1266 batch loss 0.883058071 epoch total loss 1.06475687\n",
      "Trained batch 1267 batch loss 0.99270463 epoch total loss 1.0647\n",
      "Trained batch 1268 batch loss 1.05797076 epoch total loss 1.06469464\n",
      "Trained batch 1269 batch loss 1.05740285 epoch total loss 1.06468892\n",
      "Trained batch 1270 batch loss 1.02006698 epoch total loss 1.06465375\n",
      "Trained batch 1271 batch loss 0.903805196 epoch total loss 1.06452715\n",
      "Trained batch 1272 batch loss 0.990196764 epoch total loss 1.06446874\n",
      "Trained batch 1273 batch loss 1.31371522 epoch total loss 1.0646646\n",
      "Trained batch 1274 batch loss 1.09604573 epoch total loss 1.06468916\n",
      "Trained batch 1275 batch loss 0.98094666 epoch total loss 1.06462359\n",
      "Trained batch 1276 batch loss 1.03590775 epoch total loss 1.06460106\n",
      "Trained batch 1277 batch loss 0.812036157 epoch total loss 1.06440318\n",
      "Trained batch 1278 batch loss 0.678841889 epoch total loss 1.06410158\n",
      "Trained batch 1279 batch loss 0.694883883 epoch total loss 1.06381285\n",
      "Trained batch 1280 batch loss 0.90259409 epoch total loss 1.06368685\n",
      "Trained batch 1281 batch loss 0.982097447 epoch total loss 1.06362307\n",
      "Trained batch 1282 batch loss 1.06143248 epoch total loss 1.0636214\n",
      "Trained batch 1283 batch loss 1.14063025 epoch total loss 1.06368136\n",
      "Trained batch 1284 batch loss 1.17513943 epoch total loss 1.06376827\n",
      "Trained batch 1285 batch loss 1.24709916 epoch total loss 1.06391084\n",
      "Trained batch 1286 batch loss 1.24280202 epoch total loss 1.06405\n",
      "Trained batch 1287 batch loss 1.06071925 epoch total loss 1.06404734\n",
      "Trained batch 1288 batch loss 0.951355159 epoch total loss 1.06396\n",
      "Trained batch 1289 batch loss 0.866578579 epoch total loss 1.06380677\n",
      "Trained batch 1290 batch loss 0.905370176 epoch total loss 1.06368399\n",
      "Trained batch 1291 batch loss 1.03710794 epoch total loss 1.06366336\n",
      "Trained batch 1292 batch loss 0.913126707 epoch total loss 1.0635469\n",
      "Trained batch 1293 batch loss 0.752238452 epoch total loss 1.06330609\n",
      "Trained batch 1294 batch loss 0.77263248 epoch total loss 1.06308138\n",
      "Trained batch 1295 batch loss 0.810162961 epoch total loss 1.06288612\n",
      "Trained batch 1296 batch loss 0.905407667 epoch total loss 1.06276453\n",
      "Trained batch 1297 batch loss 0.931618333 epoch total loss 1.06266344\n",
      "Trained batch 1298 batch loss 0.966123641 epoch total loss 1.06258905\n",
      "Trained batch 1299 batch loss 0.930829823 epoch total loss 1.0624876\n",
      "Trained batch 1300 batch loss 1.02244294 epoch total loss 1.06245685\n",
      "Trained batch 1301 batch loss 1.0900135 epoch total loss 1.06247795\n",
      "Trained batch 1302 batch loss 0.898515165 epoch total loss 1.06235206\n",
      "Trained batch 1303 batch loss 1.03586364 epoch total loss 1.0623318\n",
      "Trained batch 1304 batch loss 0.99409 epoch total loss 1.06227946\n",
      "Trained batch 1305 batch loss 0.981184721 epoch total loss 1.06221735\n",
      "Trained batch 1306 batch loss 1.04814589 epoch total loss 1.06220651\n",
      "Trained batch 1307 batch loss 1.04323053 epoch total loss 1.06219196\n",
      "Trained batch 1308 batch loss 1.12926352 epoch total loss 1.06224322\n",
      "Trained batch 1309 batch loss 1.20633841 epoch total loss 1.06235325\n",
      "Trained batch 1310 batch loss 1.13760495 epoch total loss 1.06241071\n",
      "Trained batch 1311 batch loss 1.04030573 epoch total loss 1.0623939\n",
      "Trained batch 1312 batch loss 1.16727734 epoch total loss 1.06247377\n",
      "Trained batch 1313 batch loss 1.00083852 epoch total loss 1.06242681\n",
      "Trained batch 1314 batch loss 1.09559429 epoch total loss 1.06245208\n",
      "Trained batch 1315 batch loss 1.17674279 epoch total loss 1.06253898\n",
      "Trained batch 1316 batch loss 1.15104544 epoch total loss 1.06260622\n",
      "Trained batch 1317 batch loss 1.07258213 epoch total loss 1.06261384\n",
      "Trained batch 1318 batch loss 1.10618615 epoch total loss 1.06264687\n",
      "Trained batch 1319 batch loss 1.18681347 epoch total loss 1.06274104\n",
      "Trained batch 1320 batch loss 1.24354076 epoch total loss 1.06287801\n",
      "Trained batch 1321 batch loss 1.08605218 epoch total loss 1.06289554\n",
      "Trained batch 1322 batch loss 1.13987279 epoch total loss 1.06295371\n",
      "Trained batch 1323 batch loss 1.09516883 epoch total loss 1.06297815\n",
      "Trained batch 1324 batch loss 1.12287378 epoch total loss 1.06302345\n",
      "Trained batch 1325 batch loss 1.10099936 epoch total loss 1.06305206\n",
      "Trained batch 1326 batch loss 1.12997246 epoch total loss 1.06310248\n",
      "Trained batch 1327 batch loss 1.13365841 epoch total loss 1.06315565\n",
      "Trained batch 1328 batch loss 1.159832 epoch total loss 1.06322849\n",
      "Trained batch 1329 batch loss 1.07747364 epoch total loss 1.06323922\n",
      "Trained batch 1330 batch loss 1.00343943 epoch total loss 1.06319427\n",
      "Trained batch 1331 batch loss 1.04956508 epoch total loss 1.06318402\n",
      "Trained batch 1332 batch loss 0.904910803 epoch total loss 1.06306517\n",
      "Trained batch 1333 batch loss 0.939579129 epoch total loss 1.06297255\n",
      "Trained batch 1334 batch loss 0.911187291 epoch total loss 1.0628587\n",
      "Trained batch 1335 batch loss 0.87121439 epoch total loss 1.06271517\n",
      "Trained batch 1336 batch loss 0.839191794 epoch total loss 1.06254792\n",
      "Trained batch 1337 batch loss 0.740621 epoch total loss 1.06230712\n",
      "Trained batch 1338 batch loss 0.796442866 epoch total loss 1.06210828\n",
      "Trained batch 1339 batch loss 0.818961322 epoch total loss 1.06192672\n",
      "Trained batch 1340 batch loss 0.852471113 epoch total loss 1.06177044\n",
      "Trained batch 1341 batch loss 0.954728 epoch total loss 1.06169057\n",
      "Trained batch 1342 batch loss 0.992051 epoch total loss 1.06163871\n",
      "Trained batch 1343 batch loss 1.0366621 epoch total loss 1.06162012\n",
      "Trained batch 1344 batch loss 0.989044964 epoch total loss 1.061566\n",
      "Trained batch 1345 batch loss 1.02797079 epoch total loss 1.06154108\n",
      "Trained batch 1346 batch loss 0.989506602 epoch total loss 1.06148756\n",
      "Trained batch 1347 batch loss 1.02951884 epoch total loss 1.06146383\n",
      "Trained batch 1348 batch loss 0.98557049 epoch total loss 1.06140757\n",
      "Trained batch 1349 batch loss 1.00947595 epoch total loss 1.06136906\n",
      "Trained batch 1350 batch loss 0.984353185 epoch total loss 1.06131208\n",
      "Trained batch 1351 batch loss 1.03278327 epoch total loss 1.06129098\n",
      "Trained batch 1352 batch loss 0.99096334 epoch total loss 1.06123888\n",
      "Trained batch 1353 batch loss 1.00805938 epoch total loss 1.06119967\n",
      "Trained batch 1354 batch loss 0.888486385 epoch total loss 1.06107199\n",
      "Trained batch 1355 batch loss 0.774516881 epoch total loss 1.06086051\n",
      "Trained batch 1356 batch loss 0.994548559 epoch total loss 1.06081164\n",
      "Trained batch 1357 batch loss 1.12722194 epoch total loss 1.06086051\n",
      "Trained batch 1358 batch loss 1.09493244 epoch total loss 1.06088567\n",
      "Trained batch 1359 batch loss 1.06064427 epoch total loss 1.06088555\n",
      "Trained batch 1360 batch loss 1.02334952 epoch total loss 1.06085789\n",
      "Trained batch 1361 batch loss 0.92567 epoch total loss 1.06075859\n",
      "Trained batch 1362 batch loss 0.936391592 epoch total loss 1.06066728\n",
      "Trained batch 1363 batch loss 1.01657 epoch total loss 1.06063497\n",
      "Trained batch 1364 batch loss 0.956817091 epoch total loss 1.0605588\n",
      "Trained batch 1365 batch loss 1.15526652 epoch total loss 1.06062818\n",
      "Trained batch 1366 batch loss 1.04595518 epoch total loss 1.06061733\n",
      "Trained batch 1367 batch loss 1.05871534 epoch total loss 1.06061602\n",
      "Trained batch 1368 batch loss 1.11295128 epoch total loss 1.06065416\n",
      "Trained batch 1369 batch loss 1.10114658 epoch total loss 1.06068385\n",
      "Trained batch 1370 batch loss 1.21826041 epoch total loss 1.06079888\n",
      "Trained batch 1371 batch loss 1.08174 epoch total loss 1.06081414\n",
      "Trained batch 1372 batch loss 1.09518862 epoch total loss 1.0608393\n",
      "Trained batch 1373 batch loss 1.03414273 epoch total loss 1.06081986\n",
      "Trained batch 1374 batch loss 1.10662746 epoch total loss 1.06085312\n",
      "Trained batch 1375 batch loss 1.10242176 epoch total loss 1.0608834\n",
      "Trained batch 1376 batch loss 0.978753626 epoch total loss 1.06082368\n",
      "Trained batch 1377 batch loss 0.93727982 epoch total loss 1.06073391\n",
      "Trained batch 1378 batch loss 1.0635848 epoch total loss 1.06073606\n",
      "Trained batch 1379 batch loss 1.06639493 epoch total loss 1.06074011\n",
      "Trained batch 1380 batch loss 0.957811475 epoch total loss 1.06066549\n",
      "Trained batch 1381 batch loss 0.972376823 epoch total loss 1.06060159\n",
      "Trained batch 1382 batch loss 0.8665483 epoch total loss 1.06046116\n",
      "Trained batch 1383 batch loss 0.943156 epoch total loss 1.06037641\n",
      "Trained batch 1384 batch loss 1.00896728 epoch total loss 1.06033921\n",
      "Trained batch 1385 batch loss 1.10229897 epoch total loss 1.06036949\n",
      "Trained batch 1386 batch loss 1.09510446 epoch total loss 1.06039453\n",
      "Trained batch 1387 batch loss 1.09058011 epoch total loss 1.06041622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 04:42:07.763305: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "2025-03-19 04:42:07.763367: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "\t [[RemoteCall]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1388 batch loss 1.08432233 epoch total loss 1.06043351\n",
      "Epoch 10 train loss 1.0604335069656372\n",
      "Validated batch 1 batch loss 1.12879705\n",
      "Validated batch 2 batch loss 1.11021161\n",
      "Validated batch 3 batch loss 1.09916568\n",
      "Validated batch 4 batch loss 1.12922525\n",
      "Validated batch 5 batch loss 1.12915194\n",
      "Validated batch 6 batch loss 1.22613382\n",
      "Validated batch 7 batch loss 1.26075101\n",
      "Validated batch 8 batch loss 1.22715819\n",
      "Validated batch 9 batch loss 1.15903318\n",
      "Validated batch 10 batch loss 1.03953409\n",
      "Validated batch 11 batch loss 1.14611053\n",
      "Validated batch 12 batch loss 1.1316216\n",
      "Validated batch 13 batch loss 1.15420461\n",
      "Validated batch 14 batch loss 1.2938664\n",
      "Validated batch 15 batch loss 1.24566758\n",
      "Validated batch 16 batch loss 1.14908266\n",
      "Validated batch 17 batch loss 1.28761435\n",
      "Validated batch 18 batch loss 1.1075114\n",
      "Validated batch 19 batch loss 1.23579097\n",
      "Validated batch 20 batch loss 0.870289147\n",
      "Validated batch 21 batch loss 1.10891378\n",
      "Validated batch 22 batch loss 1.19436431\n",
      "Validated batch 23 batch loss 0.959813297\n",
      "Validated batch 24 batch loss 1.17470169\n",
      "Validated batch 25 batch loss 1.08163643\n",
      "Validated batch 26 batch loss 1.10326302\n",
      "Validated batch 27 batch loss 1.10401738\n",
      "Validated batch 28 batch loss 1.09347296\n",
      "Validated batch 29 batch loss 1.22513974\n",
      "Validated batch 30 batch loss 1.16577458\n",
      "Validated batch 31 batch loss 1.16266775\n",
      "Validated batch 32 batch loss 1.19648111\n",
      "Validated batch 33 batch loss 1.19696558\n",
      "Validated batch 34 batch loss 1.00993228\n",
      "Validated batch 35 batch loss 1.01254165\n",
      "Validated batch 36 batch loss 1.15456176\n",
      "Validated batch 37 batch loss 1.14266038\n",
      "Validated batch 38 batch loss 1.31300271\n",
      "Validated batch 39 batch loss 1.25897658\n",
      "Validated batch 40 batch loss 1.16917884\n",
      "Validated batch 41 batch loss 1.29964602\n",
      "Validated batch 42 batch loss 1.16513228\n",
      "Validated batch 43 batch loss 1.1677897\n",
      "Validated batch 44 batch loss 1.13472414\n",
      "Validated batch 45 batch loss 0.882993\n",
      "Validated batch 46 batch loss 1.07210445\n",
      "Validated batch 47 batch loss 1.07847321\n",
      "Validated batch 48 batch loss 1.04334092\n",
      "Validated batch 49 batch loss 1.06961071\n",
      "Validated batch 50 batch loss 1.02038765\n",
      "Validated batch 51 batch loss 1.08989906\n",
      "Validated batch 52 batch loss 1.13029051\n",
      "Validated batch 53 batch loss 1.14669311\n",
      "Validated batch 54 batch loss 1.1166544\n",
      "Validated batch 55 batch loss 1.12691414\n",
      "Validated batch 56 batch loss 1.06758821\n",
      "Validated batch 57 batch loss 1.09777355\n",
      "Validated batch 58 batch loss 1.19026804\n",
      "Validated batch 59 batch loss 1.08640122\n",
      "Validated batch 60 batch loss 1.09083343\n",
      "Validated batch 61 batch loss 1.12198985\n",
      "Validated batch 62 batch loss 1.06933904\n",
      "Validated batch 63 batch loss 1.29067326\n",
      "Validated batch 64 batch loss 1.16513848\n",
      "Validated batch 65 batch loss 0.994434714\n",
      "Validated batch 66 batch loss 1.19681394\n",
      "Validated batch 67 batch loss 1.11880302\n",
      "Validated batch 68 batch loss 0.985362351\n",
      "Validated batch 69 batch loss 1.1086657\n",
      "Validated batch 70 batch loss 1.04361391\n",
      "Validated batch 71 batch loss 1.11455429\n",
      "Validated batch 72 batch loss 1.11406326\n",
      "Validated batch 73 batch loss 1.08077717\n",
      "Validated batch 74 batch loss 1.10859036\n",
      "Validated batch 75 batch loss 1.25540185\n",
      "Validated batch 76 batch loss 1.01213527\n",
      "Validated batch 77 batch loss 0.979863584\n",
      "Validated batch 78 batch loss 1.01974463\n",
      "Validated batch 79 batch loss 1.04356551\n",
      "Validated batch 80 batch loss 0.944916248\n",
      "Validated batch 81 batch loss 1.11455894\n",
      "Validated batch 82 batch loss 1.03082502\n",
      "Validated batch 83 batch loss 1.04159641\n",
      "Validated batch 84 batch loss 1.25695288\n",
      "Validated batch 85 batch loss 1.18467402\n",
      "Validated batch 86 batch loss 1.03272128\n",
      "Validated batch 87 batch loss 1.21073794\n",
      "Validated batch 88 batch loss 0.880805075\n",
      "Validated batch 89 batch loss 1.02505875\n",
      "Validated batch 90 batch loss 1.0310266\n",
      "Validated batch 91 batch loss 1.00658059\n",
      "Validated batch 92 batch loss 1.2755475\n",
      "Validated batch 93 batch loss 1.23075008\n",
      "Validated batch 94 batch loss 1.09690642\n",
      "Validated batch 95 batch loss 1.06888521\n",
      "Validated batch 96 batch loss 1.18499398\n",
      "Validated batch 97 batch loss 1.06230497\n",
      "Validated batch 98 batch loss 1.1424377\n",
      "Validated batch 99 batch loss 1.16910505\n",
      "Validated batch 100 batch loss 1.09419787\n",
      "Validated batch 101 batch loss 1.13628483\n",
      "Validated batch 102 batch loss 1.11870182\n",
      "Validated batch 103 batch loss 1.21229482\n",
      "Validated batch 104 batch loss 1.21250796\n",
      "Validated batch 105 batch loss 1.1067065\n",
      "Validated batch 106 batch loss 1.04123795\n",
      "Validated batch 107 batch loss 0.998182237\n",
      "Validated batch 108 batch loss 1.12844586\n",
      "Validated batch 109 batch loss 1.01581955\n",
      "Validated batch 110 batch loss 1.14732945\n",
      "Validated batch 111 batch loss 1.11703062\n",
      "Validated batch 112 batch loss 1.25276148\n",
      "Validated batch 113 batch loss 1.18608475\n",
      "Validated batch 114 batch loss 1.11780953\n",
      "Validated batch 115 batch loss 1.03173983\n",
      "Validated batch 116 batch loss 0.999499381\n",
      "Validated batch 117 batch loss 1.07783\n",
      "Validated batch 118 batch loss 1.04452205\n",
      "Validated batch 119 batch loss 1.0282445\n",
      "Validated batch 120 batch loss 1.04361558\n",
      "Validated batch 121 batch loss 1.2023803\n",
      "Validated batch 122 batch loss 1.17585909\n",
      "Validated batch 123 batch loss 1.15701723\n",
      "Validated batch 124 batch loss 1.12249207\n",
      "Validated batch 125 batch loss 1.14416742\n",
      "Validated batch 126 batch loss 1.14002013\n",
      "Validated batch 127 batch loss 1.31920767\n",
      "Validated batch 128 batch loss 1.20142555\n",
      "Validated batch 129 batch loss 1.28006542\n",
      "Validated batch 130 batch loss 1.23055542\n",
      "Validated batch 131 batch loss 1.32600486\n",
      "Validated batch 132 batch loss 1.27074814\n",
      "Validated batch 133 batch loss 1.11301088\n",
      "Validated batch 134 batch loss 1.1895678\n",
      "Validated batch 135 batch loss 1.26678073\n",
      "Validated batch 136 batch loss 1.26834321\n",
      "Validated batch 137 batch loss 1.07256103\n",
      "Validated batch 138 batch loss 1.18426239\n",
      "Validated batch 139 batch loss 1.14689398\n",
      "Validated batch 140 batch loss 1.13764787\n",
      "Validated batch 141 batch loss 1.11916041\n",
      "Validated batch 142 batch loss 1.10804033\n",
      "Validated batch 143 batch loss 1.2465415\n",
      "Validated batch 144 batch loss 1.33539701\n",
      "Validated batch 145 batch loss 1.12759638\n",
      "Validated batch 146 batch loss 1.19861078\n",
      "Validated batch 147 batch loss 1.04955029\n",
      "Validated batch 148 batch loss 1.26107347\n",
      "Validated batch 149 batch loss 1.21069622\n",
      "Validated batch 150 batch loss 1.14332914\n",
      "Validated batch 151 batch loss 1.25987792\n",
      "Validated batch 152 batch loss 1.16433525\n",
      "Validated batch 153 batch loss 1.23026013\n",
      "Validated batch 154 batch loss 1.22843\n",
      "Validated batch 155 batch loss 1.21664369\n",
      "Validated batch 156 batch loss 1.1307714\n",
      "Validated batch 157 batch loss 1.12134385\n",
      "Validated batch 158 batch loss 1.17859626\n",
      "Validated batch 159 batch loss 1.10494685\n",
      "Validated batch 160 batch loss 1.16981959\n",
      "Validated batch 161 batch loss 1.07738638\n",
      "Validated batch 162 batch loss 1.14596343\n",
      "Validated batch 163 batch loss 1.00397778\n",
      "Validated batch 164 batch loss 1.19141197\n",
      "Validated batch 165 batch loss 1.06494296\n",
      "Validated batch 166 batch loss 1.09125447\n",
      "Validated batch 167 batch loss 1.34079409\n",
      "Validated batch 168 batch loss 0.985875726\n",
      "Validated batch 169 batch loss 1.11719322\n",
      "Validated batch 170 batch loss 1.08149135\n",
      "Validated batch 171 batch loss 1.14602637\n",
      "Validated batch 172 batch loss 1.16383803\n",
      "Validated batch 173 batch loss 1.0525955\n",
      "Validated batch 174 batch loss 0.829275608\n",
      "Validated batch 175 batch loss 1.08464515\n",
      "Validated batch 176 batch loss 0.99490875\n",
      "Validated batch 177 batch loss 1.01676643\n",
      "Validated batch 178 batch loss 1.1729691\n",
      "Validated batch 179 batch loss 0.884179175\n",
      "Validated batch 180 batch loss 1.1167984\n",
      "Validated batch 181 batch loss 1.2181288\n",
      "Validated batch 182 batch loss 1.13789427\n",
      "Validated batch 183 batch loss 1.0995996\n",
      "Validated batch 184 batch loss 0.960219145\n",
      "Validated batch 185 batch loss 1.10722589\n",
      "Epoch 10 val loss 1.127576231956482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 04:42:23.639123: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "2025-03-19 04:42:23.639167: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "\t [[RemoteCall]]\n"
     ]
    }
   ],
   "source": [
    "# 모델 훈련 실행\n",
    "history = train(EPOCHS, LEARNING_RATE, NUM_HEATMAP, BATCH_SIZE, train_tfrecords, val_tfrecords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Start training SimpleBaseline model...\n",
      "Start epoch 1 with learning rate 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minho/miniconda3/envs/aiffel/lib/python3.11/site-packages/tensorflow/python/data/ops/dataset_ops.py:463: UserWarning: To make it possible to preserve tf.data options across serialization boundaries, their implementation has moved to be part of the TensorFlow graph. As a consequence, the options value is in general no longer known at graph construction time. Invoking this method in graph mode retains the legacy behavior of the original implementation, but note that the returned value might not reflect the actual value of the options.\n",
      "  warnings.warn(\"To make it possible to preserve tf.data options across \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start distributed training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1742320857.883657   86312 service.cc:145] XLA service 0x762ab00171d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1742320857.883695   86312 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 4070 Ti SUPER, Compute Capability 8.9\n",
      "2025-03-19 03:00:57.904640: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "I0000 00:00:1742320858.166161   86312 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1 batch loss 9.99826908 epoch total loss 9.99826908\n",
      "Trained batch 2 batch loss 12.7043037 epoch total loss 11.3512859\n",
      "Trained batch 3 batch loss 10.3710632 epoch total loss 11.0245447\n",
      "Trained batch 4 batch loss 9.8175211 epoch total loss 10.7227888\n",
      "Trained batch 5 batch loss 9.59959507 epoch total loss 10.4981499\n",
      "Trained batch 6 batch loss 8.94616604 epoch total loss 10.2394857\n",
      "Trained batch 7 batch loss 9.16797256 epoch total loss 10.0864124\n",
      "Trained batch 8 batch loss 8.77139759 epoch total loss 9.92203617\n",
      "Trained batch 9 batch loss 9.00557232 epoch total loss 9.82020664\n",
      "Trained batch 10 batch loss 8.83065414 epoch total loss 9.72125053\n",
      "Trained batch 11 batch loss 9.2351141 epoch total loss 9.67705631\n",
      "Trained batch 12 batch loss 9.14506054 epoch total loss 9.63272381\n",
      "Trained batch 13 batch loss 8.01837921 epoch total loss 9.50854301\n",
      "Trained batch 14 batch loss 7.47291279 epoch total loss 9.36314106\n",
      "Trained batch 15 batch loss 7.86757231 epoch total loss 9.26343632\n",
      "Trained batch 16 batch loss 7.21443748 epoch total loss 9.13537312\n",
      "Trained batch 17 batch loss 7.88234901 epoch total loss 9.06166649\n",
      "Trained batch 18 batch loss 8.13227272 epoch total loss 9.01003361\n",
      "Trained batch 19 batch loss 8.10509205 epoch total loss 8.9624052\n",
      "Trained batch 20 batch loss 8.67861652 epoch total loss 8.94821548\n",
      "Trained batch 21 batch loss 8.77553558 epoch total loss 8.93999195\n",
      "Trained batch 22 batch loss 8.39983273 epoch total loss 8.91544\n",
      "Trained batch 23 batch loss 8.52337646 epoch total loss 8.89839268\n",
      "Trained batch 24 batch loss 7.85680103 epoch total loss 8.85499287\n",
      "Trained batch 25 batch loss 7.77777052 epoch total loss 8.81190395\n",
      "Trained batch 26 batch loss 8.16819668 epoch total loss 8.78714657\n",
      "Trained batch 27 batch loss 8.10321522 epoch total loss 8.76181507\n",
      "Trained batch 28 batch loss 8.41832352 epoch total loss 8.74954796\n",
      "Trained batch 29 batch loss 8.23427582 epoch total loss 8.73178\n",
      "Trained batch 30 batch loss 8.19829559 epoch total loss 8.71399784\n",
      "Trained batch 31 batch loss 7.98702383 epoch total loss 8.69054699\n",
      "Trained batch 32 batch loss 7.75588894 epoch total loss 8.66133881\n",
      "Trained batch 33 batch loss 8.03223801 epoch total loss 8.64227486\n",
      "Trained batch 34 batch loss 7.93405533 epoch total loss 8.6214447\n",
      "Trained batch 35 batch loss 7.85366297 epoch total loss 8.59950829\n",
      "Trained batch 36 batch loss 8.16605091 epoch total loss 8.58746719\n",
      "Trained batch 37 batch loss 8.52914906 epoch total loss 8.58589172\n",
      "Trained batch 38 batch loss 8.29008198 epoch total loss 8.57810688\n",
      "Trained batch 39 batch loss 7.99457 epoch total loss 8.56314373\n",
      "Trained batch 40 batch loss 7.8692584 epoch total loss 8.54579735\n",
      "Trained batch 41 batch loss 7.73426867 epoch total loss 8.52600384\n",
      "Trained batch 42 batch loss 8.01257324 epoch total loss 8.51378\n",
      "Trained batch 43 batch loss 7.30486631 epoch total loss 8.48566532\n",
      "Trained batch 44 batch loss 7.20714903 epoch total loss 8.45660782\n",
      "Trained batch 45 batch loss 6.70835781 epoch total loss 8.41775799\n",
      "Trained batch 46 batch loss 7.23927498 epoch total loss 8.39213848\n",
      "Trained batch 47 batch loss 8.14099598 epoch total loss 8.38679504\n",
      "Trained batch 48 batch loss 7.8938179 epoch total loss 8.37652493\n",
      "Trained batch 49 batch loss 7.74067497 epoch total loss 8.36354828\n",
      "Trained batch 50 batch loss 7.67939758 epoch total loss 8.34986591\n",
      "Trained batch 51 batch loss 6.98629379 epoch total loss 8.3231287\n",
      "Trained batch 52 batch loss 7.77476597 epoch total loss 8.31258392\n",
      "Trained batch 53 batch loss 8.05401802 epoch total loss 8.30770493\n",
      "Trained batch 54 batch loss 7.92924547 epoch total loss 8.30069733\n",
      "Trained batch 55 batch loss 8.04825878 epoch total loss 8.29610729\n",
      "Trained batch 56 batch loss 7.59269571 epoch total loss 8.28354549\n",
      "Trained batch 57 batch loss 7.34553242 epoch total loss 8.26708889\n",
      "Trained batch 58 batch loss 7.20530272 epoch total loss 8.24878216\n",
      "Trained batch 59 batch loss 7.61880398 epoch total loss 8.23810482\n",
      "Trained batch 60 batch loss 7.59017 epoch total loss 8.22730637\n",
      "Trained batch 61 batch loss 7.86884165 epoch total loss 8.22143\n",
      "Trained batch 62 batch loss 7.76123095 epoch total loss 8.21400738\n",
      "Trained batch 63 batch loss 7.94526386 epoch total loss 8.20974064\n",
      "Trained batch 64 batch loss 7.59195375 epoch total loss 8.2000885\n",
      "Trained batch 65 batch loss 7.70844316 epoch total loss 8.19252491\n",
      "Trained batch 66 batch loss 7.60218048 epoch total loss 8.18357944\n",
      "Trained batch 67 batch loss 7.50650787 epoch total loss 8.17347431\n",
      "Trained batch 68 batch loss 7.28389 epoch total loss 8.16039276\n",
      "Trained batch 69 batch loss 7.73291 epoch total loss 8.15419674\n",
      "Trained batch 70 batch loss 7.76578665 epoch total loss 8.14864826\n",
      "Trained batch 71 batch loss 8.02188587 epoch total loss 8.14686394\n",
      "Trained batch 72 batch loss 8.06142807 epoch total loss 8.14567661\n",
      "Trained batch 73 batch loss 6.79486418 epoch total loss 8.12717247\n",
      "Trained batch 74 batch loss 6.84109974 epoch total loss 8.10979271\n",
      "Trained batch 75 batch loss 7.0380969 epoch total loss 8.09550381\n",
      "Trained batch 76 batch loss 7.94684649 epoch total loss 8.09354782\n",
      "Trained batch 77 batch loss 7.6279726 epoch total loss 8.08750153\n",
      "Trained batch 78 batch loss 7.85688 epoch total loss 8.08454418\n",
      "Trained batch 79 batch loss 7.87480974 epoch total loss 8.08189\n",
      "Trained batch 80 batch loss 7.78924465 epoch total loss 8.07823181\n",
      "Trained batch 81 batch loss 7.63465118 epoch total loss 8.07275581\n",
      "Trained batch 82 batch loss 7.60811329 epoch total loss 8.06708908\n",
      "Trained batch 83 batch loss 7.90037251 epoch total loss 8.06508064\n",
      "Trained batch 84 batch loss 8.02276134 epoch total loss 8.0645771\n",
      "Trained batch 85 batch loss 7.85482597 epoch total loss 8.06210899\n",
      "Trained batch 86 batch loss 7.95539951 epoch total loss 8.06086731\n",
      "Trained batch 87 batch loss 7.40782309 epoch total loss 8.05336189\n",
      "Trained batch 88 batch loss 7.82050276 epoch total loss 8.05071545\n",
      "Trained batch 89 batch loss 7.43877125 epoch total loss 8.04383945\n",
      "Trained batch 90 batch loss 7.55092335 epoch total loss 8.0383625\n",
      "Trained batch 91 batch loss 7.3132267 epoch total loss 8.03039455\n",
      "Trained batch 92 batch loss 7.66837502 epoch total loss 8.02646\n",
      "Trained batch 93 batch loss 7.59613657 epoch total loss 8.02183247\n",
      "Trained batch 94 batch loss 6.88401651 epoch total loss 8.00972843\n",
      "Trained batch 95 batch loss 6.79346371 epoch total loss 7.99692488\n",
      "Trained batch 96 batch loss 7.01706743 epoch total loss 7.98671865\n",
      "Trained batch 97 batch loss 6.79202509 epoch total loss 7.97440243\n",
      "Trained batch 98 batch loss 7.43614864 epoch total loss 7.96891\n",
      "Trained batch 99 batch loss 7.30858564 epoch total loss 7.96224\n",
      "Trained batch 100 batch loss 7.68702888 epoch total loss 7.95948792\n",
      "Trained batch 101 batch loss 7.65536118 epoch total loss 7.95647669\n",
      "Trained batch 102 batch loss 7.5223403 epoch total loss 7.95222044\n",
      "Trained batch 103 batch loss 7.39759922 epoch total loss 7.94683552\n",
      "Trained batch 104 batch loss 6.92503405 epoch total loss 7.93701077\n",
      "Trained batch 105 batch loss 7.22349644 epoch total loss 7.93021536\n",
      "Trained batch 106 batch loss 7.35146332 epoch total loss 7.9247551\n",
      "Trained batch 107 batch loss 7.18439913 epoch total loss 7.91783571\n",
      "Trained batch 108 batch loss 7.71569538 epoch total loss 7.91596413\n",
      "Trained batch 109 batch loss 7.60943031 epoch total loss 7.91315222\n",
      "Trained batch 110 batch loss 7.55000544 epoch total loss 7.9098506\n",
      "Trained batch 111 batch loss 7.45290709 epoch total loss 7.90573359\n",
      "Trained batch 112 batch loss 7.62911701 epoch total loss 7.90326357\n",
      "Trained batch 113 batch loss 7.1359992 epoch total loss 7.89647341\n",
      "Trained batch 114 batch loss 7.57496834 epoch total loss 7.89365339\n",
      "Trained batch 115 batch loss 7.36944389 epoch total loss 7.88909483\n",
      "Trained batch 116 batch loss 7.04860878 epoch total loss 7.88184929\n",
      "Trained batch 117 batch loss 7.22473431 epoch total loss 7.87623262\n",
      "Trained batch 118 batch loss 7.37298822 epoch total loss 7.87196779\n",
      "Trained batch 119 batch loss 7.45291185 epoch total loss 7.86844683\n",
      "Trained batch 120 batch loss 7.23959446 epoch total loss 7.86320639\n",
      "Trained batch 121 batch loss 7.30692291 epoch total loss 7.8586092\n",
      "Trained batch 122 batch loss 7.34536648 epoch total loss 7.85440207\n",
      "Trained batch 123 batch loss 7.55908108 epoch total loss 7.85200119\n",
      "Trained batch 124 batch loss 7.53166 epoch total loss 7.84941769\n",
      "Trained batch 125 batch loss 7.77118 epoch total loss 7.84879208\n",
      "Trained batch 126 batch loss 7.35464096 epoch total loss 7.84487\n",
      "Trained batch 127 batch loss 7.12152433 epoch total loss 7.83917427\n",
      "Trained batch 128 batch loss 7.42323208 epoch total loss 7.83592463\n",
      "Trained batch 129 batch loss 7.29082251 epoch total loss 7.83169889\n",
      "Trained batch 130 batch loss 6.8401289 epoch total loss 7.82407188\n",
      "Trained batch 131 batch loss 6.10742569 epoch total loss 7.81096745\n",
      "Trained batch 132 batch loss 6.49401617 epoch total loss 7.80099\n",
      "Trained batch 133 batch loss 7.45068359 epoch total loss 7.79835653\n",
      "Trained batch 134 batch loss 7.44743633 epoch total loss 7.79573727\n",
      "Trained batch 135 batch loss 7.18630362 epoch total loss 7.79122257\n",
      "Trained batch 136 batch loss 7.58104944 epoch total loss 7.78967714\n",
      "Trained batch 137 batch loss 7.49701929 epoch total loss 7.78754139\n",
      "Trained batch 138 batch loss 7.53400612 epoch total loss 7.78570461\n",
      "Trained batch 139 batch loss 7.51739788 epoch total loss 7.78377485\n",
      "Trained batch 140 batch loss 7.43235731 epoch total loss 7.78126478\n",
      "Trained batch 141 batch loss 7.49781895 epoch total loss 7.77925444\n",
      "Trained batch 142 batch loss 7.60170698 epoch total loss 7.77800417\n",
      "Trained batch 143 batch loss 7.1190176 epoch total loss 7.77339554\n",
      "Trained batch 144 batch loss 6.90583611 epoch total loss 7.76737118\n",
      "Trained batch 145 batch loss 6.80988359 epoch total loss 7.76076841\n",
      "Trained batch 146 batch loss 7.35061693 epoch total loss 7.75795889\n",
      "Trained batch 147 batch loss 7.50920773 epoch total loss 7.75626612\n",
      "Trained batch 148 batch loss 6.95591831 epoch total loss 7.75085878\n",
      "Trained batch 149 batch loss 7.03996897 epoch total loss 7.74608707\n",
      "Trained batch 150 batch loss 6.65889549 epoch total loss 7.73883963\n",
      "Trained batch 151 batch loss 7.06531906 epoch total loss 7.73437881\n",
      "Trained batch 152 batch loss 6.8617034 epoch total loss 7.7286377\n",
      "Trained batch 153 batch loss 7.19545794 epoch total loss 7.72515249\n",
      "Trained batch 154 batch loss 7.32602215 epoch total loss 7.72256136\n",
      "Trained batch 155 batch loss 7.23760653 epoch total loss 7.71943188\n",
      "Trained batch 156 batch loss 6.90654182 epoch total loss 7.714221\n",
      "Trained batch 157 batch loss 7.08053732 epoch total loss 7.71018505\n",
      "Trained batch 158 batch loss 7.32158518 epoch total loss 7.70772505\n",
      "Trained batch 159 batch loss 7.31356192 epoch total loss 7.70524645\n",
      "Trained batch 160 batch loss 6.89160872 epoch total loss 7.70016098\n",
      "Trained batch 161 batch loss 7.04344749 epoch total loss 7.69608212\n",
      "Trained batch 162 batch loss 6.84946966 epoch total loss 7.69085598\n",
      "Trained batch 163 batch loss 7.43269539 epoch total loss 7.6892724\n",
      "Trained batch 164 batch loss 6.9065032 epoch total loss 7.68449974\n",
      "Trained batch 165 batch loss 7.45173073 epoch total loss 7.68308926\n",
      "Trained batch 166 batch loss 7.41450882 epoch total loss 7.68147135\n",
      "Trained batch 167 batch loss 7.44446039 epoch total loss 7.68005228\n",
      "Trained batch 168 batch loss 7.29689932 epoch total loss 7.67777157\n",
      "Trained batch 169 batch loss 7.27627754 epoch total loss 7.67539549\n",
      "Trained batch 170 batch loss 7.35127735 epoch total loss 7.67348909\n",
      "Trained batch 171 batch loss 7.34697485 epoch total loss 7.67157936\n",
      "Trained batch 172 batch loss 6.95909691 epoch total loss 7.66743708\n",
      "Trained batch 173 batch loss 6.7007575 epoch total loss 7.6618495\n",
      "Trained batch 174 batch loss 6.66363907 epoch total loss 7.65611315\n",
      "Trained batch 175 batch loss 7.25281858 epoch total loss 7.65380859\n",
      "Trained batch 176 batch loss 7.2577424 epoch total loss 7.65155792\n",
      "Trained batch 177 batch loss 7.04309559 epoch total loss 7.6481204\n",
      "Trained batch 178 batch loss 6.83214569 epoch total loss 7.64353609\n",
      "Trained batch 179 batch loss 7.21567345 epoch total loss 7.64114618\n",
      "Trained batch 180 batch loss 6.83982563 epoch total loss 7.63669443\n",
      "Trained batch 181 batch loss 7.4473424 epoch total loss 7.63564825\n",
      "Trained batch 182 batch loss 7.23092699 epoch total loss 7.63342476\n",
      "Trained batch 183 batch loss 6.76403093 epoch total loss 7.62867403\n",
      "Trained batch 184 batch loss 7.54216719 epoch total loss 7.62820387\n",
      "Trained batch 185 batch loss 6.64972734 epoch total loss 7.62291479\n",
      "Trained batch 186 batch loss 7.12718296 epoch total loss 7.62024975\n",
      "Trained batch 187 batch loss 6.95710802 epoch total loss 7.61670399\n",
      "Trained batch 188 batch loss 6.78379393 epoch total loss 7.61227369\n",
      "Trained batch 189 batch loss 7.08897734 epoch total loss 7.6095047\n",
      "Trained batch 190 batch loss 7.05006838 epoch total loss 7.60656\n",
      "Trained batch 191 batch loss 7.18777895 epoch total loss 7.60436773\n",
      "Trained batch 192 batch loss 7.11399269 epoch total loss 7.60181379\n",
      "Trained batch 193 batch loss 7.03142405 epoch total loss 7.59885788\n",
      "Trained batch 194 batch loss 6.90315342 epoch total loss 7.59527206\n",
      "Trained batch 195 batch loss 6.6248312 epoch total loss 7.59029579\n",
      "Trained batch 196 batch loss 7.11431408 epoch total loss 7.58786678\n",
      "Trained batch 197 batch loss 7.32739925 epoch total loss 7.58654499\n",
      "Trained batch 198 batch loss 7.36932039 epoch total loss 7.58544731\n",
      "Trained batch 199 batch loss 7.53844 epoch total loss 7.58521128\n",
      "Trained batch 200 batch loss 7.5241971 epoch total loss 7.5849061\n",
      "Trained batch 201 batch loss 7.50766134 epoch total loss 7.58452177\n",
      "Trained batch 202 batch loss 7.04557896 epoch total loss 7.58185339\n",
      "Trained batch 203 batch loss 6.65894604 epoch total loss 7.57730722\n",
      "Trained batch 204 batch loss 7.37852478 epoch total loss 7.57633305\n",
      "Trained batch 205 batch loss 7.56521082 epoch total loss 7.57627869\n",
      "Trained batch 206 batch loss 7.21728897 epoch total loss 7.57453585\n",
      "Trained batch 207 batch loss 6.86546564 epoch total loss 7.57111025\n",
      "Trained batch 208 batch loss 6.7993145 epoch total loss 7.5674\n",
      "Trained batch 209 batch loss 6.96443892 epoch total loss 7.56451511\n",
      "Trained batch 210 batch loss 7.41667175 epoch total loss 7.56381083\n",
      "Trained batch 211 batch loss 7.32791567 epoch total loss 7.56269264\n",
      "Trained batch 212 batch loss 7.04496717 epoch total loss 7.56025028\n",
      "Trained batch 213 batch loss 7.22755241 epoch total loss 7.55868816\n",
      "Trained batch 214 batch loss 7.33914566 epoch total loss 7.55766249\n",
      "Trained batch 215 batch loss 7.24646235 epoch total loss 7.55621481\n",
      "Trained batch 216 batch loss 6.9174962 epoch total loss 7.55325747\n",
      "Trained batch 217 batch loss 7.22527409 epoch total loss 7.55174589\n",
      "Trained batch 218 batch loss 7.49184799 epoch total loss 7.55147123\n",
      "Trained batch 219 batch loss 6.90244 epoch total loss 7.54850769\n",
      "Trained batch 220 batch loss 7.43498611 epoch total loss 7.54799128\n",
      "Trained batch 221 batch loss 7.00163746 epoch total loss 7.54551888\n",
      "Trained batch 222 batch loss 6.33071661 epoch total loss 7.54004669\n",
      "Trained batch 223 batch loss 6.5249114 epoch total loss 7.53549433\n",
      "Trained batch 224 batch loss 6.91999674 epoch total loss 7.53274679\n",
      "Trained batch 225 batch loss 7.14882803 epoch total loss 7.53104067\n",
      "Trained batch 226 batch loss 7.03002119 epoch total loss 7.52882385\n",
      "Trained batch 227 batch loss 6.21497488 epoch total loss 7.523036\n",
      "Trained batch 228 batch loss 6.55860472 epoch total loss 7.51880598\n",
      "Trained batch 229 batch loss 6.77357864 epoch total loss 7.51555157\n",
      "Trained batch 230 batch loss 6.5468173 epoch total loss 7.51133966\n",
      "Trained batch 231 batch loss 6.9752903 epoch total loss 7.50901937\n",
      "Trained batch 232 batch loss 6.99985123 epoch total loss 7.50682497\n",
      "Trained batch 233 batch loss 7.26322174 epoch total loss 7.50577927\n",
      "Trained batch 234 batch loss 7.3789444 epoch total loss 7.5052371\n",
      "Trained batch 235 batch loss 7.58547878 epoch total loss 7.50557852\n",
      "Trained batch 236 batch loss 7.42115545 epoch total loss 7.50522041\n",
      "Trained batch 237 batch loss 7.61199856 epoch total loss 7.50567102\n",
      "Trained batch 238 batch loss 7.33655071 epoch total loss 7.50496\n",
      "Trained batch 239 batch loss 7.62760353 epoch total loss 7.50547314\n",
      "Trained batch 240 batch loss 7.50771904 epoch total loss 7.50548267\n",
      "Trained batch 241 batch loss 7.52315521 epoch total loss 7.50555611\n",
      "Trained batch 242 batch loss 7.53721237 epoch total loss 7.50568676\n",
      "Trained batch 243 batch loss 7.28403187 epoch total loss 7.50477457\n",
      "Trained batch 244 batch loss 7.28171825 epoch total loss 7.50386095\n",
      "Trained batch 245 batch loss 7.23628092 epoch total loss 7.50276852\n",
      "Trained batch 246 batch loss 7.55445099 epoch total loss 7.5029788\n",
      "Trained batch 247 batch loss 7.46189833 epoch total loss 7.50281239\n",
      "Trained batch 248 batch loss 7.61032104 epoch total loss 7.50324631\n",
      "Trained batch 249 batch loss 7.32776928 epoch total loss 7.50254154\n",
      "Trained batch 250 batch loss 7.43486881 epoch total loss 7.5022707\n",
      "Trained batch 251 batch loss 7.51713943 epoch total loss 7.50232935\n",
      "Trained batch 252 batch loss 7.42115879 epoch total loss 7.50200748\n",
      "Trained batch 253 batch loss 7.28267384 epoch total loss 7.50114059\n",
      "Trained batch 254 batch loss 7.02062225 epoch total loss 7.49924898\n",
      "Trained batch 255 batch loss 7.40818357 epoch total loss 7.49889183\n",
      "Trained batch 256 batch loss 7.36637878 epoch total loss 7.49837399\n",
      "Trained batch 257 batch loss 7.6409564 epoch total loss 7.49892902\n",
      "Trained batch 258 batch loss 7.54866934 epoch total loss 7.49912167\n",
      "Trained batch 259 batch loss 7.2548027 epoch total loss 7.49817848\n",
      "Trained batch 260 batch loss 7.3666153 epoch total loss 7.49767208\n",
      "Trained batch 261 batch loss 7.4413743 epoch total loss 7.49745655\n",
      "Trained batch 262 batch loss 7.54998112 epoch total loss 7.49765682\n",
      "Trained batch 263 batch loss 7.13870478 epoch total loss 7.49629211\n",
      "Trained batch 264 batch loss 6.45679188 epoch total loss 7.49235439\n",
      "Trained batch 265 batch loss 6.33444309 epoch total loss 7.48798513\n",
      "Trained batch 266 batch loss 6.37967253 epoch total loss 7.48381853\n",
      "Trained batch 267 batch loss 6.84092283 epoch total loss 7.4814105\n",
      "Trained batch 268 batch loss 7.02901936 epoch total loss 7.4797225\n",
      "Trained batch 269 batch loss 7.37280607 epoch total loss 7.47932529\n",
      "Trained batch 270 batch loss 7.249681 epoch total loss 7.47847462\n",
      "Trained batch 271 batch loss 7.49341297 epoch total loss 7.47852945\n",
      "Trained batch 272 batch loss 7.29612637 epoch total loss 7.47785902\n",
      "Trained batch 273 batch loss 7.29239273 epoch total loss 7.47717953\n",
      "Trained batch 274 batch loss 7.21823 epoch total loss 7.47623444\n",
      "Trained batch 275 batch loss 7.18550348 epoch total loss 7.47517776\n",
      "Trained batch 276 batch loss 7.19081068 epoch total loss 7.4741478\n",
      "Trained batch 277 batch loss 7.24659491 epoch total loss 7.47332621\n",
      "Trained batch 278 batch loss 6.87871933 epoch total loss 7.47118711\n",
      "Trained batch 279 batch loss 6.8999095 epoch total loss 7.46913958\n",
      "Trained batch 280 batch loss 7.21430492 epoch total loss 7.46822929\n",
      "Trained batch 281 batch loss 6.76472664 epoch total loss 7.46572542\n",
      "Trained batch 282 batch loss 6.63961601 epoch total loss 7.46279621\n",
      "Trained batch 283 batch loss 7.02097321 epoch total loss 7.46123505\n",
      "Trained batch 284 batch loss 7.42736626 epoch total loss 7.46111536\n",
      "Trained batch 285 batch loss 7.6158905 epoch total loss 7.46165895\n",
      "Trained batch 286 batch loss 7.49141407 epoch total loss 7.46176291\n",
      "Trained batch 287 batch loss 7.49814606 epoch total loss 7.46188927\n",
      "Trained batch 288 batch loss 7.4732008 epoch total loss 7.46192837\n",
      "Trained batch 289 batch loss 7.49516964 epoch total loss 7.46204329\n",
      "Trained batch 290 batch loss 7.52979469 epoch total loss 7.46227694\n",
      "Trained batch 291 batch loss 7.58752203 epoch total loss 7.46270704\n",
      "Trained batch 292 batch loss 7.66404057 epoch total loss 7.46339655\n",
      "Trained batch 293 batch loss 7.5274992 epoch total loss 7.46361542\n",
      "Trained batch 294 batch loss 7.39497 epoch total loss 7.46338224\n",
      "Trained batch 295 batch loss 7.26839352 epoch total loss 7.46272087\n",
      "Trained batch 296 batch loss 7.56129265 epoch total loss 7.4630537\n",
      "Trained batch 297 batch loss 7.3184886 epoch total loss 7.46256781\n",
      "Trained batch 298 batch loss 7.09239674 epoch total loss 7.46132517\n",
      "Trained batch 299 batch loss 7.48487329 epoch total loss 7.46140385\n",
      "Trained batch 300 batch loss 7.45203209 epoch total loss 7.46137285\n",
      "Trained batch 301 batch loss 7.20861244 epoch total loss 7.46053267\n",
      "Trained batch 302 batch loss 6.99302292 epoch total loss 7.45898438\n",
      "Trained batch 303 batch loss 7.19693 epoch total loss 7.45812\n",
      "Trained batch 304 batch loss 7.56315804 epoch total loss 7.45846558\n",
      "Trained batch 305 batch loss 7.34864616 epoch total loss 7.45810556\n",
      "Trained batch 306 batch loss 7.51457071 epoch total loss 7.45829\n",
      "Trained batch 307 batch loss 7.39569044 epoch total loss 7.45808649\n",
      "Trained batch 308 batch loss 7.38592 epoch total loss 7.45785236\n",
      "Trained batch 309 batch loss 7.54623604 epoch total loss 7.45813799\n",
      "Trained batch 310 batch loss 7.22296619 epoch total loss 7.45737934\n",
      "Trained batch 311 batch loss 7.17342949 epoch total loss 7.4564662\n",
      "Trained batch 312 batch loss 7.08652592 epoch total loss 7.45528\n",
      "Trained batch 313 batch loss 7.52803564 epoch total loss 7.45551252\n",
      "Trained batch 314 batch loss 7.31354523 epoch total loss 7.45506\n",
      "Trained batch 315 batch loss 7.48016071 epoch total loss 7.45514\n",
      "Trained batch 316 batch loss 7.18694592 epoch total loss 7.45429182\n",
      "Trained batch 317 batch loss 7.37966824 epoch total loss 7.45405626\n",
      "Trained batch 318 batch loss 7.4662075 epoch total loss 7.45409489\n",
      "Trained batch 319 batch loss 7.41055679 epoch total loss 7.45395851\n",
      "Trained batch 320 batch loss 7.42968798 epoch total loss 7.45388269\n",
      "Trained batch 321 batch loss 7.41007948 epoch total loss 7.45374632\n",
      "Trained batch 322 batch loss 7.26972103 epoch total loss 7.45317507\n",
      "Trained batch 323 batch loss 7.10616922 epoch total loss 7.45210075\n",
      "Trained batch 324 batch loss 7.54030752 epoch total loss 7.45237303\n",
      "Trained batch 325 batch loss 7.43327093 epoch total loss 7.45231438\n",
      "Trained batch 326 batch loss 7.56871939 epoch total loss 7.45267105\n",
      "Trained batch 327 batch loss 7.27247143 epoch total loss 7.4521203\n",
      "Trained batch 328 batch loss 6.90301609 epoch total loss 7.45044613\n",
      "Trained batch 329 batch loss 7.26719427 epoch total loss 7.44988871\n",
      "Trained batch 330 batch loss 7.47595215 epoch total loss 7.44996738\n",
      "Trained batch 331 batch loss 7.48367071 epoch total loss 7.45006895\n",
      "Trained batch 332 batch loss 7.49582529 epoch total loss 7.45020723\n",
      "Trained batch 333 batch loss 7.17995834 epoch total loss 7.44939566\n",
      "Trained batch 334 batch loss 6.72698402 epoch total loss 7.44723272\n",
      "Trained batch 335 batch loss 6.70804501 epoch total loss 7.44502592\n",
      "Trained batch 336 batch loss 6.18961668 epoch total loss 7.44129\n",
      "Trained batch 337 batch loss 6.338305 epoch total loss 7.43801737\n",
      "Trained batch 338 batch loss 6.3710022 epoch total loss 7.43486071\n",
      "Trained batch 339 batch loss 6.17541456 epoch total loss 7.43114519\n",
      "Trained batch 340 batch loss 5.87551928 epoch total loss 7.42656946\n",
      "Trained batch 341 batch loss 5.89402151 epoch total loss 7.42207527\n",
      "Trained batch 342 batch loss 6.53694248 epoch total loss 7.419487\n",
      "Trained batch 343 batch loss 7.13067913 epoch total loss 7.41864491\n",
      "Trained batch 344 batch loss 7.40276241 epoch total loss 7.41859913\n",
      "Trained batch 345 batch loss 7.43269777 epoch total loss 7.41863966\n",
      "Trained batch 346 batch loss 6.91595554 epoch total loss 7.41718674\n",
      "Trained batch 347 batch loss 7.04433107 epoch total loss 7.4161129\n",
      "Trained batch 348 batch loss 7.34668255 epoch total loss 7.41591311\n",
      "Trained batch 349 batch loss 6.78124332 epoch total loss 7.41409445\n",
      "Trained batch 350 batch loss 7.34657955 epoch total loss 7.41390228\n",
      "Trained batch 351 batch loss 7.27170944 epoch total loss 7.41349697\n",
      "Trained batch 352 batch loss 6.6246171 epoch total loss 7.41125536\n",
      "Trained batch 353 batch loss 6.46979618 epoch total loss 7.40858841\n",
      "Trained batch 354 batch loss 7.128757 epoch total loss 7.40779781\n",
      "Trained batch 355 batch loss 7.2951684 epoch total loss 7.40748024\n",
      "Trained batch 356 batch loss 7.67384195 epoch total loss 7.4082284\n",
      "Trained batch 357 batch loss 7.33061266 epoch total loss 7.40801096\n",
      "Trained batch 358 batch loss 7.55914497 epoch total loss 7.40843296\n",
      "Trained batch 359 batch loss 7.61152077 epoch total loss 7.40899897\n",
      "Trained batch 360 batch loss 7.50703621 epoch total loss 7.40927124\n",
      "Trained batch 361 batch loss 7.36057377 epoch total loss 7.4091363\n",
      "Trained batch 362 batch loss 7.25192928 epoch total loss 7.40870237\n",
      "Trained batch 363 batch loss 7.32936525 epoch total loss 7.40848351\n",
      "Trained batch 364 batch loss 7.04454517 epoch total loss 7.40748358\n",
      "Trained batch 365 batch loss 6.99611902 epoch total loss 7.40635633\n",
      "Trained batch 366 batch loss 7.26122522 epoch total loss 7.40595961\n",
      "Trained batch 367 batch loss 7.15988731 epoch total loss 7.40528917\n",
      "Trained batch 368 batch loss 6.87190342 epoch total loss 7.40383959\n",
      "Trained batch 369 batch loss 6.92732382 epoch total loss 7.40254831\n",
      "Trained batch 370 batch loss 6.97461843 epoch total loss 7.40139151\n",
      "Trained batch 371 batch loss 7.26711416 epoch total loss 7.40102959\n",
      "Trained batch 372 batch loss 6.78872585 epoch total loss 7.39938402\n",
      "Trained batch 373 batch loss 6.55070925 epoch total loss 7.39710903\n",
      "Trained batch 374 batch loss 7.00137138 epoch total loss 7.39605093\n",
      "Trained batch 375 batch loss 6.95168352 epoch total loss 7.39486599\n",
      "Trained batch 376 batch loss 6.28304625 epoch total loss 7.39190865\n",
      "Trained batch 377 batch loss 6.2359252 epoch total loss 7.38884211\n",
      "Trained batch 378 batch loss 6.12019682 epoch total loss 7.38548565\n",
      "Trained batch 379 batch loss 6.22144556 epoch total loss 7.38241434\n",
      "Trained batch 380 batch loss 6.39421368 epoch total loss 7.37981415\n",
      "Trained batch 381 batch loss 6.73208618 epoch total loss 7.37811422\n",
      "Trained batch 382 batch loss 6.81101656 epoch total loss 7.37663\n",
      "Trained batch 383 batch loss 6.69543266 epoch total loss 7.37485075\n",
      "Trained batch 384 batch loss 7.12583208 epoch total loss 7.37420225\n",
      "Trained batch 385 batch loss 6.6672349 epoch total loss 7.37236595\n",
      "Trained batch 386 batch loss 7.07648087 epoch total loss 7.3715992\n",
      "Trained batch 387 batch loss 7.25450277 epoch total loss 7.37129641\n",
      "Trained batch 388 batch loss 7.36891174 epoch total loss 7.37129\n",
      "Trained batch 389 batch loss 7.31451035 epoch total loss 7.37114382\n",
      "Trained batch 390 batch loss 7.36236572 epoch total loss 7.37112141\n",
      "Trained batch 391 batch loss 7.16328907 epoch total loss 7.37058973\n",
      "Trained batch 392 batch loss 7.34200954 epoch total loss 7.37051725\n",
      "Trained batch 393 batch loss 7.38170195 epoch total loss 7.37054539\n",
      "Trained batch 394 batch loss 7.46030474 epoch total loss 7.37077284\n",
      "Trained batch 395 batch loss 7.42275572 epoch total loss 7.37090445\n",
      "Trained batch 396 batch loss 7.4601779 epoch total loss 7.37113\n",
      "Trained batch 397 batch loss 7.61410332 epoch total loss 7.37174177\n",
      "Trained batch 398 batch loss 7.14437151 epoch total loss 7.37117052\n",
      "Trained batch 399 batch loss 6.83420181 epoch total loss 7.36982489\n",
      "Trained batch 400 batch loss 7.32128429 epoch total loss 7.36970329\n",
      "Trained batch 401 batch loss 7.32818222 epoch total loss 7.3696\n",
      "Trained batch 402 batch loss 7.2953167 epoch total loss 7.36941528\n",
      "Trained batch 403 batch loss 7.47350025 epoch total loss 7.36967325\n",
      "Trained batch 404 batch loss 7.64098167 epoch total loss 7.37034464\n",
      "Trained batch 405 batch loss 7.33901215 epoch total loss 7.37026739\n",
      "Trained batch 406 batch loss 7.51001501 epoch total loss 7.37061167\n",
      "Trained batch 407 batch loss 7.56083679 epoch total loss 7.37107897\n",
      "Trained batch 408 batch loss 7.59495735 epoch total loss 7.37162733\n",
      "Trained batch 409 batch loss 7.75202179 epoch total loss 7.37255716\n",
      "Trained batch 410 batch loss 7.60017776 epoch total loss 7.3731122\n",
      "Trained batch 411 batch loss 7.47720861 epoch total loss 7.37336588\n",
      "Trained batch 412 batch loss 7.15950298 epoch total loss 7.3728466\n",
      "Trained batch 413 batch loss 7.11479712 epoch total loss 7.37222147\n",
      "Trained batch 414 batch loss 6.9380827 epoch total loss 7.3711729\n",
      "Trained batch 415 batch loss 7.26466179 epoch total loss 7.37091589\n",
      "Trained batch 416 batch loss 7.2835021 epoch total loss 7.3707056\n",
      "Trained batch 417 batch loss 7.30683088 epoch total loss 7.37055254\n",
      "Trained batch 418 batch loss 7.48126364 epoch total loss 7.37081766\n",
      "Trained batch 419 batch loss 6.83038521 epoch total loss 7.36952734\n",
      "Trained batch 420 batch loss 7.25624323 epoch total loss 7.36925793\n",
      "Trained batch 421 batch loss 7.27340508 epoch total loss 7.36903048\n",
      "Trained batch 422 batch loss 6.55711842 epoch total loss 7.36710644\n",
      "Trained batch 423 batch loss 7.16005945 epoch total loss 7.3666172\n",
      "Trained batch 424 batch loss 6.94930077 epoch total loss 7.36563301\n",
      "Trained batch 425 batch loss 6.71288538 epoch total loss 7.36409712\n",
      "Trained batch 426 batch loss 7.55043221 epoch total loss 7.36453485\n",
      "Trained batch 427 batch loss 7.18125868 epoch total loss 7.36410522\n",
      "Trained batch 428 batch loss 7.31862354 epoch total loss 7.36399889\n",
      "Trained batch 429 batch loss 6.36896944 epoch total loss 7.36167908\n",
      "Trained batch 430 batch loss 6.65502405 epoch total loss 7.3600359\n",
      "Trained batch 431 batch loss 7.26493025 epoch total loss 7.35981512\n",
      "Trained batch 432 batch loss 7.48219109 epoch total loss 7.36009836\n",
      "Trained batch 433 batch loss 7.61285925 epoch total loss 7.36068201\n",
      "Trained batch 434 batch loss 7.73963213 epoch total loss 7.36155558\n",
      "Trained batch 435 batch loss 7.714221 epoch total loss 7.36236572\n",
      "Trained batch 436 batch loss 7.53108263 epoch total loss 7.36275244\n",
      "Trained batch 437 batch loss 6.96347141 epoch total loss 7.36183882\n",
      "Trained batch 438 batch loss 7.5208106 epoch total loss 7.36220169\n",
      "Trained batch 439 batch loss 7.50317764 epoch total loss 7.3625226\n",
      "Trained batch 440 batch loss 7.2925477 epoch total loss 7.36236334\n",
      "Trained batch 441 batch loss 7.38913965 epoch total loss 7.36242437\n",
      "Trained batch 442 batch loss 7.43494225 epoch total loss 7.36258841\n",
      "Trained batch 443 batch loss 7.3560338 epoch total loss 7.36257362\n",
      "Trained batch 444 batch loss 7.33271742 epoch total loss 7.36250639\n",
      "Trained batch 445 batch loss 6.794806 epoch total loss 7.36123085\n",
      "Trained batch 446 batch loss 7.44669867 epoch total loss 7.36142302\n",
      "Trained batch 447 batch loss 7.45858526 epoch total loss 7.36164\n",
      "Trained batch 448 batch loss 7.51008081 epoch total loss 7.36197138\n",
      "Trained batch 449 batch loss 7.41865253 epoch total loss 7.36209774\n",
      "Trained batch 450 batch loss 6.93561077 epoch total loss 7.36115\n",
      "Trained batch 451 batch loss 6.0554862 epoch total loss 7.35825443\n",
      "Trained batch 452 batch loss 6.39364576 epoch total loss 7.35612\n",
      "Trained batch 453 batch loss 7.28726196 epoch total loss 7.35596848\n",
      "Trained batch 454 batch loss 7.152946 epoch total loss 7.35552073\n",
      "Trained batch 455 batch loss 7.25491428 epoch total loss 7.3553\n",
      "Trained batch 456 batch loss 7.22429562 epoch total loss 7.35501242\n",
      "Trained batch 457 batch loss 7.17416811 epoch total loss 7.35461664\n",
      "Trained batch 458 batch loss 7.35407686 epoch total loss 7.35461521\n",
      "Trained batch 459 batch loss 7.49297 epoch total loss 7.35491657\n",
      "Trained batch 460 batch loss 7.61838245 epoch total loss 7.35548925\n",
      "Trained batch 461 batch loss 7.66759539 epoch total loss 7.35616636\n",
      "Trained batch 462 batch loss 7.53357553 epoch total loss 7.35655069\n",
      "Trained batch 463 batch loss 7.44067812 epoch total loss 7.35673237\n",
      "Trained batch 464 batch loss 7.22765493 epoch total loss 7.3564539\n",
      "Trained batch 465 batch loss 7.35732603 epoch total loss 7.3564558\n",
      "Trained batch 466 batch loss 7.49995279 epoch total loss 7.35676384\n",
      "Trained batch 467 batch loss 7.33393478 epoch total loss 7.3567152\n",
      "Trained batch 468 batch loss 7.03138828 epoch total loss 7.35602\n",
      "Trained batch 469 batch loss 7.13744831 epoch total loss 7.3555541\n",
      "Trained batch 470 batch loss 7.31353378 epoch total loss 7.35546446\n",
      "Trained batch 471 batch loss 6.86414623 epoch total loss 7.35442162\n",
      "Trained batch 472 batch loss 6.94475 epoch total loss 7.35355377\n",
      "Trained batch 473 batch loss 7.1599164 epoch total loss 7.35314465\n",
      "Trained batch 474 batch loss 7.44406128 epoch total loss 7.35333633\n",
      "Trained batch 475 batch loss 7.13010836 epoch total loss 7.35286665\n",
      "Trained batch 476 batch loss 6.85155249 epoch total loss 7.35181332\n",
      "Trained batch 477 batch loss 6.70566368 epoch total loss 7.35045862\n",
      "Trained batch 478 batch loss 6.39225864 epoch total loss 7.348454\n",
      "Trained batch 479 batch loss 6.4064765 epoch total loss 7.34648752\n",
      "Trained batch 480 batch loss 7.00780201 epoch total loss 7.3457818\n",
      "Trained batch 481 batch loss 7.36526251 epoch total loss 7.34582233\n",
      "Trained batch 482 batch loss 7.10407829 epoch total loss 7.3453207\n",
      "Trained batch 483 batch loss 7.464921 epoch total loss 7.34556818\n",
      "Trained batch 484 batch loss 7.5313096 epoch total loss 7.34595203\n",
      "Trained batch 485 batch loss 7.24565315 epoch total loss 7.34574509\n",
      "Trained batch 486 batch loss 7.22483444 epoch total loss 7.34549618\n",
      "Trained batch 487 batch loss 7.41318893 epoch total loss 7.34563494\n",
      "Trained batch 488 batch loss 7.52315378 epoch total loss 7.34599876\n",
      "Trained batch 489 batch loss 7.39489222 epoch total loss 7.34609842\n",
      "Trained batch 490 batch loss 7.38225889 epoch total loss 7.34617233\n",
      "Trained batch 491 batch loss 7.49001122 epoch total loss 7.34646559\n",
      "Trained batch 492 batch loss 7.55776453 epoch total loss 7.34689522\n",
      "Trained batch 493 batch loss 7.38585091 epoch total loss 7.3469739\n",
      "Trained batch 494 batch loss 7.37328339 epoch total loss 7.3470273\n",
      "Trained batch 495 batch loss 7.39851236 epoch total loss 7.34713078\n",
      "Trained batch 496 batch loss 7.43476057 epoch total loss 7.34730768\n",
      "Trained batch 497 batch loss 7.44025517 epoch total loss 7.3474946\n",
      "Trained batch 498 batch loss 7.63088512 epoch total loss 7.34806347\n",
      "Trained batch 499 batch loss 7.16027737 epoch total loss 7.34768724\n",
      "Trained batch 500 batch loss 7.40171671 epoch total loss 7.34779501\n",
      "Trained batch 501 batch loss 7.6273427 epoch total loss 7.34835291\n",
      "Trained batch 502 batch loss 7.06573 epoch total loss 7.34779\n",
      "Trained batch 503 batch loss 6.89849234 epoch total loss 7.34689665\n",
      "Trained batch 504 batch loss 6.19417524 epoch total loss 7.34460926\n",
      "Trained batch 505 batch loss 6.54872084 epoch total loss 7.34303331\n",
      "Trained batch 506 batch loss 6.9282732 epoch total loss 7.34221363\n",
      "Trained batch 507 batch loss 7.59424114 epoch total loss 7.34271097\n",
      "Trained batch 508 batch loss 7.63131 epoch total loss 7.34327888\n",
      "Trained batch 509 batch loss 7.56052637 epoch total loss 7.34370565\n",
      "Trained batch 510 batch loss 7.54649496 epoch total loss 7.34410334\n",
      "Trained batch 511 batch loss 7.57206154 epoch total loss 7.34454918\n",
      "Trained batch 512 batch loss 7.55862761 epoch total loss 7.34496737\n",
      "Trained batch 513 batch loss 7.49299812 epoch total loss 7.34525585\n",
      "Trained batch 514 batch loss 7.41370869 epoch total loss 7.34538937\n",
      "Trained batch 515 batch loss 7.30668688 epoch total loss 7.34531403\n",
      "Trained batch 516 batch loss 7.28347397 epoch total loss 7.34519386\n",
      "Trained batch 517 batch loss 7.3088274 epoch total loss 7.34512377\n",
      "Trained batch 518 batch loss 7.56755 epoch total loss 7.3455534\n",
      "Trained batch 519 batch loss 7.3062458 epoch total loss 7.3454771\n",
      "Trained batch 520 batch loss 7.3376646 epoch total loss 7.34546232\n",
      "Trained batch 521 batch loss 7.05739117 epoch total loss 7.34490919\n",
      "Trained batch 522 batch loss 7.415308 epoch total loss 7.34504414\n",
      "Trained batch 523 batch loss 7.18844461 epoch total loss 7.34474468\n",
      "Trained batch 524 batch loss 7.28432274 epoch total loss 7.34463\n",
      "Trained batch 525 batch loss 7.59289646 epoch total loss 7.34510279\n",
      "Trained batch 526 batch loss 7.51088858 epoch total loss 7.34541798\n",
      "Trained batch 527 batch loss 7.35414171 epoch total loss 7.34543467\n",
      "Trained batch 528 batch loss 7.23882151 epoch total loss 7.34523296\n",
      "Trained batch 529 batch loss 7.09078121 epoch total loss 7.34475183\n",
      "Trained batch 530 batch loss 7.45271301 epoch total loss 7.34495544\n",
      "Trained batch 531 batch loss 7.45994186 epoch total loss 7.34517193\n",
      "Trained batch 532 batch loss 7.40484047 epoch total loss 7.34528399\n",
      "Trained batch 533 batch loss 7.34766197 epoch total loss 7.34528875\n",
      "Trained batch 534 batch loss 7.30949545 epoch total loss 7.34522152\n",
      "Trained batch 535 batch loss 7.52318811 epoch total loss 7.34555435\n",
      "Trained batch 536 batch loss 7.09067488 epoch total loss 7.34507847\n",
      "Trained batch 537 batch loss 7.48806667 epoch total loss 7.34534502\n",
      "Trained batch 538 batch loss 6.82325172 epoch total loss 7.34437466\n",
      "Trained batch 539 batch loss 7.24792147 epoch total loss 7.34419537\n",
      "Trained batch 540 batch loss 7.02724552 epoch total loss 7.34360838\n",
      "Trained batch 541 batch loss 6.30558205 epoch total loss 7.34169\n",
      "Trained batch 542 batch loss 6.30051041 epoch total loss 7.33976889\n",
      "Trained batch 543 batch loss 5.82859039 epoch total loss 7.33698606\n",
      "Trained batch 544 batch loss 6.57066059 epoch total loss 7.33557701\n",
      "Trained batch 545 batch loss 7.03333855 epoch total loss 7.33502293\n",
      "Trained batch 546 batch loss 7.38912392 epoch total loss 7.33512211\n",
      "Trained batch 547 batch loss 7.2111516 epoch total loss 7.33489513\n",
      "Trained batch 548 batch loss 7.33357477 epoch total loss 7.33489275\n",
      "Trained batch 549 batch loss 7.53806496 epoch total loss 7.33526278\n",
      "Trained batch 550 batch loss 7.36263132 epoch total loss 7.33531237\n",
      "Trained batch 551 batch loss 7.34051561 epoch total loss 7.3353219\n",
      "Trained batch 552 batch loss 7.45930529 epoch total loss 7.33554649\n",
      "Trained batch 553 batch loss 7.50338316 epoch total loss 7.33585024\n",
      "Trained batch 554 batch loss 7.58448792 epoch total loss 7.33629894\n",
      "Trained batch 555 batch loss 7.16917086 epoch total loss 7.33599758\n",
      "Trained batch 556 batch loss 6.86302328 epoch total loss 7.3351469\n",
      "Trained batch 557 batch loss 6.77794838 epoch total loss 7.3341465\n",
      "Trained batch 558 batch loss 6.54296589 epoch total loss 7.33272886\n",
      "Trained batch 559 batch loss 6.95745039 epoch total loss 7.33205748\n",
      "Trained batch 560 batch loss 7.2925849 epoch total loss 7.3319869\n",
      "Trained batch 561 batch loss 6.73236036 epoch total loss 7.33091784\n",
      "Trained batch 562 batch loss 7.45232344 epoch total loss 7.33113384\n",
      "Trained batch 563 batch loss 6.84106922 epoch total loss 7.33026361\n",
      "Trained batch 564 batch loss 6.78802681 epoch total loss 7.32930231\n",
      "Trained batch 565 batch loss 7.00963497 epoch total loss 7.32873678\n",
      "Trained batch 566 batch loss 6.80906963 epoch total loss 7.32781887\n",
      "Trained batch 567 batch loss 7.26436472 epoch total loss 7.32770634\n",
      "Trained batch 568 batch loss 6.86888361 epoch total loss 7.3268981\n",
      "Trained batch 569 batch loss 6.5623374 epoch total loss 7.32555485\n",
      "Trained batch 570 batch loss 7.05561924 epoch total loss 7.32508135\n",
      "Trained batch 571 batch loss 7.20633745 epoch total loss 7.32487392\n",
      "Trained batch 572 batch loss 7.35293818 epoch total loss 7.32492304\n",
      "Trained batch 573 batch loss 7.35388613 epoch total loss 7.32497358\n",
      "Trained batch 574 batch loss 7.25083208 epoch total loss 7.32484484\n",
      "Trained batch 575 batch loss 6.93227768 epoch total loss 7.32416201\n",
      "Trained batch 576 batch loss 7.34787464 epoch total loss 7.32420254\n",
      "Trained batch 577 batch loss 7.01695776 epoch total loss 7.32367039\n",
      "Trained batch 578 batch loss 6.82952118 epoch total loss 7.32281542\n",
      "Trained batch 579 batch loss 7.17264318 epoch total loss 7.3225565\n",
      "Trained batch 580 batch loss 7.18824911 epoch total loss 7.32232523\n",
      "Trained batch 581 batch loss 7.54918432 epoch total loss 7.32271624\n",
      "Trained batch 582 batch loss 7.52065945 epoch total loss 7.32305574\n",
      "Trained batch 583 batch loss 7.53497505 epoch total loss 7.32341957\n",
      "Trained batch 584 batch loss 7.46511412 epoch total loss 7.32366276\n",
      "Trained batch 585 batch loss 7.18806076 epoch total loss 7.32343102\n",
      "Trained batch 586 batch loss 7.09814692 epoch total loss 7.32304621\n",
      "Trained batch 587 batch loss 7.25084 epoch total loss 7.32292366\n",
      "Trained batch 588 batch loss 6.90968513 epoch total loss 7.3222208\n",
      "Trained batch 589 batch loss 6.34250641 epoch total loss 7.32055712\n",
      "Trained batch 590 batch loss 6.71031475 epoch total loss 7.31952286\n",
      "Trained batch 591 batch loss 6.7867589 epoch total loss 7.31862116\n",
      "Trained batch 592 batch loss 6.5514 epoch total loss 7.31732512\n",
      "Trained batch 593 batch loss 6.90053129 epoch total loss 7.31662178\n",
      "Trained batch 594 batch loss 6.85770893 epoch total loss 7.31585\n",
      "Trained batch 595 batch loss 6.75237751 epoch total loss 7.31490278\n",
      "Trained batch 596 batch loss 7.06313276 epoch total loss 7.3144803\n",
      "Trained batch 597 batch loss 7.03998899 epoch total loss 7.31402063\n",
      "Trained batch 598 batch loss 7.39562511 epoch total loss 7.31415653\n",
      "Trained batch 599 batch loss 7.55447292 epoch total loss 7.31455851\n",
      "Trained batch 600 batch loss 6.62632465 epoch total loss 7.31341124\n",
      "Trained batch 601 batch loss 6.0229125 epoch total loss 7.31126404\n",
      "Trained batch 602 batch loss 5.60221291 epoch total loss 7.30842495\n",
      "Trained batch 603 batch loss 6.41494036 epoch total loss 7.30694342\n",
      "Trained batch 604 batch loss 7.35734749 epoch total loss 7.30702686\n",
      "Trained batch 605 batch loss 7.49070358 epoch total loss 7.30733061\n",
      "Trained batch 606 batch loss 7.39557934 epoch total loss 7.30747604\n",
      "Trained batch 607 batch loss 7.47537899 epoch total loss 7.30775309\n",
      "Trained batch 608 batch loss 7.35701942 epoch total loss 7.30783415\n",
      "Trained batch 609 batch loss 7.48549843 epoch total loss 7.3081255\n",
      "Trained batch 610 batch loss 7.58229876 epoch total loss 7.30857515\n",
      "Trained batch 611 batch loss 7.46358538 epoch total loss 7.30882883\n",
      "Trained batch 612 batch loss 7.36565924 epoch total loss 7.30892181\n",
      "Trained batch 613 batch loss 7.36722183 epoch total loss 7.3090167\n",
      "Trained batch 614 batch loss 7.25394678 epoch total loss 7.30892706\n",
      "Trained batch 615 batch loss 6.93581867 epoch total loss 7.30832052\n",
      "Trained batch 616 batch loss 6.96295071 epoch total loss 7.30776\n",
      "Trained batch 617 batch loss 6.79316187 epoch total loss 7.3069253\n",
      "Trained batch 618 batch loss 6.605443 epoch total loss 7.30579042\n",
      "Trained batch 619 batch loss 7.25124788 epoch total loss 7.30570269\n",
      "Trained batch 620 batch loss 7.08919907 epoch total loss 7.30535364\n",
      "Trained batch 621 batch loss 7.17515135 epoch total loss 7.30514431\n",
      "Trained batch 622 batch loss 6.93291044 epoch total loss 7.30454636\n",
      "Trained batch 623 batch loss 6.95321226 epoch total loss 7.30398226\n",
      "Trained batch 624 batch loss 6.6664 epoch total loss 7.3029604\n",
      "Trained batch 625 batch loss 6.71259928 epoch total loss 7.30201578\n",
      "Trained batch 626 batch loss 7.16710091 epoch total loss 7.3018\n",
      "Trained batch 627 batch loss 7.2131629 epoch total loss 7.30165911\n",
      "Trained batch 628 batch loss 6.98664665 epoch total loss 7.30115747\n",
      "Trained batch 629 batch loss 7.39832687 epoch total loss 7.30131245\n",
      "Trained batch 630 batch loss 6.79080772 epoch total loss 7.3005023\n",
      "Trained batch 631 batch loss 6.93979168 epoch total loss 7.29993105\n",
      "Trained batch 632 batch loss 7.48658705 epoch total loss 7.30022669\n",
      "Trained batch 633 batch loss 7.00862789 epoch total loss 7.29976606\n",
      "Trained batch 634 batch loss 6.76137829 epoch total loss 7.29891682\n",
      "Trained batch 635 batch loss 6.72811651 epoch total loss 7.2980175\n",
      "Trained batch 636 batch loss 7.03902531 epoch total loss 7.29761028\n",
      "Trained batch 637 batch loss 6.4618783 epoch total loss 7.2962985\n",
      "Trained batch 638 batch loss 7.05045128 epoch total loss 7.29591274\n",
      "Trained batch 639 batch loss 6.47120619 epoch total loss 7.29462242\n",
      "Trained batch 640 batch loss 6.70854378 epoch total loss 7.29370642\n",
      "Trained batch 641 batch loss 7.22048187 epoch total loss 7.29359245\n",
      "Trained batch 642 batch loss 7.19750214 epoch total loss 7.29344273\n",
      "Trained batch 643 batch loss 7.4075036 epoch total loss 7.29362\n",
      "Trained batch 644 batch loss 6.85236502 epoch total loss 7.29293537\n",
      "Trained batch 645 batch loss 7.12762547 epoch total loss 7.29267883\n",
      "Trained batch 646 batch loss 7.27763176 epoch total loss 7.29265594\n",
      "Trained batch 647 batch loss 7.01320744 epoch total loss 7.29222393\n",
      "Trained batch 648 batch loss 6.93134689 epoch total loss 7.29166651\n",
      "Trained batch 649 batch loss 7.21203 epoch total loss 7.29154396\n",
      "Trained batch 650 batch loss 6.58829117 epoch total loss 7.29046202\n",
      "Trained batch 651 batch loss 7.61602688 epoch total loss 7.29096222\n",
      "Trained batch 652 batch loss 7.55225134 epoch total loss 7.29136324\n",
      "Trained batch 653 batch loss 7.43595362 epoch total loss 7.29158449\n",
      "Trained batch 654 batch loss 7.14432096 epoch total loss 7.29136\n",
      "Trained batch 655 batch loss 6.94607449 epoch total loss 7.290833\n",
      "Trained batch 656 batch loss 6.98766232 epoch total loss 7.29037094\n",
      "Trained batch 657 batch loss 7.24742889 epoch total loss 7.29030609\n",
      "Trained batch 658 batch loss 7.26588917 epoch total loss 7.2902689\n",
      "Trained batch 659 batch loss 7.27768326 epoch total loss 7.2902503\n",
      "Trained batch 660 batch loss 7.4241724 epoch total loss 7.29045343\n",
      "Trained batch 661 batch loss 7.44258928 epoch total loss 7.29068327\n",
      "Trained batch 662 batch loss 7.55386591 epoch total loss 7.29108047\n",
      "Trained batch 663 batch loss 7.26267147 epoch total loss 7.29103756\n",
      "Trained batch 664 batch loss 7.16869497 epoch total loss 7.29085302\n",
      "Trained batch 665 batch loss 7.39168787 epoch total loss 7.29100466\n",
      "Trained batch 666 batch loss 7.21364594 epoch total loss 7.29088879\n",
      "Trained batch 667 batch loss 7.18493557 epoch total loss 7.29073\n",
      "Trained batch 668 batch loss 6.68362093 epoch total loss 7.28982115\n",
      "Trained batch 669 batch loss 7.3710103 epoch total loss 7.28994274\n",
      "Trained batch 670 batch loss 6.71830845 epoch total loss 7.28908968\n",
      "Trained batch 671 batch loss 6.37013817 epoch total loss 7.28771973\n",
      "Trained batch 672 batch loss 6.20481539 epoch total loss 7.28610802\n",
      "Trained batch 673 batch loss 6.92572308 epoch total loss 7.28557253\n",
      "Trained batch 674 batch loss 7.46994972 epoch total loss 7.28584576\n",
      "Trained batch 675 batch loss 6.96408606 epoch total loss 7.28536892\n",
      "Trained batch 676 batch loss 7.32733679 epoch total loss 7.28543091\n",
      "Trained batch 677 batch loss 7.03076601 epoch total loss 7.28505468\n",
      "Trained batch 678 batch loss 7.32886314 epoch total loss 7.28511953\n",
      "Trained batch 679 batch loss 7.18941164 epoch total loss 7.28497839\n",
      "Trained batch 680 batch loss 6.15218878 epoch total loss 7.2833128\n",
      "Trained batch 681 batch loss 6.13203049 epoch total loss 7.28162193\n",
      "Trained batch 682 batch loss 5.4624157 epoch total loss 7.27895451\n",
      "Trained batch 683 batch loss 6.29074287 epoch total loss 7.27750731\n",
      "Trained batch 684 batch loss 6.83033514 epoch total loss 7.27685404\n",
      "Trained batch 685 batch loss 7.17534256 epoch total loss 7.27670574\n",
      "Trained batch 686 batch loss 6.91098309 epoch total loss 7.27617264\n",
      "Trained batch 687 batch loss 7.19099808 epoch total loss 7.27604866\n",
      "Trained batch 688 batch loss 7.27917767 epoch total loss 7.27605343\n",
      "Trained batch 689 batch loss 6.94287348 epoch total loss 7.27557\n",
      "Trained batch 690 batch loss 6.35577 epoch total loss 7.27423716\n",
      "Trained batch 691 batch loss 6.76565075 epoch total loss 7.27350092\n",
      "Trained batch 692 batch loss 7.03679609 epoch total loss 7.27315855\n",
      "Trained batch 693 batch loss 6.48688602 epoch total loss 7.27202415\n",
      "Trained batch 694 batch loss 6.74817181 epoch total loss 7.27126884\n",
      "Trained batch 695 batch loss 7.0544858 epoch total loss 7.27095747\n",
      "Trained batch 696 batch loss 6.87049818 epoch total loss 7.2703824\n",
      "Trained batch 697 batch loss 7.04905891 epoch total loss 7.27006435\n",
      "Trained batch 698 batch loss 7.38858604 epoch total loss 7.27023411\n",
      "Trained batch 699 batch loss 7.50421143 epoch total loss 7.27056932\n",
      "Trained batch 700 batch loss 7.55090714 epoch total loss 7.27096939\n",
      "Trained batch 701 batch loss 7.36574554 epoch total loss 7.27110481\n",
      "Trained batch 702 batch loss 6.66203499 epoch total loss 7.27023745\n",
      "Trained batch 703 batch loss 6.91812801 epoch total loss 7.26973629\n",
      "Trained batch 704 batch loss 7.17504835 epoch total loss 7.26960135\n",
      "Trained batch 705 batch loss 7.35332155 epoch total loss 7.26972055\n",
      "Trained batch 706 batch loss 7.0183959 epoch total loss 7.26936436\n",
      "Trained batch 707 batch loss 6.22519541 epoch total loss 7.26788759\n",
      "Trained batch 708 batch loss 6.5915761 epoch total loss 7.26693249\n",
      "Trained batch 709 batch loss 7.05916 epoch total loss 7.26663923\n",
      "Trained batch 710 batch loss 7.45282888 epoch total loss 7.26690149\n",
      "Trained batch 711 batch loss 7.30633402 epoch total loss 7.26695681\n",
      "Trained batch 712 batch loss 7.48602724 epoch total loss 7.26726389\n",
      "Trained batch 713 batch loss 7.47717285 epoch total loss 7.2675581\n",
      "Trained batch 714 batch loss 7.41903687 epoch total loss 7.26777029\n",
      "Trained batch 715 batch loss 6.26660824 epoch total loss 7.26637\n",
      "Trained batch 716 batch loss 6.33980703 epoch total loss 7.26507616\n",
      "Trained batch 717 batch loss 6.88004494 epoch total loss 7.26453876\n",
      "Trained batch 718 batch loss 6.41977024 epoch total loss 7.26336241\n",
      "Trained batch 719 batch loss 6.44448185 epoch total loss 7.26222324\n",
      "Trained batch 720 batch loss 6.18023396 epoch total loss 7.26072025\n",
      "Trained batch 721 batch loss 6.4442215 epoch total loss 7.25958824\n",
      "Trained batch 722 batch loss 6.48123264 epoch total loss 7.25851059\n",
      "Trained batch 723 batch loss 7.28644657 epoch total loss 7.25854921\n",
      "Trained batch 724 batch loss 7.43690062 epoch total loss 7.25879574\n",
      "Trained batch 725 batch loss 7.52607727 epoch total loss 7.25916433\n",
      "Trained batch 726 batch loss 7.42870808 epoch total loss 7.25939751\n",
      "Trained batch 727 batch loss 7.38796806 epoch total loss 7.25957489\n",
      "Trained batch 728 batch loss 7.42063951 epoch total loss 7.25979567\n",
      "Trained batch 729 batch loss 7.52377844 epoch total loss 7.26015806\n",
      "Trained batch 730 batch loss 7.47329044 epoch total loss 7.26045\n",
      "Trained batch 731 batch loss 7.49830055 epoch total loss 7.26077557\n",
      "Trained batch 732 batch loss 7.53140259 epoch total loss 7.26114511\n",
      "Trained batch 733 batch loss 7.10017872 epoch total loss 7.26092529\n",
      "Trained batch 734 batch loss 7.25266552 epoch total loss 7.26091385\n",
      "Trained batch 735 batch loss 7.18689823 epoch total loss 7.26081324\n",
      "Trained batch 736 batch loss 6.63454914 epoch total loss 7.25996256\n",
      "Trained batch 737 batch loss 6.69247341 epoch total loss 7.25919247\n",
      "Trained batch 738 batch loss 6.74307823 epoch total loss 7.25849342\n",
      "Trained batch 739 batch loss 7.21262169 epoch total loss 7.25843096\n",
      "Trained batch 740 batch loss 7.13638687 epoch total loss 7.25826597\n",
      "Trained batch 741 batch loss 6.84799767 epoch total loss 7.25771236\n",
      "Trained batch 742 batch loss 7.51775122 epoch total loss 7.25806236\n",
      "Trained batch 743 batch loss 7.45842648 epoch total loss 7.25833225\n",
      "Trained batch 744 batch loss 7.46145058 epoch total loss 7.25860548\n",
      "Trained batch 745 batch loss 7.47066116 epoch total loss 7.25889\n",
      "Trained batch 746 batch loss 7.22788429 epoch total loss 7.25884867\n",
      "Trained batch 747 batch loss 7.4492054 epoch total loss 7.2591033\n",
      "Trained batch 748 batch loss 7.59361887 epoch total loss 7.25955105\n",
      "Trained batch 749 batch loss 7.49366665 epoch total loss 7.25986338\n",
      "Trained batch 750 batch loss 6.67222357 epoch total loss 7.25908\n",
      "Trained batch 751 batch loss 6.94147 epoch total loss 7.25865698\n",
      "Trained batch 752 batch loss 7.48981333 epoch total loss 7.25896454\n",
      "Trained batch 753 batch loss 7.59013414 epoch total loss 7.25940466\n",
      "Trained batch 754 batch loss 7.64689589 epoch total loss 7.25991869\n",
      "Trained batch 755 batch loss 7.49423122 epoch total loss 7.26022863\n",
      "Trained batch 756 batch loss 7.46608543 epoch total loss 7.26050138\n",
      "Trained batch 757 batch loss 7.57921171 epoch total loss 7.26092196\n",
      "Trained batch 758 batch loss 7.35504198 epoch total loss 7.26104641\n",
      "Trained batch 759 batch loss 7.10442781 epoch total loss 7.26084\n",
      "Trained batch 760 batch loss 6.75218391 epoch total loss 7.26017046\n",
      "Trained batch 761 batch loss 7.03781843 epoch total loss 7.25987768\n",
      "Trained batch 762 batch loss 7.19164562 epoch total loss 7.25978804\n",
      "Trained batch 763 batch loss 6.94130707 epoch total loss 7.2593708\n",
      "Trained batch 764 batch loss 7.21633148 epoch total loss 7.25931454\n",
      "Trained batch 765 batch loss 7.60879898 epoch total loss 7.25977135\n",
      "Trained batch 766 batch loss 6.65867615 epoch total loss 7.25898647\n",
      "Trained batch 767 batch loss 7.07618713 epoch total loss 7.25874853\n",
      "Trained batch 768 batch loss 6.91296482 epoch total loss 7.2582984\n",
      "Trained batch 769 batch loss 6.9492507 epoch total loss 7.25789642\n",
      "Trained batch 770 batch loss 7.46299887 epoch total loss 7.2581625\n",
      "Trained batch 771 batch loss 7.35872841 epoch total loss 7.25829315\n",
      "Trained batch 772 batch loss 7.13938141 epoch total loss 7.25813866\n",
      "Trained batch 773 batch loss 7.42340946 epoch total loss 7.25835276\n",
      "Trained batch 774 batch loss 7.47297144 epoch total loss 7.25863028\n",
      "Trained batch 775 batch loss 7.45320225 epoch total loss 7.25888109\n",
      "Trained batch 776 batch loss 7.43344879 epoch total loss 7.25910616\n",
      "Trained batch 777 batch loss 7.32140779 epoch total loss 7.25918627\n",
      "Trained batch 778 batch loss 6.95271206 epoch total loss 7.2587924\n",
      "Trained batch 779 batch loss 7.37435246 epoch total loss 7.2589407\n",
      "Trained batch 780 batch loss 6.8552947 epoch total loss 7.25842333\n",
      "Trained batch 781 batch loss 7.30557632 epoch total loss 7.25848389\n",
      "Trained batch 782 batch loss 7.23772 epoch total loss 7.25845766\n",
      "Trained batch 783 batch loss 6.38984108 epoch total loss 7.25734806\n",
      "Trained batch 784 batch loss 6.79787207 epoch total loss 7.25676203\n",
      "Trained batch 785 batch loss 6.68868446 epoch total loss 7.25603771\n",
      "Trained batch 786 batch loss 6.47823286 epoch total loss 7.25504827\n",
      "Trained batch 787 batch loss 7.30025101 epoch total loss 7.2551055\n",
      "Trained batch 788 batch loss 7.4431572 epoch total loss 7.25534439\n",
      "Trained batch 789 batch loss 7.44909859 epoch total loss 7.25559\n",
      "Trained batch 790 batch loss 7.59192848 epoch total loss 7.25601578\n",
      "Trained batch 791 batch loss 7.56700945 epoch total loss 7.25640869\n",
      "Trained batch 792 batch loss 7.35185337 epoch total loss 7.25652933\n",
      "Trained batch 793 batch loss 7.40028906 epoch total loss 7.25671101\n",
      "Trained batch 794 batch loss 7.17334557 epoch total loss 7.2566061\n",
      "Trained batch 795 batch loss 7.26417 epoch total loss 7.25661564\n",
      "Trained batch 796 batch loss 7.28308058 epoch total loss 7.25664902\n",
      "Trained batch 797 batch loss 7.34173584 epoch total loss 7.25675583\n",
      "Trained batch 798 batch loss 7.35087 epoch total loss 7.25687408\n",
      "Trained batch 799 batch loss 7.41661024 epoch total loss 7.25707388\n",
      "Trained batch 800 batch loss 7.20159626 epoch total loss 7.25700426\n",
      "Trained batch 801 batch loss 7.2931509 epoch total loss 7.25704908\n",
      "Trained batch 802 batch loss 7.50389814 epoch total loss 7.25735712\n",
      "Trained batch 803 batch loss 7.35238934 epoch total loss 7.25747585\n",
      "Trained batch 804 batch loss 7.54468632 epoch total loss 7.257833\n",
      "Trained batch 805 batch loss 7.49753761 epoch total loss 7.25813103\n",
      "Trained batch 806 batch loss 7.46799469 epoch total loss 7.2583909\n",
      "Trained batch 807 batch loss 7.48231316 epoch total loss 7.25866842\n",
      "Trained batch 808 batch loss 7.47316408 epoch total loss 7.25893402\n",
      "Trained batch 809 batch loss 7.37316322 epoch total loss 7.25907516\n",
      "Trained batch 810 batch loss 7.41081095 epoch total loss 7.25926208\n",
      "Trained batch 811 batch loss 7.25656652 epoch total loss 7.25925875\n",
      "Trained batch 812 batch loss 7.20071268 epoch total loss 7.25918674\n",
      "Trained batch 813 batch loss 7.44249916 epoch total loss 7.25941181\n",
      "Trained batch 814 batch loss 7.14340973 epoch total loss 7.25926971\n",
      "Trained batch 815 batch loss 7.24949265 epoch total loss 7.25925779\n",
      "Trained batch 816 batch loss 7.34690905 epoch total loss 7.2593646\n",
      "Trained batch 817 batch loss 7.55729866 epoch total loss 7.25972939\n",
      "Trained batch 818 batch loss 7.1130414 epoch total loss 7.25955\n",
      "Trained batch 819 batch loss 7.44305134 epoch total loss 7.25977421\n",
      "Trained batch 820 batch loss 7.11143875 epoch total loss 7.25959301\n",
      "Trained batch 821 batch loss 6.76050282 epoch total loss 7.25898552\n",
      "Trained batch 822 batch loss 7.09198952 epoch total loss 7.25878191\n",
      "Trained batch 823 batch loss 7.34323359 epoch total loss 7.25888443\n",
      "Trained batch 824 batch loss 7.52113581 epoch total loss 7.25920248\n",
      "Trained batch 825 batch loss 7.46421242 epoch total loss 7.25945139\n",
      "Trained batch 826 batch loss 7.54211 epoch total loss 7.25979328\n",
      "Trained batch 827 batch loss 7.3674612 epoch total loss 7.25992393\n",
      "Trained batch 828 batch loss 7.49571657 epoch total loss 7.26020861\n",
      "Trained batch 829 batch loss 7.17627525 epoch total loss 7.26010704\n",
      "Trained batch 830 batch loss 6.87137794 epoch total loss 7.25963926\n",
      "Trained batch 831 batch loss 6.8872695 epoch total loss 7.25919104\n",
      "Trained batch 832 batch loss 7.15763664 epoch total loss 7.25906897\n",
      "Trained batch 833 batch loss 6.81404638 epoch total loss 7.25853443\n",
      "Trained batch 834 batch loss 6.85987234 epoch total loss 7.25805664\n",
      "Trained batch 835 batch loss 7.38361406 epoch total loss 7.25820732\n",
      "Trained batch 836 batch loss 7.01120377 epoch total loss 7.25791168\n",
      "Trained batch 837 batch loss 6.74271345 epoch total loss 7.25729609\n",
      "Trained batch 838 batch loss 6.57437134 epoch total loss 7.25648117\n",
      "Trained batch 839 batch loss 7.11701488 epoch total loss 7.25631523\n",
      "Trained batch 840 batch loss 6.876369 epoch total loss 7.25586271\n",
      "Trained batch 841 batch loss 7.01988792 epoch total loss 7.25558233\n",
      "Trained batch 842 batch loss 7.14739037 epoch total loss 7.25545406\n",
      "Trained batch 843 batch loss 6.88162231 epoch total loss 7.2550106\n",
      "Trained batch 844 batch loss 6.81258631 epoch total loss 7.25448656\n",
      "Trained batch 845 batch loss 6.6871295 epoch total loss 7.25381517\n",
      "Trained batch 846 batch loss 6.44627094 epoch total loss 7.25286055\n",
      "Trained batch 847 batch loss 6.38188505 epoch total loss 7.25183201\n",
      "Trained batch 848 batch loss 6.79908609 epoch total loss 7.25129843\n",
      "Trained batch 849 batch loss 7.28693914 epoch total loss 7.25134039\n",
      "Trained batch 850 batch loss 7.11176872 epoch total loss 7.25117636\n",
      "Trained batch 851 batch loss 7.22641325 epoch total loss 7.25114775\n",
      "Trained batch 852 batch loss 7.13851786 epoch total loss 7.25101566\n",
      "Trained batch 853 batch loss 6.58187628 epoch total loss 7.25023127\n",
      "Trained batch 854 batch loss 6.99228573 epoch total loss 7.24992895\n",
      "Trained batch 855 batch loss 6.64491 epoch total loss 7.2492218\n",
      "Trained batch 856 batch loss 7.14084196 epoch total loss 7.24909496\n",
      "Trained batch 857 batch loss 7.03090143 epoch total loss 7.24884\n",
      "Trained batch 858 batch loss 6.46135473 epoch total loss 7.24792242\n",
      "Trained batch 859 batch loss 6.56628656 epoch total loss 7.24712896\n",
      "Trained batch 860 batch loss 6.11051941 epoch total loss 7.24580717\n",
      "Trained batch 861 batch loss 7.09617186 epoch total loss 7.24563313\n",
      "Trained batch 862 batch loss 6.80566883 epoch total loss 7.24512291\n",
      "Trained batch 863 batch loss 7.11449957 epoch total loss 7.24497128\n",
      "Trained batch 864 batch loss 6.97405958 epoch total loss 7.24465752\n",
      "Trained batch 865 batch loss 7.37974262 epoch total loss 7.24481392\n",
      "Trained batch 866 batch loss 7.39191198 epoch total loss 7.24498415\n",
      "Trained batch 867 batch loss 6.98289728 epoch total loss 7.24468184\n",
      "Trained batch 868 batch loss 7.56457233 epoch total loss 7.24505043\n",
      "Trained batch 869 batch loss 7.22690678 epoch total loss 7.24502945\n",
      "Trained batch 870 batch loss 6.67358398 epoch total loss 7.24437284\n",
      "Trained batch 871 batch loss 5.90344477 epoch total loss 7.24283314\n",
      "Trained batch 872 batch loss 6.29730368 epoch total loss 7.24174929\n",
      "Trained batch 873 batch loss 7.26157713 epoch total loss 7.2417717\n",
      "Trained batch 874 batch loss 6.98605537 epoch total loss 7.24147892\n",
      "Trained batch 875 batch loss 6.92809963 epoch total loss 7.24112129\n",
      "Trained batch 876 batch loss 7.4444418 epoch total loss 7.24135303\n",
      "Trained batch 877 batch loss 7.19928 epoch total loss 7.24130487\n",
      "Trained batch 878 batch loss 7.60035133 epoch total loss 7.241714\n",
      "Trained batch 879 batch loss 6.97560596 epoch total loss 7.24141169\n",
      "Trained batch 880 batch loss 6.46045113 epoch total loss 7.24052382\n",
      "Trained batch 881 batch loss 6.67693853 epoch total loss 7.2398839\n",
      "Trained batch 882 batch loss 7.37155151 epoch total loss 7.24003363\n",
      "Trained batch 883 batch loss 7.14797306 epoch total loss 7.2399292\n",
      "Trained batch 884 batch loss 7.17128611 epoch total loss 7.23985147\n",
      "Trained batch 885 batch loss 7.47660398 epoch total loss 7.24011898\n",
      "Trained batch 886 batch loss 7.24823046 epoch total loss 7.24012804\n",
      "Trained batch 887 batch loss 7.59701633 epoch total loss 7.24053049\n",
      "Trained batch 888 batch loss 7.55065489 epoch total loss 7.24088\n",
      "Trained batch 889 batch loss 7.38881302 epoch total loss 7.24104595\n",
      "Trained batch 890 batch loss 7.21296692 epoch total loss 7.24101448\n",
      "Trained batch 891 batch loss 7.23795271 epoch total loss 7.24101114\n",
      "Trained batch 892 batch loss 7.11774111 epoch total loss 7.24087286\n",
      "Trained batch 893 batch loss 7.05296803 epoch total loss 7.2406621\n",
      "Trained batch 894 batch loss 7.13729429 epoch total loss 7.24054623\n",
      "Trained batch 895 batch loss 7.06185675 epoch total loss 7.24034691\n",
      "Trained batch 896 batch loss 7.18928194 epoch total loss 7.24029\n",
      "Trained batch 897 batch loss 6.89925146 epoch total loss 7.23991\n",
      "Trained batch 898 batch loss 7.0013895 epoch total loss 7.23964453\n",
      "Trained batch 899 batch loss 6.64641571 epoch total loss 7.23898458\n",
      "Trained batch 900 batch loss 7.15367413 epoch total loss 7.23889\n",
      "Trained batch 901 batch loss 7.30703449 epoch total loss 7.23896551\n",
      "Trained batch 902 batch loss 7.39341164 epoch total loss 7.23913717\n",
      "Trained batch 903 batch loss 7.52767944 epoch total loss 7.23945665\n",
      "Trained batch 904 batch loss 7.4978075 epoch total loss 7.23974276\n",
      "Trained batch 905 batch loss 7.2605834 epoch total loss 7.23976612\n",
      "Trained batch 906 batch loss 7.27130461 epoch total loss 7.23980093\n",
      "Trained batch 907 batch loss 6.07795477 epoch total loss 7.23852\n",
      "Trained batch 908 batch loss 6.14494896 epoch total loss 7.23731613\n",
      "Trained batch 909 batch loss 6.2233243 epoch total loss 7.23620033\n",
      "Trained batch 910 batch loss 6.29620266 epoch total loss 7.2351675\n",
      "Trained batch 911 batch loss 6.69121361 epoch total loss 7.2345705\n",
      "Trained batch 912 batch loss 7.08605862 epoch total loss 7.23440742\n",
      "Trained batch 913 batch loss 7.42704535 epoch total loss 7.23461866\n",
      "Trained batch 914 batch loss 6.86562824 epoch total loss 7.23421526\n",
      "Trained batch 915 batch loss 7.15191126 epoch total loss 7.23412514\n",
      "Trained batch 916 batch loss 7.12753487 epoch total loss 7.23400879\n",
      "Trained batch 917 batch loss 6.93795586 epoch total loss 7.23368597\n",
      "Trained batch 918 batch loss 6.841784 epoch total loss 7.2332592\n",
      "Trained batch 919 batch loss 6.4376235 epoch total loss 7.23239326\n",
      "Trained batch 920 batch loss 7.21736431 epoch total loss 7.23237658\n",
      "Trained batch 921 batch loss 7.28110504 epoch total loss 7.23243\n",
      "Trained batch 922 batch loss 7.41755152 epoch total loss 7.23263073\n",
      "Trained batch 923 batch loss 7.49227 epoch total loss 7.23291159\n",
      "Trained batch 924 batch loss 7.43181276 epoch total loss 7.23312664\n",
      "Trained batch 925 batch loss 7.53039122 epoch total loss 7.23344803\n",
      "Trained batch 926 batch loss 7.67264 epoch total loss 7.23392248\n",
      "Trained batch 927 batch loss 7.07356453 epoch total loss 7.23375\n",
      "Trained batch 928 batch loss 7.23281908 epoch total loss 7.23374891\n",
      "Trained batch 929 batch loss 7.44537306 epoch total loss 7.23397636\n",
      "Trained batch 930 batch loss 7.47220325 epoch total loss 7.2342329\n",
      "Trained batch 931 batch loss 7.44509029 epoch total loss 7.2344594\n",
      "Trained batch 932 batch loss 7.3636651 epoch total loss 7.23459816\n",
      "Trained batch 933 batch loss 6.58057117 epoch total loss 7.23389721\n",
      "Trained batch 934 batch loss 7.15738487 epoch total loss 7.23381519\n",
      "Trained batch 935 batch loss 7.28143597 epoch total loss 7.23386574\n",
      "Trained batch 936 batch loss 6.92180109 epoch total loss 7.23353243\n",
      "Trained batch 937 batch loss 6.61479425 epoch total loss 7.23287201\n",
      "Trained batch 938 batch loss 7.34582329 epoch total loss 7.23299217\n",
      "Trained batch 939 batch loss 7.45676899 epoch total loss 7.23323059\n",
      "Trained batch 940 batch loss 7.22247458 epoch total loss 7.23321915\n",
      "Trained batch 941 batch loss 7.48903894 epoch total loss 7.23349142\n",
      "Trained batch 942 batch loss 7.30661917 epoch total loss 7.23356915\n",
      "Trained batch 943 batch loss 6.71380234 epoch total loss 7.23301792\n",
      "Trained batch 944 batch loss 6.6933527 epoch total loss 7.23244619\n",
      "Trained batch 945 batch loss 6.83170223 epoch total loss 7.23202181\n",
      "Trained batch 946 batch loss 7.20765209 epoch total loss 7.23199606\n",
      "Trained batch 947 batch loss 7.43016338 epoch total loss 7.23220539\n",
      "Trained batch 948 batch loss 7.30788803 epoch total loss 7.2322855\n",
      "Trained batch 949 batch loss 7.07620144 epoch total loss 7.23212099\n",
      "Trained batch 950 batch loss 6.82266521 epoch total loss 7.23169\n",
      "Trained batch 951 batch loss 6.86267233 epoch total loss 7.23130226\n",
      "Trained batch 952 batch loss 7.18306446 epoch total loss 7.23125124\n",
      "Trained batch 953 batch loss 7.48926306 epoch total loss 7.23152208\n",
      "Trained batch 954 batch loss 7.06801319 epoch total loss 7.23135042\n",
      "Trained batch 955 batch loss 7.01389122 epoch total loss 7.23112249\n",
      "Trained batch 956 batch loss 7.28167391 epoch total loss 7.23117542\n",
      "Trained batch 957 batch loss 7.16492462 epoch total loss 7.23110676\n",
      "Trained batch 958 batch loss 7.2822504 epoch total loss 7.23115969\n",
      "Trained batch 959 batch loss 7.57603407 epoch total loss 7.2315197\n",
      "Trained batch 960 batch loss 7.46114922 epoch total loss 7.23175859\n",
      "Trained batch 961 batch loss 7.21434164 epoch total loss 7.23174047\n",
      "Trained batch 962 batch loss 7.08367109 epoch total loss 7.23158646\n",
      "Trained batch 963 batch loss 6.82727671 epoch total loss 7.23116636\n",
      "Trained batch 964 batch loss 6.34569502 epoch total loss 7.23024797\n",
      "Trained batch 965 batch loss 7.22859764 epoch total loss 7.23024607\n",
      "Trained batch 966 batch loss 7.18787861 epoch total loss 7.2302022\n",
      "Trained batch 967 batch loss 7.09659386 epoch total loss 7.23006439\n",
      "Trained batch 968 batch loss 6.6954093 epoch total loss 7.22951174\n",
      "Trained batch 969 batch loss 6.77101231 epoch total loss 7.22903872\n",
      "Trained batch 970 batch loss 6.91076374 epoch total loss 7.22871065\n",
      "Trained batch 971 batch loss 7.44738388 epoch total loss 7.22893572\n",
      "Trained batch 972 batch loss 6.38031912 epoch total loss 7.22806263\n",
      "Trained batch 973 batch loss 7.08479 epoch total loss 7.22791529\n",
      "Trained batch 974 batch loss 6.80368805 epoch total loss 7.22748\n",
      "Trained batch 975 batch loss 7.0572505 epoch total loss 7.22730541\n",
      "Trained batch 976 batch loss 7.50296068 epoch total loss 7.2275877\n",
      "Trained batch 977 batch loss 7.25063133 epoch total loss 7.22761106\n",
      "Trained batch 978 batch loss 7.5941515 epoch total loss 7.22798586\n",
      "Trained batch 979 batch loss 7.34186745 epoch total loss 7.22810221\n",
      "Trained batch 980 batch loss 7.61142731 epoch total loss 7.22849321\n",
      "Trained batch 981 batch loss 7.58171129 epoch total loss 7.22885323\n",
      "Trained batch 982 batch loss 7.27009153 epoch total loss 7.22889519\n",
      "Trained batch 983 batch loss 6.71934319 epoch total loss 7.22837639\n",
      "Trained batch 984 batch loss 6.94513321 epoch total loss 7.22808886\n",
      "Trained batch 985 batch loss 6.41774273 epoch total loss 7.22726631\n",
      "Trained batch 986 batch loss 6.30756 epoch total loss 7.22633362\n",
      "Trained batch 987 batch loss 6.3625617 epoch total loss 7.22545862\n",
      "Trained batch 988 batch loss 6.46132517 epoch total loss 7.22468567\n",
      "Trained batch 989 batch loss 5.7799449 epoch total loss 7.22322464\n",
      "Trained batch 990 batch loss 5.82075882 epoch total loss 7.22180796\n",
      "Trained batch 991 batch loss 6.50252438 epoch total loss 7.22108221\n",
      "Trained batch 992 batch loss 6.3654809 epoch total loss 7.22021961\n",
      "Trained batch 993 batch loss 6.78241396 epoch total loss 7.21977854\n",
      "Trained batch 994 batch loss 7.33391619 epoch total loss 7.21989346\n",
      "Trained batch 995 batch loss 7.46897554 epoch total loss 7.2201438\n",
      "Trained batch 996 batch loss 7.56318188 epoch total loss 7.22048807\n",
      "Trained batch 997 batch loss 7.39500237 epoch total loss 7.22066307\n",
      "Trained batch 998 batch loss 7.08943605 epoch total loss 7.22053146\n",
      "Trained batch 999 batch loss 6.7162714 epoch total loss 7.22002649\n",
      "Trained batch 1000 batch loss 6.97784185 epoch total loss 7.21978474\n",
      "Trained batch 1001 batch loss 6.94746351 epoch total loss 7.21951246\n",
      "Trained batch 1002 batch loss 7.14593 epoch total loss 7.21943903\n",
      "Trained batch 1003 batch loss 6.54454088 epoch total loss 7.21876621\n",
      "Trained batch 1004 batch loss 6.90147877 epoch total loss 7.21845\n",
      "Trained batch 1005 batch loss 7.06869316 epoch total loss 7.2183013\n",
      "Trained batch 1006 batch loss 7.3678689 epoch total loss 7.21844959\n",
      "Trained batch 1007 batch loss 6.46846628 epoch total loss 7.21770477\n",
      "Trained batch 1008 batch loss 6.36943197 epoch total loss 7.21686316\n",
      "Trained batch 1009 batch loss 6.12302923 epoch total loss 7.2157793\n",
      "Trained batch 1010 batch loss 6.5645647 epoch total loss 7.21513414\n",
      "Trained batch 1011 batch loss 6.81013155 epoch total loss 7.2147336\n",
      "Trained batch 1012 batch loss 6.81886578 epoch total loss 7.21434259\n",
      "Trained batch 1013 batch loss 7.07488155 epoch total loss 7.21420479\n",
      "Trained batch 1014 batch loss 7.23408365 epoch total loss 7.21422386\n",
      "Trained batch 1015 batch loss 7.02777767 epoch total loss 7.21404028\n",
      "Trained batch 1016 batch loss 6.84171772 epoch total loss 7.21367407\n",
      "Trained batch 1017 batch loss 6.77379847 epoch total loss 7.21324158\n",
      "Trained batch 1018 batch loss 7.24639416 epoch total loss 7.21327448\n",
      "Trained batch 1019 batch loss 7.62778282 epoch total loss 7.21368122\n",
      "Trained batch 1020 batch loss 7.44237614 epoch total loss 7.21390533\n",
      "Trained batch 1021 batch loss 7.57223606 epoch total loss 7.21425629\n",
      "Trained batch 1022 batch loss 7.46731567 epoch total loss 7.21450424\n",
      "Trained batch 1023 batch loss 7.03451443 epoch total loss 7.21432829\n",
      "Trained batch 1024 batch loss 6.9156251 epoch total loss 7.21403646\n",
      "Trained batch 1025 batch loss 6.56322193 epoch total loss 7.21340132\n",
      "Trained batch 1026 batch loss 7.18218374 epoch total loss 7.2133708\n",
      "Trained batch 1027 batch loss 7.42199945 epoch total loss 7.21357393\n",
      "Trained batch 1028 batch loss 7.11082125 epoch total loss 7.2134738\n",
      "Trained batch 1029 batch loss 7.24154186 epoch total loss 7.21350145\n",
      "Trained batch 1030 batch loss 6.79308271 epoch total loss 7.21309328\n",
      "Trained batch 1031 batch loss 6.72859621 epoch total loss 7.21262312\n",
      "Trained batch 1032 batch loss 7.36402178 epoch total loss 7.21277\n",
      "Trained batch 1033 batch loss 7.33031082 epoch total loss 7.21288347\n",
      "Trained batch 1034 batch loss 7.4615221 epoch total loss 7.2131238\n",
      "Trained batch 1035 batch loss 7.44462442 epoch total loss 7.21334791\n",
      "Trained batch 1036 batch loss 7.47054625 epoch total loss 7.21359634\n",
      "Trained batch 1037 batch loss 7.34187 epoch total loss 7.21372\n",
      "Trained batch 1038 batch loss 7.35730076 epoch total loss 7.21385813\n",
      "Trained batch 1039 batch loss 7.31681728 epoch total loss 7.21395731\n",
      "Trained batch 1040 batch loss 7.40141916 epoch total loss 7.21413755\n",
      "Trained batch 1041 batch loss 7.61455631 epoch total loss 7.21452236\n",
      "Trained batch 1042 batch loss 7.50627327 epoch total loss 7.21480274\n",
      "Trained batch 1043 batch loss 7.25065422 epoch total loss 7.2148366\n",
      "Trained batch 1044 batch loss 7.19358778 epoch total loss 7.21481609\n",
      "Trained batch 1045 batch loss 7.34659338 epoch total loss 7.21494246\n",
      "Trained batch 1046 batch loss 6.98413 epoch total loss 7.21472168\n",
      "Trained batch 1047 batch loss 7.29563713 epoch total loss 7.21479845\n",
      "Trained batch 1048 batch loss 7.36677551 epoch total loss 7.21494341\n",
      "Trained batch 1049 batch loss 7.21200275 epoch total loss 7.21494055\n",
      "Trained batch 1050 batch loss 7.2942977 epoch total loss 7.21501637\n",
      "Trained batch 1051 batch loss 7.42222404 epoch total loss 7.21521378\n",
      "Trained batch 1052 batch loss 7.39071226 epoch total loss 7.21538\n",
      "Trained batch 1053 batch loss 7.53168821 epoch total loss 7.2156806\n",
      "Trained batch 1054 batch loss 7.54898691 epoch total loss 7.21599674\n",
      "Trained batch 1055 batch loss 7.48314667 epoch total loss 7.21625\n",
      "Trained batch 1056 batch loss 7.51313925 epoch total loss 7.2165308\n",
      "Trained batch 1057 batch loss 7.37101173 epoch total loss 7.21667719\n",
      "Trained batch 1058 batch loss 7.12822676 epoch total loss 7.21659374\n",
      "Trained batch 1059 batch loss 7.37415695 epoch total loss 7.21674252\n",
      "Trained batch 1060 batch loss 7.20404673 epoch total loss 7.21673059\n",
      "Trained batch 1061 batch loss 7.26398087 epoch total loss 7.21677542\n",
      "Trained batch 1062 batch loss 7.18547058 epoch total loss 7.21674585\n",
      "Trained batch 1063 batch loss 6.9850378 epoch total loss 7.21652746\n",
      "Trained batch 1064 batch loss 7.08152437 epoch total loss 7.21640062\n",
      "Trained batch 1065 batch loss 6.70267 epoch total loss 7.21591854\n",
      "Trained batch 1066 batch loss 7.00465679 epoch total loss 7.21572065\n",
      "Trained batch 1067 batch loss 6.59250498 epoch total loss 7.21513605\n",
      "Trained batch 1068 batch loss 6.65058041 epoch total loss 7.21460724\n",
      "Trained batch 1069 batch loss 7.21184778 epoch total loss 7.21460485\n",
      "Trained batch 1070 batch loss 6.60613441 epoch total loss 7.21403599\n",
      "Trained batch 1071 batch loss 6.94642353 epoch total loss 7.21378613\n",
      "Trained batch 1072 batch loss 7.37487602 epoch total loss 7.21393633\n",
      "Trained batch 1073 batch loss 7.08258533 epoch total loss 7.21381378\n",
      "Trained batch 1074 batch loss 7.19325972 epoch total loss 7.21379471\n",
      "Trained batch 1075 batch loss 6.90706301 epoch total loss 7.21350956\n",
      "Trained batch 1076 batch loss 6.74749613 epoch total loss 7.21307659\n",
      "Trained batch 1077 batch loss 7.33567953 epoch total loss 7.21319\n",
      "Trained batch 1078 batch loss 7.14242697 epoch total loss 7.21312475\n",
      "Trained batch 1079 batch loss 7.28064442 epoch total loss 7.21318769\n",
      "Trained batch 1080 batch loss 7.30275869 epoch total loss 7.21327\n",
      "Trained batch 1081 batch loss 7.47524071 epoch total loss 7.21351242\n",
      "Trained batch 1082 batch loss 6.73091221 epoch total loss 7.21306658\n",
      "Trained batch 1083 batch loss 6.60496521 epoch total loss 7.21250534\n",
      "Trained batch 1084 batch loss 7.17451382 epoch total loss 7.21247\n",
      "Trained batch 1085 batch loss 7.23723602 epoch total loss 7.21249294\n",
      "Trained batch 1086 batch loss 7.44039869 epoch total loss 7.21270275\n",
      "Trained batch 1087 batch loss 7.16585493 epoch total loss 7.21266\n",
      "Trained batch 1088 batch loss 7.06603909 epoch total loss 7.21252489\n",
      "Trained batch 1089 batch loss 7.33054543 epoch total loss 7.21263313\n",
      "Trained batch 1090 batch loss 6.91961718 epoch total loss 7.2123642\n",
      "Trained batch 1091 batch loss 7.29017305 epoch total loss 7.21243525\n",
      "Trained batch 1092 batch loss 7.30298901 epoch total loss 7.21251869\n",
      "Trained batch 1093 batch loss 6.63182497 epoch total loss 7.2119875\n",
      "Trained batch 1094 batch loss 7.09722519 epoch total loss 7.21188259\n",
      "Trained batch 1095 batch loss 7.35124779 epoch total loss 7.21200943\n",
      "Trained batch 1096 batch loss 7.1969676 epoch total loss 7.2119956\n",
      "Trained batch 1097 batch loss 7.26691 epoch total loss 7.21204567\n",
      "Trained batch 1098 batch loss 7.59149647 epoch total loss 7.21239138\n",
      "Trained batch 1099 batch loss 7.27937222 epoch total loss 7.21245193\n",
      "Trained batch 1100 batch loss 7.33211708 epoch total loss 7.21256065\n",
      "Trained batch 1101 batch loss 7.41611624 epoch total loss 7.21274567\n",
      "Trained batch 1102 batch loss 6.98676634 epoch total loss 7.21254063\n",
      "Trained batch 1103 batch loss 6.67729235 epoch total loss 7.21205521\n",
      "Trained batch 1104 batch loss 6.227 epoch total loss 7.21116304\n",
      "Trained batch 1105 batch loss 7.3518014 epoch total loss 7.21129036\n",
      "Trained batch 1106 batch loss 7.40337133 epoch total loss 7.21146393\n",
      "Trained batch 1107 batch loss 7.06262112 epoch total loss 7.21132898\n",
      "Trained batch 1108 batch loss 6.80015802 epoch total loss 7.210958\n",
      "Trained batch 1109 batch loss 6.993 epoch total loss 7.21076202\n",
      "Trained batch 1110 batch loss 6.84495878 epoch total loss 7.21043205\n",
      "Trained batch 1111 batch loss 7.26082897 epoch total loss 7.21047735\n",
      "Trained batch 1112 batch loss 7.37713909 epoch total loss 7.21062708\n",
      "Trained batch 1113 batch loss 7.37482119 epoch total loss 7.2107749\n",
      "Trained batch 1114 batch loss 7.35509872 epoch total loss 7.21090412\n",
      "Trained batch 1115 batch loss 7.30306149 epoch total loss 7.21098709\n",
      "Trained batch 1116 batch loss 6.9780879 epoch total loss 7.21077824\n",
      "Trained batch 1117 batch loss 7.06829786 epoch total loss 7.21065092\n",
      "Trained batch 1118 batch loss 6.39262724 epoch total loss 7.20991898\n",
      "Trained batch 1119 batch loss 6.40199375 epoch total loss 7.20919704\n",
      "Trained batch 1120 batch loss 6.36759329 epoch total loss 7.20844555\n",
      "Trained batch 1121 batch loss 6.92840433 epoch total loss 7.20819569\n",
      "Trained batch 1122 batch loss 6.52149 epoch total loss 7.20758343\n",
      "Trained batch 1123 batch loss 7.27115 epoch total loss 7.20764\n",
      "Trained batch 1124 batch loss 7.03369856 epoch total loss 7.2074852\n",
      "Trained batch 1125 batch loss 7.02772427 epoch total loss 7.20732546\n",
      "Trained batch 1126 batch loss 6.69585705 epoch total loss 7.20687103\n",
      "Trained batch 1127 batch loss 6.79006577 epoch total loss 7.20650148\n",
      "Trained batch 1128 batch loss 6.3090868 epoch total loss 7.20570564\n",
      "Trained batch 1129 batch loss 5.88274813 epoch total loss 7.20453405\n",
      "Trained batch 1130 batch loss 5.87750101 epoch total loss 7.2033596\n",
      "Trained batch 1131 batch loss 6.78875923 epoch total loss 7.20299292\n",
      "Trained batch 1132 batch loss 6.94107866 epoch total loss 7.20276117\n",
      "Trained batch 1133 batch loss 7.54564857 epoch total loss 7.20306396\n",
      "Trained batch 1134 batch loss 6.7875886 epoch total loss 7.20269728\n",
      "Trained batch 1135 batch loss 6.55564547 epoch total loss 7.20212746\n",
      "Trained batch 1136 batch loss 6.85418177 epoch total loss 7.20182085\n",
      "Trained batch 1137 batch loss 7.48606396 epoch total loss 7.20207071\n",
      "Trained batch 1138 batch loss 7.25612211 epoch total loss 7.2021184\n",
      "Trained batch 1139 batch loss 7.47729254 epoch total loss 7.20236\n",
      "Trained batch 1140 batch loss 7.36804914 epoch total loss 7.20250559\n",
      "Trained batch 1141 batch loss 7.59545422 epoch total loss 7.20285034\n",
      "Trained batch 1142 batch loss 7.29513693 epoch total loss 7.20293093\n",
      "Trained batch 1143 batch loss 7.50767899 epoch total loss 7.20319748\n",
      "Trained batch 1144 batch loss 7.39323139 epoch total loss 7.2033639\n",
      "Trained batch 1145 batch loss 6.64676714 epoch total loss 7.20287752\n",
      "Trained batch 1146 batch loss 7.34142065 epoch total loss 7.20299911\n",
      "Trained batch 1147 batch loss 7.38375378 epoch total loss 7.20315647\n",
      "Trained batch 1148 batch loss 7.18530178 epoch total loss 7.20314121\n",
      "Trained batch 1149 batch loss 6.11621571 epoch total loss 7.20219517\n",
      "Trained batch 1150 batch loss 6.60844755 epoch total loss 7.20167875\n",
      "Trained batch 1151 batch loss 7.20875263 epoch total loss 7.20168495\n",
      "Trained batch 1152 batch loss 7.157938 epoch total loss 7.20164728\n",
      "Trained batch 1153 batch loss 7.37893629 epoch total loss 7.2018013\n",
      "Trained batch 1154 batch loss 7.4135046 epoch total loss 7.20198441\n",
      "Trained batch 1155 batch loss 7.17959166 epoch total loss 7.20196486\n",
      "Trained batch 1156 batch loss 7.46320438 epoch total loss 7.20219088\n",
      "Trained batch 1157 batch loss 7.33846188 epoch total loss 7.20230865\n",
      "Trained batch 1158 batch loss 7.53920937 epoch total loss 7.20259953\n",
      "Trained batch 1159 batch loss 7.41871119 epoch total loss 7.20278645\n",
      "Trained batch 1160 batch loss 7.29117107 epoch total loss 7.20286226\n",
      "Trained batch 1161 batch loss 6.97547388 epoch total loss 7.20266676\n",
      "Trained batch 1162 batch loss 6.36834955 epoch total loss 7.20194864\n",
      "Trained batch 1163 batch loss 6.3213625 epoch total loss 7.20119143\n",
      "Trained batch 1164 batch loss 6.05204678 epoch total loss 7.2002039\n",
      "Trained batch 1165 batch loss 6.89742708 epoch total loss 7.19994402\n",
      "Trained batch 1166 batch loss 6.73170185 epoch total loss 7.19954205\n",
      "Trained batch 1167 batch loss 6.41789865 epoch total loss 7.19887209\n",
      "Trained batch 1168 batch loss 6.87764549 epoch total loss 7.19859743\n",
      "Trained batch 1169 batch loss 6.74477196 epoch total loss 7.19821\n",
      "Trained batch 1170 batch loss 6.70088673 epoch total loss 7.1977849\n",
      "Trained batch 1171 batch loss 6.57055855 epoch total loss 7.19724894\n",
      "Trained batch 1172 batch loss 7.22773266 epoch total loss 7.19727468\n",
      "Trained batch 1173 batch loss 6.9185853 epoch total loss 7.1970377\n",
      "Trained batch 1174 batch loss 6.39829969 epoch total loss 7.19635725\n",
      "Trained batch 1175 batch loss 7.33786631 epoch total loss 7.19647789\n",
      "Trained batch 1176 batch loss 7.05683708 epoch total loss 7.19635868\n",
      "Trained batch 1177 batch loss 7.30251551 epoch total loss 7.19644928\n",
      "Trained batch 1178 batch loss 6.84195 epoch total loss 7.19614792\n",
      "Trained batch 1179 batch loss 7.0145154 epoch total loss 7.19599438\n",
      "Trained batch 1180 batch loss 7.48827791 epoch total loss 7.19624186\n",
      "Trained batch 1181 batch loss 7.28417873 epoch total loss 7.19631624\n",
      "Trained batch 1182 batch loss 7.06199551 epoch total loss 7.19620228\n",
      "Trained batch 1183 batch loss 6.77667522 epoch total loss 7.19584751\n",
      "Trained batch 1184 batch loss 7.05331135 epoch total loss 7.19572735\n",
      "Trained batch 1185 batch loss 6.77063465 epoch total loss 7.19536877\n",
      "Trained batch 1186 batch loss 6.90798664 epoch total loss 7.19512653\n",
      "Trained batch 1187 batch loss 7.00285721 epoch total loss 7.19496441\n",
      "Trained batch 1188 batch loss 6.9773078 epoch total loss 7.1947813\n",
      "Trained batch 1189 batch loss 6.59034634 epoch total loss 7.19427347\n",
      "Trained batch 1190 batch loss 6.97717333 epoch total loss 7.19409132\n",
      "Trained batch 1191 batch loss 7.33697367 epoch total loss 7.19421148\n",
      "Trained batch 1192 batch loss 7.14524412 epoch total loss 7.19417048\n",
      "Trained batch 1193 batch loss 7.42399597 epoch total loss 7.19436312\n",
      "Trained batch 1194 batch loss 7.6262722 epoch total loss 7.19472456\n",
      "Trained batch 1195 batch loss 7.31046152 epoch total loss 7.19482136\n",
      "Trained batch 1196 batch loss 6.91398859 epoch total loss 7.19458675\n",
      "Trained batch 1197 batch loss 7.13786793 epoch total loss 7.19453907\n",
      "Trained batch 1198 batch loss 7.03971958 epoch total loss 7.19441032\n",
      "Trained batch 1199 batch loss 6.89753771 epoch total loss 7.19416237\n",
      "Trained batch 1200 batch loss 6.72671318 epoch total loss 7.19377279\n",
      "Trained batch 1201 batch loss 6.8531065 epoch total loss 7.19348955\n",
      "Trained batch 1202 batch loss 6.72565508 epoch total loss 7.1931\n",
      "Trained batch 1203 batch loss 7.30750751 epoch total loss 7.19319534\n",
      "Trained batch 1204 batch loss 7.23034954 epoch total loss 7.19322634\n",
      "Trained batch 1205 batch loss 7.44860458 epoch total loss 7.19343805\n",
      "Trained batch 1206 batch loss 7.36715269 epoch total loss 7.19358206\n",
      "Trained batch 1207 batch loss 7.18185091 epoch total loss 7.19357204\n",
      "Trained batch 1208 batch loss 7.40144444 epoch total loss 7.19374418\n",
      "Trained batch 1209 batch loss 7.24997 epoch total loss 7.19379091\n",
      "Trained batch 1210 batch loss 7.0217557 epoch total loss 7.19364834\n",
      "Trained batch 1211 batch loss 7.34827042 epoch total loss 7.19377613\n",
      "Trained batch 1212 batch loss 7.09922886 epoch total loss 7.19369841\n",
      "Trained batch 1213 batch loss 6.92006922 epoch total loss 7.19347286\n",
      "Trained batch 1214 batch loss 6.8047843 epoch total loss 7.19315243\n",
      "Trained batch 1215 batch loss 6.87978649 epoch total loss 7.19289494\n",
      "Trained batch 1216 batch loss 6.9050436 epoch total loss 7.19265842\n",
      "Trained batch 1217 batch loss 6.71042109 epoch total loss 7.1922617\n",
      "Trained batch 1218 batch loss 6.49131393 epoch total loss 7.19168615\n",
      "Trained batch 1219 batch loss 6.65701199 epoch total loss 7.19124746\n",
      "Trained batch 1220 batch loss 6.61357307 epoch total loss 7.19077396\n",
      "Trained batch 1221 batch loss 6.18498659 epoch total loss 7.18995\n",
      "Trained batch 1222 batch loss 6.21404028 epoch total loss 7.18915081\n",
      "Trained batch 1223 batch loss 6.48350382 epoch total loss 7.18857384\n",
      "Trained batch 1224 batch loss 7.10886383 epoch total loss 7.18850851\n",
      "Trained batch 1225 batch loss 7.46351433 epoch total loss 7.1887331\n",
      "Trained batch 1226 batch loss 7.19232225 epoch total loss 7.18873644\n",
      "Trained batch 1227 batch loss 6.78748322 epoch total loss 7.18840885\n",
      "Trained batch 1228 batch loss 6.68170166 epoch total loss 7.18799639\n",
      "Trained batch 1229 batch loss 7.10133696 epoch total loss 7.18792582\n",
      "Trained batch 1230 batch loss 6.86687708 epoch total loss 7.18766499\n",
      "Trained batch 1231 batch loss 7.14992 epoch total loss 7.18763494\n",
      "Trained batch 1232 batch loss 6.87562847 epoch total loss 7.18738174\n",
      "Trained batch 1233 batch loss 6.98564529 epoch total loss 7.18721819\n",
      "Trained batch 1234 batch loss 7.33884 epoch total loss 7.18734074\n",
      "Trained batch 1235 batch loss 7.32518578 epoch total loss 7.18745279\n",
      "Trained batch 1236 batch loss 7.65924549 epoch total loss 7.18783426\n",
      "Trained batch 1237 batch loss 7.12841034 epoch total loss 7.18778563\n",
      "Trained batch 1238 batch loss 7.35830593 epoch total loss 7.18792343\n",
      "Trained batch 1239 batch loss 6.92887 epoch total loss 7.18771458\n",
      "Trained batch 1240 batch loss 7.28589916 epoch total loss 7.18779373\n",
      "Trained batch 1241 batch loss 6.99545288 epoch total loss 7.18763828\n",
      "Trained batch 1242 batch loss 6.43589878 epoch total loss 7.18703318\n",
      "Trained batch 1243 batch loss 6.84760332 epoch total loss 7.18676\n",
      "Trained batch 1244 batch loss 7.09218311 epoch total loss 7.18668365\n",
      "Trained batch 1245 batch loss 7.09187841 epoch total loss 7.18660736\n",
      "Trained batch 1246 batch loss 7.20941639 epoch total loss 7.18662548\n",
      "Trained batch 1247 batch loss 7.17427683 epoch total loss 7.18661499\n",
      "Trained batch 1248 batch loss 7.30726385 epoch total loss 7.18671179\n",
      "Trained batch 1249 batch loss 7.34607458 epoch total loss 7.1868391\n",
      "Trained batch 1250 batch loss 6.89293909 epoch total loss 7.18660402\n",
      "Trained batch 1251 batch loss 6.84741688 epoch total loss 7.18633318\n",
      "Trained batch 1252 batch loss 6.80079174 epoch total loss 7.18602514\n",
      "Trained batch 1253 batch loss 6.70124483 epoch total loss 7.18563795\n",
      "Trained batch 1254 batch loss 6.98655605 epoch total loss 7.18547916\n",
      "Trained batch 1255 batch loss 7.23381853 epoch total loss 7.18551731\n",
      "Trained batch 1256 batch loss 7.15884781 epoch total loss 7.18549633\n",
      "Trained batch 1257 batch loss 7.36361599 epoch total loss 7.18563795\n",
      "Trained batch 1258 batch loss 7.01528597 epoch total loss 7.18550253\n",
      "Trained batch 1259 batch loss 6.88076258 epoch total loss 7.18526077\n",
      "Trained batch 1260 batch loss 6.94866896 epoch total loss 7.18507242\n",
      "Trained batch 1261 batch loss 6.45334482 epoch total loss 7.18449211\n",
      "Trained batch 1262 batch loss 6.14965105 epoch total loss 7.18367195\n",
      "Trained batch 1263 batch loss 6.32238865 epoch total loss 7.18299\n",
      "Trained batch 1264 batch loss 6.84400654 epoch total loss 7.18272161\n",
      "Trained batch 1265 batch loss 6.34461117 epoch total loss 7.18205881\n",
      "Trained batch 1266 batch loss 7.22118092 epoch total loss 7.18208933\n",
      "Trained batch 1267 batch loss 7.23758316 epoch total loss 7.1821332\n",
      "Trained batch 1268 batch loss 7.4280262 epoch total loss 7.18232679\n",
      "Trained batch 1269 batch loss 7.44048691 epoch total loss 7.1825304\n",
      "Trained batch 1270 batch loss 7.43859291 epoch total loss 7.18273163\n",
      "Trained batch 1271 batch loss 7.33406258 epoch total loss 7.18285084\n",
      "Trained batch 1272 batch loss 7.14196157 epoch total loss 7.18281841\n",
      "Trained batch 1273 batch loss 6.35461521 epoch total loss 7.18216753\n",
      "Trained batch 1274 batch loss 6.41848803 epoch total loss 7.18156862\n",
      "Trained batch 1275 batch loss 7.34507895 epoch total loss 7.18169641\n",
      "Trained batch 1276 batch loss 7.38164759 epoch total loss 7.18185329\n",
      "Trained batch 1277 batch loss 7.18630648 epoch total loss 7.18185711\n",
      "Trained batch 1278 batch loss 7.32796144 epoch total loss 7.18197155\n",
      "Trained batch 1279 batch loss 7.38315535 epoch total loss 7.18212843\n",
      "Trained batch 1280 batch loss 6.89875555 epoch total loss 7.1819067\n",
      "Trained batch 1281 batch loss 7.0148344 epoch total loss 7.18177652\n",
      "Trained batch 1282 batch loss 6.74121141 epoch total loss 7.18143272\n",
      "Trained batch 1283 batch loss 7.06269503 epoch total loss 7.18133974\n",
      "Trained batch 1284 batch loss 6.93966103 epoch total loss 7.18115139\n",
      "Trained batch 1285 batch loss 6.82147455 epoch total loss 7.18087149\n",
      "Trained batch 1286 batch loss 6.72832203 epoch total loss 7.18051958\n",
      "Trained batch 1287 batch loss 7.1073308 epoch total loss 7.18046284\n",
      "Trained batch 1288 batch loss 7.01527071 epoch total loss 7.18033504\n",
      "Trained batch 1289 batch loss 6.71299934 epoch total loss 7.17997217\n",
      "Trained batch 1290 batch loss 7.35673714 epoch total loss 7.18010902\n",
      "Trained batch 1291 batch loss 7.03541565 epoch total loss 7.17999697\n",
      "Trained batch 1292 batch loss 6.41643572 epoch total loss 7.17940569\n",
      "Trained batch 1293 batch loss 7.06418276 epoch total loss 7.17931652\n",
      "Trained batch 1294 batch loss 7.49841881 epoch total loss 7.17956305\n",
      "Trained batch 1295 batch loss 7.37296391 epoch total loss 7.1797123\n",
      "Trained batch 1296 batch loss 7.38899708 epoch total loss 7.17987347\n",
      "Trained batch 1297 batch loss 7.14058352 epoch total loss 7.17984343\n",
      "Trained batch 1298 batch loss 7.18345308 epoch total loss 7.17984629\n",
      "Trained batch 1299 batch loss 6.68217468 epoch total loss 7.17946339\n",
      "Trained batch 1300 batch loss 7.03053522 epoch total loss 7.17934895\n",
      "Trained batch 1301 batch loss 6.52200508 epoch total loss 7.17884398\n",
      "Trained batch 1302 batch loss 6.90466309 epoch total loss 7.17863274\n",
      "Trained batch 1303 batch loss 7.18165588 epoch total loss 7.17863512\n",
      "Trained batch 1304 batch loss 7.35168362 epoch total loss 7.17876768\n",
      "Trained batch 1305 batch loss 7.41030884 epoch total loss 7.17894506\n",
      "Trained batch 1306 batch loss 7.33516693 epoch total loss 7.17906475\n",
      "Trained batch 1307 batch loss 7.31356144 epoch total loss 7.17916727\n",
      "Trained batch 1308 batch loss 7.19703722 epoch total loss 7.1791811\n",
      "Trained batch 1309 batch loss 7.29222679 epoch total loss 7.17926741\n",
      "Trained batch 1310 batch loss 7.33161068 epoch total loss 7.17938423\n",
      "Trained batch 1311 batch loss 7.29476261 epoch total loss 7.17947245\n",
      "Trained batch 1312 batch loss 7.17945623 epoch total loss 7.17947245\n",
      "Trained batch 1313 batch loss 6.87510109 epoch total loss 7.1792407\n",
      "Trained batch 1314 batch loss 6.48836517 epoch total loss 7.17871475\n",
      "Trained batch 1315 batch loss 7.32752705 epoch total loss 7.17882776\n",
      "Trained batch 1316 batch loss 7.45339298 epoch total loss 7.17903614\n",
      "Trained batch 1317 batch loss 6.97369432 epoch total loss 7.17888\n",
      "Trained batch 1318 batch loss 6.94581747 epoch total loss 7.17870331\n",
      "Trained batch 1319 batch loss 7.47186661 epoch total loss 7.17892551\n",
      "Trained batch 1320 batch loss 7.29376507 epoch total loss 7.17901278\n",
      "Trained batch 1321 batch loss 7.19944668 epoch total loss 7.17902803\n",
      "Trained batch 1322 batch loss 7.18158484 epoch total loss 7.17903\n",
      "Trained batch 1323 batch loss 6.59287119 epoch total loss 7.17858696\n",
      "Trained batch 1324 batch loss 5.72082281 epoch total loss 7.17748594\n",
      "Trained batch 1325 batch loss 6.61586761 epoch total loss 7.17706203\n",
      "Trained batch 1326 batch loss 6.28181314 epoch total loss 7.17638731\n",
      "Trained batch 1327 batch loss 7.17999554 epoch total loss 7.17638969\n",
      "Trained batch 1328 batch loss 7.09409523 epoch total loss 7.17632771\n",
      "Trained batch 1329 batch loss 7.32406902 epoch total loss 7.17643881\n",
      "Trained batch 1330 batch loss 6.9159 epoch total loss 7.17624331\n",
      "Trained batch 1331 batch loss 7.14957714 epoch total loss 7.1762228\n",
      "Trained batch 1332 batch loss 7.17554188 epoch total loss 7.1762228\n",
      "Trained batch 1333 batch loss 7.28355 epoch total loss 7.17630291\n",
      "Trained batch 1334 batch loss 6.70178652 epoch total loss 7.17594719\n",
      "Trained batch 1335 batch loss 6.43823338 epoch total loss 7.17539501\n",
      "Trained batch 1336 batch loss 7.26913548 epoch total loss 7.17546558\n",
      "Trained batch 1337 batch loss 7.29291391 epoch total loss 7.17555332\n",
      "Trained batch 1338 batch loss 6.78014 epoch total loss 7.17525816\n",
      "Trained batch 1339 batch loss 6.95379734 epoch total loss 7.1750927\n",
      "Trained batch 1340 batch loss 7.17718935 epoch total loss 7.17509413\n",
      "Trained batch 1341 batch loss 7.01830912 epoch total loss 7.1749773\n",
      "Trained batch 1342 batch loss 6.74548054 epoch total loss 7.17465687\n",
      "Trained batch 1343 batch loss 6.08996868 epoch total loss 7.17384911\n",
      "Trained batch 1344 batch loss 6.28673887 epoch total loss 7.17318964\n",
      "Trained batch 1345 batch loss 7.07228088 epoch total loss 7.1731143\n",
      "Trained batch 1346 batch loss 6.96354246 epoch total loss 7.17295885\n",
      "Trained batch 1347 batch loss 7.08262968 epoch total loss 7.17289209\n",
      "Trained batch 1348 batch loss 7.43225 epoch total loss 7.17308474\n",
      "Trained batch 1349 batch loss 7.50528431 epoch total loss 7.17333078\n",
      "Trained batch 1350 batch loss 7.39376068 epoch total loss 7.17349386\n",
      "Trained batch 1351 batch loss 7.45393181 epoch total loss 7.17370176\n",
      "Trained batch 1352 batch loss 7.36270571 epoch total loss 7.173841\n",
      "Trained batch 1353 batch loss 6.72714615 epoch total loss 7.17351103\n",
      "Trained batch 1354 batch loss 7.11957502 epoch total loss 7.17347097\n",
      "Trained batch 1355 batch loss 7.39511347 epoch total loss 7.17363501\n",
      "Trained batch 1356 batch loss 7.02720118 epoch total loss 7.17352724\n",
      "Trained batch 1357 batch loss 6.81041288 epoch total loss 7.17325974\n",
      "Trained batch 1358 batch loss 7.31326723 epoch total loss 7.17336273\n",
      "Trained batch 1359 batch loss 6.85089874 epoch total loss 7.17312527\n",
      "Trained batch 1360 batch loss 6.81158829 epoch total loss 7.17285967\n",
      "Trained batch 1361 batch loss 7.12522459 epoch total loss 7.17282438\n",
      "Trained batch 1362 batch loss 7.28456116 epoch total loss 7.17290592\n",
      "Trained batch 1363 batch loss 7.41449308 epoch total loss 7.17308283\n",
      "Trained batch 1364 batch loss 7.31522465 epoch total loss 7.17318726\n",
      "Trained batch 1365 batch loss 7.47922754 epoch total loss 7.17341185\n",
      "Trained batch 1366 batch loss 7.58808231 epoch total loss 7.17371511\n",
      "Trained batch 1367 batch loss 7.26046276 epoch total loss 7.17377901\n",
      "Trained batch 1368 batch loss 7.0186739 epoch total loss 7.17366552\n",
      "Trained batch 1369 batch loss 7.33843327 epoch total loss 7.17378616\n",
      "Trained batch 1370 batch loss 7.40354061 epoch total loss 7.17395353\n",
      "Trained batch 1371 batch loss 7.11129856 epoch total loss 7.17390776\n",
      "Trained batch 1372 batch loss 7.01760817 epoch total loss 7.17379379\n",
      "Trained batch 1373 batch loss 7.18718624 epoch total loss 7.17380381\n",
      "Trained batch 1374 batch loss 6.8139267 epoch total loss 7.17354155\n",
      "Trained batch 1375 batch loss 7.07940197 epoch total loss 7.17347288\n",
      "Trained batch 1376 batch loss 6.85818958 epoch total loss 7.173244\n",
      "Trained batch 1377 batch loss 7.04892159 epoch total loss 7.17315388\n",
      "Trained batch 1378 batch loss 7.30316639 epoch total loss 7.17324781\n",
      "Trained batch 1379 batch loss 7.3523674 epoch total loss 7.17337751\n",
      "Trained batch 1380 batch loss 7.48463821 epoch total loss 7.17360306\n",
      "Trained batch 1381 batch loss 6.72747135 epoch total loss 7.17328024\n",
      "Trained batch 1382 batch loss 7.39832973 epoch total loss 7.17344284\n",
      "Trained batch 1383 batch loss 6.8966465 epoch total loss 7.17324257\n",
      "Trained batch 1384 batch loss 7.30985498 epoch total loss 7.17334127\n",
      "Trained batch 1385 batch loss 7.20210457 epoch total loss 7.17336226\n",
      "Trained batch 1386 batch loss 7.14733887 epoch total loss 7.17334318\n",
      "Trained batch 1387 batch loss 6.93126154 epoch total loss 7.17316914\n",
      "Trained batch 1388 batch loss 6.7292161 epoch total loss 7.17284966\n",
      "Epoch 1 train loss 7.172849655151367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 03:01:57.753605: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "2025-03-19 03:01:57.753662: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "\t [[RemoteCall]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 1 batch loss 7.3765173\n",
      "Validated batch 2 batch loss 7.14347792\n",
      "Validated batch 3 batch loss 7.04288197\n",
      "Validated batch 4 batch loss 6.79136\n",
      "Validated batch 5 batch loss 6.93003321\n",
      "Validated batch 6 batch loss 7.1625886\n",
      "Validated batch 7 batch loss 7.05498123\n",
      "Validated batch 8 batch loss 7.33963633\n",
      "Validated batch 9 batch loss 7.39343929\n",
      "Validated batch 10 batch loss 6.99930143\n",
      "Validated batch 11 batch loss 7.1839571\n",
      "Validated batch 12 batch loss 6.63680887\n",
      "Validated batch 13 batch loss 7.25484753\n",
      "Validated batch 14 batch loss 7.03480339\n",
      "Validated batch 15 batch loss 7.20408583\n",
      "Validated batch 16 batch loss 7.32905388\n",
      "Validated batch 17 batch loss 7.0706625\n",
      "Validated batch 18 batch loss 6.38422251\n",
      "Validated batch 19 batch loss 6.80314\n",
      "Validated batch 20 batch loss 7.32411528\n",
      "Validated batch 21 batch loss 6.81283426\n",
      "Validated batch 22 batch loss 6.86860228\n",
      "Validated batch 23 batch loss 7.02140284\n",
      "Validated batch 24 batch loss 6.97261381\n",
      "Validated batch 25 batch loss 6.6649394\n",
      "Validated batch 26 batch loss 7.24478817\n",
      "Validated batch 27 batch loss 7.34069586\n",
      "Validated batch 28 batch loss 6.93098593\n",
      "Validated batch 29 batch loss 7.18893623\n",
      "Validated batch 30 batch loss 7.14862967\n",
      "Validated batch 31 batch loss 7.34014416\n",
      "Validated batch 32 batch loss 6.99229097\n",
      "Validated batch 33 batch loss 7.23005581\n",
      "Validated batch 34 batch loss 6.91152716\n",
      "Validated batch 35 batch loss 7.0529809\n",
      "Validated batch 36 batch loss 7.17186928\n",
      "Validated batch 37 batch loss 7.37123\n",
      "Validated batch 38 batch loss 7.2793417\n",
      "Validated batch 39 batch loss 6.99010229\n",
      "Validated batch 40 batch loss 7.49329948\n",
      "Validated batch 41 batch loss 6.57345819\n",
      "Validated batch 42 batch loss 7.35916138\n",
      "Validated batch 43 batch loss 7.50059414\n",
      "Validated batch 44 batch loss 7.27886963\n",
      "Validated batch 45 batch loss 7.3996253\n",
      "Validated batch 46 batch loss 6.93571568\n",
      "Validated batch 47 batch loss 7.01158094\n",
      "Validated batch 48 batch loss 7.20249891\n",
      "Validated batch 49 batch loss 6.94298506\n",
      "Validated batch 50 batch loss 7.40558767\n",
      "Validated batch 51 batch loss 7.52008915\n",
      "Validated batch 52 batch loss 7.53265905\n",
      "Validated batch 53 batch loss 7.3784256\n",
      "Validated batch 54 batch loss 7.22909\n",
      "Validated batch 55 batch loss 7.30096\n",
      "Validated batch 56 batch loss 7.45484638\n",
      "Validated batch 57 batch loss 7.57022524\n",
      "Validated batch 58 batch loss 7.34947348\n",
      "Validated batch 59 batch loss 6.91110754\n",
      "Validated batch 60 batch loss 7.31098604\n",
      "Validated batch 61 batch loss 7.00076294\n",
      "Validated batch 62 batch loss 7.01301718\n",
      "Validated batch 63 batch loss 7.3247056\n",
      "Validated batch 64 batch loss 6.26727676\n",
      "Validated batch 65 batch loss 6.99020815\n",
      "Validated batch 66 batch loss 7.51148367\n",
      "Validated batch 67 batch loss 7.02489281\n",
      "Validated batch 68 batch loss 7.52241373\n",
      "Validated batch 69 batch loss 7.23741627\n",
      "Validated batch 70 batch loss 7.06110525\n",
      "Validated batch 71 batch loss 7.19517756\n",
      "Validated batch 72 batch loss 6.72775269\n",
      "Validated batch 73 batch loss 6.4708786\n",
      "Validated batch 74 batch loss 6.8075\n",
      "Validated batch 75 batch loss 7.32871246\n",
      "Validated batch 76 batch loss 6.81445599\n",
      "Validated batch 77 batch loss 7.18441963\n",
      "Validated batch 78 batch loss 7.39816475\n",
      "Validated batch 79 batch loss 7.37889385\n",
      "Validated batch 80 batch loss 7.35257292\n",
      "Validated batch 81 batch loss 6.82454062\n",
      "Validated batch 82 batch loss 7.15371084\n",
      "Validated batch 83 batch loss 7.15071106\n",
      "Validated batch 84 batch loss 7.32870865\n",
      "Validated batch 85 batch loss 7.47784853\n",
      "Validated batch 86 batch loss 7.07577753\n",
      "Validated batch 87 batch loss 7.36827898\n",
      "Validated batch 88 batch loss 7.50803328\n",
      "Validated batch 89 batch loss 7.19500828\n",
      "Validated batch 90 batch loss 7.36307526\n",
      "Validated batch 91 batch loss 7.34101486\n",
      "Validated batch 92 batch loss 7.46953\n",
      "Validated batch 93 batch loss 7.62913847\n",
      "Validated batch 94 batch loss 7.16968155\n",
      "Validated batch 95 batch loss 7.27970266\n",
      "Validated batch 96 batch loss 7.1918087\n",
      "Validated batch 97 batch loss 7.37147045\n",
      "Validated batch 98 batch loss 7.48487854\n",
      "Validated batch 99 batch loss 7.18729639\n",
      "Validated batch 100 batch loss 6.88843536\n",
      "Validated batch 101 batch loss 6.99906683\n",
      "Validated batch 102 batch loss 7.01034832\n",
      "Validated batch 103 batch loss 7.29816341\n",
      "Validated batch 104 batch loss 7.15651\n",
      "Validated batch 105 batch loss 6.76553345\n",
      "Validated batch 106 batch loss 6.77921724\n",
      "Validated batch 107 batch loss 6.99291086\n",
      "Validated batch 108 batch loss 7.47918463\n",
      "Validated batch 109 batch loss 7.42124081\n",
      "Validated batch 110 batch loss 7.23473692\n",
      "Validated batch 111 batch loss 7.61242247\n",
      "Validated batch 112 batch loss 7.51174784\n",
      "Validated batch 113 batch loss 7.52628088\n",
      "Validated batch 114 batch loss 6.98931742\n",
      "Validated batch 115 batch loss 6.81391907\n",
      "Validated batch 116 batch loss 6.56183529\n",
      "Validated batch 117 batch loss 7.34421825\n",
      "Validated batch 118 batch loss 7.12014866\n",
      "Validated batch 119 batch loss 7.32638359\n",
      "Validated batch 120 batch loss 7.10200882\n",
      "Validated batch 121 batch loss 7.40962934\n",
      "Validated batch 122 batch loss 6.95398808\n",
      "Validated batch 123 batch loss 7.44831181\n",
      "Validated batch 124 batch loss 7.06387758\n",
      "Validated batch 125 batch loss 6.92730808\n",
      "Validated batch 126 batch loss 7.19336605\n",
      "Validated batch 127 batch loss 6.92938709\n",
      "Validated batch 128 batch loss 6.29434\n",
      "Validated batch 129 batch loss 7.44117212\n",
      "Validated batch 130 batch loss 7.36443138\n",
      "Validated batch 131 batch loss 7.13596582\n",
      "Validated batch 132 batch loss 7.24039078\n",
      "Validated batch 133 batch loss 7.35228634\n",
      "Validated batch 134 batch loss 7.4322381\n",
      "Validated batch 135 batch loss 7.43755198\n",
      "Validated batch 136 batch loss 7.46354294\n",
      "Validated batch 137 batch loss 7.31312752\n",
      "Validated batch 138 batch loss 6.64500189\n",
      "Validated batch 139 batch loss 7.26202297\n",
      "Validated batch 140 batch loss 6.92812252\n",
      "Validated batch 141 batch loss 6.93096399\n",
      "Validated batch 142 batch loss 6.99363184\n",
      "Validated batch 143 batch loss 6.9568162\n",
      "Validated batch 144 batch loss 7.2467742\n",
      "Validated batch 145 batch loss 6.97215033\n",
      "Validated batch 146 batch loss 7.40410709\n",
      "Validated batch 147 batch loss 7.17545128\n",
      "Validated batch 148 batch loss 7.1765008\n",
      "Validated batch 149 batch loss 7.51107216\n",
      "Validated batch 150 batch loss 7.52337742\n",
      "Validated batch 151 batch loss 6.93841696\n",
      "Validated batch 152 batch loss 6.98053646\n",
      "Validated batch 153 batch loss 7.55123663\n",
      "Validated batch 154 batch loss 6.8547368\n",
      "Validated batch 155 batch loss 7.33341599\n",
      "Validated batch 156 batch loss 6.88266134\n",
      "Validated batch 157 batch loss 7.49703836\n",
      "Validated batch 158 batch loss 6.83266687\n",
      "Validated batch 159 batch loss 7.06382656\n",
      "Validated batch 160 batch loss 7.10712051\n",
      "Validated batch 161 batch loss 7.17945433\n",
      "Validated batch 162 batch loss 7.37624788\n",
      "Validated batch 163 batch loss 7.58849096\n",
      "Validated batch 164 batch loss 7.11112976\n",
      "Validated batch 165 batch loss 6.48548937\n",
      "Validated batch 166 batch loss 7.04365587\n",
      "Validated batch 167 batch loss 6.83551788\n",
      "Validated batch 168 batch loss 6.78419209\n",
      "Validated batch 169 batch loss 6.85281849\n",
      "Validated batch 170 batch loss 6.54473829\n",
      "Validated batch 171 batch loss 7.35283232\n",
      "Validated batch 172 batch loss 7.19296503\n",
      "Validated batch 173 batch loss 6.66466618\n",
      "Validated batch 174 batch loss 6.50267029\n",
      "Validated batch 175 batch loss 7.38633347\n",
      "Validated batch 176 batch loss 6.82563305\n",
      "Validated batch 177 batch loss 7.2882843\n",
      "Validated batch 178 batch loss 6.99150705\n",
      "Validated batch 179 batch loss 7.49349451\n",
      "Validated batch 180 batch loss 6.99725199\n",
      "Validated batch 181 batch loss 7.05554\n",
      "Validated batch 182 batch loss 7.20629072\n",
      "Validated batch 183 batch loss 6.35451794\n",
      "Validated batch 184 batch loss 7.13486099\n",
      "Validated batch 185 batch loss 3.71447039\n",
      "Epoch 1 val loss 7.118393898010254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 03:02:06.507735: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "2025-03-19 03:02:06.507801: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "\t [[RemoteCall]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model /home/minho/Desktop/aiffel/pose_estimation/models/model-epoch-1-loss-7.1184.weights.h5 saved.\n",
      "Start epoch 2 with learning rate 0.001\n",
      "Start distributed training...\n",
      "Trained batch 1 batch loss 6.61769295 epoch total loss 6.61769295\n",
      "Trained batch 2 batch loss 7.03147364 epoch total loss 6.82458305\n",
      "Trained batch 3 batch loss 6.81523371 epoch total loss 6.82146645\n",
      "Trained batch 4 batch loss 7.00322294 epoch total loss 6.86690569\n",
      "Trained batch 5 batch loss 6.94633579 epoch total loss 6.882792\n",
      "Trained batch 6 batch loss 7.37225914 epoch total loss 6.9643693\n",
      "Trained batch 7 batch loss 7.28154755 epoch total loss 7.00968075\n",
      "Trained batch 8 batch loss 7.15796232 epoch total loss 7.02821589\n",
      "Trained batch 9 batch loss 7.39990139 epoch total loss 7.06951427\n",
      "Trained batch 10 batch loss 7.16413641 epoch total loss 7.07897663\n",
      "Trained batch 11 batch loss 6.43176365 epoch total loss 7.02013874\n",
      "Trained batch 12 batch loss 6.91500378 epoch total loss 7.01137733\n",
      "Trained batch 13 batch loss 6.8765192 epoch total loss 7.00100374\n",
      "Trained batch 14 batch loss 7.12413073 epoch total loss 7.00979853\n",
      "Trained batch 15 batch loss 7.30920029 epoch total loss 7.02975893\n",
      "Trained batch 16 batch loss 7.30280495 epoch total loss 7.04682398\n",
      "Trained batch 17 batch loss 6.7249856 epoch total loss 7.02789211\n",
      "Trained batch 18 batch loss 7.01852751 epoch total loss 7.02737188\n",
      "Trained batch 19 batch loss 7.24103308 epoch total loss 7.03861666\n",
      "Trained batch 20 batch loss 7.32795334 epoch total loss 7.0530839\n",
      "Trained batch 21 batch loss 7.2310915 epoch total loss 7.06156063\n",
      "Trained batch 22 batch loss 7.21778393 epoch total loss 7.06866169\n",
      "Trained batch 23 batch loss 7.71153069 epoch total loss 7.09661245\n",
      "Trained batch 24 batch loss 7.03656149 epoch total loss 7.09411049\n",
      "Trained batch 25 batch loss 7.3428793 epoch total loss 7.10406113\n",
      "Trained batch 26 batch loss 7.45628405 epoch total loss 7.11760807\n",
      "Trained batch 27 batch loss 7.06267357 epoch total loss 7.11557341\n",
      "Trained batch 28 batch loss 6.88017464 epoch total loss 7.10716629\n",
      "Trained batch 29 batch loss 7.17231321 epoch total loss 7.10941267\n",
      "Trained batch 30 batch loss 7.18054914 epoch total loss 7.11178398\n",
      "Trained batch 31 batch loss 7.31424904 epoch total loss 7.11831522\n",
      "Trained batch 32 batch loss 6.84714746 epoch total loss 7.10984135\n",
      "Trained batch 33 batch loss 7.31857157 epoch total loss 7.11616659\n",
      "Trained batch 34 batch loss 7.23104572 epoch total loss 7.11954546\n",
      "Trained batch 35 batch loss 7.41012669 epoch total loss 7.12784767\n",
      "Trained batch 36 batch loss 7.30604315 epoch total loss 7.13279724\n",
      "Trained batch 37 batch loss 7.3835845 epoch total loss 7.139575\n",
      "Trained batch 38 batch loss 6.46467304 epoch total loss 7.12181425\n",
      "Trained batch 39 batch loss 6.42563295 epoch total loss 7.10396338\n",
      "Trained batch 40 batch loss 6.50847626 epoch total loss 7.08907604\n",
      "Trained batch 41 batch loss 7.10630798 epoch total loss 7.08949709\n",
      "Trained batch 42 batch loss 7.10418463 epoch total loss 7.08984661\n",
      "Trained batch 43 batch loss 7.56205702 epoch total loss 7.10082817\n",
      "Trained batch 44 batch loss 7.33759212 epoch total loss 7.1062088\n",
      "Trained batch 45 batch loss 7.44539928 epoch total loss 7.11374664\n",
      "Trained batch 46 batch loss 6.93268299 epoch total loss 7.10981035\n",
      "Trained batch 47 batch loss 7.1541338 epoch total loss 7.11075354\n",
      "Trained batch 48 batch loss 7.31550646 epoch total loss 7.11501932\n",
      "Trained batch 49 batch loss 6.94925737 epoch total loss 7.11163664\n",
      "Trained batch 50 batch loss 7.29615688 epoch total loss 7.1153264\n",
      "Trained batch 51 batch loss 7.41322708 epoch total loss 7.12116814\n",
      "Trained batch 52 batch loss 6.76981544 epoch total loss 7.11441088\n",
      "Trained batch 53 batch loss 7.23235083 epoch total loss 7.11663628\n",
      "Trained batch 54 batch loss 7.32110405 epoch total loss 7.12042284\n",
      "Trained batch 55 batch loss 7.10937357 epoch total loss 7.12022209\n",
      "Trained batch 56 batch loss 7.42798519 epoch total loss 7.12571764\n",
      "Trained batch 57 batch loss 7.1573844 epoch total loss 7.12627316\n",
      "Trained batch 58 batch loss 7.41697 epoch total loss 7.13128519\n",
      "Trained batch 59 batch loss 7.28031635 epoch total loss 7.133811\n",
      "Trained batch 60 batch loss 7.46072 epoch total loss 7.13925934\n",
      "Trained batch 61 batch loss 7.21382236 epoch total loss 7.14048195\n",
      "Trained batch 62 batch loss 6.62206602 epoch total loss 7.13212061\n",
      "Trained batch 63 batch loss 7.07811165 epoch total loss 7.13126326\n",
      "Trained batch 64 batch loss 6.59575891 epoch total loss 7.12289619\n",
      "Trained batch 65 batch loss 7.10469532 epoch total loss 7.12261629\n",
      "Trained batch 66 batch loss 7.07859945 epoch total loss 7.12194967\n",
      "Trained batch 67 batch loss 7.48845434 epoch total loss 7.12742\n",
      "Trained batch 68 batch loss 7.44470215 epoch total loss 7.1320858\n",
      "Trained batch 69 batch loss 7.43399143 epoch total loss 7.13646126\n",
      "Trained batch 70 batch loss 7.37426376 epoch total loss 7.13985872\n",
      "Trained batch 71 batch loss 7.31716633 epoch total loss 7.14235592\n",
      "Trained batch 72 batch loss 7.55551291 epoch total loss 7.14809418\n",
      "Trained batch 73 batch loss 7.25636625 epoch total loss 7.14957714\n",
      "Trained batch 74 batch loss 6.91150379 epoch total loss 7.14636\n",
      "Trained batch 75 batch loss 7.46922779 epoch total loss 7.15066481\n",
      "Trained batch 76 batch loss 7.55821371 epoch total loss 7.15602732\n",
      "Trained batch 77 batch loss 7.24120426 epoch total loss 7.15713406\n",
      "Trained batch 78 batch loss 7.33589554 epoch total loss 7.15942526\n",
      "Trained batch 79 batch loss 7.29827833 epoch total loss 7.16118288\n",
      "Trained batch 80 batch loss 7.19072771 epoch total loss 7.16155243\n",
      "Trained batch 81 batch loss 7.30104876 epoch total loss 7.16327429\n",
      "Trained batch 82 batch loss 7.25605202 epoch total loss 7.16440582\n",
      "Trained batch 83 batch loss 6.47796154 epoch total loss 7.15613508\n",
      "Trained batch 84 batch loss 7.18272638 epoch total loss 7.15645218\n",
      "Trained batch 85 batch loss 7.00125456 epoch total loss 7.15462637\n",
      "Trained batch 86 batch loss 7.58943558 epoch total loss 7.15968227\n",
      "Trained batch 87 batch loss 7.1156497 epoch total loss 7.15917635\n",
      "Trained batch 88 batch loss 7.38865948 epoch total loss 7.16178417\n",
      "Trained batch 89 batch loss 6.99243736 epoch total loss 7.15988111\n",
      "Trained batch 90 batch loss 6.78354216 epoch total loss 7.1557\n",
      "Trained batch 91 batch loss 7.33204126 epoch total loss 7.1576376\n",
      "Trained batch 92 batch loss 7.34617138 epoch total loss 7.15968704\n",
      "Trained batch 93 batch loss 6.92563391 epoch total loss 7.15717077\n",
      "Trained batch 94 batch loss 6.95385885 epoch total loss 7.15500784\n",
      "Trained batch 95 batch loss 6.63183594 epoch total loss 7.14950085\n",
      "Trained batch 96 batch loss 6.71276093 epoch total loss 7.14495134\n",
      "Trained batch 97 batch loss 6.98173618 epoch total loss 7.14326906\n",
      "Trained batch 98 batch loss 7.34094334 epoch total loss 7.14528608\n",
      "Trained batch 99 batch loss 7.46396637 epoch total loss 7.14850521\n",
      "Trained batch 100 batch loss 7.3453846 epoch total loss 7.15047407\n",
      "Trained batch 101 batch loss 7.44650221 epoch total loss 7.15340471\n",
      "Trained batch 102 batch loss 7.41301823 epoch total loss 7.15595\n",
      "Trained batch 103 batch loss 7.34846497 epoch total loss 7.15781927\n",
      "Trained batch 104 batch loss 6.87270737 epoch total loss 7.15507746\n",
      "Trained batch 105 batch loss 6.05144596 epoch total loss 7.14456654\n",
      "Trained batch 106 batch loss 5.98689604 epoch total loss 7.13364506\n",
      "Trained batch 107 batch loss 6.29662848 epoch total loss 7.12582254\n",
      "Trained batch 108 batch loss 6.25668383 epoch total loss 7.11777544\n",
      "Trained batch 109 batch loss 6.88241529 epoch total loss 7.11561584\n",
      "Trained batch 110 batch loss 6.9458704 epoch total loss 7.11407232\n",
      "Trained batch 111 batch loss 7.06955576 epoch total loss 7.11367178\n",
      "Trained batch 112 batch loss 6.8655405 epoch total loss 7.11145639\n",
      "Trained batch 113 batch loss 6.86764288 epoch total loss 7.10929823\n",
      "Trained batch 114 batch loss 7.37304211 epoch total loss 7.11161184\n",
      "Trained batch 115 batch loss 7.51333141 epoch total loss 7.11510468\n",
      "Trained batch 116 batch loss 6.83081484 epoch total loss 7.11265421\n",
      "Trained batch 117 batch loss 7.36501169 epoch total loss 7.11481094\n",
      "Trained batch 118 batch loss 6.75685 epoch total loss 7.11177731\n",
      "Trained batch 119 batch loss 7.28584337 epoch total loss 7.11324\n",
      "Trained batch 120 batch loss 6.85056877 epoch total loss 7.11105108\n",
      "Trained batch 121 batch loss 6.94069052 epoch total loss 7.10964298\n",
      "Trained batch 122 batch loss 7.24041176 epoch total loss 7.11071491\n",
      "Trained batch 123 batch loss 7.12766 epoch total loss 7.11085272\n",
      "Trained batch 124 batch loss 7.0406208 epoch total loss 7.11028671\n",
      "Trained batch 125 batch loss 7.54347849 epoch total loss 7.11375189\n",
      "Trained batch 126 batch loss 7.50729227 epoch total loss 7.11687517\n",
      "Trained batch 127 batch loss 7.02292061 epoch total loss 7.1161356\n",
      "Trained batch 128 batch loss 6.89206648 epoch total loss 7.11438513\n",
      "Trained batch 129 batch loss 7.08037043 epoch total loss 7.11412144\n",
      "Trained batch 130 batch loss 6.76545095 epoch total loss 7.11143923\n",
      "Trained batch 131 batch loss 6.5743103 epoch total loss 7.10733891\n",
      "Trained batch 132 batch loss 6.74462414 epoch total loss 7.10459137\n",
      "Trained batch 133 batch loss 6.8477478 epoch total loss 7.10266\n",
      "Trained batch 134 batch loss 6.3775053 epoch total loss 7.09724855\n",
      "Trained batch 135 batch loss 6.12410402 epoch total loss 7.09003973\n",
      "Trained batch 136 batch loss 6.95107889 epoch total loss 7.08901787\n",
      "Trained batch 137 batch loss 6.39036179 epoch total loss 7.08391857\n",
      "Trained batch 138 batch loss 7.01277256 epoch total loss 7.08340263\n",
      "Trained batch 139 batch loss 7.3712616 epoch total loss 7.08547401\n",
      "Trained batch 140 batch loss 7.47693443 epoch total loss 7.08826971\n",
      "Trained batch 141 batch loss 7.38455725 epoch total loss 7.09037161\n",
      "Trained batch 142 batch loss 7.42849398 epoch total loss 7.09275246\n",
      "Trained batch 143 batch loss 7.49608278 epoch total loss 7.09557295\n",
      "Trained batch 144 batch loss 6.98713303 epoch total loss 7.09482\n",
      "Trained batch 145 batch loss 6.33595896 epoch total loss 7.08958626\n",
      "Trained batch 146 batch loss 6.50142717 epoch total loss 7.08555794\n",
      "Trained batch 147 batch loss 7.20167542 epoch total loss 7.08634758\n",
      "Trained batch 148 batch loss 7.2102685 epoch total loss 7.08718538\n",
      "Trained batch 149 batch loss 7.23993778 epoch total loss 7.08821106\n",
      "Trained batch 150 batch loss 7.39947796 epoch total loss 7.09028625\n",
      "Trained batch 151 batch loss 7.32807493 epoch total loss 7.09186172\n",
      "Trained batch 152 batch loss 6.96910667 epoch total loss 7.09105396\n",
      "Trained batch 153 batch loss 7.51262188 epoch total loss 7.09380913\n",
      "Trained batch 154 batch loss 7.2647686 epoch total loss 7.0949192\n",
      "Trained batch 155 batch loss 7.05010748 epoch total loss 7.09463\n",
      "Trained batch 156 batch loss 6.98214293 epoch total loss 7.09390879\n",
      "Trained batch 157 batch loss 6.39633036 epoch total loss 7.08946609\n",
      "Trained batch 158 batch loss 6.07365 epoch total loss 7.08303642\n",
      "Trained batch 159 batch loss 7.05441427 epoch total loss 7.08285666\n",
      "Trained batch 160 batch loss 7.44708443 epoch total loss 7.08513355\n",
      "Trained batch 161 batch loss 7.57659245 epoch total loss 7.08818579\n",
      "Trained batch 162 batch loss 7.47519493 epoch total loss 7.09057474\n",
      "Trained batch 163 batch loss 7.51806641 epoch total loss 7.09319735\n",
      "Trained batch 164 batch loss 7.51561737 epoch total loss 7.09577322\n",
      "Trained batch 165 batch loss 7.51071405 epoch total loss 7.09828806\n",
      "Trained batch 166 batch loss 7.49084 epoch total loss 7.10065269\n",
      "Trained batch 167 batch loss 7.13477278 epoch total loss 7.10085726\n",
      "Trained batch 168 batch loss 7.2928648 epoch total loss 7.10199976\n",
      "Trained batch 169 batch loss 7.23648739 epoch total loss 7.1027956\n",
      "Trained batch 170 batch loss 7.24526596 epoch total loss 7.1036334\n",
      "Trained batch 171 batch loss 7.45981216 epoch total loss 7.10571623\n",
      "Trained batch 172 batch loss 7.13101387 epoch total loss 7.10586357\n",
      "Trained batch 173 batch loss 7.36453104 epoch total loss 7.10735846\n",
      "Trained batch 174 batch loss 7.09259224 epoch total loss 7.10727406\n",
      "Trained batch 175 batch loss 7.03876 epoch total loss 7.10688257\n",
      "Trained batch 176 batch loss 7.22641468 epoch total loss 7.10756207\n",
      "Trained batch 177 batch loss 7.3182745 epoch total loss 7.10875225\n",
      "Trained batch 178 batch loss 7.29247761 epoch total loss 7.1097846\n",
      "Trained batch 179 batch loss 7.06463146 epoch total loss 7.10953188\n",
      "Trained batch 180 batch loss 7.1242609 epoch total loss 7.1096139\n",
      "Trained batch 181 batch loss 6.45737648 epoch total loss 7.10601044\n",
      "Trained batch 182 batch loss 6.1458683 epoch total loss 7.10073471\n",
      "Trained batch 183 batch loss 5.48432255 epoch total loss 7.09190226\n",
      "Trained batch 184 batch loss 5.64232492 epoch total loss 7.08402395\n",
      "Trained batch 185 batch loss 6.9833169 epoch total loss 7.0834794\n",
      "Trained batch 186 batch loss 7.49766302 epoch total loss 7.08570671\n",
      "Trained batch 187 batch loss 7.07291317 epoch total loss 7.08563805\n",
      "Trained batch 188 batch loss 7.21713686 epoch total loss 7.08633757\n",
      "Trained batch 189 batch loss 6.99008846 epoch total loss 7.0858283\n",
      "Trained batch 190 batch loss 7.01864862 epoch total loss 7.08547497\n",
      "Trained batch 191 batch loss 6.20592594 epoch total loss 7.08087\n",
      "Trained batch 192 batch loss 6.39975786 epoch total loss 7.07732248\n",
      "Trained batch 193 batch loss 6.93563557 epoch total loss 7.07658863\n",
      "Trained batch 194 batch loss 6.50003719 epoch total loss 7.0736165\n",
      "Trained batch 195 batch loss 7.02508068 epoch total loss 7.0733676\n",
      "Trained batch 196 batch loss 7.31635571 epoch total loss 7.07460737\n",
      "Trained batch 197 batch loss 7.51477432 epoch total loss 7.07684183\n",
      "Trained batch 198 batch loss 7.50296688 epoch total loss 7.0789938\n",
      "Trained batch 199 batch loss 7.34093475 epoch total loss 7.08031\n",
      "Trained batch 200 batch loss 7.32494688 epoch total loss 7.08153343\n",
      "Trained batch 201 batch loss 7.02927208 epoch total loss 7.08127356\n",
      "Trained batch 202 batch loss 7.39545107 epoch total loss 7.082829\n",
      "Trained batch 203 batch loss 7.47499752 epoch total loss 7.08476067\n",
      "Trained batch 204 batch loss 7.47301 epoch total loss 7.08666372\n",
      "Trained batch 205 batch loss 7.4959321 epoch total loss 7.08866072\n",
      "Trained batch 206 batch loss 7.21195698 epoch total loss 7.08925867\n",
      "Trained batch 207 batch loss 7.38516569 epoch total loss 7.09068823\n",
      "Trained batch 208 batch loss 7.44717121 epoch total loss 7.09240198\n",
      "Trained batch 209 batch loss 7.24588203 epoch total loss 7.09313631\n",
      "Trained batch 210 batch loss 7.02397633 epoch total loss 7.09280634\n",
      "Trained batch 211 batch loss 7.25492859 epoch total loss 7.09357452\n",
      "Trained batch 212 batch loss 7.12833595 epoch total loss 7.09373856\n",
      "Trained batch 213 batch loss 7.62331915 epoch total loss 7.09622478\n",
      "Trained batch 214 batch loss 7.5237112 epoch total loss 7.09822226\n",
      "Trained batch 215 batch loss 7.38486385 epoch total loss 7.09955549\n",
      "Trained batch 216 batch loss 7.10235882 epoch total loss 7.09956884\n",
      "Trained batch 217 batch loss 7.21482754 epoch total loss 7.1001\n",
      "Trained batch 218 batch loss 7.17603874 epoch total loss 7.10044813\n",
      "Trained batch 219 batch loss 7.1491127 epoch total loss 7.10067081\n",
      "Trained batch 220 batch loss 7.47821665 epoch total loss 7.10238695\n",
      "Trained batch 221 batch loss 6.77961254 epoch total loss 7.10092688\n",
      "Trained batch 222 batch loss 6.39885569 epoch total loss 7.09776402\n",
      "Trained batch 223 batch loss 6.50606775 epoch total loss 7.09511089\n",
      "Trained batch 224 batch loss 6.71262121 epoch total loss 7.09340334\n",
      "Trained batch 225 batch loss 7.09734869 epoch total loss 7.09342051\n",
      "Trained batch 226 batch loss 6.57721806 epoch total loss 7.09113693\n",
      "Trained batch 227 batch loss 6.40587807 epoch total loss 7.08811808\n",
      "Trained batch 228 batch loss 6.60645 epoch total loss 7.08600569\n",
      "Trained batch 229 batch loss 6.68881035 epoch total loss 7.08427095\n",
      "Trained batch 230 batch loss 6.35289478 epoch total loss 7.0810914\n",
      "Trained batch 231 batch loss 7.03897142 epoch total loss 7.08090878\n",
      "Trained batch 232 batch loss 7.0755353 epoch total loss 7.08088589\n",
      "Trained batch 233 batch loss 6.9527421 epoch total loss 7.08033609\n",
      "Trained batch 234 batch loss 7.26053524 epoch total loss 7.08110571\n",
      "Trained batch 235 batch loss 7.42784548 epoch total loss 7.08258152\n",
      "Trained batch 236 batch loss 7.49759769 epoch total loss 7.08433962\n",
      "Trained batch 237 batch loss 7.53424692 epoch total loss 7.08623838\n",
      "Trained batch 238 batch loss 7.28015614 epoch total loss 7.08705282\n",
      "Trained batch 239 batch loss 7.04148626 epoch total loss 7.08686256\n",
      "Trained batch 240 batch loss 7.00895214 epoch total loss 7.08653784\n",
      "Trained batch 241 batch loss 7.33928776 epoch total loss 7.0875864\n",
      "Trained batch 242 batch loss 7.13993 epoch total loss 7.08780241\n",
      "Trained batch 243 batch loss 7.53598785 epoch total loss 7.08964682\n",
      "Trained batch 244 batch loss 7.4493475 epoch total loss 7.0911212\n",
      "Trained batch 245 batch loss 7.52109909 epoch total loss 7.09287596\n",
      "Trained batch 246 batch loss 6.94415617 epoch total loss 7.0922718\n",
      "Trained batch 247 batch loss 7.01403189 epoch total loss 7.09195518\n",
      "Trained batch 248 batch loss 7.22113943 epoch total loss 7.09247637\n",
      "Trained batch 249 batch loss 7.10211086 epoch total loss 7.09251451\n",
      "Trained batch 250 batch loss 7.10155153 epoch total loss 7.09255075\n",
      "Trained batch 251 batch loss 7.25706768 epoch total loss 7.09320641\n",
      "Trained batch 252 batch loss 7.55550289 epoch total loss 7.0950408\n",
      "Trained batch 253 batch loss 7.37131929 epoch total loss 7.09613323\n",
      "Trained batch 254 batch loss 7.46558571 epoch total loss 7.09758759\n",
      "Trained batch 255 batch loss 7.37730646 epoch total loss 7.09868431\n",
      "Trained batch 256 batch loss 7.57357645 epoch total loss 7.10053968\n",
      "Trained batch 257 batch loss 7.68166256 epoch total loss 7.10280085\n",
      "Trained batch 258 batch loss 7.53849936 epoch total loss 7.10448933\n",
      "Trained batch 259 batch loss 7.31610584 epoch total loss 7.10530663\n",
      "Trained batch 260 batch loss 7.43280029 epoch total loss 7.10656643\n",
      "Trained batch 261 batch loss 7.04010201 epoch total loss 7.1063118\n",
      "Trained batch 262 batch loss 7.08293 epoch total loss 7.10622263\n",
      "Trained batch 263 batch loss 7.35690117 epoch total loss 7.10717583\n",
      "Trained batch 264 batch loss 7.12492275 epoch total loss 7.10724306\n",
      "Trained batch 265 batch loss 7.30843306 epoch total loss 7.10800219\n",
      "Trained batch 266 batch loss 7.45976257 epoch total loss 7.10932446\n",
      "Trained batch 267 batch loss 7.10459042 epoch total loss 7.10930681\n",
      "Trained batch 268 batch loss 7.03835964 epoch total loss 7.10904217\n",
      "Trained batch 269 batch loss 7.31291771 epoch total loss 7.1098\n",
      "Trained batch 270 batch loss 7.30821514 epoch total loss 7.11053467\n",
      "Trained batch 271 batch loss 7.50613451 epoch total loss 7.11199427\n",
      "Trained batch 272 batch loss 7.30905724 epoch total loss 7.11271906\n",
      "Trained batch 273 batch loss 7.4696269 epoch total loss 7.11402607\n",
      "Trained batch 274 batch loss 7.21896172 epoch total loss 7.11440945\n",
      "Trained batch 275 batch loss 7.34500217 epoch total loss 7.11524773\n",
      "Trained batch 276 batch loss 6.92426968 epoch total loss 7.11455584\n",
      "Trained batch 277 batch loss 7.30740404 epoch total loss 7.11525202\n",
      "Trained batch 278 batch loss 7.08546638 epoch total loss 7.11514473\n",
      "Trained batch 279 batch loss 7.14293337 epoch total loss 7.11524439\n",
      "Trained batch 280 batch loss 7.02664375 epoch total loss 7.11492777\n",
      "Trained batch 281 batch loss 6.57462025 epoch total loss 7.11300516\n",
      "Trained batch 282 batch loss 6.32158613 epoch total loss 7.1101985\n",
      "Trained batch 283 batch loss 6.96184206 epoch total loss 7.10967398\n",
      "Trained batch 284 batch loss 7.31738758 epoch total loss 7.11040545\n",
      "Trained batch 285 batch loss 7.34030819 epoch total loss 7.11121225\n",
      "Trained batch 286 batch loss 7.45940113 epoch total loss 7.11242914\n",
      "Trained batch 287 batch loss 7.33298 epoch total loss 7.1131978\n",
      "Trained batch 288 batch loss 7.41859102 epoch total loss 7.11425877\n",
      "Trained batch 289 batch loss 6.98981142 epoch total loss 7.11382771\n",
      "Trained batch 290 batch loss 6.70521212 epoch total loss 7.11241913\n",
      "Trained batch 291 batch loss 7.26815939 epoch total loss 7.11295414\n",
      "Trained batch 292 batch loss 6.91789675 epoch total loss 7.11228609\n",
      "Trained batch 293 batch loss 7.22334385 epoch total loss 7.11266565\n",
      "Trained batch 294 batch loss 6.67881536 epoch total loss 7.11118937\n",
      "Trained batch 295 batch loss 6.91310787 epoch total loss 7.11051798\n",
      "Trained batch 296 batch loss 7.14619493 epoch total loss 7.11063862\n",
      "Trained batch 297 batch loss 7.3635397 epoch total loss 7.11149025\n",
      "Trained batch 298 batch loss 7.04330921 epoch total loss 7.11126089\n",
      "Trained batch 299 batch loss 7.20209694 epoch total loss 7.11156511\n",
      "Trained batch 300 batch loss 6.76107645 epoch total loss 7.11039639\n",
      "Trained batch 301 batch loss 7.22244835 epoch total loss 7.11076832\n",
      "Trained batch 302 batch loss 7.24324942 epoch total loss 7.11120701\n",
      "Trained batch 303 batch loss 6.51234818 epoch total loss 7.109231\n",
      "Trained batch 304 batch loss 6.97411776 epoch total loss 7.10878611\n",
      "Trained batch 305 batch loss 6.80423689 epoch total loss 7.10778761\n",
      "Trained batch 306 batch loss 6.42853165 epoch total loss 7.10556746\n",
      "Trained batch 307 batch loss 5.93299294 epoch total loss 7.10174847\n",
      "Trained batch 308 batch loss 6.01390457 epoch total loss 7.09821653\n",
      "Trained batch 309 batch loss 6.64854908 epoch total loss 7.09676123\n",
      "Trained batch 310 batch loss 6.59001493 epoch total loss 7.09512663\n",
      "Trained batch 311 batch loss 6.95469618 epoch total loss 7.09467459\n",
      "Trained batch 312 batch loss 7.25051737 epoch total loss 7.09517431\n",
      "Trained batch 313 batch loss 7.23182154 epoch total loss 7.0956111\n",
      "Trained batch 314 batch loss 7.4746151 epoch total loss 7.09681797\n",
      "Trained batch 315 batch loss 7.38892221 epoch total loss 7.09774542\n",
      "Trained batch 316 batch loss 7.40120554 epoch total loss 7.09870529\n",
      "Trained batch 317 batch loss 7.32532501 epoch total loss 7.09942055\n",
      "Trained batch 318 batch loss 7.49770594 epoch total loss 7.10067368\n",
      "Trained batch 319 batch loss 7.23779058 epoch total loss 7.10110331\n",
      "Trained batch 320 batch loss 7.47959137 epoch total loss 7.10228586\n",
      "Trained batch 321 batch loss 6.94015694 epoch total loss 7.10178089\n",
      "Trained batch 322 batch loss 7.04378366 epoch total loss 7.10160065\n",
      "Trained batch 323 batch loss 7.31288719 epoch total loss 7.10225487\n",
      "Trained batch 324 batch loss 6.24067831 epoch total loss 7.09959602\n",
      "Trained batch 325 batch loss 6.83462763 epoch total loss 7.09878063\n",
      "Trained batch 326 batch loss 6.97798109 epoch total loss 7.09841061\n",
      "Trained batch 327 batch loss 7.13797 epoch total loss 7.09853125\n",
      "Trained batch 328 batch loss 7.12631941 epoch total loss 7.09861565\n",
      "Trained batch 329 batch loss 7.08552 epoch total loss 7.09857559\n",
      "Trained batch 330 batch loss 7.24551344 epoch total loss 7.09902143\n",
      "Trained batch 331 batch loss 7.37443161 epoch total loss 7.09985352\n",
      "Trained batch 332 batch loss 7.40264845 epoch total loss 7.10076523\n",
      "Trained batch 333 batch loss 7.43114233 epoch total loss 7.10175753\n",
      "Trained batch 334 batch loss 7.35824299 epoch total loss 7.10252523\n",
      "Trained batch 335 batch loss 7.41976 epoch total loss 7.10347176\n",
      "Trained batch 336 batch loss 7.37406492 epoch total loss 7.10427713\n",
      "Trained batch 337 batch loss 7.31320143 epoch total loss 7.10489702\n",
      "Trained batch 338 batch loss 7.04669952 epoch total loss 7.10472488\n",
      "Trained batch 339 batch loss 6.82900095 epoch total loss 7.10391188\n",
      "Trained batch 340 batch loss 7.38266659 epoch total loss 7.10473108\n",
      "Trained batch 341 batch loss 7.4168129 epoch total loss 7.10564613\n",
      "Trained batch 342 batch loss 7.50218248 epoch total loss 7.1068058\n",
      "Trained batch 343 batch loss 7.55012131 epoch total loss 7.10809803\n",
      "Trained batch 344 batch loss 7.47855139 epoch total loss 7.10917473\n",
      "Trained batch 345 batch loss 7.5448 epoch total loss 7.11043787\n",
      "Trained batch 346 batch loss 7.34743881 epoch total loss 7.11112261\n",
      "Trained batch 347 batch loss 7.34752131 epoch total loss 7.11180353\n",
      "Trained batch 348 batch loss 7.17211866 epoch total loss 7.1119771\n",
      "Trained batch 349 batch loss 7.05557585 epoch total loss 7.11181593\n",
      "Trained batch 350 batch loss 6.9691453 epoch total loss 7.11140823\n",
      "Trained batch 351 batch loss 7.15457296 epoch total loss 7.11153126\n",
      "Trained batch 352 batch loss 7.358284 epoch total loss 7.11223269\n",
      "Trained batch 353 batch loss 7.36389303 epoch total loss 7.11294603\n",
      "Trained batch 354 batch loss 7.36742687 epoch total loss 7.11366463\n",
      "Trained batch 355 batch loss 7.13634968 epoch total loss 7.11372805\n",
      "Trained batch 356 batch loss 6.18291 epoch total loss 7.11111355\n",
      "Trained batch 357 batch loss 6.6949172 epoch total loss 7.1099472\n",
      "Trained batch 358 batch loss 6.3161211 epoch total loss 7.10773\n",
      "Trained batch 359 batch loss 6.31987667 epoch total loss 7.10553551\n",
      "Trained batch 360 batch loss 6.20865202 epoch total loss 7.10304451\n",
      "Trained batch 361 batch loss 6.01930523 epoch total loss 7.10004234\n",
      "Trained batch 362 batch loss 5.7700429 epoch total loss 7.09636831\n",
      "Trained batch 363 batch loss 5.92377377 epoch total loss 7.09313774\n",
      "Trained batch 364 batch loss 6.76589298 epoch total loss 7.0922389\n",
      "Trained batch 365 batch loss 7.31516933 epoch total loss 7.09284973\n",
      "Trained batch 366 batch loss 7.21589 epoch total loss 7.0931859\n",
      "Trained batch 367 batch loss 7.24002504 epoch total loss 7.09358549\n",
      "Trained batch 368 batch loss 7.1319623 epoch total loss 7.0936904\n",
      "Trained batch 369 batch loss 6.2200222 epoch total loss 7.09132242\n",
      "Trained batch 370 batch loss 6.02661753 epoch total loss 7.08844471\n",
      "Trained batch 371 batch loss 6.74195576 epoch total loss 7.08751106\n",
      "Trained batch 372 batch loss 7.29305792 epoch total loss 7.08806324\n",
      "Trained batch 373 batch loss 7.44392443 epoch total loss 7.08901691\n",
      "Trained batch 374 batch loss 7.31548119 epoch total loss 7.0896225\n",
      "Trained batch 375 batch loss 7.30367041 epoch total loss 7.09019327\n",
      "Trained batch 376 batch loss 7.3831706 epoch total loss 7.09097242\n",
      "Trained batch 377 batch loss 7.21795893 epoch total loss 7.09130907\n",
      "Trained batch 378 batch loss 7.43569899 epoch total loss 7.09222078\n",
      "Trained batch 379 batch loss 7.49926901 epoch total loss 7.09329462\n",
      "Trained batch 380 batch loss 7.57945251 epoch total loss 7.0945735\n",
      "Trained batch 381 batch loss 7.41402674 epoch total loss 7.09541225\n",
      "Trained batch 382 batch loss 7.25218773 epoch total loss 7.09582281\n",
      "Trained batch 383 batch loss 6.49506807 epoch total loss 7.09425402\n",
      "Trained batch 384 batch loss 6.52454281 epoch total loss 7.09277105\n",
      "Trained batch 385 batch loss 6.70048761 epoch total loss 7.09175205\n",
      "Trained batch 386 batch loss 7.17318 epoch total loss 7.09196281\n",
      "Trained batch 387 batch loss 6.45115662 epoch total loss 7.09030676\n",
      "Trained batch 388 batch loss 7.22147942 epoch total loss 7.09064484\n",
      "Trained batch 389 batch loss 6.87338972 epoch total loss 7.09008598\n",
      "Trained batch 390 batch loss 6.97326183 epoch total loss 7.08978605\n",
      "Trained batch 391 batch loss 7.0974741 epoch total loss 7.0898056\n",
      "Trained batch 392 batch loss 6.86497164 epoch total loss 7.08923197\n",
      "Trained batch 393 batch loss 7.22499514 epoch total loss 7.08957767\n",
      "Trained batch 394 batch loss 7.29858112 epoch total loss 7.09010839\n",
      "Trained batch 395 batch loss 7.38153744 epoch total loss 7.09084606\n",
      "Trained batch 396 batch loss 7.28131676 epoch total loss 7.09132719\n",
      "Trained batch 397 batch loss 7.11324453 epoch total loss 7.0913825\n",
      "Trained batch 398 batch loss 7.41305304 epoch total loss 7.09219074\n",
      "Trained batch 399 batch loss 7.52062416 epoch total loss 7.0932641\n",
      "Trained batch 400 batch loss 7.50552654 epoch total loss 7.09429502\n",
      "Trained batch 401 batch loss 7.28039789 epoch total loss 7.09475946\n",
      "Trained batch 402 batch loss 7.06979322 epoch total loss 7.09469748\n",
      "Trained batch 403 batch loss 6.27686691 epoch total loss 7.09266806\n",
      "Trained batch 404 batch loss 6.17265224 epoch total loss 7.09039068\n",
      "Trained batch 405 batch loss 6.19523287 epoch total loss 7.08818054\n",
      "Trained batch 406 batch loss 6.78362036 epoch total loss 7.08743048\n",
      "Trained batch 407 batch loss 6.77312136 epoch total loss 7.08665848\n",
      "Trained batch 408 batch loss 6.63645172 epoch total loss 7.08555508\n",
      "Trained batch 409 batch loss 6.36342955 epoch total loss 7.08379\n",
      "Trained batch 410 batch loss 6.77285385 epoch total loss 7.08303165\n",
      "Trained batch 411 batch loss 6.6118083 epoch total loss 7.08188534\n",
      "Trained batch 412 batch loss 6.8423 epoch total loss 7.0813036\n",
      "Trained batch 413 batch loss 7.21828699 epoch total loss 7.081635\n",
      "Trained batch 414 batch loss 7.16058683 epoch total loss 7.08182621\n",
      "Trained batch 415 batch loss 6.81080675 epoch total loss 7.08117294\n",
      "Trained batch 416 batch loss 6.23077297 epoch total loss 7.07912874\n",
      "Trained batch 417 batch loss 6.10880899 epoch total loss 7.07680178\n",
      "Trained batch 418 batch loss 6.4791007 epoch total loss 7.07537174\n",
      "Trained batch 419 batch loss 6.67780876 epoch total loss 7.07442284\n",
      "Trained batch 420 batch loss 7.02530622 epoch total loss 7.07430601\n",
      "Trained batch 421 batch loss 7.26408958 epoch total loss 7.0747571\n",
      "Trained batch 422 batch loss 7.32278776 epoch total loss 7.07534456\n",
      "Trained batch 423 batch loss 7.10075951 epoch total loss 7.07540464\n",
      "Trained batch 424 batch loss 7.37493324 epoch total loss 7.07611132\n",
      "Trained batch 425 batch loss 7.19551611 epoch total loss 7.07639265\n",
      "Trained batch 426 batch loss 7.15916443 epoch total loss 7.07658672\n",
      "Trained batch 427 batch loss 7.05933619 epoch total loss 7.07654619\n",
      "Trained batch 428 batch loss 6.79743338 epoch total loss 7.07589388\n",
      "Trained batch 429 batch loss 7.12111855 epoch total loss 7.07599926\n",
      "Trained batch 430 batch loss 6.89789915 epoch total loss 7.07558537\n",
      "Trained batch 431 batch loss 6.84078312 epoch total loss 7.07504082\n",
      "Trained batch 432 batch loss 6.89574385 epoch total loss 7.07462549\n",
      "Trained batch 433 batch loss 6.65091848 epoch total loss 7.07364702\n",
      "Trained batch 434 batch loss 6.86517668 epoch total loss 7.07316685\n",
      "Trained batch 435 batch loss 7.2976408 epoch total loss 7.07368279\n",
      "Trained batch 436 batch loss 7.52979088 epoch total loss 7.07472897\n",
      "Trained batch 437 batch loss 7.45616913 epoch total loss 7.07560158\n",
      "Trained batch 438 batch loss 7.36760139 epoch total loss 7.0762682\n",
      "Trained batch 439 batch loss 7.2893095 epoch total loss 7.07675362\n",
      "Trained batch 440 batch loss 7.13801384 epoch total loss 7.07689285\n",
      "Trained batch 441 batch loss 6.89200974 epoch total loss 7.07647371\n",
      "Trained batch 442 batch loss 7.21763134 epoch total loss 7.07679272\n",
      "Trained batch 443 batch loss 6.88293076 epoch total loss 7.07635498\n",
      "Trained batch 444 batch loss 6.45800543 epoch total loss 7.07496214\n",
      "Trained batch 445 batch loss 6.85853052 epoch total loss 7.07447624\n",
      "Trained batch 446 batch loss 6.90108967 epoch total loss 7.07408714\n",
      "Trained batch 447 batch loss 7.0789361 epoch total loss 7.07409811\n",
      "Trained batch 448 batch loss 6.93232393 epoch total loss 7.07378149\n",
      "Trained batch 449 batch loss 7.04460573 epoch total loss 7.07371664\n",
      "Trained batch 450 batch loss 6.64649343 epoch total loss 7.07276726\n",
      "Trained batch 451 batch loss 6.78432417 epoch total loss 7.0721283\n",
      "Trained batch 452 batch loss 7.08252764 epoch total loss 7.07215118\n",
      "Trained batch 453 batch loss 6.99255562 epoch total loss 7.07197571\n",
      "Trained batch 454 batch loss 7.19754362 epoch total loss 7.07225227\n",
      "Trained batch 455 batch loss 7.20067549 epoch total loss 7.07253456\n",
      "Trained batch 456 batch loss 5.6524086 epoch total loss 7.06942\n",
      "Trained batch 457 batch loss 5.8623457 epoch total loss 7.06677866\n",
      "Trained batch 458 batch loss 6.51288319 epoch total loss 7.0655694\n",
      "Trained batch 459 batch loss 6.96683073 epoch total loss 7.06535435\n",
      "Trained batch 460 batch loss 6.8163867 epoch total loss 7.06481314\n",
      "Trained batch 461 batch loss 7.37356377 epoch total loss 7.06548262\n",
      "Trained batch 462 batch loss 7.2774725 epoch total loss 7.06594181\n",
      "Trained batch 463 batch loss 6.99763489 epoch total loss 7.06579399\n",
      "Trained batch 464 batch loss 7.20368338 epoch total loss 7.06609106\n",
      "Trained batch 465 batch loss 7.25087166 epoch total loss 7.06648874\n",
      "Trained batch 466 batch loss 6.73966265 epoch total loss 7.06578732\n",
      "Trained batch 467 batch loss 6.57201624 epoch total loss 7.06473\n",
      "Trained batch 468 batch loss 6.99026489 epoch total loss 7.0645709\n",
      "Trained batch 469 batch loss 7.06355524 epoch total loss 7.06456852\n",
      "Trained batch 470 batch loss 7.0491 epoch total loss 7.06453562\n",
      "Trained batch 471 batch loss 6.88376188 epoch total loss 7.06415176\n",
      "Trained batch 472 batch loss 7.04910946 epoch total loss 7.06412\n",
      "Trained batch 473 batch loss 7.25670481 epoch total loss 7.06452703\n",
      "Trained batch 474 batch loss 6.85400343 epoch total loss 7.06408262\n",
      "Trained batch 475 batch loss 6.29704428 epoch total loss 7.06246805\n",
      "Trained batch 476 batch loss 6.5098505 epoch total loss 7.06130695\n",
      "Trained batch 477 batch loss 6.47365952 epoch total loss 7.06007481\n",
      "Trained batch 478 batch loss 6.93234634 epoch total loss 7.05980778\n",
      "Trained batch 479 batch loss 6.65254593 epoch total loss 7.05895758\n",
      "Trained batch 480 batch loss 6.84435415 epoch total loss 7.0585103\n",
      "Trained batch 481 batch loss 6.88740063 epoch total loss 7.05815458\n",
      "Trained batch 482 batch loss 6.64911509 epoch total loss 7.05730629\n",
      "Trained batch 483 batch loss 6.28371763 epoch total loss 7.05570459\n",
      "Trained batch 484 batch loss 6.94315 epoch total loss 7.0554719\n",
      "Trained batch 485 batch loss 7.09402084 epoch total loss 7.05555153\n",
      "Trained batch 486 batch loss 7.18531084 epoch total loss 7.05581808\n",
      "Trained batch 487 batch loss 7.17585659 epoch total loss 7.05606461\n",
      "Trained batch 488 batch loss 7.1510663 epoch total loss 7.05625963\n",
      "Trained batch 489 batch loss 7.04817438 epoch total loss 7.05624294\n",
      "Trained batch 490 batch loss 6.61120081 epoch total loss 7.05533409\n",
      "Trained batch 491 batch loss 7.06229734 epoch total loss 7.0553484\n",
      "Trained batch 492 batch loss 7.21238899 epoch total loss 7.0556674\n",
      "Trained batch 493 batch loss 7.00868 epoch total loss 7.05557251\n",
      "Trained batch 494 batch loss 7.30583239 epoch total loss 7.05607939\n",
      "Trained batch 495 batch loss 7.50794506 epoch total loss 7.05699205\n",
      "Trained batch 496 batch loss 7.13638783 epoch total loss 7.05715275\n",
      "Trained batch 497 batch loss 6.41279364 epoch total loss 7.05585623\n",
      "Trained batch 498 batch loss 6.96464777 epoch total loss 7.05567312\n",
      "Trained batch 499 batch loss 7.12079144 epoch total loss 7.0558033\n",
      "Trained batch 500 batch loss 6.81360483 epoch total loss 7.05531931\n",
      "Trained batch 501 batch loss 7.09231329 epoch total loss 7.05539322\n",
      "Trained batch 502 batch loss 6.87507296 epoch total loss 7.05503368\n",
      "Trained batch 503 batch loss 6.62321806 epoch total loss 7.05417538\n",
      "Trained batch 504 batch loss 6.9920392 epoch total loss 7.05405188\n",
      "Trained batch 505 batch loss 6.97845793 epoch total loss 7.05390215\n",
      "Trained batch 506 batch loss 7.28256845 epoch total loss 7.05435419\n",
      "Trained batch 507 batch loss 7.12592 epoch total loss 7.05449533\n",
      "Trained batch 508 batch loss 6.94491959 epoch total loss 7.05427933\n",
      "Trained batch 509 batch loss 6.69752216 epoch total loss 7.05357838\n",
      "Trained batch 510 batch loss 6.74483204 epoch total loss 7.05297327\n",
      "Trained batch 511 batch loss 6.9239912 epoch total loss 7.05272102\n",
      "Trained batch 512 batch loss 6.89421463 epoch total loss 7.05241156\n",
      "Trained batch 513 batch loss 6.61303139 epoch total loss 7.05155516\n",
      "Trained batch 514 batch loss 7.34809208 epoch total loss 7.05213213\n",
      "Trained batch 515 batch loss 6.45821524 epoch total loss 7.05097914\n",
      "Trained batch 516 batch loss 7.30251884 epoch total loss 7.05146646\n",
      "Trained batch 517 batch loss 7.10294533 epoch total loss 7.05156612\n",
      "Trained batch 518 batch loss 6.90060616 epoch total loss 7.05127478\n",
      "Trained batch 519 batch loss 6.58750105 epoch total loss 7.05038071\n",
      "Trained batch 520 batch loss 6.97578239 epoch total loss 7.05023766\n",
      "Trained batch 521 batch loss 6.95946 epoch total loss 7.05006313\n",
      "Trained batch 522 batch loss 7.33558512 epoch total loss 7.05061054\n",
      "Trained batch 523 batch loss 7.46295786 epoch total loss 7.05139875\n",
      "Trained batch 524 batch loss 7.06516409 epoch total loss 7.05142498\n",
      "Trained batch 525 batch loss 7.02131176 epoch total loss 7.05136776\n",
      "Trained batch 526 batch loss 6.9633584 epoch total loss 7.05120039\n",
      "Trained batch 527 batch loss 6.95698547 epoch total loss 7.05102158\n",
      "Trained batch 528 batch loss 7.34765768 epoch total loss 7.05158329\n",
      "Trained batch 529 batch loss 7.33176756 epoch total loss 7.05211306\n",
      "Trained batch 530 batch loss 7.21286 epoch total loss 7.05241632\n",
      "Trained batch 531 batch loss 7.43442059 epoch total loss 7.05313587\n",
      "Trained batch 532 batch loss 7.25119209 epoch total loss 7.05350828\n",
      "Trained batch 533 batch loss 7.24904346 epoch total loss 7.05387497\n",
      "Trained batch 534 batch loss 7.2536087 epoch total loss 7.05424929\n",
      "Trained batch 535 batch loss 7.27195692 epoch total loss 7.05465603\n",
      "Trained batch 536 batch loss 7.32031202 epoch total loss 7.05515146\n",
      "Trained batch 537 batch loss 6.73362637 epoch total loss 7.05455303\n",
      "Trained batch 538 batch loss 7.0086031 epoch total loss 7.0544672\n",
      "Trained batch 539 batch loss 7.05507565 epoch total loss 7.05446863\n",
      "Trained batch 540 batch loss 6.56008244 epoch total loss 7.0535531\n",
      "Trained batch 541 batch loss 6.52719116 epoch total loss 7.05258\n",
      "Trained batch 542 batch loss 6.07618046 epoch total loss 7.05077839\n",
      "Trained batch 543 batch loss 6.48365259 epoch total loss 7.04973412\n",
      "Trained batch 544 batch loss 6.80562115 epoch total loss 7.04928541\n",
      "Trained batch 545 batch loss 6.85451317 epoch total loss 7.04892778\n",
      "Trained batch 546 batch loss 6.99804974 epoch total loss 7.0488348\n",
      "Trained batch 547 batch loss 6.50884771 epoch total loss 7.04784775\n",
      "Trained batch 548 batch loss 7.08978748 epoch total loss 7.04792404\n",
      "Trained batch 549 batch loss 6.79416037 epoch total loss 7.04746199\n",
      "Trained batch 550 batch loss 7.08821297 epoch total loss 7.0475359\n",
      "Trained batch 551 batch loss 6.85872173 epoch total loss 7.04719305\n",
      "Trained batch 552 batch loss 6.75778294 epoch total loss 7.04666901\n",
      "Trained batch 553 batch loss 7.44075108 epoch total loss 7.0473814\n",
      "Trained batch 554 batch loss 6.71714401 epoch total loss 7.04678488\n",
      "Trained batch 555 batch loss 6.85001 epoch total loss 7.04643059\n",
      "Trained batch 556 batch loss 6.81674147 epoch total loss 7.04601765\n",
      "Trained batch 557 batch loss 7.46689558 epoch total loss 7.04677296\n",
      "Trained batch 558 batch loss 7.42519808 epoch total loss 7.04745102\n",
      "Trained batch 559 batch loss 7.20642805 epoch total loss 7.04773569\n",
      "Trained batch 560 batch loss 7.00106621 epoch total loss 7.04765224\n",
      "Trained batch 561 batch loss 7.05374432 epoch total loss 7.04766321\n",
      "Trained batch 562 batch loss 6.8917079 epoch total loss 7.04738522\n",
      "Trained batch 563 batch loss 6.63584423 epoch total loss 7.04665422\n",
      "Trained batch 564 batch loss 6.66669178 epoch total loss 7.04598045\n",
      "Trained batch 565 batch loss 6.84181213 epoch total loss 7.04561949\n",
      "Trained batch 566 batch loss 6.74915457 epoch total loss 7.04509592\n",
      "Trained batch 567 batch loss 6.2829051 epoch total loss 7.04375172\n",
      "Trained batch 568 batch loss 6.93652487 epoch total loss 7.04356289\n",
      "Trained batch 569 batch loss 6.38193607 epoch total loss 7.0424\n",
      "Trained batch 570 batch loss 6.28949738 epoch total loss 7.04107904\n",
      "Trained batch 571 batch loss 6.35993147 epoch total loss 7.039886\n",
      "Trained batch 572 batch loss 6.34629154 epoch total loss 7.0386734\n",
      "Trained batch 573 batch loss 6.85181332 epoch total loss 7.03834724\n",
      "Trained batch 574 batch loss 7.10907269 epoch total loss 7.03847027\n",
      "Trained batch 575 batch loss 7.41682577 epoch total loss 7.0391283\n",
      "Trained batch 576 batch loss 6.87241364 epoch total loss 7.03883886\n",
      "Trained batch 577 batch loss 6.53826427 epoch total loss 7.0379715\n",
      "Trained batch 578 batch loss 6.64318275 epoch total loss 7.03728819\n",
      "Trained batch 579 batch loss 7.19764853 epoch total loss 7.03756523\n",
      "Trained batch 580 batch loss 6.95697784 epoch total loss 7.03742647\n",
      "Trained batch 581 batch loss 6.9473443 epoch total loss 7.03727102\n",
      "Trained batch 582 batch loss 6.86440563 epoch total loss 7.03697443\n",
      "Trained batch 583 batch loss 7.20444345 epoch total loss 7.03726196\n",
      "Trained batch 584 batch loss 7.23858404 epoch total loss 7.03760672\n",
      "Trained batch 585 batch loss 7.44809 epoch total loss 7.03830862\n",
      "Trained batch 586 batch loss 7.30371857 epoch total loss 7.03876162\n",
      "Trained batch 587 batch loss 7.27471972 epoch total loss 7.03916407\n",
      "Trained batch 588 batch loss 7.51909208 epoch total loss 7.03998\n",
      "Trained batch 589 batch loss 7.27131796 epoch total loss 7.04037333\n",
      "Trained batch 590 batch loss 7.51500654 epoch total loss 7.04117775\n",
      "Trained batch 591 batch loss 7.45663357 epoch total loss 7.04188061\n",
      "Trained batch 592 batch loss 7.07096 epoch total loss 7.04192972\n",
      "Trained batch 593 batch loss 6.59082079 epoch total loss 7.04116869\n",
      "Trained batch 594 batch loss 6.67764854 epoch total loss 7.04055691\n",
      "Trained batch 595 batch loss 6.09747171 epoch total loss 7.03897238\n",
      "Trained batch 596 batch loss 6.22336626 epoch total loss 7.03760338\n",
      "Trained batch 597 batch loss 6.78783321 epoch total loss 7.03718472\n",
      "Trained batch 598 batch loss 6.13115692 epoch total loss 7.03567\n",
      "Trained batch 599 batch loss 5.77178431 epoch total loss 7.03356028\n",
      "Trained batch 600 batch loss 5.87402725 epoch total loss 7.03162766\n",
      "Trained batch 601 batch loss 6.23589754 epoch total loss 7.03030348\n",
      "Trained batch 602 batch loss 6.30004644 epoch total loss 7.02909\n",
      "Trained batch 603 batch loss 7.06029 epoch total loss 7.02914143\n",
      "Trained batch 604 batch loss 7.39410877 epoch total loss 7.02974558\n",
      "Trained batch 605 batch loss 7.49814606 epoch total loss 7.03051949\n",
      "Trained batch 606 batch loss 7.41291142 epoch total loss 7.03115082\n",
      "Trained batch 607 batch loss 7.3287425 epoch total loss 7.03164101\n",
      "Trained batch 608 batch loss 7.05614376 epoch total loss 7.03168106\n",
      "Trained batch 609 batch loss 6.9241786 epoch total loss 7.03150511\n",
      "Trained batch 610 batch loss 6.97592926 epoch total loss 7.03141403\n",
      "Trained batch 611 batch loss 7.15232086 epoch total loss 7.03161192\n",
      "Trained batch 612 batch loss 6.95489788 epoch total loss 7.03148699\n",
      "Trained batch 613 batch loss 6.39660788 epoch total loss 7.0304513\n",
      "Trained batch 614 batch loss 6.31128693 epoch total loss 7.02928\n",
      "Trained batch 615 batch loss 7.37773132 epoch total loss 7.02984715\n",
      "Trained batch 616 batch loss 7.45711708 epoch total loss 7.03054047\n",
      "Trained batch 617 batch loss 7.38906193 epoch total loss 7.03112173\n",
      "Trained batch 618 batch loss 7.32266665 epoch total loss 7.0315938\n",
      "Trained batch 619 batch loss 7.45849419 epoch total loss 7.03228331\n",
      "Trained batch 620 batch loss 7.46884966 epoch total loss 7.03298712\n",
      "Trained batch 621 batch loss 7.20293283 epoch total loss 7.0332613\n",
      "Trained batch 622 batch loss 7.10316086 epoch total loss 7.03337336\n",
      "Trained batch 623 batch loss 6.72343969 epoch total loss 7.03287649\n",
      "Trained batch 624 batch loss 7.23480415 epoch total loss 7.0332\n",
      "Trained batch 625 batch loss 7.26030684 epoch total loss 7.03356314\n",
      "Trained batch 626 batch loss 7.22592354 epoch total loss 7.0338707\n",
      "Trained batch 627 batch loss 6.80207205 epoch total loss 7.03350163\n",
      "Trained batch 628 batch loss 6.69447517 epoch total loss 7.03296137\n",
      "Trained batch 629 batch loss 6.51573133 epoch total loss 7.03213882\n",
      "Trained batch 630 batch loss 6.9255147 epoch total loss 7.03196907\n",
      "Trained batch 631 batch loss 6.75727177 epoch total loss 7.03153419\n",
      "Trained batch 632 batch loss 6.97529888 epoch total loss 7.03144455\n",
      "Trained batch 633 batch loss 7.35048103 epoch total loss 7.03194904\n",
      "Trained batch 634 batch loss 7.37872 epoch total loss 7.03249598\n",
      "Trained batch 635 batch loss 7.3862915 epoch total loss 7.0330534\n",
      "Trained batch 636 batch loss 7.44385052 epoch total loss 7.03369904\n",
      "Trained batch 637 batch loss 6.97459126 epoch total loss 7.03360653\n",
      "Trained batch 638 batch loss 6.47525787 epoch total loss 7.03273106\n",
      "Trained batch 639 batch loss 7.1130352 epoch total loss 7.03285599\n",
      "Trained batch 640 batch loss 7.11157894 epoch total loss 7.03297949\n",
      "Trained batch 641 batch loss 7.14893436 epoch total loss 7.03316069\n",
      "Trained batch 642 batch loss 6.6284771 epoch total loss 7.03253\n",
      "Trained batch 643 batch loss 6.48129034 epoch total loss 7.03167295\n",
      "Trained batch 644 batch loss 6.97887325 epoch total loss 7.03159142\n",
      "Trained batch 645 batch loss 7.08278227 epoch total loss 7.03167105\n",
      "Trained batch 646 batch loss 7.18131256 epoch total loss 7.03190231\n",
      "Trained batch 647 batch loss 7.3904376 epoch total loss 7.03245687\n",
      "Trained batch 648 batch loss 7.37569 epoch total loss 7.03298616\n",
      "Trained batch 649 batch loss 7.22513819 epoch total loss 7.03328228\n",
      "Trained batch 650 batch loss 6.51766539 epoch total loss 7.03248882\n",
      "Trained batch 651 batch loss 5.90318108 epoch total loss 7.03075409\n",
      "Trained batch 652 batch loss 5.64739132 epoch total loss 7.02863264\n",
      "Trained batch 653 batch loss 6.18463755 epoch total loss 7.02734\n",
      "Trained batch 654 batch loss 6.70963 epoch total loss 7.02685404\n",
      "Trained batch 655 batch loss 7.13793945 epoch total loss 7.02702332\n",
      "Trained batch 656 batch loss 7.03911829 epoch total loss 7.02704144\n",
      "Trained batch 657 batch loss 6.9697566 epoch total loss 7.02695417\n",
      "Trained batch 658 batch loss 6.11257 epoch total loss 7.02556515\n",
      "Trained batch 659 batch loss 7.16276932 epoch total loss 7.02577305\n",
      "Trained batch 660 batch loss 7.27330303 epoch total loss 7.02614832\n",
      "Trained batch 661 batch loss 7.48771667 epoch total loss 7.02684641\n",
      "Trained batch 662 batch loss 7.51622581 epoch total loss 7.02758551\n",
      "Trained batch 663 batch loss 7.51460409 epoch total loss 7.02832031\n",
      "Trained batch 664 batch loss 7.44256973 epoch total loss 7.02894402\n",
      "Trained batch 665 batch loss 7.29425049 epoch total loss 7.02934313\n",
      "Trained batch 666 batch loss 7.22988 epoch total loss 7.02964449\n",
      "Trained batch 667 batch loss 7.3101635 epoch total loss 7.03006458\n",
      "Trained batch 668 batch loss 6.97585773 epoch total loss 7.029984\n",
      "Trained batch 669 batch loss 7.34838867 epoch total loss 7.03046036\n",
      "Trained batch 670 batch loss 7.26302195 epoch total loss 7.0308075\n",
      "Trained batch 671 batch loss 6.94351721 epoch total loss 7.03067732\n",
      "Trained batch 672 batch loss 6.36094904 epoch total loss 7.02968073\n",
      "Trained batch 673 batch loss 6.83849716 epoch total loss 7.02939606\n",
      "Trained batch 674 batch loss 6.93364716 epoch total loss 7.02925396\n",
      "Trained batch 675 batch loss 7.05087 epoch total loss 7.02928591\n",
      "Trained batch 676 batch loss 7.03497219 epoch total loss 7.02929449\n",
      "Trained batch 677 batch loss 6.99870825 epoch total loss 7.02924919\n",
      "Trained batch 678 batch loss 7.10647774 epoch total loss 7.02936316\n",
      "Trained batch 679 batch loss 6.78626442 epoch total loss 7.02900505\n",
      "Trained batch 680 batch loss 6.85375738 epoch total loss 7.0287466\n",
      "Trained batch 681 batch loss 7.22868109 epoch total loss 7.02904034\n",
      "Trained batch 682 batch loss 7.0174222 epoch total loss 7.02902317\n",
      "Trained batch 683 batch loss 6.90637 epoch total loss 7.0288434\n",
      "Trained batch 684 batch loss 7.07714701 epoch total loss 7.02891445\n",
      "Trained batch 685 batch loss 7.10025835 epoch total loss 7.02901793\n",
      "Trained batch 686 batch loss 7.27784252 epoch total loss 7.0293808\n",
      "Trained batch 687 batch loss 7.32353115 epoch total loss 7.02980947\n",
      "Trained batch 688 batch loss 7.38732672 epoch total loss 7.03032875\n",
      "Trained batch 689 batch loss 7.11673212 epoch total loss 7.03045416\n",
      "Trained batch 690 batch loss 7.18121624 epoch total loss 7.03067255\n",
      "Trained batch 691 batch loss 7.06454611 epoch total loss 7.03072166\n",
      "Trained batch 692 batch loss 7.11618757 epoch total loss 7.03084517\n",
      "Trained batch 693 batch loss 6.47546864 epoch total loss 7.0300436\n",
      "Trained batch 694 batch loss 7.09316921 epoch total loss 7.03013468\n",
      "Trained batch 695 batch loss 6.89892769 epoch total loss 7.02994585\n",
      "Trained batch 696 batch loss 6.83251619 epoch total loss 7.02966261\n",
      "Trained batch 697 batch loss 7.05769062 epoch total loss 7.02970266\n",
      "Trained batch 698 batch loss 7.19296026 epoch total loss 7.02993631\n",
      "Trained batch 699 batch loss 7.23884678 epoch total loss 7.03023481\n",
      "Trained batch 700 batch loss 7.14976788 epoch total loss 7.030406\n",
      "Trained batch 701 batch loss 7.18842793 epoch total loss 7.03063154\n",
      "Trained batch 702 batch loss 7.22669888 epoch total loss 7.03091049\n",
      "Trained batch 703 batch loss 6.89142799 epoch total loss 7.0307126\n",
      "Trained batch 704 batch loss 6.69410372 epoch total loss 7.03023481\n",
      "Trained batch 705 batch loss 6.94097853 epoch total loss 7.03010798\n",
      "Trained batch 706 batch loss 6.75538063 epoch total loss 7.02971888\n",
      "Trained batch 707 batch loss 6.7569294 epoch total loss 7.02933264\n",
      "Trained batch 708 batch loss 7.31039524 epoch total loss 7.02973\n",
      "Trained batch 709 batch loss 7.20974636 epoch total loss 7.029984\n",
      "Trained batch 710 batch loss 7.27854109 epoch total loss 7.030334\n",
      "Trained batch 711 batch loss 6.50887346 epoch total loss 7.02960062\n",
      "Trained batch 712 batch loss 6.80615377 epoch total loss 7.02928638\n",
      "Trained batch 713 batch loss 7.03957081 epoch total loss 7.02930117\n",
      "Trained batch 714 batch loss 7.09063244 epoch total loss 7.029387\n",
      "Trained batch 715 batch loss 7.51662254 epoch total loss 7.0300684\n",
      "Trained batch 716 batch loss 7.50260592 epoch total loss 7.03072834\n",
      "Trained batch 717 batch loss 7.17356443 epoch total loss 7.03092718\n",
      "Trained batch 718 batch loss 7.21581 epoch total loss 7.03118467\n",
      "Trained batch 719 batch loss 7.2374692 epoch total loss 7.03147125\n",
      "Trained batch 720 batch loss 7.33408499 epoch total loss 7.03189135\n",
      "Trained batch 721 batch loss 7.21564436 epoch total loss 7.03214645\n",
      "Trained batch 722 batch loss 6.83416748 epoch total loss 7.03187227\n",
      "Trained batch 723 batch loss 7.36988497 epoch total loss 7.03234\n",
      "Trained batch 724 batch loss 7.1487112 epoch total loss 7.03250122\n",
      "Trained batch 725 batch loss 7.35752869 epoch total loss 7.03294945\n",
      "Trained batch 726 batch loss 7.39834595 epoch total loss 7.03345251\n",
      "Trained batch 727 batch loss 7.4877286 epoch total loss 7.03407764\n",
      "Trained batch 728 batch loss 7.46428919 epoch total loss 7.03466845\n",
      "Trained batch 729 batch loss 7.49954176 epoch total loss 7.03530645\n",
      "Trained batch 730 batch loss 7.38301945 epoch total loss 7.03578234\n",
      "Trained batch 731 batch loss 7.44575834 epoch total loss 7.0363431\n",
      "Trained batch 732 batch loss 7.14096737 epoch total loss 7.03648615\n",
      "Trained batch 733 batch loss 7.2359066 epoch total loss 7.03675842\n",
      "Trained batch 734 batch loss 7.17229891 epoch total loss 7.03694296\n",
      "Trained batch 735 batch loss 7.40298605 epoch total loss 7.03744078\n",
      "Trained batch 736 batch loss 7.08007574 epoch total loss 7.03749895\n",
      "Trained batch 737 batch loss 7.04462481 epoch total loss 7.03750801\n",
      "Trained batch 738 batch loss 5.98846436 epoch total loss 7.03608656\n",
      "Trained batch 739 batch loss 6.60354424 epoch total loss 7.035501\n",
      "Trained batch 740 batch loss 7.23153067 epoch total loss 7.03576612\n",
      "Trained batch 741 batch loss 6.82253075 epoch total loss 7.03547859\n",
      "Trained batch 742 batch loss 6.81234503 epoch total loss 7.03517818\n",
      "Trained batch 743 batch loss 7.23404598 epoch total loss 7.03544521\n",
      "Trained batch 744 batch loss 7.30085564 epoch total loss 7.03580189\n",
      "Trained batch 745 batch loss 7.32878304 epoch total loss 7.03619528\n",
      "Trained batch 746 batch loss 7.43122482 epoch total loss 7.03672457\n",
      "Trained batch 747 batch loss 7.37375975 epoch total loss 7.03717518\n",
      "Trained batch 748 batch loss 7.49238825 epoch total loss 7.03778362\n",
      "Trained batch 749 batch loss 7.28867722 epoch total loss 7.03811836\n",
      "Trained batch 750 batch loss 7.19911957 epoch total loss 7.03833342\n",
      "Trained batch 751 batch loss 6.47145844 epoch total loss 7.03757858\n",
      "Trained batch 752 batch loss 7.20342875 epoch total loss 7.03779936\n",
      "Trained batch 753 batch loss 7.03081846 epoch total loss 7.0377903\n",
      "Trained batch 754 batch loss 7.31289196 epoch total loss 7.03815508\n",
      "Trained batch 755 batch loss 7.15579939 epoch total loss 7.038311\n",
      "Trained batch 756 batch loss 6.37207174 epoch total loss 7.03743\n",
      "Trained batch 757 batch loss 6.77184772 epoch total loss 7.03707886\n",
      "Trained batch 758 batch loss 6.46529 epoch total loss 7.03632498\n",
      "Trained batch 759 batch loss 6.54946423 epoch total loss 7.03568316\n",
      "Trained batch 760 batch loss 6.75843477 epoch total loss 7.03531837\n",
      "Trained batch 761 batch loss 6.62392807 epoch total loss 7.03477764\n",
      "Trained batch 762 batch loss 7.03619337 epoch total loss 7.03477955\n",
      "Trained batch 763 batch loss 6.91323328 epoch total loss 7.03462\n",
      "Trained batch 764 batch loss 7.43224764 epoch total loss 7.03514\n",
      "Trained batch 765 batch loss 6.93759155 epoch total loss 7.03501272\n",
      "Trained batch 766 batch loss 6.65106392 epoch total loss 7.03451109\n",
      "Trained batch 767 batch loss 7.07318592 epoch total loss 7.03456163\n",
      "Trained batch 768 batch loss 6.67611551 epoch total loss 7.03409529\n",
      "Trained batch 769 batch loss 6.70077848 epoch total loss 7.03366137\n",
      "Trained batch 770 batch loss 7.13642645 epoch total loss 7.03379488\n",
      "Trained batch 771 batch loss 6.56919909 epoch total loss 7.03319216\n",
      "Trained batch 772 batch loss 6.30385113 epoch total loss 7.03224754\n",
      "Trained batch 773 batch loss 6.58411217 epoch total loss 7.03166771\n",
      "Trained batch 774 batch loss 6.45298767 epoch total loss 7.03092\n",
      "Trained batch 775 batch loss 7.20646906 epoch total loss 7.03114653\n",
      "Trained batch 776 batch loss 6.7806735 epoch total loss 7.03082418\n",
      "Trained batch 777 batch loss 7.0855608 epoch total loss 7.03089428\n",
      "Trained batch 778 batch loss 7.103 epoch total loss 7.03098726\n",
      "Trained batch 779 batch loss 7.38634396 epoch total loss 7.03144312\n",
      "Trained batch 780 batch loss 7.17992926 epoch total loss 7.0316329\n",
      "Trained batch 781 batch loss 6.57263422 epoch total loss 7.03104544\n",
      "Trained batch 782 batch loss 6.60778141 epoch total loss 7.0305047\n",
      "Trained batch 783 batch loss 7.41369915 epoch total loss 7.03099346\n",
      "Trained batch 784 batch loss 7.02821445 epoch total loss 7.03099\n",
      "Trained batch 785 batch loss 7.36762905 epoch total loss 7.03141928\n",
      "Trained batch 786 batch loss 6.88325167 epoch total loss 7.03123093\n",
      "Trained batch 787 batch loss 7.04116774 epoch total loss 7.03124332\n",
      "Trained batch 788 batch loss 7.01074 epoch total loss 7.0312171\n",
      "Trained batch 789 batch loss 7.10225439 epoch total loss 7.03130674\n",
      "Trained batch 790 batch loss 7.26900816 epoch total loss 7.0316081\n",
      "Trained batch 791 batch loss 7.21259356 epoch total loss 7.03183651\n",
      "Trained batch 792 batch loss 7.36200905 epoch total loss 7.03225327\n",
      "Trained batch 793 batch loss 7.13318539 epoch total loss 7.03238058\n",
      "Trained batch 794 batch loss 6.83382797 epoch total loss 7.03213072\n",
      "Trained batch 795 batch loss 6.60461903 epoch total loss 7.03159285\n",
      "Trained batch 796 batch loss 6.09038639 epoch total loss 7.03041029\n",
      "Trained batch 797 batch loss 6.44808817 epoch total loss 7.02968\n",
      "Trained batch 798 batch loss 6.43761492 epoch total loss 7.02893782\n",
      "Trained batch 799 batch loss 6.79546118 epoch total loss 7.02864552\n",
      "Trained batch 800 batch loss 7.05959511 epoch total loss 7.02868414\n",
      "Trained batch 801 batch loss 7.25648832 epoch total loss 7.02896833\n",
      "Trained batch 802 batch loss 7.27606058 epoch total loss 7.02927637\n",
      "Trained batch 803 batch loss 6.60906172 epoch total loss 7.0287528\n",
      "Trained batch 804 batch loss 7.33230686 epoch total loss 7.02913046\n",
      "Trained batch 805 batch loss 7.2606225 epoch total loss 7.02941799\n",
      "Trained batch 806 batch loss 7.50379753 epoch total loss 7.03000689\n",
      "Trained batch 807 batch loss 7.4814229 epoch total loss 7.03056622\n",
      "Trained batch 808 batch loss 7.40675354 epoch total loss 7.03103161\n",
      "Trained batch 809 batch loss 7.2992816 epoch total loss 7.03136349\n",
      "Trained batch 810 batch loss 7.00739956 epoch total loss 7.03133392\n",
      "Trained batch 811 batch loss 6.83052063 epoch total loss 7.03108644\n",
      "Trained batch 812 batch loss 7.17644596 epoch total loss 7.03126526\n",
      "Trained batch 813 batch loss 7.34044409 epoch total loss 7.0316453\n",
      "Trained batch 814 batch loss 6.74976635 epoch total loss 7.03129911\n",
      "Trained batch 815 batch loss 7.18692589 epoch total loss 7.03149033\n",
      "Trained batch 816 batch loss 7.20936632 epoch total loss 7.03170824\n",
      "Trained batch 817 batch loss 6.6632 epoch total loss 7.03125715\n",
      "Trained batch 818 batch loss 7.16015 epoch total loss 7.03141499\n",
      "Trained batch 819 batch loss 6.92953396 epoch total loss 7.03129053\n",
      "Trained batch 820 batch loss 7.29622126 epoch total loss 7.03161383\n",
      "Trained batch 821 batch loss 7.3062315 epoch total loss 7.03194809\n",
      "Trained batch 822 batch loss 7.43468046 epoch total loss 7.0324378\n",
      "Trained batch 823 batch loss 7.50987196 epoch total loss 7.03301811\n",
      "Trained batch 824 batch loss 7.34333706 epoch total loss 7.03339434\n",
      "Trained batch 825 batch loss 7.30900192 epoch total loss 7.0337286\n",
      "Trained batch 826 batch loss 6.69091749 epoch total loss 7.03331375\n",
      "Trained batch 827 batch loss 7.23894215 epoch total loss 7.03356218\n",
      "Trained batch 828 batch loss 6.86595392 epoch total loss 7.03335953\n",
      "Trained batch 829 batch loss 6.70801115 epoch total loss 7.03296709\n",
      "Trained batch 830 batch loss 6.87049389 epoch total loss 7.03277111\n",
      "Trained batch 831 batch loss 6.80796051 epoch total loss 7.03250074\n",
      "Trained batch 832 batch loss 6.71869946 epoch total loss 7.03212404\n",
      "Trained batch 833 batch loss 6.64257908 epoch total loss 7.03165627\n",
      "Trained batch 834 batch loss 6.85060549 epoch total loss 7.0314393\n",
      "Trained batch 835 batch loss 6.64678478 epoch total loss 7.03097868\n",
      "Trained batch 836 batch loss 7.05794239 epoch total loss 7.0310111\n",
      "Trained batch 837 batch loss 7.09085 epoch total loss 7.03108263\n",
      "Trained batch 838 batch loss 7.14359665 epoch total loss 7.03121662\n",
      "Trained batch 839 batch loss 7.22155857 epoch total loss 7.0314436\n",
      "Trained batch 840 batch loss 6.99278879 epoch total loss 7.03139782\n",
      "Trained batch 841 batch loss 7.05998421 epoch total loss 7.03143167\n",
      "Trained batch 842 batch loss 6.94937563 epoch total loss 7.03133392\n",
      "Trained batch 843 batch loss 7.02061939 epoch total loss 7.03132105\n",
      "Trained batch 844 batch loss 6.854949 epoch total loss 7.03111219\n",
      "Trained batch 845 batch loss 6.67791319 epoch total loss 7.03069401\n",
      "Trained batch 846 batch loss 7.15549612 epoch total loss 7.03084135\n",
      "Trained batch 847 batch loss 6.95495701 epoch total loss 7.03075171\n",
      "Trained batch 848 batch loss 6.6102109 epoch total loss 7.03025627\n",
      "Trained batch 849 batch loss 7.14956379 epoch total loss 7.03039646\n",
      "Trained batch 850 batch loss 7.26647711 epoch total loss 7.03067446\n",
      "Trained batch 851 batch loss 7.45531416 epoch total loss 7.03117323\n",
      "Trained batch 852 batch loss 7.42033291 epoch total loss 7.03163\n",
      "Trained batch 853 batch loss 7.27039289 epoch total loss 7.03191\n",
      "Trained batch 854 batch loss 6.32945251 epoch total loss 7.0310874\n",
      "Trained batch 855 batch loss 6.47118235 epoch total loss 7.0304327\n",
      "Trained batch 856 batch loss 6.97543192 epoch total loss 7.0303688\n",
      "Trained batch 857 batch loss 7.24457884 epoch total loss 7.03061867\n",
      "Trained batch 858 batch loss 7.44031239 epoch total loss 7.03109646\n",
      "Trained batch 859 batch loss 7.10000181 epoch total loss 7.03117657\n",
      "Trained batch 860 batch loss 7.46289158 epoch total loss 7.03167868\n",
      "Trained batch 861 batch loss 7.40356874 epoch total loss 7.03211069\n",
      "Trained batch 862 batch loss 7.27722216 epoch total loss 7.03239536\n",
      "Trained batch 863 batch loss 7.34928322 epoch total loss 7.03276253\n",
      "Trained batch 864 batch loss 7.38969183 epoch total loss 7.03317547\n",
      "Trained batch 865 batch loss 7.52875853 epoch total loss 7.03374863\n",
      "Trained batch 866 batch loss 7.45625925 epoch total loss 7.03423595\n",
      "Trained batch 867 batch loss 7.35756683 epoch total loss 7.03460884\n",
      "Trained batch 868 batch loss 7.48296309 epoch total loss 7.03512526\n",
      "Trained batch 869 batch loss 7.45148373 epoch total loss 7.03560448\n",
      "Trained batch 870 batch loss 7.42345619 epoch total loss 7.03605032\n",
      "Trained batch 871 batch loss 7.35651541 epoch total loss 7.03641796\n",
      "Trained batch 872 batch loss 7.35292244 epoch total loss 7.03678131\n",
      "Trained batch 873 batch loss 7.43149614 epoch total loss 7.03723335\n",
      "Trained batch 874 batch loss 6.97981119 epoch total loss 7.03716803\n",
      "Trained batch 875 batch loss 7.05624199 epoch total loss 7.03719\n",
      "Trained batch 876 batch loss 7.08716297 epoch total loss 7.03724718\n",
      "Trained batch 877 batch loss 7.47200871 epoch total loss 7.03774309\n",
      "Trained batch 878 batch loss 7.45104122 epoch total loss 7.03821373\n",
      "Trained batch 879 batch loss 7.27604961 epoch total loss 7.0384841\n",
      "Trained batch 880 batch loss 7.17039204 epoch total loss 7.0386343\n",
      "Trained batch 881 batch loss 7.30915165 epoch total loss 7.03894091\n",
      "Trained batch 882 batch loss 7.33775806 epoch total loss 7.03928\n",
      "Trained batch 883 batch loss 7.3467083 epoch total loss 7.03962803\n",
      "Trained batch 884 batch loss 7.2039 epoch total loss 7.03981447\n",
      "Trained batch 885 batch loss 7.39759779 epoch total loss 7.04021835\n",
      "Trained batch 886 batch loss 7.13312292 epoch total loss 7.04032326\n",
      "Trained batch 887 batch loss 6.91401386 epoch total loss 7.04018116\n",
      "Trained batch 888 batch loss 7.3787756 epoch total loss 7.04056263\n",
      "Trained batch 889 batch loss 7.06841946 epoch total loss 7.04059362\n",
      "Trained batch 890 batch loss 6.87615871 epoch total loss 7.04040861\n",
      "Trained batch 891 batch loss 6.53255558 epoch total loss 7.03983927\n",
      "Trained batch 892 batch loss 6.86683 epoch total loss 7.03964472\n",
      "Trained batch 893 batch loss 7.13819933 epoch total loss 7.03975534\n",
      "Trained batch 894 batch loss 7.23103619 epoch total loss 7.03996897\n",
      "Trained batch 895 batch loss 7.21362257 epoch total loss 7.04016304\n",
      "Trained batch 896 batch loss 7.17126513 epoch total loss 7.04030943\n",
      "Trained batch 897 batch loss 7.3619256 epoch total loss 7.04066801\n",
      "Trained batch 898 batch loss 7.29251337 epoch total loss 7.04094839\n",
      "Trained batch 899 batch loss 7.19461 epoch total loss 7.04111958\n",
      "Trained batch 900 batch loss 7.38487434 epoch total loss 7.04150105\n",
      "Trained batch 901 batch loss 7.31492329 epoch total loss 7.04180479\n",
      "Trained batch 902 batch loss 7.27639437 epoch total loss 7.04206467\n",
      "Trained batch 903 batch loss 7.51834726 epoch total loss 7.04259253\n",
      "Trained batch 904 batch loss 7.38668871 epoch total loss 7.04297304\n",
      "Trained batch 905 batch loss 7.30214 epoch total loss 7.04325962\n",
      "Trained batch 906 batch loss 7.25044346 epoch total loss 7.0434885\n",
      "Trained batch 907 batch loss 7.21706 epoch total loss 7.04367971\n",
      "Trained batch 908 batch loss 7.3665123 epoch total loss 7.04403543\n",
      "Trained batch 909 batch loss 7.3597374 epoch total loss 7.04438305\n",
      "Trained batch 910 batch loss 7.55787373 epoch total loss 7.04494762\n",
      "Trained batch 911 batch loss 7.32836246 epoch total loss 7.04525852\n",
      "Trained batch 912 batch loss 7.08104849 epoch total loss 7.04529762\n",
      "Trained batch 913 batch loss 6.62296343 epoch total loss 7.04483509\n",
      "Trained batch 914 batch loss 6.38213825 epoch total loss 7.0441103\n",
      "Trained batch 915 batch loss 6.86359882 epoch total loss 7.04391336\n",
      "Trained batch 916 batch loss 6.52611589 epoch total loss 7.04334784\n",
      "Trained batch 917 batch loss 7.29015303 epoch total loss 7.04361677\n",
      "Trained batch 918 batch loss 7.16912222 epoch total loss 7.04375315\n",
      "Trained batch 919 batch loss 7.55072117 epoch total loss 7.04430485\n",
      "Trained batch 920 batch loss 7.54587698 epoch total loss 7.04485035\n",
      "Trained batch 921 batch loss 7.49206 epoch total loss 7.04533577\n",
      "Trained batch 922 batch loss 7.49626684 epoch total loss 7.045825\n",
      "Trained batch 923 batch loss 7.22929859 epoch total loss 7.04602385\n",
      "Trained batch 924 batch loss 6.99559784 epoch total loss 7.04596949\n",
      "Trained batch 925 batch loss 7.17939186 epoch total loss 7.04611349\n",
      "Trained batch 926 batch loss 7.38078451 epoch total loss 7.04647493\n",
      "Trained batch 927 batch loss 7.01793051 epoch total loss 7.04644394\n",
      "Trained batch 928 batch loss 7.41207266 epoch total loss 7.04683828\n",
      "Trained batch 929 batch loss 7.45359468 epoch total loss 7.04727602\n",
      "Trained batch 930 batch loss 7.05155754 epoch total loss 7.04728079\n",
      "Trained batch 931 batch loss 7.40898561 epoch total loss 7.04766941\n",
      "Trained batch 932 batch loss 7.27103281 epoch total loss 7.04790926\n",
      "Trained batch 933 batch loss 7.35486746 epoch total loss 7.04823828\n",
      "Trained batch 934 batch loss 6.71785879 epoch total loss 7.04788446\n",
      "Trained batch 935 batch loss 7.04935741 epoch total loss 7.04788589\n",
      "Trained batch 936 batch loss 6.88457632 epoch total loss 7.04771185\n",
      "Trained batch 937 batch loss 7.25571108 epoch total loss 7.04793406\n",
      "Trained batch 938 batch loss 6.76800919 epoch total loss 7.04763556\n",
      "Trained batch 939 batch loss 7.09803581 epoch total loss 7.04768944\n",
      "Trained batch 940 batch loss 7.1180625 epoch total loss 7.0477643\n",
      "Trained batch 941 batch loss 7.28629065 epoch total loss 7.0480175\n",
      "Trained batch 942 batch loss 7.44762182 epoch total loss 7.04844189\n",
      "Trained batch 943 batch loss 6.98739195 epoch total loss 7.04837704\n",
      "Trained batch 944 batch loss 6.90653849 epoch total loss 7.04822731\n",
      "Trained batch 945 batch loss 6.89309168 epoch total loss 7.0480628\n",
      "Trained batch 946 batch loss 6.78508902 epoch total loss 7.04778481\n",
      "Trained batch 947 batch loss 7.07116508 epoch total loss 7.0478096\n",
      "Trained batch 948 batch loss 6.8847084 epoch total loss 7.04763794\n",
      "Trained batch 949 batch loss 6.64126205 epoch total loss 7.04720926\n",
      "Trained batch 950 batch loss 6.9781847 epoch total loss 7.04713678\n",
      "Trained batch 951 batch loss 6.79551411 epoch total loss 7.04687214\n",
      "Trained batch 952 batch loss 6.96980858 epoch total loss 7.04679108\n",
      "Trained batch 953 batch loss 7.29724598 epoch total loss 7.04705381\n",
      "Trained batch 954 batch loss 7.16227627 epoch total loss 7.04717445\n",
      "Trained batch 955 batch loss 7.12983131 epoch total loss 7.04726124\n",
      "Trained batch 956 batch loss 7.31764221 epoch total loss 7.047544\n",
      "Trained batch 957 batch loss 6.92977762 epoch total loss 7.04742098\n",
      "Trained batch 958 batch loss 6.77652359 epoch total loss 7.04713821\n",
      "Trained batch 959 batch loss 6.99527645 epoch total loss 7.04708385\n",
      "Trained batch 960 batch loss 7.14209461 epoch total loss 7.04718256\n",
      "Trained batch 961 batch loss 6.81325197 epoch total loss 7.04693937\n",
      "Trained batch 962 batch loss 6.45013285 epoch total loss 7.04631901\n",
      "Trained batch 963 batch loss 6.88941574 epoch total loss 7.04615641\n",
      "Trained batch 964 batch loss 7.25917959 epoch total loss 7.04637766\n",
      "Trained batch 965 batch loss 7.40845442 epoch total loss 7.04675293\n",
      "Trained batch 966 batch loss 7.3509 epoch total loss 7.04706812\n",
      "Trained batch 967 batch loss 7.42971516 epoch total loss 7.04746389\n",
      "Trained batch 968 batch loss 7.53764248 epoch total loss 7.04797029\n",
      "Trained batch 969 batch loss 7.5040288 epoch total loss 7.04844046\n",
      "Trained batch 970 batch loss 7.30404425 epoch total loss 7.04870415\n",
      "Trained batch 971 batch loss 7.1127553 epoch total loss 7.04877043\n",
      "Trained batch 972 batch loss 7.15895557 epoch total loss 7.04888391\n",
      "Trained batch 973 batch loss 7.39062071 epoch total loss 7.04923534\n",
      "Trained batch 974 batch loss 7.39308071 epoch total loss 7.0495882\n",
      "Trained batch 975 batch loss 7.42554474 epoch total loss 7.04997396\n",
      "Trained batch 976 batch loss 7.06630659 epoch total loss 7.04999065\n",
      "Trained batch 977 batch loss 7.36141348 epoch total loss 7.05030966\n",
      "Trained batch 978 batch loss 7.1288228 epoch total loss 7.05039\n",
      "Trained batch 979 batch loss 7.02763796 epoch total loss 7.05036688\n",
      "Trained batch 980 batch loss 7.10971069 epoch total loss 7.05042744\n",
      "Trained batch 981 batch loss 6.82269096 epoch total loss 7.05019522\n",
      "Trained batch 982 batch loss 6.47430944 epoch total loss 7.04960871\n",
      "Trained batch 983 batch loss 6.97327328 epoch total loss 7.04953098\n",
      "Trained batch 984 batch loss 7.00652218 epoch total loss 7.04948711\n",
      "Trained batch 985 batch loss 6.95628119 epoch total loss 7.04939222\n",
      "Trained batch 986 batch loss 6.9157052 epoch total loss 7.04925632\n",
      "Trained batch 987 batch loss 7.32556391 epoch total loss 7.04953671\n",
      "Trained batch 988 batch loss 7.21210861 epoch total loss 7.04970074\n",
      "Trained batch 989 batch loss 7.13139057 epoch total loss 7.04978323\n",
      "Trained batch 990 batch loss 7.20921898 epoch total loss 7.0499444\n",
      "Trained batch 991 batch loss 6.80843925 epoch total loss 7.04970074\n",
      "Trained batch 992 batch loss 6.45935965 epoch total loss 7.04910564\n",
      "Trained batch 993 batch loss 6.92479277 epoch total loss 7.04898071\n",
      "Trained batch 994 batch loss 6.91496277 epoch total loss 7.04884577\n",
      "Trained batch 995 batch loss 7.41211748 epoch total loss 7.04921103\n",
      "Trained batch 996 batch loss 7.30240154 epoch total loss 7.04946518\n",
      "Trained batch 997 batch loss 7.21354961 epoch total loss 7.04962921\n",
      "Trained batch 998 batch loss 7.10112524 epoch total loss 7.04968071\n",
      "Trained batch 999 batch loss 7.19022036 epoch total loss 7.04982185\n",
      "Trained batch 1000 batch loss 6.93954802 epoch total loss 7.04971123\n",
      "Trained batch 1001 batch loss 6.20674944 epoch total loss 7.04886913\n",
      "Trained batch 1002 batch loss 6.15918684 epoch total loss 7.04798126\n",
      "Trained batch 1003 batch loss 6.9520669 epoch total loss 7.04788542\n",
      "Trained batch 1004 batch loss 6.71082306 epoch total loss 7.04755\n",
      "Trained batch 1005 batch loss 6.66977739 epoch total loss 7.04717445\n",
      "Trained batch 1006 batch loss 6.64102697 epoch total loss 7.04677057\n",
      "Trained batch 1007 batch loss 6.81381512 epoch total loss 7.04653931\n",
      "Trained batch 1008 batch loss 6.91142035 epoch total loss 7.04640579\n",
      "Trained batch 1009 batch loss 7.17533207 epoch total loss 7.04653358\n",
      "Trained batch 1010 batch loss 7.21454859 epoch total loss 7.04669952\n",
      "Trained batch 1011 batch loss 7.33200598 epoch total loss 7.04698181\n",
      "Trained batch 1012 batch loss 7.28935671 epoch total loss 7.04722166\n",
      "Trained batch 1013 batch loss 5.93980694 epoch total loss 7.04612827\n",
      "Trained batch 1014 batch loss 5.22665787 epoch total loss 7.04433393\n",
      "Trained batch 1015 batch loss 5.92276335 epoch total loss 7.0432291\n",
      "Trained batch 1016 batch loss 7.31479359 epoch total loss 7.04349661\n",
      "Trained batch 1017 batch loss 7.43977928 epoch total loss 7.04388618\n",
      "Trained batch 1018 batch loss 7.32446909 epoch total loss 7.04416227\n",
      "Trained batch 1019 batch loss 7.22275829 epoch total loss 7.04433727\n",
      "Trained batch 1020 batch loss 7.28043079 epoch total loss 7.04456854\n",
      "Trained batch 1021 batch loss 7.34056044 epoch total loss 7.04485846\n",
      "Trained batch 1022 batch loss 7.21488285 epoch total loss 7.04502439\n",
      "Trained batch 1023 batch loss 7.2551012 epoch total loss 7.04523\n",
      "Trained batch 1024 batch loss 7.4041338 epoch total loss 7.04558039\n",
      "Trained batch 1025 batch loss 6.80097389 epoch total loss 7.04534149\n",
      "Trained batch 1026 batch loss 7.01346922 epoch total loss 7.0453105\n",
      "Trained batch 1027 batch loss 7.0252471 epoch total loss 7.04529142\n",
      "Trained batch 1028 batch loss 6.55023289 epoch total loss 7.04481\n",
      "Trained batch 1029 batch loss 6.71055174 epoch total loss 7.04448462\n",
      "Trained batch 1030 batch loss 7.08578777 epoch total loss 7.04452515\n",
      "Trained batch 1031 batch loss 6.87093449 epoch total loss 7.04435682\n",
      "Trained batch 1032 batch loss 7.04802513 epoch total loss 7.04436\n",
      "Trained batch 1033 batch loss 7.40887165 epoch total loss 7.04471302\n",
      "Trained batch 1034 batch loss 7.33510065 epoch total loss 7.04499388\n",
      "Trained batch 1035 batch loss 6.94427061 epoch total loss 7.0448966\n",
      "Trained batch 1036 batch loss 6.78337812 epoch total loss 7.04464388\n",
      "Trained batch 1037 batch loss 6.97525024 epoch total loss 7.04457664\n",
      "Trained batch 1038 batch loss 6.91406775 epoch total loss 7.04445124\n",
      "Trained batch 1039 batch loss 7.111588 epoch total loss 7.04451561\n",
      "Trained batch 1040 batch loss 7.40634584 epoch total loss 7.0448637\n",
      "Trained batch 1041 batch loss 7.54507923 epoch total loss 7.04534388\n",
      "Trained batch 1042 batch loss 7.42144442 epoch total loss 7.04570484\n",
      "Trained batch 1043 batch loss 7.45189857 epoch total loss 7.04609394\n",
      "Trained batch 1044 batch loss 6.97246933 epoch total loss 7.04602385\n",
      "Trained batch 1045 batch loss 7.02269363 epoch total loss 7.04600143\n",
      "Trained batch 1046 batch loss 6.83741283 epoch total loss 7.04580164\n",
      "Trained batch 1047 batch loss 6.97074413 epoch total loss 7.04573\n",
      "Trained batch 1048 batch loss 6.63282 epoch total loss 7.04533625\n",
      "Trained batch 1049 batch loss 7.11200905 epoch total loss 7.04539967\n",
      "Trained batch 1050 batch loss 6.69672 epoch total loss 7.04506731\n",
      "Trained batch 1051 batch loss 7.04536104 epoch total loss 7.04506779\n",
      "Trained batch 1052 batch loss 7.01338387 epoch total loss 7.04503727\n",
      "Trained batch 1053 batch loss 7.41583633 epoch total loss 7.04538965\n",
      "Trained batch 1054 batch loss 7.40545464 epoch total loss 7.04573107\n",
      "Trained batch 1055 batch loss 7.23499727 epoch total loss 7.04591036\n",
      "Trained batch 1056 batch loss 7.26632118 epoch total loss 7.04611921\n",
      "Trained batch 1057 batch loss 7.281919 epoch total loss 7.0463419\n",
      "Trained batch 1058 batch loss 7.04503 epoch total loss 7.04634047\n",
      "Trained batch 1059 batch loss 7.17250919 epoch total loss 7.04645967\n",
      "Trained batch 1060 batch loss 6.96928 epoch total loss 7.04638672\n",
      "Trained batch 1061 batch loss 6.95976067 epoch total loss 7.04630518\n",
      "Trained batch 1062 batch loss 6.84722424 epoch total loss 7.04611778\n",
      "Trained batch 1063 batch loss 6.86799383 epoch total loss 7.04595041\n",
      "Trained batch 1064 batch loss 7.07368374 epoch total loss 7.04597664\n",
      "Trained batch 1065 batch loss 7.32976198 epoch total loss 7.04624271\n",
      "Trained batch 1066 batch loss 7.40615273 epoch total loss 7.04658031\n",
      "Trained batch 1067 batch loss 7.14840317 epoch total loss 7.04667616\n",
      "Trained batch 1068 batch loss 7.34823 epoch total loss 7.04695845\n",
      "Trained batch 1069 batch loss 7.02515554 epoch total loss 7.04693794\n",
      "Trained batch 1070 batch loss 7.22699261 epoch total loss 7.04710627\n",
      "Trained batch 1071 batch loss 7.40184259 epoch total loss 7.04743767\n",
      "Trained batch 1072 batch loss 7.15066862 epoch total loss 7.04753399\n",
      "Trained batch 1073 batch loss 7.40099478 epoch total loss 7.04786348\n",
      "Trained batch 1074 batch loss 6.6859417 epoch total loss 7.04752636\n",
      "Trained batch 1075 batch loss 6.99619198 epoch total loss 7.04747868\n",
      "Trained batch 1076 batch loss 6.7616396 epoch total loss 7.04721308\n",
      "Trained batch 1077 batch loss 7.33793354 epoch total loss 7.04748297\n",
      "Trained batch 1078 batch loss 7.35065699 epoch total loss 7.0477643\n",
      "Trained batch 1079 batch loss 6.88831329 epoch total loss 7.04761648\n",
      "Trained batch 1080 batch loss 7.23555183 epoch total loss 7.04779\n",
      "Trained batch 1081 batch loss 7.2243104 epoch total loss 7.04795313\n",
      "Trained batch 1082 batch loss 7.23256588 epoch total loss 7.04812384\n",
      "Trained batch 1083 batch loss 7.21709061 epoch total loss 7.04828\n",
      "Trained batch 1084 batch loss 7.16027832 epoch total loss 7.04838324\n",
      "Trained batch 1085 batch loss 7.25523663 epoch total loss 7.04857397\n",
      "Trained batch 1086 batch loss 6.82746601 epoch total loss 7.04837036\n",
      "Trained batch 1087 batch loss 6.90023279 epoch total loss 7.04823446\n",
      "Trained batch 1088 batch loss 7.08492279 epoch total loss 7.04826784\n",
      "Trained batch 1089 batch loss 7.32555628 epoch total loss 7.04852295\n",
      "Trained batch 1090 batch loss 7.28974771 epoch total loss 7.04874372\n",
      "Trained batch 1091 batch loss 7.17318487 epoch total loss 7.04885817\n",
      "Trained batch 1092 batch loss 7.41518593 epoch total loss 7.04919338\n",
      "Trained batch 1093 batch loss 7.23570347 epoch total loss 7.04936409\n",
      "Trained batch 1094 batch loss 6.75921631 epoch total loss 7.04909897\n",
      "Trained batch 1095 batch loss 7.27213383 epoch total loss 7.04930258\n",
      "Trained batch 1096 batch loss 6.85728645 epoch total loss 7.04912758\n",
      "Trained batch 1097 batch loss 6.58601284 epoch total loss 7.0487051\n",
      "Trained batch 1098 batch loss 5.93909025 epoch total loss 7.04769468\n",
      "Trained batch 1099 batch loss 6.74039745 epoch total loss 7.04741478\n",
      "Trained batch 1100 batch loss 6.81183434 epoch total loss 7.04720068\n",
      "Trained batch 1101 batch loss 6.980618 epoch total loss 7.04714\n",
      "Trained batch 1102 batch loss 7.17076206 epoch total loss 7.04725266\n",
      "Trained batch 1103 batch loss 6.89636421 epoch total loss 7.0471158\n",
      "Trained batch 1104 batch loss 6.58627462 epoch total loss 7.04669857\n",
      "Trained batch 1105 batch loss 6.79188442 epoch total loss 7.04646826\n",
      "Trained batch 1106 batch loss 6.74785423 epoch total loss 7.04619837\n",
      "Trained batch 1107 batch loss 6.65379 epoch total loss 7.0458436\n",
      "Trained batch 1108 batch loss 7.1832571 epoch total loss 7.04596758\n",
      "Trained batch 1109 batch loss 7.35040522 epoch total loss 7.04624224\n",
      "Trained batch 1110 batch loss 7.42373228 epoch total loss 7.0465827\n",
      "Trained batch 1111 batch loss 7.41432095 epoch total loss 7.04691362\n",
      "Trained batch 1112 batch loss 7.44386721 epoch total loss 7.04727077\n",
      "Trained batch 1113 batch loss 7.29367781 epoch total loss 7.04749203\n",
      "Trained batch 1114 batch loss 7.24567795 epoch total loss 7.04767\n",
      "Trained batch 1115 batch loss 7.41061974 epoch total loss 7.04799509\n",
      "Trained batch 1116 batch loss 7.33733082 epoch total loss 7.04825449\n",
      "Trained batch 1117 batch loss 7.2866044 epoch total loss 7.04846811\n",
      "Trained batch 1118 batch loss 7.12908268 epoch total loss 7.04853964\n",
      "Trained batch 1119 batch loss 6.95694494 epoch total loss 7.0484581\n",
      "Trained batch 1120 batch loss 7.01111 epoch total loss 7.04842472\n",
      "Trained batch 1121 batch loss 6.6763339 epoch total loss 7.04809284\n",
      "Trained batch 1122 batch loss 6.41442585 epoch total loss 7.04752827\n",
      "Trained batch 1123 batch loss 7.24383307 epoch total loss 7.04770279\n",
      "Trained batch 1124 batch loss 7.41241312 epoch total loss 7.04802752\n",
      "Trained batch 1125 batch loss 7.46258163 epoch total loss 7.04839563\n",
      "Trained batch 1126 batch loss 6.76028824 epoch total loss 7.04814\n",
      "Trained batch 1127 batch loss 6.2669735 epoch total loss 7.04744673\n",
      "Trained batch 1128 batch loss 7.19783688 epoch total loss 7.04758024\n",
      "Trained batch 1129 batch loss 7.44978952 epoch total loss 7.04793644\n",
      "Trained batch 1130 batch loss 6.60424042 epoch total loss 7.04754353\n",
      "Trained batch 1131 batch loss 5.77561 epoch total loss 7.04641867\n",
      "Trained batch 1132 batch loss 6.58452415 epoch total loss 7.04601049\n",
      "Trained batch 1133 batch loss 6.895226 epoch total loss 7.04587746\n",
      "Trained batch 1134 batch loss 6.94573641 epoch total loss 7.04578924\n",
      "Trained batch 1135 batch loss 7.16811419 epoch total loss 7.04589653\n",
      "Trained batch 1136 batch loss 7.3155303 epoch total loss 7.046134\n",
      "Trained batch 1137 batch loss 7.23956 epoch total loss 7.04630423\n",
      "Trained batch 1138 batch loss 7.28001308 epoch total loss 7.04650927\n",
      "Trained batch 1139 batch loss 6.86317158 epoch total loss 7.04634857\n",
      "Trained batch 1140 batch loss 6.52512932 epoch total loss 7.04589128\n",
      "Trained batch 1141 batch loss 6.73535204 epoch total loss 7.04561901\n",
      "Trained batch 1142 batch loss 6.96463919 epoch total loss 7.04554844\n",
      "Trained batch 1143 batch loss 7.05442715 epoch total loss 7.04555559\n",
      "Trained batch 1144 batch loss 7.20014811 epoch total loss 7.04569101\n",
      "Trained batch 1145 batch loss 7.1960125 epoch total loss 7.04582214\n",
      "Trained batch 1146 batch loss 7.39789438 epoch total loss 7.04612923\n",
      "Trained batch 1147 batch loss 7.37989759 epoch total loss 7.04642057\n",
      "Trained batch 1148 batch loss 7.41638613 epoch total loss 7.04674292\n",
      "Trained batch 1149 batch loss 7.26196718 epoch total loss 7.04693031\n",
      "Trained batch 1150 batch loss 7.44127846 epoch total loss 7.04727316\n",
      "Trained batch 1151 batch loss 7.43646669 epoch total loss 7.04761124\n",
      "Trained batch 1152 batch loss 7.23758 epoch total loss 7.0477767\n",
      "Trained batch 1153 batch loss 7.41767025 epoch total loss 7.04809713\n",
      "Trained batch 1154 batch loss 7.34677315 epoch total loss 7.04835606\n",
      "Trained batch 1155 batch loss 7.35182047 epoch total loss 7.04861879\n",
      "Trained batch 1156 batch loss 7.32453775 epoch total loss 7.04885769\n",
      "Trained batch 1157 batch loss 7.49487448 epoch total loss 7.04924345\n",
      "Trained batch 1158 batch loss 7.47015333 epoch total loss 7.0496068\n",
      "Trained batch 1159 batch loss 7.49254131 epoch total loss 7.04998922\n",
      "Trained batch 1160 batch loss 7.43436766 epoch total loss 7.05032063\n",
      "Trained batch 1161 batch loss 7.41688871 epoch total loss 7.05063677\n",
      "Trained batch 1162 batch loss 7.38950968 epoch total loss 7.05092812\n",
      "Trained batch 1163 batch loss 7.32697248 epoch total loss 7.05116606\n",
      "Trained batch 1164 batch loss 7.29275894 epoch total loss 7.05137348\n",
      "Trained batch 1165 batch loss 7.01692295 epoch total loss 7.05134392\n",
      "Trained batch 1166 batch loss 7.15855122 epoch total loss 7.05143547\n",
      "Trained batch 1167 batch loss 7.04871464 epoch total loss 7.05143309\n",
      "Trained batch 1168 batch loss 7.44295883 epoch total loss 7.05176878\n",
      "Trained batch 1169 batch loss 6.96951437 epoch total loss 7.05169868\n",
      "Trained batch 1170 batch loss 6.9586606 epoch total loss 7.05161905\n",
      "Trained batch 1171 batch loss 7.05666351 epoch total loss 7.05162334\n",
      "Trained batch 1172 batch loss 6.74526739 epoch total loss 7.05136204\n",
      "Trained batch 1173 batch loss 6.74141502 epoch total loss 7.05109739\n",
      "Trained batch 1174 batch loss 6.85795403 epoch total loss 7.05093336\n",
      "Trained batch 1175 batch loss 6.97226286 epoch total loss 7.05086708\n",
      "Trained batch 1176 batch loss 7.00521374 epoch total loss 7.05082798\n",
      "Trained batch 1177 batch loss 6.6908617 epoch total loss 7.05052137\n",
      "Trained batch 1178 batch loss 6.91379166 epoch total loss 7.0504055\n",
      "Trained batch 1179 batch loss 7.04935932 epoch total loss 7.05040503\n",
      "Trained batch 1180 batch loss 7.13096476 epoch total loss 7.05047321\n",
      "Trained batch 1181 batch loss 6.24810839 epoch total loss 7.04979372\n",
      "Trained batch 1182 batch loss 5.97731543 epoch total loss 7.04888678\n",
      "Trained batch 1183 batch loss 6.40433693 epoch total loss 7.04834175\n",
      "Trained batch 1184 batch loss 6.64588976 epoch total loss 7.04800177\n",
      "Trained batch 1185 batch loss 6.75742245 epoch total loss 7.04775667\n",
      "Trained batch 1186 batch loss 7.10919952 epoch total loss 7.04780865\n",
      "Trained batch 1187 batch loss 6.99310589 epoch total loss 7.04776287\n",
      "Trained batch 1188 batch loss 7.15572119 epoch total loss 7.04785299\n",
      "Trained batch 1189 batch loss 6.86056042 epoch total loss 7.04769564\n",
      "Trained batch 1190 batch loss 7.11465025 epoch total loss 7.04775143\n",
      "Trained batch 1191 batch loss 6.91295385 epoch total loss 7.04763842\n",
      "Trained batch 1192 batch loss 7.37044954 epoch total loss 7.04790878\n",
      "Trained batch 1193 batch loss 7.30073738 epoch total loss 7.04812098\n",
      "Trained batch 1194 batch loss 7.12124205 epoch total loss 7.04818201\n",
      "Trained batch 1195 batch loss 6.96983862 epoch total loss 7.04811621\n",
      "Trained batch 1196 batch loss 7.20618534 epoch total loss 7.04824829\n",
      "Trained batch 1197 batch loss 6.72732496 epoch total loss 7.04798031\n",
      "Trained batch 1198 batch loss 6.38757753 epoch total loss 7.04742908\n",
      "Trained batch 1199 batch loss 6.98735142 epoch total loss 7.04737902\n",
      "Trained batch 1200 batch loss 6.95473814 epoch total loss 7.04730225\n",
      "Trained batch 1201 batch loss 7.32791853 epoch total loss 7.0475359\n",
      "Trained batch 1202 batch loss 7.44986248 epoch total loss 7.04787111\n",
      "Trained batch 1203 batch loss 7.19704103 epoch total loss 7.04799509\n",
      "Trained batch 1204 batch loss 6.68928766 epoch total loss 7.04769754\n",
      "Trained batch 1205 batch loss 7.07426739 epoch total loss 7.04771948\n",
      "Trained batch 1206 batch loss 7.00620127 epoch total loss 7.04768467\n",
      "Trained batch 1207 batch loss 6.96304131 epoch total loss 7.04761457\n",
      "Trained batch 1208 batch loss 6.88585758 epoch total loss 7.04748058\n",
      "Trained batch 1209 batch loss 7.19430351 epoch total loss 7.04760218\n",
      "Trained batch 1210 batch loss 7.38042545 epoch total loss 7.04787731\n",
      "Trained batch 1211 batch loss 7.35658216 epoch total loss 7.04813242\n",
      "Trained batch 1212 batch loss 7.58430195 epoch total loss 7.04857445\n",
      "Trained batch 1213 batch loss 7.468009 epoch total loss 7.04891968\n",
      "Trained batch 1214 batch loss 7.16068077 epoch total loss 7.04901218\n",
      "Trained batch 1215 batch loss 7.02237797 epoch total loss 7.04899025\n",
      "Trained batch 1216 batch loss 7.23507309 epoch total loss 7.04914379\n",
      "Trained batch 1217 batch loss 7.2035675 epoch total loss 7.04927\n",
      "Trained batch 1218 batch loss 7.0106349 epoch total loss 7.04923868\n",
      "Trained batch 1219 batch loss 7.08496523 epoch total loss 7.04926777\n",
      "Trained batch 1220 batch loss 7.15440798 epoch total loss 7.04935408\n",
      "Trained batch 1221 batch loss 7.21693659 epoch total loss 7.04949093\n",
      "Trained batch 1222 batch loss 7.0516 epoch total loss 7.04949284\n",
      "Trained batch 1223 batch loss 7.20174694 epoch total loss 7.04961777\n",
      "Trained batch 1224 batch loss 7.006814 epoch total loss 7.04958296\n",
      "Trained batch 1225 batch loss 7.35981512 epoch total loss 7.04983568\n",
      "Trained batch 1226 batch loss 7.01198387 epoch total loss 7.04980469\n",
      "Trained batch 1227 batch loss 7.0488019 epoch total loss 7.04980373\n",
      "Trained batch 1228 batch loss 7.13698578 epoch total loss 7.04987478\n",
      "Trained batch 1229 batch loss 7.53726149 epoch total loss 7.05027103\n",
      "Trained batch 1230 batch loss 7.48148584 epoch total loss 7.05062151\n",
      "Trained batch 1231 batch loss 7.50729 epoch total loss 7.05099249\n",
      "Trained batch 1232 batch loss 7.1813221 epoch total loss 7.05109835\n",
      "Trained batch 1233 batch loss 7.17259264 epoch total loss 7.05119705\n",
      "Trained batch 1234 batch loss 7.14656734 epoch total loss 7.0512743\n",
      "Trained batch 1235 batch loss 7.22707701 epoch total loss 7.05141687\n",
      "Trained batch 1236 batch loss 6.98992682 epoch total loss 7.05136728\n",
      "Trained batch 1237 batch loss 7.09736586 epoch total loss 7.05140495\n",
      "Trained batch 1238 batch loss 6.84043598 epoch total loss 7.05123472\n",
      "Trained batch 1239 batch loss 7.21561861 epoch total loss 7.05136776\n",
      "Trained batch 1240 batch loss 7.20601654 epoch total loss 7.05149221\n",
      "Trained batch 1241 batch loss 7.25205374 epoch total loss 7.05165386\n",
      "Trained batch 1242 batch loss 7.05459452 epoch total loss 7.05165625\n",
      "Trained batch 1243 batch loss 7.07202387 epoch total loss 7.05167294\n",
      "Trained batch 1244 batch loss 6.89442396 epoch total loss 7.05154657\n",
      "Trained batch 1245 batch loss 6.58187342 epoch total loss 7.0511694\n",
      "Trained batch 1246 batch loss 6.98437929 epoch total loss 7.05111599\n",
      "Trained batch 1247 batch loss 7.26723194 epoch total loss 7.05128956\n",
      "Trained batch 1248 batch loss 7.30037403 epoch total loss 7.05148935\n",
      "Trained batch 1249 batch loss 7.35341692 epoch total loss 7.05173111\n",
      "Trained batch 1250 batch loss 7.17391634 epoch total loss 7.05182886\n",
      "Trained batch 1251 batch loss 6.5533433 epoch total loss 7.0514307\n",
      "Trained batch 1252 batch loss 7.18617392 epoch total loss 7.05153847\n",
      "Trained batch 1253 batch loss 7.39655828 epoch total loss 7.05181408\n",
      "Trained batch 1254 batch loss 7.30434275 epoch total loss 7.05201578\n",
      "Trained batch 1255 batch loss 7.16181231 epoch total loss 7.05210352\n",
      "Trained batch 1256 batch loss 6.71166801 epoch total loss 7.05183268\n",
      "Trained batch 1257 batch loss 6.63113213 epoch total loss 7.05149746\n",
      "Trained batch 1258 batch loss 7.234025 epoch total loss 7.05164289\n",
      "Trained batch 1259 batch loss 7.25123119 epoch total loss 7.0518012\n",
      "Trained batch 1260 batch loss 6.91259766 epoch total loss 7.05169106\n",
      "Trained batch 1261 batch loss 6.80725527 epoch total loss 7.05149746\n",
      "Trained batch 1262 batch loss 6.34259605 epoch total loss 7.05093622\n",
      "Trained batch 1263 batch loss 6.98145819 epoch total loss 7.05088091\n",
      "Trained batch 1264 batch loss 6.88636255 epoch total loss 7.05075121\n",
      "Trained batch 1265 batch loss 6.29614162 epoch total loss 7.05015421\n",
      "Trained batch 1266 batch loss 6.04548168 epoch total loss 7.04936123\n",
      "Trained batch 1267 batch loss 5.86543226 epoch total loss 7.04842663\n",
      "Trained batch 1268 batch loss 6.18873453 epoch total loss 7.04774857\n",
      "Trained batch 1269 batch loss 6.43173313 epoch total loss 7.04726267\n",
      "Trained batch 1270 batch loss 6.54358292 epoch total loss 7.04686642\n",
      "Trained batch 1271 batch loss 6.33354712 epoch total loss 7.04630566\n",
      "Trained batch 1272 batch loss 7.14890718 epoch total loss 7.04638577\n",
      "Trained batch 1273 batch loss 6.70934296 epoch total loss 7.04612112\n",
      "Trained batch 1274 batch loss 6.7061429 epoch total loss 7.04585409\n",
      "Trained batch 1275 batch loss 6.62115908 epoch total loss 7.04552078\n",
      "Trained batch 1276 batch loss 6.75834894 epoch total loss 7.04529619\n",
      "Trained batch 1277 batch loss 7.41665316 epoch total loss 7.04558706\n",
      "Trained batch 1278 batch loss 7.43038416 epoch total loss 7.04588842\n",
      "Trained batch 1279 batch loss 7.37949848 epoch total loss 7.04614973\n",
      "Trained batch 1280 batch loss 7.46088028 epoch total loss 7.0464735\n",
      "Trained batch 1281 batch loss 7.52563381 epoch total loss 7.04684734\n",
      "Trained batch 1282 batch loss 7.34234858 epoch total loss 7.04707861\n",
      "Trained batch 1283 batch loss 7.43557596 epoch total loss 7.0473814\n",
      "Trained batch 1284 batch loss 7.084692 epoch total loss 7.04741049\n",
      "Trained batch 1285 batch loss 7.04204178 epoch total loss 7.0474062\n",
      "Trained batch 1286 batch loss 6.90873 epoch total loss 7.04729891\n",
      "Trained batch 1287 batch loss 6.85906506 epoch total loss 7.04715252\n",
      "Trained batch 1288 batch loss 7.20103216 epoch total loss 7.04727221\n",
      "Trained batch 1289 batch loss 6.91380072 epoch total loss 7.04716873\n",
      "Trained batch 1290 batch loss 6.7800889 epoch total loss 7.04696226\n",
      "Trained batch 1291 batch loss 7.15510321 epoch total loss 7.04704618\n",
      "Trained batch 1292 batch loss 6.86550713 epoch total loss 7.04690504\n",
      "Trained batch 1293 batch loss 6.62150192 epoch total loss 7.04657602\n",
      "Trained batch 1294 batch loss 6.92317867 epoch total loss 7.04648\n",
      "Trained batch 1295 batch loss 7.11568642 epoch total loss 7.04653358\n",
      "Trained batch 1296 batch loss 6.64296865 epoch total loss 7.04622173\n",
      "Trained batch 1297 batch loss 7.16255474 epoch total loss 7.0463109\n",
      "Trained batch 1298 batch loss 6.92374706 epoch total loss 7.04621649\n",
      "Trained batch 1299 batch loss 6.48062801 epoch total loss 7.04578114\n",
      "Trained batch 1300 batch loss 6.46290636 epoch total loss 7.04533291\n",
      "Trained batch 1301 batch loss 6.68165159 epoch total loss 7.04505301\n",
      "Trained batch 1302 batch loss 6.81667471 epoch total loss 7.04487753\n",
      "Trained batch 1303 batch loss 6.87370205 epoch total loss 7.0447464\n",
      "Trained batch 1304 batch loss 7.20368385 epoch total loss 7.04486847\n",
      "Trained batch 1305 batch loss 7.3317728 epoch total loss 7.04508877\n",
      "Trained batch 1306 batch loss 7.0351181 epoch total loss 7.04508114\n",
      "Trained batch 1307 batch loss 6.85917425 epoch total loss 7.04493904\n",
      "Trained batch 1308 batch loss 7.31221867 epoch total loss 7.0451436\n",
      "Trained batch 1309 batch loss 6.75704956 epoch total loss 7.04492331\n",
      "Trained batch 1310 batch loss 6.35977507 epoch total loss 7.0444\n",
      "Trained batch 1311 batch loss 7.13183117 epoch total loss 7.04446697\n",
      "Trained batch 1312 batch loss 6.95473289 epoch total loss 7.04439878\n",
      "Trained batch 1313 batch loss 7.49387264 epoch total loss 7.04474115\n",
      "Trained batch 1314 batch loss 7.27381706 epoch total loss 7.0449152\n",
      "Trained batch 1315 batch loss 7.45294762 epoch total loss 7.04522562\n",
      "Trained batch 1316 batch loss 7.42806864 epoch total loss 7.04551649\n",
      "Trained batch 1317 batch loss 7.38788414 epoch total loss 7.04577589\n",
      "Trained batch 1318 batch loss 7.49870586 epoch total loss 7.04612\n",
      "Trained batch 1319 batch loss 7.05220318 epoch total loss 7.04612446\n",
      "Trained batch 1320 batch loss 6.98325825 epoch total loss 7.04607677\n",
      "Trained batch 1321 batch loss 7.11331 epoch total loss 7.0461278\n",
      "Trained batch 1322 batch loss 7.16698933 epoch total loss 7.04621887\n",
      "Trained batch 1323 batch loss 6.58217335 epoch total loss 7.0458684\n",
      "Trained batch 1324 batch loss 6.30470276 epoch total loss 7.04530859\n",
      "Trained batch 1325 batch loss 6.10263443 epoch total loss 7.04459667\n",
      "Trained batch 1326 batch loss 7.11799145 epoch total loss 7.04465246\n",
      "Trained batch 1327 batch loss 7.4861846 epoch total loss 7.04498529\n",
      "Trained batch 1328 batch loss 6.89431381 epoch total loss 7.04487181\n",
      "Trained batch 1329 batch loss 7.10396528 epoch total loss 7.04491615\n",
      "Trained batch 1330 batch loss 6.38392115 epoch total loss 7.04441881\n",
      "Trained batch 1331 batch loss 6.69223356 epoch total loss 7.04415464\n",
      "Trained batch 1332 batch loss 6.99487782 epoch total loss 7.04411745\n",
      "Trained batch 1333 batch loss 7.35936594 epoch total loss 7.04435396\n",
      "Trained batch 1334 batch loss 7.43269157 epoch total loss 7.04464531\n",
      "Trained batch 1335 batch loss 7.0852685 epoch total loss 7.04467535\n",
      "Trained batch 1336 batch loss 7.22274303 epoch total loss 7.04480839\n",
      "Trained batch 1337 batch loss 6.74753523 epoch total loss 7.0445857\n",
      "Trained batch 1338 batch loss 6.9873929 epoch total loss 7.04454327\n",
      "Trained batch 1339 batch loss 6.80160809 epoch total loss 7.04436159\n",
      "Trained batch 1340 batch loss 7.22282839 epoch total loss 7.04449463\n",
      "Trained batch 1341 batch loss 7.3739996 epoch total loss 7.04474068\n",
      "Trained batch 1342 batch loss 7.0551362 epoch total loss 7.04474783\n",
      "Trained batch 1343 batch loss 7.26503134 epoch total loss 7.04491186\n",
      "Trained batch 1344 batch loss 7.18126 epoch total loss 7.04501343\n",
      "Trained batch 1345 batch loss 7.17225647 epoch total loss 7.04510784\n",
      "Trained batch 1346 batch loss 7.54192924 epoch total loss 7.04547691\n",
      "Trained batch 1347 batch loss 7.29265261 epoch total loss 7.0456605\n",
      "Trained batch 1348 batch loss 7.10053682 epoch total loss 7.0457015\n",
      "Trained batch 1349 batch loss 6.90778589 epoch total loss 7.04559946\n",
      "Trained batch 1350 batch loss 6.81868076 epoch total loss 7.04543114\n",
      "Trained batch 1351 batch loss 7.05367184 epoch total loss 7.04543734\n",
      "Trained batch 1352 batch loss 7.12217045 epoch total loss 7.04549408\n",
      "Trained batch 1353 batch loss 7.21242619 epoch total loss 7.04561758\n",
      "Trained batch 1354 batch loss 7.18597555 epoch total loss 7.04572105\n",
      "Trained batch 1355 batch loss 7.24421692 epoch total loss 7.04586744\n",
      "Trained batch 1356 batch loss 6.62828875 epoch total loss 7.04555941\n",
      "Trained batch 1357 batch loss 7.14652872 epoch total loss 7.04563379\n",
      "Trained batch 1358 batch loss 7.13543415 epoch total loss 7.0457\n",
      "Trained batch 1359 batch loss 7.37420368 epoch total loss 7.04594135\n",
      "Trained batch 1360 batch loss 7.34179544 epoch total loss 7.04615927\n",
      "Trained batch 1361 batch loss 7.35635853 epoch total loss 7.0463872\n",
      "Trained batch 1362 batch loss 7.38912392 epoch total loss 7.04663849\n",
      "Trained batch 1363 batch loss 7.22875357 epoch total loss 7.046772\n",
      "Trained batch 1364 batch loss 7.20012045 epoch total loss 7.04688454\n",
      "Trained batch 1365 batch loss 7.35349512 epoch total loss 7.04710913\n",
      "Trained batch 1366 batch loss 7.55990696 epoch total loss 7.04748392\n",
      "Trained batch 1367 batch loss 7.28847694 epoch total loss 7.04766\n",
      "Trained batch 1368 batch loss 7.0596056 epoch total loss 7.04766893\n",
      "Trained batch 1369 batch loss 7.00149345 epoch total loss 7.04763556\n",
      "Trained batch 1370 batch loss 7.54790545 epoch total loss 7.04800034\n",
      "Trained batch 1371 batch loss 7.4637022 epoch total loss 7.04830408\n",
      "Trained batch 1372 batch loss 7.52613449 epoch total loss 7.04865217\n",
      "Trained batch 1373 batch loss 7.04908371 epoch total loss 7.04865265\n",
      "Trained batch 1374 batch loss 7.27419567 epoch total loss 7.04881668\n",
      "Trained batch 1375 batch loss 6.97009468 epoch total loss 7.04875946\n",
      "Trained batch 1376 batch loss 6.16341162 epoch total loss 7.04811573\n",
      "Trained batch 1377 batch loss 7.09815693 epoch total loss 7.04815245\n",
      "Trained batch 1378 batch loss 7.17316866 epoch total loss 7.04824257\n",
      "Trained batch 1379 batch loss 7.16159582 epoch total loss 7.04832458\n",
      "Trained batch 1380 batch loss 6.61106396 epoch total loss 7.04800797\n",
      "Trained batch 1381 batch loss 6.94472647 epoch total loss 7.0479331\n",
      "Trained batch 1382 batch loss 6.81219482 epoch total loss 7.04776239\n",
      "Trained batch 1383 batch loss 7.32343292 epoch total loss 7.04796171\n",
      "Trained batch 1384 batch loss 6.56545877 epoch total loss 7.04761314\n",
      "Trained batch 1385 batch loss 7.05902576 epoch total loss 7.04762077\n",
      "Trained batch 1386 batch loss 6.79767847 epoch total loss 7.04744101\n",
      "Trained batch 1387 batch loss 6.76651764 epoch total loss 7.04723835\n",
      "Trained batch 1388 batch loss 7.3956542 epoch total loss 7.04748917\n",
      "Epoch 2 train loss 7.047489166259766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 03:03:06.411481: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "2025-03-19 03:03:06.411537: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "\t [[RemoteCall]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 1 batch loss 6.45684052\n",
      "Validated batch 2 batch loss 7.49210882\n",
      "Validated batch 3 batch loss 7.23971796\n",
      "Validated batch 4 batch loss 7.22810936\n",
      "Validated batch 5 batch loss 7.12327433\n",
      "Validated batch 6 batch loss 7.57336378\n",
      "Validated batch 7 batch loss 7.00229549\n",
      "Validated batch 8 batch loss 7.45819616\n",
      "Validated batch 9 batch loss 6.88812399\n",
      "Validated batch 10 batch loss 7.15492582\n",
      "Validated batch 11 batch loss 6.96221781\n",
      "Validated batch 12 batch loss 6.42196655\n",
      "Validated batch 13 batch loss 6.72814417\n",
      "Validated batch 14 batch loss 7.53808784\n",
      "Validated batch 15 batch loss 7.30534887\n",
      "Validated batch 16 batch loss 6.7679863\n",
      "Validated batch 17 batch loss 7.43158817\n",
      "Validated batch 18 batch loss 7.38247442\n",
      "Validated batch 19 batch loss 7.34717846\n",
      "Validated batch 20 batch loss 7.53246355\n",
      "Validated batch 21 batch loss 7.45910454\n",
      "Validated batch 22 batch loss 7.0086832\n",
      "Validated batch 23 batch loss 6.79675913\n",
      "Validated batch 24 batch loss 7.28337193\n",
      "Validated batch 25 batch loss 7.28222513\n",
      "Validated batch 26 batch loss 7.24715471\n",
      "Validated batch 27 batch loss 6.4114542\n",
      "Validated batch 28 batch loss 7.03790522\n",
      "Validated batch 29 batch loss 7.09081888\n",
      "Validated batch 30 batch loss 6.56918859\n",
      "Validated batch 31 batch loss 7.00223351\n",
      "Validated batch 32 batch loss 6.80457067\n",
      "Validated batch 33 batch loss 7.14851\n",
      "Validated batch 34 batch loss 6.91650772\n",
      "Validated batch 35 batch loss 6.73417425\n",
      "Validated batch 36 batch loss 6.8629117\n",
      "Validated batch 37 batch loss 6.79121161\n",
      "Validated batch 38 batch loss 7.22087288\n",
      "Validated batch 39 batch loss 7.17568398\n",
      "Validated batch 40 batch loss 7.11405277\n",
      "Validated batch 41 batch loss 7.35600233\n",
      "Validated batch 42 batch loss 7.22310829\n",
      "Validated batch 43 batch loss 6.60118818\n",
      "Validated batch 44 batch loss 7.35630274\n",
      "Validated batch 45 batch loss 6.12708712\n",
      "Validated batch 46 batch loss 7.48831415\n",
      "Validated batch 47 batch loss 7.3971014\n",
      "Validated batch 48 batch loss 7.12754726\n",
      "Validated batch 49 batch loss 7.09753752\n",
      "Validated batch 50 batch loss 6.88961744\n",
      "Validated batch 51 batch loss 6.77439451\n",
      "Validated batch 52 batch loss 7.18766975\n",
      "Validated batch 53 batch loss 6.97576809\n",
      "Validated batch 54 batch loss 7.29692841\n",
      "Validated batch 55 batch loss 7.33310366\n",
      "Validated batch 56 batch loss 7.07669401\n",
      "Validated batch 57 batch loss 7.10843325\n",
      "Validated batch 58 batch loss 6.6393342\n",
      "Validated batch 59 batch loss 7.08853054\n",
      "Validated batch 60 batch loss 7.03552103\n",
      "Validated batch 61 batch loss 7.54522133\n",
      "Validated batch 62 batch loss 7.11631966\n",
      "Validated batch 63 batch loss 7.04812098\n",
      "Validated batch 64 batch loss 6.47428083\n",
      "Validated batch 65 batch loss 6.87078238\n",
      "Validated batch 66 batch loss 7.23861217\n",
      "Validated batch 67 batch loss 6.78224945\n",
      "Validated batch 68 batch loss 6.91508102\n",
      "Validated batch 69 batch loss 6.97755575\n",
      "Validated batch 70 batch loss 7.06193399\n",
      "Validated batch 71 batch loss 7.08448315\n",
      "Validated batch 72 batch loss 6.9318\n",
      "Validated batch 73 batch loss 7.41472292\n",
      "Validated batch 74 batch loss 7.65213394\n",
      "Validated batch 75 batch loss 7.46247149\n",
      "Validated batch 76 batch loss 7.35301304\n",
      "Validated batch 77 batch loss 7.18662643\n",
      "Validated batch 78 batch loss 7.3494525\n",
      "Validated batch 79 batch loss 7.42953253\n",
      "Validated batch 80 batch loss 7.56871271\n",
      "Validated batch 81 batch loss 7.32026339\n",
      "Validated batch 82 batch loss 6.90787029\n",
      "Validated batch 83 batch loss 7.38894129\n",
      "Validated batch 84 batch loss 7.12583303\n",
      "Validated batch 85 batch loss 6.99290657\n",
      "Validated batch 86 batch loss 7.2901845\n",
      "Validated batch 87 batch loss 6.42762756\n",
      "Validated batch 88 batch loss 6.78485489\n",
      "Validated batch 89 batch loss 7.55946398\n",
      "Validated batch 90 batch loss 7.08599949\n",
      "Validated batch 91 batch loss 7.45977449\n",
      "Validated batch 92 batch loss 7.20337868\n",
      "Validated batch 93 batch loss 7.44474125\n",
      "Validated batch 94 batch loss 6.99629879\n",
      "Validated batch 95 batch loss 6.88958693\n",
      "Validated batch 96 batch loss 6.84916258\n",
      "Validated batch 97 batch loss 7.12133312\n",
      "Validated batch 98 batch loss 7.2749691\n",
      "Validated batch 99 batch loss 7.02336836\n",
      "Validated batch 100 batch loss 7.39212751\n",
      "Validated batch 101 batch loss 7.05312729\n",
      "Validated batch 102 batch loss 7.50752306\n",
      "Validated batch 103 batch loss 7.55104828\n",
      "Validated batch 104 batch loss 7.18441\n",
      "Validated batch 105 batch loss 7.15829945\n",
      "Validated batch 106 batch loss 7.02781725\n",
      "Validated batch 107 batch loss 7.52835894\n",
      "Validated batch 108 batch loss 7.02250624\n",
      "Validated batch 109 batch loss 7.38007212\n",
      "Validated batch 110 batch loss 6.89327908\n",
      "Validated batch 111 batch loss 7.34241629\n",
      "Validated batch 112 batch loss 7.04909945\n",
      "Validated batch 113 batch loss 7.18060207\n",
      "Validated batch 114 batch loss 7.04757166\n",
      "Validated batch 115 batch loss 7.20692921\n",
      "Validated batch 116 batch loss 7.57262754\n",
      "Validated batch 117 batch loss 7.18919563\n",
      "Validated batch 118 batch loss 7.31841946\n",
      "Validated batch 119 batch loss 7.21003389\n",
      "Validated batch 120 batch loss 7.33538771\n",
      "Validated batch 121 batch loss 7.48197699\n",
      "Validated batch 122 batch loss 7.07540035\n",
      "Validated batch 123 batch loss 6.9766326\n",
      "Validated batch 124 batch loss 6.90614605\n",
      "Validated batch 125 batch loss 7.02799416\n",
      "Validated batch 126 batch loss 7.37350368\n",
      "Validated batch 127 batch loss 7.16332483\n",
      "Validated batch 128 batch loss 6.88332033\n",
      "Validated batch 129 batch loss 6.80623722\n",
      "Validated batch 130 batch loss 7.01296616\n",
      "Validated batch 131 batch loss 7.38165951\n",
      "Validated batch 132 batch loss 7.54970551\n",
      "Validated batch 133 batch loss 7.29349613\n",
      "Validated batch 134 batch loss 7.57897472\n",
      "Validated batch 135 batch loss 7.6377039\n",
      "Validated batch 136 batch loss 7.45535898\n",
      "Validated batch 137 batch loss 7.0868268\n",
      "Validated batch 138 batch loss 6.78313637\n",
      "Validated batch 139 batch loss 6.74352026\n",
      "Validated batch 140 batch loss 6.86923647\n",
      "Validated batch 141 batch loss 7.23543739\n",
      "Validated batch 142 batch loss 7.39604187\n",
      "Validated batch 143 batch loss 6.70296097\n",
      "Validated batch 144 batch loss 7.25604391\n",
      "Validated batch 145 batch loss 7.17111063\n",
      "Validated batch 146 batch loss 7.27261829\n",
      "Validated batch 147 batch loss 7.09510183\n",
      "Validated batch 148 batch loss 7.21619177\n",
      "Validated batch 149 batch loss 7.16861725\n",
      "Validated batch 150 batch loss 6.97946024\n",
      "Validated batch 151 batch loss 7.22120047\n",
      "Validated batch 152 batch loss 7.17475367\n",
      "Validated batch 153 batch loss 7.43539476\n",
      "Validated batch 154 batch loss 7.28796148\n",
      "Validated batch 155 batch loss 7.02312708\n",
      "Validated batch 156 batch loss 6.74579287\n",
      "Validated batch 157 batch loss 7.3423295\n",
      "Validated batch 158 batch loss 7.5432272\n",
      "Validated batch 159 batch loss 7.41498804\n",
      "Validated batch 160 batch loss 7.46595716\n",
      "Validated batch 161 batch loss 7.09082079\n",
      "Validated batch 162 batch loss 7.00868511\n",
      "Validated batch 163 batch loss 7.07182217\n",
      "Validated batch 164 batch loss 6.85697937\n",
      "Validated batch 165 batch loss 6.32372618\n",
      "Validated batch 166 batch loss 7.07520247\n",
      "Validated batch 167 batch loss 7.24470901\n",
      "Validated batch 168 batch loss 6.63455677\n",
      "Validated batch 169 batch loss 7.18898344\n",
      "Validated batch 170 batch loss 7.29527903\n",
      "Validated batch 171 batch loss 7.29666471\n",
      "Validated batch 172 batch loss 7.45648766\n",
      "Validated batch 173 batch loss 7.12247849\n",
      "Validated batch 174 batch loss 6.78034449\n",
      "Validated batch 175 batch loss 7.25098324\n",
      "Validated batch 176 batch loss 7.33230972\n",
      "Validated batch 177 batch loss 7.4535327\n",
      "Validated batch 178 batch loss 7.25981283\n",
      "Validated batch 179 batch loss 7.19155\n",
      "Validated batch 180 batch loss 7.5041337\n",
      "Validated batch 181 batch loss 7.0952673\n",
      "Validated batch 182 batch loss 7.4718442\n",
      "Validated batch 183 batch loss 7.2006216\n",
      "Validated batch 184 batch loss 7.42871284\n",
      "Validated batch 185 batch loss 3.76669335\n",
      "Epoch 2 val loss 7.121998310089111\n",
      "Start epoch 3 with learning rate 0.001\n",
      "Start distributed training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 03:03:14.988388: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "2025-03-19 03:03:14.988442: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "\t [[RemoteCall]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1 batch loss 7.12349844 epoch total loss 7.12349844\n",
      "Trained batch 2 batch loss 7.29281378 epoch total loss 7.20815611\n",
      "Trained batch 3 batch loss 7.43505192 epoch total loss 7.2837882\n",
      "Trained batch 4 batch loss 7.48824596 epoch total loss 7.33490276\n",
      "Trained batch 5 batch loss 7.35794449 epoch total loss 7.33951092\n",
      "Trained batch 6 batch loss 7.30259132 epoch total loss 7.33335733\n",
      "Trained batch 7 batch loss 7.37479258 epoch total loss 7.33927679\n",
      "Trained batch 8 batch loss 7.0938139 epoch total loss 7.30859423\n",
      "Trained batch 9 batch loss 7.50137 epoch total loss 7.33001375\n",
      "Trained batch 10 batch loss 7.43638802 epoch total loss 7.34065104\n",
      "Trained batch 11 batch loss 7.36934042 epoch total loss 7.34325886\n",
      "Trained batch 12 batch loss 6.66353703 epoch total loss 7.28661537\n",
      "Trained batch 13 batch loss 6.38757563 epoch total loss 7.21745825\n",
      "Trained batch 14 batch loss 5.81014681 epoch total loss 7.11693621\n",
      "Trained batch 15 batch loss 6.69264507 epoch total loss 7.08865\n",
      "Trained batch 16 batch loss 6.79097843 epoch total loss 7.07004547\n",
      "Trained batch 17 batch loss 6.52564049 epoch total loss 7.03802156\n",
      "Trained batch 18 batch loss 6.61508036 epoch total loss 7.01452494\n",
      "Trained batch 19 batch loss 6.45497322 epoch total loss 6.98507547\n",
      "Trained batch 20 batch loss 6.91626 epoch total loss 6.98163462\n",
      "Trained batch 21 batch loss 6.14911222 epoch total loss 6.94199038\n",
      "Trained batch 22 batch loss 7.22692537 epoch total loss 6.95494223\n",
      "Trained batch 23 batch loss 7.20056343 epoch total loss 6.96562147\n",
      "Trained batch 24 batch loss 7.42248821 epoch total loss 6.98465729\n",
      "Trained batch 25 batch loss 7.32700682 epoch total loss 6.99835157\n",
      "Trained batch 26 batch loss 6.87003183 epoch total loss 6.99341583\n",
      "Trained batch 27 batch loss 6.77643061 epoch total loss 6.98537922\n",
      "Trained batch 28 batch loss 6.9708972 epoch total loss 6.98486233\n",
      "Trained batch 29 batch loss 7.0378418 epoch total loss 6.98668909\n",
      "Trained batch 30 batch loss 7.20027924 epoch total loss 6.99380922\n",
      "Trained batch 31 batch loss 7.33924818 epoch total loss 7.00495243\n",
      "Trained batch 32 batch loss 6.94669104 epoch total loss 7.00313139\n",
      "Trained batch 33 batch loss 6.67763758 epoch total loss 6.99326801\n",
      "Trained batch 34 batch loss 7.0938983 epoch total loss 6.99622774\n",
      "Trained batch 35 batch loss 7.28512192 epoch total loss 7.00448227\n",
      "Trained batch 36 batch loss 7.45091057 epoch total loss 7.0168829\n",
      "Trained batch 37 batch loss 7.3846221 epoch total loss 7.02682161\n",
      "Trained batch 38 batch loss 7.29384279 epoch total loss 7.03384876\n",
      "Trained batch 39 batch loss 7.34375858 epoch total loss 7.04179478\n",
      "Trained batch 40 batch loss 7.23068523 epoch total loss 7.04651737\n",
      "Trained batch 41 batch loss 7.37900496 epoch total loss 7.05462646\n",
      "Trained batch 42 batch loss 7.26358557 epoch total loss 7.05960178\n",
      "Trained batch 43 batch loss 7.37057781 epoch total loss 7.0668335\n",
      "Trained batch 44 batch loss 7.39960623 epoch total loss 7.07439613\n",
      "Trained batch 45 batch loss 6.9063158 epoch total loss 7.07066107\n",
      "Trained batch 46 batch loss 6.58684731 epoch total loss 7.06014347\n",
      "Trained batch 47 batch loss 6.3952713 epoch total loss 7.04599714\n",
      "Trained batch 48 batch loss 6.46211767 epoch total loss 7.03383303\n",
      "Trained batch 49 batch loss 6.80575085 epoch total loss 7.02917862\n",
      "Trained batch 50 batch loss 7.10543108 epoch total loss 7.03070354\n",
      "Trained batch 51 batch loss 7.44269657 epoch total loss 7.03878164\n",
      "Trained batch 52 batch loss 7.53199482 epoch total loss 7.04826641\n",
      "Trained batch 53 batch loss 7.47388792 epoch total loss 7.05629683\n",
      "Trained batch 54 batch loss 7.43056059 epoch total loss 7.06322813\n",
      "Trained batch 55 batch loss 7.2383852 epoch total loss 7.06641245\n",
      "Trained batch 56 batch loss 7.16074705 epoch total loss 7.06809664\n",
      "Trained batch 57 batch loss 7.27584648 epoch total loss 7.07174158\n",
      "Trained batch 58 batch loss 7.08055115 epoch total loss 7.07189322\n",
      "Trained batch 59 batch loss 7.06402302 epoch total loss 7.0717597\n",
      "Trained batch 60 batch loss 7.36315155 epoch total loss 7.07661629\n",
      "Trained batch 61 batch loss 7.38650703 epoch total loss 7.08169651\n",
      "Trained batch 62 batch loss 7.06180429 epoch total loss 7.0813756\n",
      "Trained batch 63 batch loss 7.3564682 epoch total loss 7.08574247\n",
      "Trained batch 64 batch loss 7.25552607 epoch total loss 7.08839512\n",
      "Trained batch 65 batch loss 7.22670412 epoch total loss 7.09052324\n",
      "Trained batch 66 batch loss 7.13308954 epoch total loss 7.09116793\n",
      "Trained batch 67 batch loss 7.04401493 epoch total loss 7.09046412\n",
      "Trained batch 68 batch loss 7.34550238 epoch total loss 7.09421444\n",
      "Trained batch 69 batch loss 7.08724737 epoch total loss 7.09411335\n",
      "Trained batch 70 batch loss 7.09421968 epoch total loss 7.09411478\n",
      "Trained batch 71 batch loss 7.05602169 epoch total loss 7.09357834\n",
      "Trained batch 72 batch loss 7.18987417 epoch total loss 7.09491587\n",
      "Trained batch 73 batch loss 6.82944298 epoch total loss 7.09127951\n",
      "Trained batch 74 batch loss 6.41397429 epoch total loss 7.08212709\n",
      "Trained batch 75 batch loss 7.06834126 epoch total loss 7.08194351\n",
      "Trained batch 76 batch loss 7.37994337 epoch total loss 7.08586454\n",
      "Trained batch 77 batch loss 7.33738708 epoch total loss 7.08913136\n",
      "Trained batch 78 batch loss 7.05089045 epoch total loss 7.08864117\n",
      "Trained batch 79 batch loss 7.3468647 epoch total loss 7.09191\n",
      "Trained batch 80 batch loss 7.12611866 epoch total loss 7.09233713\n",
      "Trained batch 81 batch loss 7.06093 epoch total loss 7.09194899\n",
      "Trained batch 82 batch loss 7.09341717 epoch total loss 7.09196711\n",
      "Trained batch 83 batch loss 7.27432775 epoch total loss 7.09416485\n",
      "Trained batch 84 batch loss 6.93630266 epoch total loss 7.09228516\n",
      "Trained batch 85 batch loss 6.99976254 epoch total loss 7.09119654\n",
      "Trained batch 86 batch loss 6.7845664 epoch total loss 7.08763075\n",
      "Trained batch 87 batch loss 7.0160079 epoch total loss 7.08680725\n",
      "Trained batch 88 batch loss 7.36631536 epoch total loss 7.08998394\n",
      "Trained batch 89 batch loss 7.46404934 epoch total loss 7.09418678\n",
      "Trained batch 90 batch loss 7.38557434 epoch total loss 7.09742451\n",
      "Trained batch 91 batch loss 7.3707962 epoch total loss 7.1004281\n",
      "Trained batch 92 batch loss 7.21340132 epoch total loss 7.10165596\n",
      "Trained batch 93 batch loss 7.54517031 epoch total loss 7.10642481\n",
      "Trained batch 94 batch loss 7.56445742 epoch total loss 7.11129761\n",
      "Trained batch 95 batch loss 6.73103619 epoch total loss 7.10729456\n",
      "Trained batch 96 batch loss 7.20537138 epoch total loss 7.10831642\n",
      "Trained batch 97 batch loss 7.5127039 epoch total loss 7.11248541\n",
      "Trained batch 98 batch loss 7.08977461 epoch total loss 7.11225367\n",
      "Trained batch 99 batch loss 7.11439896 epoch total loss 7.11227512\n",
      "Trained batch 100 batch loss 7.55045748 epoch total loss 7.11665726\n",
      "Trained batch 101 batch loss 7.07221174 epoch total loss 7.11621714\n",
      "Trained batch 102 batch loss 7.58444738 epoch total loss 7.12080765\n",
      "Trained batch 103 batch loss 7.18274403 epoch total loss 7.12140894\n",
      "Trained batch 104 batch loss 6.65041161 epoch total loss 7.11688\n",
      "Trained batch 105 batch loss 7.26794434 epoch total loss 7.11831856\n",
      "Trained batch 106 batch loss 7.1402669 epoch total loss 7.11852551\n",
      "Trained batch 107 batch loss 6.92792559 epoch total loss 7.11674452\n",
      "Trained batch 108 batch loss 7.39561 epoch total loss 7.11932659\n",
      "Trained batch 109 batch loss 7.11624241 epoch total loss 7.11929846\n",
      "Trained batch 110 batch loss 7.0043335 epoch total loss 7.11825323\n",
      "Trained batch 111 batch loss 6.40978622 epoch total loss 7.11187077\n",
      "Trained batch 112 batch loss 7.1059556 epoch total loss 7.11181784\n",
      "Trained batch 113 batch loss 6.51276541 epoch total loss 7.10651684\n",
      "Trained batch 114 batch loss 6.41420698 epoch total loss 7.10044336\n",
      "Trained batch 115 batch loss 6.33699751 epoch total loss 7.09380484\n",
      "Trained batch 116 batch loss 6.13441849 epoch total loss 7.0855341\n",
      "Trained batch 117 batch loss 6.2296567 epoch total loss 7.07821894\n",
      "Trained batch 118 batch loss 7.04888868 epoch total loss 7.0779705\n",
      "Trained batch 119 batch loss 7.16318035 epoch total loss 7.07868671\n",
      "Trained batch 120 batch loss 7.18405485 epoch total loss 7.07956505\n",
      "Trained batch 121 batch loss 6.94504881 epoch total loss 7.07845354\n",
      "Trained batch 122 batch loss 6.42528582 epoch total loss 7.07309961\n",
      "Trained batch 123 batch loss 6.90996933 epoch total loss 7.07177353\n",
      "Trained batch 124 batch loss 7.09795952 epoch total loss 7.07198477\n",
      "Trained batch 125 batch loss 7.05797434 epoch total loss 7.07187271\n",
      "Trained batch 126 batch loss 6.98083496 epoch total loss 7.07115\n",
      "Trained batch 127 batch loss 6.88612843 epoch total loss 7.06969309\n",
      "Trained batch 128 batch loss 7.18115425 epoch total loss 7.07056379\n",
      "Trained batch 129 batch loss 7.29037094 epoch total loss 7.07226753\n",
      "Trained batch 130 batch loss 7.04652166 epoch total loss 7.07206917\n",
      "Trained batch 131 batch loss 7.41035843 epoch total loss 7.07465172\n",
      "Trained batch 132 batch loss 7.27590466 epoch total loss 7.07617617\n",
      "Trained batch 133 batch loss 7.27163601 epoch total loss 7.0776453\n",
      "Trained batch 134 batch loss 7.38125944 epoch total loss 7.07991123\n",
      "Trained batch 135 batch loss 7.35891056 epoch total loss 7.08197784\n",
      "Trained batch 136 batch loss 7.22434092 epoch total loss 7.08302498\n",
      "Trained batch 137 batch loss 7.01433611 epoch total loss 7.08252335\n",
      "Trained batch 138 batch loss 7.47566462 epoch total loss 7.08537245\n",
      "Trained batch 139 batch loss 7.50756645 epoch total loss 7.08840942\n",
      "Trained batch 140 batch loss 7.23189592 epoch total loss 7.08943415\n",
      "Trained batch 141 batch loss 7.11912727 epoch total loss 7.08964491\n",
      "Trained batch 142 batch loss 6.64899683 epoch total loss 7.08654165\n",
      "Trained batch 143 batch loss 6.54729128 epoch total loss 7.08277082\n",
      "Trained batch 144 batch loss 7.32239771 epoch total loss 7.08443499\n",
      "Trained batch 145 batch loss 7.37044811 epoch total loss 7.08640718\n",
      "Trained batch 146 batch loss 7.20728874 epoch total loss 7.08723497\n",
      "Trained batch 147 batch loss 6.9068 epoch total loss 7.08600807\n",
      "Trained batch 148 batch loss 7.17876339 epoch total loss 7.08663464\n",
      "Trained batch 149 batch loss 7.37691116 epoch total loss 7.08858299\n",
      "Trained batch 150 batch loss 7.23534441 epoch total loss 7.08956146\n",
      "Trained batch 151 batch loss 7.15730476 epoch total loss 7.09001\n",
      "Trained batch 152 batch loss 7.35717678 epoch total loss 7.09176779\n",
      "Trained batch 153 batch loss 7.5471158 epoch total loss 7.09474421\n",
      "Trained batch 154 batch loss 7.23532104 epoch total loss 7.09565735\n",
      "Trained batch 155 batch loss 7.22365284 epoch total loss 7.09648275\n",
      "Trained batch 156 batch loss 6.93365383 epoch total loss 7.09543848\n",
      "Trained batch 157 batch loss 7.42086172 epoch total loss 7.09751177\n",
      "Trained batch 158 batch loss 7.22313213 epoch total loss 7.09830666\n",
      "Trained batch 159 batch loss 7.09235716 epoch total loss 7.09826946\n",
      "Trained batch 160 batch loss 7.13537 epoch total loss 7.09850168\n",
      "Trained batch 161 batch loss 7.38464451 epoch total loss 7.10027885\n",
      "Trained batch 162 batch loss 6.59235573 epoch total loss 7.09714365\n",
      "Trained batch 163 batch loss 7.3349576 epoch total loss 7.09860277\n",
      "Trained batch 164 batch loss 7.18033409 epoch total loss 7.09910107\n",
      "Trained batch 165 batch loss 7.43813753 epoch total loss 7.10115576\n",
      "Trained batch 166 batch loss 7.20600033 epoch total loss 7.10178757\n",
      "Trained batch 167 batch loss 7.48868465 epoch total loss 7.10410404\n",
      "Trained batch 168 batch loss 6.65357161 epoch total loss 7.10142231\n",
      "Trained batch 169 batch loss 6.26665735 epoch total loss 7.09648228\n",
      "Trained batch 170 batch loss 6.31336403 epoch total loss 7.09187603\n",
      "Trained batch 171 batch loss 7.17222261 epoch total loss 7.09234571\n",
      "Trained batch 172 batch loss 7.33724117 epoch total loss 7.09377\n",
      "Trained batch 173 batch loss 7.23078346 epoch total loss 7.09456205\n",
      "Trained batch 174 batch loss 7.38573885 epoch total loss 7.09623575\n",
      "Trained batch 175 batch loss 7.26681709 epoch total loss 7.09721041\n",
      "Trained batch 176 batch loss 7.30229282 epoch total loss 7.09837532\n",
      "Trained batch 177 batch loss 7.36751461 epoch total loss 7.09989643\n",
      "Trained batch 178 batch loss 7.08543587 epoch total loss 7.09981489\n",
      "Trained batch 179 batch loss 7.13584 epoch total loss 7.10001659\n",
      "Trained batch 180 batch loss 6.94189262 epoch total loss 7.09913826\n",
      "Trained batch 181 batch loss 7.330688 epoch total loss 7.10041714\n",
      "Trained batch 182 batch loss 7.03616381 epoch total loss 7.10006428\n",
      "Trained batch 183 batch loss 7.3743825 epoch total loss 7.10156298\n",
      "Trained batch 184 batch loss 7.31534863 epoch total loss 7.10272503\n",
      "Trained batch 185 batch loss 7.19825172 epoch total loss 7.10324097\n",
      "Trained batch 186 batch loss 7.32284355 epoch total loss 7.10442209\n",
      "Trained batch 187 batch loss 7.24964285 epoch total loss 7.10519838\n",
      "Trained batch 188 batch loss 7.19798279 epoch total loss 7.10569191\n",
      "Trained batch 189 batch loss 7.02803802 epoch total loss 7.10528135\n",
      "Trained batch 190 batch loss 6.89375591 epoch total loss 7.10416842\n",
      "Trained batch 191 batch loss 6.85598421 epoch total loss 7.10286903\n",
      "Trained batch 192 batch loss 6.99326086 epoch total loss 7.10229826\n",
      "Trained batch 193 batch loss 6.89570951 epoch total loss 7.10122776\n",
      "Trained batch 194 batch loss 6.4118681 epoch total loss 7.09767437\n",
      "Trained batch 195 batch loss 6.29875898 epoch total loss 7.09357738\n",
      "Trained batch 196 batch loss 6.77482271 epoch total loss 7.09195089\n",
      "Trained batch 197 batch loss 6.90768 epoch total loss 7.09101534\n",
      "Trained batch 198 batch loss 7.03072739 epoch total loss 7.09071112\n",
      "Trained batch 199 batch loss 7.21361685 epoch total loss 7.09132862\n",
      "Trained batch 200 batch loss 7.2318821 epoch total loss 7.09203196\n",
      "Trained batch 201 batch loss 7.18496799 epoch total loss 7.09249401\n",
      "Trained batch 202 batch loss 7.29671049 epoch total loss 7.09350538\n",
      "Trained batch 203 batch loss 7.28507185 epoch total loss 7.09444857\n",
      "Trained batch 204 batch loss 7.11792374 epoch total loss 7.09456396\n",
      "Trained batch 205 batch loss 7.43027449 epoch total loss 7.09620142\n",
      "Trained batch 206 batch loss 7.52985764 epoch total loss 7.09830713\n",
      "Trained batch 207 batch loss 7.15523624 epoch total loss 7.09858227\n",
      "Trained batch 208 batch loss 6.79914045 epoch total loss 7.0971427\n",
      "Trained batch 209 batch loss 7.20836782 epoch total loss 7.09767485\n",
      "Trained batch 210 batch loss 7.19924068 epoch total loss 7.09815836\n",
      "Trained batch 211 batch loss 7.50856256 epoch total loss 7.10010338\n",
      "Trained batch 212 batch loss 7.46725225 epoch total loss 7.10183525\n",
      "Trained batch 213 batch loss 7.60555935 epoch total loss 7.10420036\n",
      "Trained batch 214 batch loss 7.41489697 epoch total loss 7.10565233\n",
      "Trained batch 215 batch loss 7.52999783 epoch total loss 7.10762644\n",
      "Trained batch 216 batch loss 7.03697729 epoch total loss 7.10729933\n",
      "Trained batch 217 batch loss 7.26569128 epoch total loss 7.10802937\n",
      "Trained batch 218 batch loss 7.36948299 epoch total loss 7.10922909\n",
      "Trained batch 219 batch loss 7.17268276 epoch total loss 7.109519\n",
      "Trained batch 220 batch loss 7.08670521 epoch total loss 7.10941505\n",
      "Trained batch 221 batch loss 7.38715696 epoch total loss 7.110672\n",
      "Trained batch 222 batch loss 7.06194973 epoch total loss 7.11045218\n",
      "Trained batch 223 batch loss 7.09373903 epoch total loss 7.11037731\n",
      "Trained batch 224 batch loss 7.22862816 epoch total loss 7.11090517\n",
      "Trained batch 225 batch loss 6.80578423 epoch total loss 7.10954905\n",
      "Trained batch 226 batch loss 7.19402456 epoch total loss 7.10992289\n",
      "Trained batch 227 batch loss 7.00723076 epoch total loss 7.10947037\n",
      "Trained batch 228 batch loss 7.07880592 epoch total loss 7.1093359\n",
      "Trained batch 229 batch loss 7.31179237 epoch total loss 7.11022\n",
      "Trained batch 230 batch loss 7.35111856 epoch total loss 7.11126709\n",
      "Trained batch 231 batch loss 7.30033159 epoch total loss 7.11208534\n",
      "Trained batch 232 batch loss 7.18697548 epoch total loss 7.11240816\n",
      "Trained batch 233 batch loss 7.40367126 epoch total loss 7.11365843\n",
      "Trained batch 234 batch loss 7.24008942 epoch total loss 7.11419868\n",
      "Trained batch 235 batch loss 6.722857 epoch total loss 7.11253357\n",
      "Trained batch 236 batch loss 6.9528656 epoch total loss 7.11185741\n",
      "Trained batch 237 batch loss 6.53796911 epoch total loss 7.10943556\n",
      "Trained batch 238 batch loss 6.9450078 epoch total loss 7.1087451\n",
      "Trained batch 239 batch loss 6.63979 epoch total loss 7.10678291\n",
      "Trained batch 240 batch loss 6.70726204 epoch total loss 7.10511827\n",
      "Trained batch 241 batch loss 6.94484043 epoch total loss 7.10445309\n",
      "Trained batch 242 batch loss 7.14669609 epoch total loss 7.10462809\n",
      "Trained batch 243 batch loss 7.20103884 epoch total loss 7.10502481\n",
      "Trained batch 244 batch loss 6.84139776 epoch total loss 7.1039443\n",
      "Trained batch 245 batch loss 7.18238592 epoch total loss 7.10426426\n",
      "Trained batch 246 batch loss 7.00272322 epoch total loss 7.1038518\n",
      "Trained batch 247 batch loss 7.02602 epoch total loss 7.10353661\n",
      "Trained batch 248 batch loss 7.29886532 epoch total loss 7.10432386\n",
      "Trained batch 249 batch loss 7.1517725 epoch total loss 7.10451412\n",
      "Trained batch 250 batch loss 7.19140625 epoch total loss 7.10486174\n",
      "Trained batch 251 batch loss 7.11463881 epoch total loss 7.10490084\n",
      "Trained batch 252 batch loss 6.39322662 epoch total loss 7.10207653\n",
      "Trained batch 253 batch loss 6.4764 epoch total loss 7.09960365\n",
      "Trained batch 254 batch loss 6.19962072 epoch total loss 7.09606028\n",
      "Trained batch 255 batch loss 6.53923416 epoch total loss 7.09387636\n",
      "Trained batch 256 batch loss 6.55634451 epoch total loss 7.09177685\n",
      "Trained batch 257 batch loss 6.98908043 epoch total loss 7.09137726\n",
      "Trained batch 258 batch loss 7.2240653 epoch total loss 7.09189177\n",
      "Trained batch 259 batch loss 7.37648106 epoch total loss 7.09299088\n",
      "Trained batch 260 batch loss 6.96274376 epoch total loss 7.09248972\n",
      "Trained batch 261 batch loss 7.372684 epoch total loss 7.09356356\n",
      "Trained batch 262 batch loss 7.39559412 epoch total loss 7.09471655\n",
      "Trained batch 263 batch loss 7.3606987 epoch total loss 7.09572792\n",
      "Trained batch 264 batch loss 7.39595604 epoch total loss 7.09686518\n",
      "Trained batch 265 batch loss 7.16248798 epoch total loss 7.09711266\n",
      "Trained batch 266 batch loss 7.40180349 epoch total loss 7.0982585\n",
      "Trained batch 267 batch loss 7.37533522 epoch total loss 7.09929609\n",
      "Trained batch 268 batch loss 7.45236444 epoch total loss 7.10061359\n",
      "Trained batch 269 batch loss 7.33925056 epoch total loss 7.10150099\n",
      "Trained batch 270 batch loss 7.45444679 epoch total loss 7.102808\n",
      "Trained batch 271 batch loss 7.38551092 epoch total loss 7.10385132\n",
      "Trained batch 272 batch loss 7.38536024 epoch total loss 7.10488605\n",
      "Trained batch 273 batch loss 7.26752853 epoch total loss 7.1054821\n",
      "Trained batch 274 batch loss 7.18532658 epoch total loss 7.10577345\n",
      "Trained batch 275 batch loss 7.40081453 epoch total loss 7.10684633\n",
      "Trained batch 276 batch loss 6.99424744 epoch total loss 7.10643816\n",
      "Trained batch 277 batch loss 6.95817375 epoch total loss 7.10590267\n",
      "Trained batch 278 batch loss 7.30589437 epoch total loss 7.10662222\n",
      "Trained batch 279 batch loss 7.21559525 epoch total loss 7.10701275\n",
      "Trained batch 280 batch loss 7.08499908 epoch total loss 7.10693407\n",
      "Trained batch 281 batch loss 7.01366472 epoch total loss 7.10660219\n",
      "Trained batch 282 batch loss 6.87667894 epoch total loss 7.1057868\n",
      "Trained batch 283 batch loss 6.89243793 epoch total loss 7.10503292\n",
      "Trained batch 284 batch loss 7.42973948 epoch total loss 7.10617638\n",
      "Trained batch 285 batch loss 7.43677139 epoch total loss 7.10733604\n",
      "Trained batch 286 batch loss 6.89307213 epoch total loss 7.10658693\n",
      "Trained batch 287 batch loss 6.34956074 epoch total loss 7.10394955\n",
      "Trained batch 288 batch loss 6.25834322 epoch total loss 7.10101318\n",
      "Trained batch 289 batch loss 7.19782925 epoch total loss 7.10134792\n",
      "Trained batch 290 batch loss 7.23658895 epoch total loss 7.10181427\n",
      "Trained batch 291 batch loss 7.29808521 epoch total loss 7.10248852\n",
      "Trained batch 292 batch loss 7.50464392 epoch total loss 7.1038661\n",
      "Trained batch 293 batch loss 7.40481472 epoch total loss 7.10489321\n",
      "Trained batch 294 batch loss 7.48598957 epoch total loss 7.10618973\n",
      "Trained batch 295 batch loss 7.49060917 epoch total loss 7.10749292\n",
      "Trained batch 296 batch loss 7.51388025 epoch total loss 7.10886621\n",
      "Trained batch 297 batch loss 7.20768356 epoch total loss 7.10919905\n",
      "Trained batch 298 batch loss 7.28701448 epoch total loss 7.10979605\n",
      "Trained batch 299 batch loss 7.06327772 epoch total loss 7.1096406\n",
      "Trained batch 300 batch loss 7.08881712 epoch total loss 7.10957098\n",
      "Trained batch 301 batch loss 7.50354767 epoch total loss 7.11088037\n",
      "Trained batch 302 batch loss 7.41660595 epoch total loss 7.11189222\n",
      "Trained batch 303 batch loss 7.10779476 epoch total loss 7.11187935\n",
      "Trained batch 304 batch loss 7.19750452 epoch total loss 7.11216116\n",
      "Trained batch 305 batch loss 7.11272335 epoch total loss 7.11216307\n",
      "Trained batch 306 batch loss 6.97185469 epoch total loss 7.11170483\n",
      "Trained batch 307 batch loss 6.86613941 epoch total loss 7.11090517\n",
      "Trained batch 308 batch loss 6.50626421 epoch total loss 7.10894203\n",
      "Trained batch 309 batch loss 6.62786055 epoch total loss 7.10738564\n",
      "Trained batch 310 batch loss 6.84602118 epoch total loss 7.10654211\n",
      "Trained batch 311 batch loss 6.51984 epoch total loss 7.10465527\n",
      "Trained batch 312 batch loss 5.97349 epoch total loss 7.1010294\n",
      "Trained batch 313 batch loss 6.06238604 epoch total loss 7.09771156\n",
      "Trained batch 314 batch loss 6.35382462 epoch total loss 7.09534216\n",
      "Trained batch 315 batch loss 6.43638659 epoch total loss 7.09325027\n",
      "Trained batch 316 batch loss 6.2864027 epoch total loss 7.09069681\n",
      "Trained batch 317 batch loss 6.76529312 epoch total loss 7.08967066\n",
      "Trained batch 318 batch loss 6.62628937 epoch total loss 7.08821297\n",
      "Trained batch 319 batch loss 7.28704453 epoch total loss 7.08883667\n",
      "Trained batch 320 batch loss 6.57900429 epoch total loss 7.08724356\n",
      "Trained batch 321 batch loss 6.7312727 epoch total loss 7.08613443\n",
      "Trained batch 322 batch loss 7.00381 epoch total loss 7.08587933\n",
      "Trained batch 323 batch loss 7.03595448 epoch total loss 7.08572435\n",
      "Trained batch 324 batch loss 7.37234974 epoch total loss 7.08660889\n",
      "Trained batch 325 batch loss 7.05641222 epoch total loss 7.0865159\n",
      "Trained batch 326 batch loss 7.01178265 epoch total loss 7.08628654\n",
      "Trained batch 327 batch loss 7.36050749 epoch total loss 7.0871253\n",
      "Trained batch 328 batch loss 7.08883429 epoch total loss 7.08713055\n",
      "Trained batch 329 batch loss 7.46340799 epoch total loss 7.08827448\n",
      "Trained batch 330 batch loss 7.39146662 epoch total loss 7.08919287\n",
      "Trained batch 331 batch loss 7.48887157 epoch total loss 7.09039974\n",
      "Trained batch 332 batch loss 7.41470623 epoch total loss 7.09137678\n",
      "Trained batch 333 batch loss 7.19805574 epoch total loss 7.09169722\n",
      "Trained batch 334 batch loss 7.19237041 epoch total loss 7.09199858\n",
      "Trained batch 335 batch loss 7.3500042 epoch total loss 7.09276915\n",
      "Trained batch 336 batch loss 7.51853371 epoch total loss 7.0940361\n",
      "Trained batch 337 batch loss 7.02778435 epoch total loss 7.09383965\n",
      "Trained batch 338 batch loss 7.05716276 epoch total loss 7.0937314\n",
      "Trained batch 339 batch loss 7.06764889 epoch total loss 7.09365416\n",
      "Trained batch 340 batch loss 6.69711065 epoch total loss 7.09248781\n",
      "Trained batch 341 batch loss 6.5684433 epoch total loss 7.09095049\n",
      "Trained batch 342 batch loss 7.28903 epoch total loss 7.09153\n",
      "Trained batch 343 batch loss 7.18877792 epoch total loss 7.09181309\n",
      "Trained batch 344 batch loss 7.31119061 epoch total loss 7.0924511\n",
      "Trained batch 345 batch loss 7.01835155 epoch total loss 7.09223652\n",
      "Trained batch 346 batch loss 6.2976346 epoch total loss 7.08993959\n",
      "Trained batch 347 batch loss 7.13599 epoch total loss 7.09007263\n",
      "Trained batch 348 batch loss 7.20569706 epoch total loss 7.09040499\n",
      "Trained batch 349 batch loss 7.06777573 epoch total loss 7.09034061\n",
      "Trained batch 350 batch loss 6.86117697 epoch total loss 7.08968544\n",
      "Trained batch 351 batch loss 7.3725605 epoch total loss 7.09049129\n",
      "Trained batch 352 batch loss 7.16389275 epoch total loss 7.09069967\n",
      "Trained batch 353 batch loss 7.32266617 epoch total loss 7.09135723\n",
      "Trained batch 354 batch loss 7.27750254 epoch total loss 7.09188318\n",
      "Trained batch 355 batch loss 6.95556831 epoch total loss 7.09149933\n",
      "Trained batch 356 batch loss 6.39253569 epoch total loss 7.08953571\n",
      "Trained batch 357 batch loss 6.4066577 epoch total loss 7.08762312\n",
      "Trained batch 358 batch loss 6.07124376 epoch total loss 7.08478451\n",
      "Trained batch 359 batch loss 6.6462822 epoch total loss 7.08356285\n",
      "Trained batch 360 batch loss 5.99770784 epoch total loss 7.08054686\n",
      "Trained batch 361 batch loss 5.93995094 epoch total loss 7.07738733\n",
      "Trained batch 362 batch loss 5.79550934 epoch total loss 7.07384586\n",
      "Trained batch 363 batch loss 5.74131107 epoch total loss 7.07017469\n",
      "Trained batch 364 batch loss 6.80355501 epoch total loss 7.0694418\n",
      "Trained batch 365 batch loss 7.15496397 epoch total loss 7.0696764\n",
      "Trained batch 366 batch loss 7.11002874 epoch total loss 7.06978703\n",
      "Trained batch 367 batch loss 7.19343615 epoch total loss 7.07012367\n",
      "Trained batch 368 batch loss 7.05482864 epoch total loss 7.07008219\n",
      "Trained batch 369 batch loss 7.27284431 epoch total loss 7.07063198\n",
      "Trained batch 370 batch loss 7.29450846 epoch total loss 7.07123709\n",
      "Trained batch 371 batch loss 6.75439024 epoch total loss 7.07038307\n",
      "Trained batch 372 batch loss 6.36519909 epoch total loss 7.06848717\n",
      "Trained batch 373 batch loss 6.43728828 epoch total loss 7.06679487\n",
      "Trained batch 374 batch loss 6.57010078 epoch total loss 7.06546688\n",
      "Trained batch 375 batch loss 6.97954845 epoch total loss 7.06523752\n",
      "Trained batch 376 batch loss 6.59396 epoch total loss 7.06398439\n",
      "Trained batch 377 batch loss 6.96049 epoch total loss 7.06370974\n",
      "Trained batch 378 batch loss 6.28379059 epoch total loss 7.06164598\n",
      "Trained batch 379 batch loss 7.27735758 epoch total loss 7.06221533\n",
      "Trained batch 380 batch loss 7.21815586 epoch total loss 7.06262589\n",
      "Trained batch 381 batch loss 7.19453812 epoch total loss 7.06297207\n",
      "Trained batch 382 batch loss 7.12760878 epoch total loss 7.06314182\n",
      "Trained batch 383 batch loss 7.22495794 epoch total loss 7.06356382\n",
      "Trained batch 384 batch loss 5.74614859 epoch total loss 7.06013298\n",
      "Trained batch 385 batch loss 5.3048377 epoch total loss 7.05557394\n",
      "Trained batch 386 batch loss 6.22808361 epoch total loss 7.05343\n",
      "Trained batch 387 batch loss 7.0685997 epoch total loss 7.05346918\n",
      "Trained batch 388 batch loss 7.31506395 epoch total loss 7.05414391\n",
      "Trained batch 389 batch loss 7.48313332 epoch total loss 7.05524683\n",
      "Trained batch 390 batch loss 7.44337559 epoch total loss 7.05624199\n",
      "Trained batch 391 batch loss 7.31386328 epoch total loss 7.05690098\n",
      "Trained batch 392 batch loss 6.94751072 epoch total loss 7.05662203\n",
      "Trained batch 393 batch loss 7.01213551 epoch total loss 7.05650902\n",
      "Trained batch 394 batch loss 7.02505112 epoch total loss 7.05642939\n",
      "Trained batch 395 batch loss 7.36037064 epoch total loss 7.05719852\n",
      "Trained batch 396 batch loss 7.16408348 epoch total loss 7.05746841\n",
      "Trained batch 397 batch loss 7.48700094 epoch total loss 7.05855083\n",
      "Trained batch 398 batch loss 7.28574896 epoch total loss 7.05912113\n",
      "Trained batch 399 batch loss 7.10397243 epoch total loss 7.05923367\n",
      "Trained batch 400 batch loss 6.99208117 epoch total loss 7.0590663\n",
      "Trained batch 401 batch loss 7.31844521 epoch total loss 7.05971289\n",
      "Trained batch 402 batch loss 6.52337456 epoch total loss 7.0583787\n",
      "Trained batch 403 batch loss 6.43368912 epoch total loss 7.0568285\n",
      "Trained batch 404 batch loss 6.14156151 epoch total loss 7.05456305\n",
      "Trained batch 405 batch loss 6.86856365 epoch total loss 7.05410385\n",
      "Trained batch 406 batch loss 6.61408854 epoch total loss 7.05302\n",
      "Trained batch 407 batch loss 6.93476486 epoch total loss 7.05272961\n",
      "Trained batch 408 batch loss 7.09560823 epoch total loss 7.05283499\n",
      "Trained batch 409 batch loss 6.88319874 epoch total loss 7.05242062\n",
      "Trained batch 410 batch loss 6.57738352 epoch total loss 7.0512619\n",
      "Trained batch 411 batch loss 6.74806881 epoch total loss 7.05052423\n",
      "Trained batch 412 batch loss 7.26361704 epoch total loss 7.0510416\n",
      "Trained batch 413 batch loss 7.20524 epoch total loss 7.05141497\n",
      "Trained batch 414 batch loss 7.32281637 epoch total loss 7.05207\n",
      "Trained batch 415 batch loss 7.38123322 epoch total loss 7.0528636\n",
      "Trained batch 416 batch loss 7.34658384 epoch total loss 7.05357027\n",
      "Trained batch 417 batch loss 7.14381123 epoch total loss 7.05378628\n",
      "Trained batch 418 batch loss 6.74818134 epoch total loss 7.05305576\n",
      "Trained batch 419 batch loss 6.97620773 epoch total loss 7.05287266\n",
      "Trained batch 420 batch loss 7.00146341 epoch total loss 7.05275\n",
      "Trained batch 421 batch loss 6.86802435 epoch total loss 7.05231094\n",
      "Trained batch 422 batch loss 6.91996098 epoch total loss 7.05199718\n",
      "Trained batch 423 batch loss 7.17457867 epoch total loss 7.0522871\n",
      "Trained batch 424 batch loss 6.84422636 epoch total loss 7.05179644\n",
      "Trained batch 425 batch loss 6.67809343 epoch total loss 7.05091667\n",
      "Trained batch 426 batch loss 6.80360222 epoch total loss 7.05033636\n",
      "Trained batch 427 batch loss 6.92157221 epoch total loss 7.050035\n",
      "Trained batch 428 batch loss 7.05306339 epoch total loss 7.05004215\n",
      "Trained batch 429 batch loss 6.84898329 epoch total loss 7.04957294\n",
      "Trained batch 430 batch loss 6.87968159 epoch total loss 7.04917765\n",
      "Trained batch 431 batch loss 6.71857119 epoch total loss 7.04841042\n",
      "Trained batch 432 batch loss 6.51770067 epoch total loss 7.04718256\n",
      "Trained batch 433 batch loss 6.91772461 epoch total loss 7.04688358\n",
      "Trained batch 434 batch loss 5.89000273 epoch total loss 7.04421759\n",
      "Trained batch 435 batch loss 6.14543629 epoch total loss 7.04215145\n",
      "Trained batch 436 batch loss 6.56081676 epoch total loss 7.04104757\n",
      "Trained batch 437 batch loss 7.19017172 epoch total loss 7.04138899\n",
      "Trained batch 438 batch loss 7.06630087 epoch total loss 7.04144573\n",
      "Trained batch 439 batch loss 7.07944441 epoch total loss 7.04153204\n",
      "Trained batch 440 batch loss 7.24292946 epoch total loss 7.04199\n",
      "Trained batch 441 batch loss 7.34694195 epoch total loss 7.04268122\n",
      "Trained batch 442 batch loss 7.48708391 epoch total loss 7.04368687\n",
      "Trained batch 443 batch loss 7.34710264 epoch total loss 7.04437208\n",
      "Trained batch 444 batch loss 7.39395237 epoch total loss 7.04515934\n",
      "Trained batch 445 batch loss 7.30288458 epoch total loss 7.0457387\n",
      "Trained batch 446 batch loss 7.36172676 epoch total loss 7.04644728\n",
      "Trained batch 447 batch loss 6.53256083 epoch total loss 7.04529762\n",
      "Trained batch 448 batch loss 6.98113108 epoch total loss 7.04515457\n",
      "Trained batch 449 batch loss 7.20175743 epoch total loss 7.04550314\n",
      "Trained batch 450 batch loss 7.30411386 epoch total loss 7.04607821\n",
      "Trained batch 451 batch loss 7.12081718 epoch total loss 7.04624367\n",
      "Trained batch 452 batch loss 6.34122562 epoch total loss 7.04468441\n",
      "Trained batch 453 batch loss 6.70430565 epoch total loss 7.04393291\n",
      "Trained batch 454 batch loss 6.30686474 epoch total loss 7.04230928\n",
      "Trained batch 455 batch loss 7.12807608 epoch total loss 7.04249811\n",
      "Trained batch 456 batch loss 7.24940586 epoch total loss 7.04295206\n",
      "Trained batch 457 batch loss 6.75186443 epoch total loss 7.04231548\n",
      "Trained batch 458 batch loss 6.77973938 epoch total loss 7.04174232\n",
      "Trained batch 459 batch loss 6.9891367 epoch total loss 7.04162788\n",
      "Trained batch 460 batch loss 7.06971 epoch total loss 7.04168892\n",
      "Trained batch 461 batch loss 7.2132926 epoch total loss 7.04206133\n",
      "Trained batch 462 batch loss 6.89307499 epoch total loss 7.04173899\n",
      "Trained batch 463 batch loss 6.93248177 epoch total loss 7.04150295\n",
      "Trained batch 464 batch loss 7.34317636 epoch total loss 7.04215336\n",
      "Trained batch 465 batch loss 7.23308134 epoch total loss 7.04256392\n",
      "Trained batch 466 batch loss 7.30186939 epoch total loss 7.04312038\n",
      "Trained batch 467 batch loss 6.92660141 epoch total loss 7.04287052\n",
      "Trained batch 468 batch loss 6.72530937 epoch total loss 7.04219198\n",
      "Trained batch 469 batch loss 6.93748951 epoch total loss 7.04196882\n",
      "Trained batch 470 batch loss 6.73552752 epoch total loss 7.04131699\n",
      "Trained batch 471 batch loss 6.91546535 epoch total loss 7.04105\n",
      "Trained batch 472 batch loss 6.72038221 epoch total loss 7.04037046\n",
      "Trained batch 473 batch loss 6.27582264 epoch total loss 7.03875446\n",
      "Trained batch 474 batch loss 7.27277565 epoch total loss 7.03924799\n",
      "Trained batch 475 batch loss 7.10596704 epoch total loss 7.03938818\n",
      "Trained batch 476 batch loss 7.21991253 epoch total loss 7.03976774\n",
      "Trained batch 477 batch loss 7.09345293 epoch total loss 7.03988028\n",
      "Trained batch 478 batch loss 6.7276926 epoch total loss 7.03922749\n",
      "Trained batch 479 batch loss 7.03024101 epoch total loss 7.03920889\n",
      "Trained batch 480 batch loss 7.19340658 epoch total loss 7.03953\n",
      "Trained batch 481 batch loss 7.49019527 epoch total loss 7.04046679\n",
      "Trained batch 482 batch loss 7.29573345 epoch total loss 7.04099655\n",
      "Trained batch 483 batch loss 7.46459055 epoch total loss 7.04187346\n",
      "Trained batch 484 batch loss 7.37731075 epoch total loss 7.0425663\n",
      "Trained batch 485 batch loss 6.92705202 epoch total loss 7.04232788\n",
      "Trained batch 486 batch loss 7.036376 epoch total loss 7.04231548\n",
      "Trained batch 487 batch loss 7.13915586 epoch total loss 7.04251432\n",
      "Trained batch 488 batch loss 6.90274096 epoch total loss 7.04222822\n",
      "Trained batch 489 batch loss 7.06201124 epoch total loss 7.04226875\n",
      "Trained batch 490 batch loss 7.24716663 epoch total loss 7.04268694\n",
      "Trained batch 491 batch loss 7.19283485 epoch total loss 7.04299259\n",
      "Trained batch 492 batch loss 6.55040026 epoch total loss 7.04199123\n",
      "Trained batch 493 batch loss 6.65374565 epoch total loss 7.04120398\n",
      "Trained batch 494 batch loss 7.26068878 epoch total loss 7.04164839\n",
      "Trained batch 495 batch loss 7.52409458 epoch total loss 7.04262304\n",
      "Trained batch 496 batch loss 7.12903214 epoch total loss 7.04279757\n",
      "Trained batch 497 batch loss 7.50523186 epoch total loss 7.04372787\n",
      "Trained batch 498 batch loss 7.50002766 epoch total loss 7.04464388\n",
      "Trained batch 499 batch loss 7.04270887 epoch total loss 7.04464\n",
      "Trained batch 500 batch loss 7.24643278 epoch total loss 7.04504347\n",
      "Trained batch 501 batch loss 7.04523087 epoch total loss 7.04504347\n",
      "Trained batch 502 batch loss 7.03677511 epoch total loss 7.04502726\n",
      "Trained batch 503 batch loss 6.69755077 epoch total loss 7.04433632\n",
      "Trained batch 504 batch loss 6.92784929 epoch total loss 7.04410505\n",
      "Trained batch 505 batch loss 6.95116138 epoch total loss 7.04392099\n",
      "Trained batch 506 batch loss 7.22714424 epoch total loss 7.04428291\n",
      "Trained batch 507 batch loss 6.85380936 epoch total loss 7.04390717\n",
      "Trained batch 508 batch loss 6.98118 epoch total loss 7.04378366\n",
      "Trained batch 509 batch loss 6.94701099 epoch total loss 7.04359388\n",
      "Trained batch 510 batch loss 6.60599518 epoch total loss 7.04273558\n",
      "Trained batch 511 batch loss 7.24109888 epoch total loss 7.0431242\n",
      "Trained batch 512 batch loss 6.58836126 epoch total loss 7.04223585\n",
      "Trained batch 513 batch loss 6.9565649 epoch total loss 7.04206896\n",
      "Trained batch 514 batch loss 7.02719307 epoch total loss 7.04204\n",
      "Trained batch 515 batch loss 6.51963568 epoch total loss 7.04102516\n",
      "Trained batch 516 batch loss 7.09604454 epoch total loss 7.0411315\n",
      "Trained batch 517 batch loss 6.98602295 epoch total loss 7.04102516\n",
      "Trained batch 518 batch loss 6.99596214 epoch total loss 7.0409379\n",
      "Trained batch 519 batch loss 6.7073431 epoch total loss 7.04029512\n",
      "Trained batch 520 batch loss 6.49163389 epoch total loss 7.03924\n",
      "Trained batch 521 batch loss 7.23744 epoch total loss 7.0396204\n",
      "Trained batch 522 batch loss 7.26907158 epoch total loss 7.04006\n",
      "Trained batch 523 batch loss 7.14879942 epoch total loss 7.04026794\n",
      "Trained batch 524 batch loss 6.54360962 epoch total loss 7.03932\n",
      "Trained batch 525 batch loss 6.09369802 epoch total loss 7.03751898\n",
      "Trained batch 526 batch loss 6.24614668 epoch total loss 7.03601456\n",
      "Trained batch 527 batch loss 6.46582794 epoch total loss 7.03493261\n",
      "Trained batch 528 batch loss 6.87042856 epoch total loss 7.03462076\n",
      "Trained batch 529 batch loss 7.08796549 epoch total loss 7.03472137\n",
      "Trained batch 530 batch loss 7.22086239 epoch total loss 7.0350728\n",
      "Trained batch 531 batch loss 7.04962492 epoch total loss 7.0351\n",
      "Trained batch 532 batch loss 7.22795677 epoch total loss 7.03546286\n",
      "Trained batch 533 batch loss 7.30177402 epoch total loss 7.03596258\n",
      "Trained batch 534 batch loss 7.14628172 epoch total loss 7.03616905\n",
      "Trained batch 535 batch loss 7.11196 epoch total loss 7.03631067\n",
      "Trained batch 536 batch loss 6.90102768 epoch total loss 7.03605843\n",
      "Trained batch 537 batch loss 7.15534496 epoch total loss 7.03628063\n",
      "Trained batch 538 batch loss 6.77859974 epoch total loss 7.03580141\n",
      "Trained batch 539 batch loss 6.48333025 epoch total loss 7.03477669\n",
      "Trained batch 540 batch loss 7.1856513 epoch total loss 7.03505611\n",
      "Trained batch 541 batch loss 6.73065853 epoch total loss 7.03449345\n",
      "Trained batch 542 batch loss 6.91437197 epoch total loss 7.03427172\n",
      "Trained batch 543 batch loss 6.94371653 epoch total loss 7.03410482\n",
      "Trained batch 544 batch loss 7.38047886 epoch total loss 7.03474092\n",
      "Trained batch 545 batch loss 7.48374176 epoch total loss 7.0355649\n",
      "Trained batch 546 batch loss 7.44536972 epoch total loss 7.03631544\n",
      "Trained batch 547 batch loss 7.23838902 epoch total loss 7.03668451\n",
      "Trained batch 548 batch loss 7.16642046 epoch total loss 7.0369215\n",
      "Trained batch 549 batch loss 7.15557528 epoch total loss 7.03713751\n",
      "Trained batch 550 batch loss 6.83311558 epoch total loss 7.03676605\n",
      "Trained batch 551 batch loss 6.90379477 epoch total loss 7.03652477\n",
      "Trained batch 552 batch loss 6.66056919 epoch total loss 7.03584385\n",
      "Trained batch 553 batch loss 6.73875523 epoch total loss 7.03530693\n",
      "Trained batch 554 batch loss 6.91503429 epoch total loss 7.03508949\n",
      "Trained batch 555 batch loss 7.0929141 epoch total loss 7.03519392\n",
      "Trained batch 556 batch loss 6.93699455 epoch total loss 7.03501749\n",
      "Trained batch 557 batch loss 6.98454809 epoch total loss 7.03492689\n",
      "Trained batch 558 batch loss 6.87593222 epoch total loss 7.03464222\n",
      "Trained batch 559 batch loss 6.26887846 epoch total loss 7.03327227\n",
      "Trained batch 560 batch loss 7.06452656 epoch total loss 7.03332758\n",
      "Trained batch 561 batch loss 6.80625343 epoch total loss 7.03292274\n",
      "Trained batch 562 batch loss 6.86821365 epoch total loss 7.03262949\n",
      "Trained batch 563 batch loss 7.30076 epoch total loss 7.03310585\n",
      "Trained batch 564 batch loss 7.38212204 epoch total loss 7.03372478\n",
      "Trained batch 565 batch loss 7.27839661 epoch total loss 7.03415775\n",
      "Trained batch 566 batch loss 7.00530624 epoch total loss 7.03410673\n",
      "Trained batch 567 batch loss 7.47199726 epoch total loss 7.03487873\n",
      "Trained batch 568 batch loss 7.44930267 epoch total loss 7.03560829\n",
      "Trained batch 569 batch loss 6.8271327 epoch total loss 7.03524208\n",
      "Trained batch 570 batch loss 7.03349304 epoch total loss 7.03523874\n",
      "Trained batch 571 batch loss 7.2505827 epoch total loss 7.03561592\n",
      "Trained batch 572 batch loss 7.33476925 epoch total loss 7.03613901\n",
      "Trained batch 573 batch loss 7.34595537 epoch total loss 7.03667927\n",
      "Trained batch 574 batch loss 7.40473 epoch total loss 7.03732061\n",
      "Trained batch 575 batch loss 7.35902405 epoch total loss 7.03788042\n",
      "Trained batch 576 batch loss 7.55244493 epoch total loss 7.03877401\n",
      "Trained batch 577 batch loss 7.54611301 epoch total loss 7.0396533\n",
      "Trained batch 578 batch loss 7.37370777 epoch total loss 7.04023123\n",
      "Trained batch 579 batch loss 7.35643244 epoch total loss 7.04077721\n",
      "Trained batch 580 batch loss 7.41728735 epoch total loss 7.04142666\n",
      "Trained batch 581 batch loss 7.12180948 epoch total loss 7.04156494\n",
      "Trained batch 582 batch loss 7.18776798 epoch total loss 7.04181623\n",
      "Trained batch 583 batch loss 7.17991877 epoch total loss 7.04205227\n",
      "Trained batch 584 batch loss 7.3745203 epoch total loss 7.04262161\n",
      "Trained batch 585 batch loss 7.15032578 epoch total loss 7.04280615\n",
      "Trained batch 586 batch loss 6.98214483 epoch total loss 7.0427022\n",
      "Trained batch 587 batch loss 7.28995752 epoch total loss 7.04312325\n",
      "Trained batch 588 batch loss 7.19660378 epoch total loss 7.04338455\n",
      "Trained batch 589 batch loss 6.82441473 epoch total loss 7.04301262\n",
      "Trained batch 590 batch loss 7.08864784 epoch total loss 7.04309034\n",
      "Trained batch 591 batch loss 7.22561264 epoch total loss 7.04339933\n",
      "Trained batch 592 batch loss 6.6584959 epoch total loss 7.0427494\n",
      "Trained batch 593 batch loss 6.53479528 epoch total loss 7.04189253\n",
      "Trained batch 594 batch loss 6.65049696 epoch total loss 7.04123354\n",
      "Trained batch 595 batch loss 7.27218866 epoch total loss 7.04162121\n",
      "Trained batch 596 batch loss 7.28364372 epoch total loss 7.04202747\n",
      "Trained batch 597 batch loss 7.34143162 epoch total loss 7.04252863\n",
      "Trained batch 598 batch loss 7.36557865 epoch total loss 7.04306936\n",
      "Trained batch 599 batch loss 7.45100069 epoch total loss 7.04375029\n",
      "Trained batch 600 batch loss 7.54207611 epoch total loss 7.04458094\n",
      "Trained batch 601 batch loss 7.53248882 epoch total loss 7.04539299\n",
      "Trained batch 602 batch loss 7.17937326 epoch total loss 7.0456152\n",
      "Trained batch 603 batch loss 7.22687817 epoch total loss 7.04591608\n",
      "Trained batch 604 batch loss 7.28104067 epoch total loss 7.04630566\n",
      "Trained batch 605 batch loss 7.27327347 epoch total loss 7.0466814\n",
      "Trained batch 606 batch loss 7.20688248 epoch total loss 7.04694605\n",
      "Trained batch 607 batch loss 7.45335627 epoch total loss 7.04761505\n",
      "Trained batch 608 batch loss 7.04606485 epoch total loss 7.04761219\n",
      "Trained batch 609 batch loss 6.68507767 epoch total loss 7.0470171\n",
      "Trained batch 610 batch loss 7.01573515 epoch total loss 7.0469656\n",
      "Trained batch 611 batch loss 7.35098934 epoch total loss 7.04746294\n",
      "Trained batch 612 batch loss 7.27932501 epoch total loss 7.04784203\n",
      "Trained batch 613 batch loss 7.46323633 epoch total loss 7.04852\n",
      "Trained batch 614 batch loss 7.23967028 epoch total loss 7.04883146\n",
      "Trained batch 615 batch loss 7.02933311 epoch total loss 7.04879951\n",
      "Trained batch 616 batch loss 6.59443235 epoch total loss 7.04806185\n",
      "Trained batch 617 batch loss 7.06869698 epoch total loss 7.04809523\n",
      "Trained batch 618 batch loss 7.27283716 epoch total loss 7.04845905\n",
      "Trained batch 619 batch loss 6.93774605 epoch total loss 7.04828072\n",
      "Trained batch 620 batch loss 6.83354759 epoch total loss 7.04793406\n",
      "Trained batch 621 batch loss 6.33828354 epoch total loss 7.04679155\n",
      "Trained batch 622 batch loss 6.9430089 epoch total loss 7.04662466\n",
      "Trained batch 623 batch loss 7.18790627 epoch total loss 7.04685163\n",
      "Trained batch 624 batch loss 7.30266476 epoch total loss 7.04726171\n",
      "Trained batch 625 batch loss 6.8285861 epoch total loss 7.04691172\n",
      "Trained batch 626 batch loss 7.29934168 epoch total loss 7.04731512\n",
      "Trained batch 627 batch loss 7.23131418 epoch total loss 7.04760838\n",
      "Trained batch 628 batch loss 6.38195419 epoch total loss 7.04654837\n",
      "Trained batch 629 batch loss 6.66271 epoch total loss 7.04593801\n",
      "Trained batch 630 batch loss 6.9951992 epoch total loss 7.04585743\n",
      "Trained batch 631 batch loss 7.06106472 epoch total loss 7.04588127\n",
      "Trained batch 632 batch loss 7.17297554 epoch total loss 7.0460825\n",
      "Trained batch 633 batch loss 7.040874 epoch total loss 7.04607439\n",
      "Trained batch 634 batch loss 7.36720467 epoch total loss 7.04658079\n",
      "Trained batch 635 batch loss 6.89952707 epoch total loss 7.04634905\n",
      "Trained batch 636 batch loss 7.1057992 epoch total loss 7.04644299\n",
      "Trained batch 637 batch loss 6.13906097 epoch total loss 7.04501867\n",
      "Trained batch 638 batch loss 6.75214434 epoch total loss 7.044559\n",
      "Trained batch 639 batch loss 6.54763746 epoch total loss 7.04378176\n",
      "Trained batch 640 batch loss 7.08145523 epoch total loss 7.04384089\n",
      "Trained batch 641 batch loss 7.19659424 epoch total loss 7.0440793\n",
      "Trained batch 642 batch loss 7.16251755 epoch total loss 7.04426384\n",
      "Trained batch 643 batch loss 7.25927258 epoch total loss 7.04459858\n",
      "Trained batch 644 batch loss 7.22369051 epoch total loss 7.04487658\n",
      "Trained batch 645 batch loss 7.01565409 epoch total loss 7.0448308\n",
      "Trained batch 646 batch loss 7.06668758 epoch total loss 7.04486513\n",
      "Trained batch 647 batch loss 7.30740356 epoch total loss 7.0452714\n",
      "Trained batch 648 batch loss 6.5482645 epoch total loss 7.04450464\n",
      "Trained batch 649 batch loss 6.94298553 epoch total loss 7.04434776\n",
      "Trained batch 650 batch loss 7.18176603 epoch total loss 7.044559\n",
      "Trained batch 651 batch loss 6.79504204 epoch total loss 7.04417562\n",
      "Trained batch 652 batch loss 6.03367138 epoch total loss 7.0426259\n",
      "Trained batch 653 batch loss 6.74322748 epoch total loss 7.04216719\n",
      "Trained batch 654 batch loss 7.36503172 epoch total loss 7.04266119\n",
      "Trained batch 655 batch loss 7.36206675 epoch total loss 7.04314899\n",
      "Trained batch 656 batch loss 6.84588528 epoch total loss 7.04284811\n",
      "Trained batch 657 batch loss 6.74651909 epoch total loss 7.04239702\n",
      "Trained batch 658 batch loss 6.58396816 epoch total loss 7.04170036\n",
      "Trained batch 659 batch loss 6.95298767 epoch total loss 7.04156637\n",
      "Trained batch 660 batch loss 7.14755869 epoch total loss 7.04172659\n",
      "Trained batch 661 batch loss 7.26977 epoch total loss 7.04207134\n",
      "Trained batch 662 batch loss 7.32005644 epoch total loss 7.04249096\n",
      "Trained batch 663 batch loss 6.88444376 epoch total loss 7.04225206\n",
      "Trained batch 664 batch loss 6.99599934 epoch total loss 7.04218245\n",
      "Trained batch 665 batch loss 6.29746389 epoch total loss 7.04106283\n",
      "Trained batch 666 batch loss 7.2922883 epoch total loss 7.04144\n",
      "Trained batch 667 batch loss 7.0989604 epoch total loss 7.04152679\n",
      "Trained batch 668 batch loss 7.23851538 epoch total loss 7.041821\n",
      "Trained batch 669 batch loss 7.12185431 epoch total loss 7.04194117\n",
      "Trained batch 670 batch loss 7.16772413 epoch total loss 7.04212856\n",
      "Trained batch 671 batch loss 6.97238493 epoch total loss 7.04202414\n",
      "Trained batch 672 batch loss 7.29491282 epoch total loss 7.04240036\n",
      "Trained batch 673 batch loss 7.50806427 epoch total loss 7.04309273\n",
      "Trained batch 674 batch loss 7.48788548 epoch total loss 7.04375267\n",
      "Trained batch 675 batch loss 6.94919729 epoch total loss 7.04361248\n",
      "Trained batch 676 batch loss 6.87545633 epoch total loss 7.04336405\n",
      "Trained batch 677 batch loss 6.86720085 epoch total loss 7.04310369\n",
      "Trained batch 678 batch loss 6.94526768 epoch total loss 7.04295921\n",
      "Trained batch 679 batch loss 6.86072922 epoch total loss 7.04269123\n",
      "Trained batch 680 batch loss 6.58100653 epoch total loss 7.04201221\n",
      "Trained batch 681 batch loss 6.84014893 epoch total loss 7.0417161\n",
      "Trained batch 682 batch loss 7.06301928 epoch total loss 7.04174709\n",
      "Trained batch 683 batch loss 7.52263927 epoch total loss 7.04245138\n",
      "Trained batch 684 batch loss 7.4087348 epoch total loss 7.04298639\n",
      "Trained batch 685 batch loss 7.19522762 epoch total loss 7.04320908\n",
      "Trained batch 686 batch loss 7.20397472 epoch total loss 7.04344368\n",
      "Trained batch 687 batch loss 7.20611429 epoch total loss 7.04368\n",
      "Trained batch 688 batch loss 7.24644852 epoch total loss 7.04397535\n",
      "Trained batch 689 batch loss 6.99253464 epoch total loss 7.04390049\n",
      "Trained batch 690 batch loss 7.21447802 epoch total loss 7.04414749\n",
      "Trained batch 691 batch loss 6.95613146 epoch total loss 7.04402\n",
      "Trained batch 692 batch loss 7.03129625 epoch total loss 7.04400158\n",
      "Trained batch 693 batch loss 6.63172483 epoch total loss 7.04340696\n",
      "Trained batch 694 batch loss 6.8818779 epoch total loss 7.04317427\n",
      "Trained batch 695 batch loss 6.78611755 epoch total loss 7.04280424\n",
      "Trained batch 696 batch loss 6.88864231 epoch total loss 7.04258299\n",
      "Trained batch 697 batch loss 6.68766356 epoch total loss 7.04207325\n",
      "Trained batch 698 batch loss 6.89694881 epoch total loss 7.04186535\n",
      "Trained batch 699 batch loss 6.76480103 epoch total loss 7.0414691\n",
      "Trained batch 700 batch loss 6.57364702 epoch total loss 7.04080057\n",
      "Trained batch 701 batch loss 6.96730566 epoch total loss 7.04069614\n",
      "Trained batch 702 batch loss 7.24060678 epoch total loss 7.04098082\n",
      "Trained batch 703 batch loss 6.86761427 epoch total loss 7.04073429\n",
      "Trained batch 704 batch loss 6.87091 epoch total loss 7.04049349\n",
      "Trained batch 705 batch loss 7.13831377 epoch total loss 7.04063177\n",
      "Trained batch 706 batch loss 6.67069483 epoch total loss 7.0401082\n",
      "Trained batch 707 batch loss 6.87169218 epoch total loss 7.03987\n",
      "Trained batch 708 batch loss 7.08034134 epoch total loss 7.03992748\n",
      "Trained batch 709 batch loss 7.39454 epoch total loss 7.04042768\n",
      "Trained batch 710 batch loss 7.33699846 epoch total loss 7.04084492\n",
      "Trained batch 711 batch loss 7.18546629 epoch total loss 7.04104853\n",
      "Trained batch 712 batch loss 6.83878231 epoch total loss 7.04076481\n",
      "Trained batch 713 batch loss 6.92256498 epoch total loss 7.04059839\n",
      "Trained batch 714 batch loss 6.97445774 epoch total loss 7.04050636\n",
      "Trained batch 715 batch loss 6.42163897 epoch total loss 7.0396409\n",
      "Trained batch 716 batch loss 6.90268278 epoch total loss 7.03944969\n",
      "Trained batch 717 batch loss 6.87428617 epoch total loss 7.03922\n",
      "Trained batch 718 batch loss 7.02093601 epoch total loss 7.03919458\n",
      "Trained batch 719 batch loss 7.06440163 epoch total loss 7.03922939\n",
      "Trained batch 720 batch loss 6.46774864 epoch total loss 7.03843594\n",
      "Trained batch 721 batch loss 6.78068638 epoch total loss 7.03807831\n",
      "Trained batch 722 batch loss 6.80876827 epoch total loss 7.03776073\n",
      "Trained batch 723 batch loss 7.12298679 epoch total loss 7.03787851\n",
      "Trained batch 724 batch loss 7.33440733 epoch total loss 7.03828812\n",
      "Trained batch 725 batch loss 7.31569099 epoch total loss 7.03867102\n",
      "Trained batch 726 batch loss 7.14897299 epoch total loss 7.03882313\n",
      "Trained batch 727 batch loss 6.99329853 epoch total loss 7.03876\n",
      "Trained batch 728 batch loss 7.08815193 epoch total loss 7.03882837\n",
      "Trained batch 729 batch loss 6.57692337 epoch total loss 7.03819513\n",
      "Trained batch 730 batch loss 6.95759583 epoch total loss 7.03808451\n",
      "Trained batch 731 batch loss 7.08078766 epoch total loss 7.03814268\n",
      "Trained batch 732 batch loss 7.18692541 epoch total loss 7.03834629\n",
      "Trained batch 733 batch loss 7.44847584 epoch total loss 7.03890514\n",
      "Trained batch 734 batch loss 7.34529209 epoch total loss 7.03932238\n",
      "Trained batch 735 batch loss 7.5150547 epoch total loss 7.03997\n",
      "Trained batch 736 batch loss 7.40136623 epoch total loss 7.04046106\n",
      "Trained batch 737 batch loss 7.06885767 epoch total loss 7.04049969\n",
      "Trained batch 738 batch loss 7.24309301 epoch total loss 7.04077435\n",
      "Trained batch 739 batch loss 7.38584518 epoch total loss 7.04124117\n",
      "Trained batch 740 batch loss 7.48166227 epoch total loss 7.04183578\n",
      "Trained batch 741 batch loss 7.14383459 epoch total loss 7.04197359\n",
      "Trained batch 742 batch loss 7.14445734 epoch total loss 7.04211187\n",
      "Trained batch 743 batch loss 7.29339933 epoch total loss 7.04245043\n",
      "Trained batch 744 batch loss 7.34279919 epoch total loss 7.04285383\n",
      "Trained batch 745 batch loss 7.24621725 epoch total loss 7.04312658\n",
      "Trained batch 746 batch loss 7.21791077 epoch total loss 7.04336071\n",
      "Trained batch 747 batch loss 7.31166553 epoch total loss 7.04372\n",
      "Trained batch 748 batch loss 7.45687628 epoch total loss 7.04427242\n",
      "Trained batch 749 batch loss 7.27599621 epoch total loss 7.04458141\n",
      "Trained batch 750 batch loss 7.40205526 epoch total loss 7.04505777\n",
      "Trained batch 751 batch loss 7.13695097 epoch total loss 7.04518\n",
      "Trained batch 752 batch loss 6.94347525 epoch total loss 7.04504442\n",
      "Trained batch 753 batch loss 6.95528173 epoch total loss 7.04492521\n",
      "Trained batch 754 batch loss 7.36963367 epoch total loss 7.0453558\n",
      "Trained batch 755 batch loss 7.41889095 epoch total loss 7.04585075\n",
      "Trained batch 756 batch loss 7.40684414 epoch total loss 7.04632807\n",
      "Trained batch 757 batch loss 7.22973 epoch total loss 7.04657\n",
      "Trained batch 758 batch loss 7.30551958 epoch total loss 7.04691172\n",
      "Trained batch 759 batch loss 7.09475 epoch total loss 7.04697466\n",
      "Trained batch 760 batch loss 6.46419811 epoch total loss 7.0462079\n",
      "Trained batch 761 batch loss 7.09052753 epoch total loss 7.04626608\n",
      "Trained batch 762 batch loss 6.84678507 epoch total loss 7.0460043\n",
      "Trained batch 763 batch loss 7.08002043 epoch total loss 7.04604864\n",
      "Trained batch 764 batch loss 7.37905 epoch total loss 7.04648447\n",
      "Trained batch 765 batch loss 6.77332258 epoch total loss 7.0461278\n",
      "Trained batch 766 batch loss 6.65173817 epoch total loss 7.04561281\n",
      "Trained batch 767 batch loss 7.01032925 epoch total loss 7.04556656\n",
      "Trained batch 768 batch loss 6.66235685 epoch total loss 7.04506826\n",
      "Trained batch 769 batch loss 6.92669296 epoch total loss 7.04491425\n",
      "Trained batch 770 batch loss 6.93014717 epoch total loss 7.04476547\n",
      "Trained batch 771 batch loss 6.28175306 epoch total loss 7.04377556\n",
      "Trained batch 772 batch loss 6.74289513 epoch total loss 7.04338551\n",
      "Trained batch 773 batch loss 6.17833805 epoch total loss 7.04226637\n",
      "Trained batch 774 batch loss 6.99051237 epoch total loss 7.04219961\n",
      "Trained batch 775 batch loss 6.67072535 epoch total loss 7.04172087\n",
      "Trained batch 776 batch loss 6.98324156 epoch total loss 7.04164553\n",
      "Trained batch 777 batch loss 7.17481661 epoch total loss 7.04181671\n",
      "Trained batch 778 batch loss 7.04538441 epoch total loss 7.04182148\n",
      "Trained batch 779 batch loss 7.39858055 epoch total loss 7.04227924\n",
      "Trained batch 780 batch loss 7.23204136 epoch total loss 7.04252243\n",
      "Trained batch 781 batch loss 7.04660511 epoch total loss 7.0425272\n",
      "Trained batch 782 batch loss 7.23335648 epoch total loss 7.04277134\n",
      "Trained batch 783 batch loss 7.34258032 epoch total loss 7.04315472\n",
      "Trained batch 784 batch loss 7.00864315 epoch total loss 7.04311085\n",
      "Trained batch 785 batch loss 7.04977 epoch total loss 7.04311943\n",
      "Trained batch 786 batch loss 7.11220741 epoch total loss 7.04320717\n",
      "Trained batch 787 batch loss 7.05370712 epoch total loss 7.04322052\n",
      "Trained batch 788 batch loss 6.47807264 epoch total loss 7.04250336\n",
      "Trained batch 789 batch loss 6.42181206 epoch total loss 7.04171658\n",
      "Trained batch 790 batch loss 5.61057 epoch total loss 7.03990507\n",
      "Trained batch 791 batch loss 5.45345449 epoch total loss 7.03789949\n",
      "Trained batch 792 batch loss 6.49782753 epoch total loss 7.03721809\n",
      "Trained batch 793 batch loss 7.29911423 epoch total loss 7.03754854\n",
      "Trained batch 794 batch loss 7.3776722 epoch total loss 7.03797626\n",
      "Trained batch 795 batch loss 6.54602957 epoch total loss 7.03735733\n",
      "Trained batch 796 batch loss 6.93146849 epoch total loss 7.03722477\n",
      "Trained batch 797 batch loss 7.14329767 epoch total loss 7.03735733\n",
      "Trained batch 798 batch loss 6.41792297 epoch total loss 7.03658152\n",
      "Trained batch 799 batch loss 6.32533216 epoch total loss 7.03569078\n",
      "Trained batch 800 batch loss 6.86132574 epoch total loss 7.03547287\n",
      "Trained batch 801 batch loss 6.69087601 epoch total loss 7.03504276\n",
      "Trained batch 802 batch loss 6.90477276 epoch total loss 7.03488064\n",
      "Trained batch 803 batch loss 7.05644321 epoch total loss 7.03490734\n",
      "Trained batch 804 batch loss 7.09880352 epoch total loss 7.03498697\n",
      "Trained batch 805 batch loss 7.33190536 epoch total loss 7.03535604\n",
      "Trained batch 806 batch loss 7.49872112 epoch total loss 7.03593063\n",
      "Trained batch 807 batch loss 7.33438683 epoch total loss 7.0363\n",
      "Trained batch 808 batch loss 7.52013111 epoch total loss 7.03689909\n",
      "Trained batch 809 batch loss 6.86723852 epoch total loss 7.03668928\n",
      "Trained batch 810 batch loss 7.02787924 epoch total loss 7.03667831\n",
      "Trained batch 811 batch loss 7.02916241 epoch total loss 7.03666925\n",
      "Trained batch 812 batch loss 7.40640736 epoch total loss 7.03712463\n",
      "Trained batch 813 batch loss 7.35093641 epoch total loss 7.0375104\n",
      "Trained batch 814 batch loss 7.13639879 epoch total loss 7.03763199\n",
      "Trained batch 815 batch loss 7.38020849 epoch total loss 7.03805256\n",
      "Trained batch 816 batch loss 7.19234896 epoch total loss 7.03824139\n",
      "Trained batch 817 batch loss 7.1038146 epoch total loss 7.03832197\n",
      "Trained batch 818 batch loss 7.42269707 epoch total loss 7.03879213\n",
      "Trained batch 819 batch loss 7.40319967 epoch total loss 7.03923702\n",
      "Trained batch 820 batch loss 7.17725372 epoch total loss 7.03940535\n",
      "Trained batch 821 batch loss 7.11814785 epoch total loss 7.03950119\n",
      "Trained batch 822 batch loss 6.93506336 epoch total loss 7.03937435\n",
      "Trained batch 823 batch loss 7.11996889 epoch total loss 7.03947258\n",
      "Trained batch 824 batch loss 7.49955177 epoch total loss 7.04003096\n",
      "Trained batch 825 batch loss 7.40301609 epoch total loss 7.0404706\n",
      "Trained batch 826 batch loss 7.28799295 epoch total loss 7.04077053\n",
      "Trained batch 827 batch loss 7.22861671 epoch total loss 7.04099751\n",
      "Trained batch 828 batch loss 7.35542488 epoch total loss 7.04137707\n",
      "Trained batch 829 batch loss 7.45012712 epoch total loss 7.04187\n",
      "Trained batch 830 batch loss 7.27797937 epoch total loss 7.04215479\n",
      "Trained batch 831 batch loss 6.73339939 epoch total loss 7.04178286\n",
      "Trained batch 832 batch loss 6.88732243 epoch total loss 7.04159737\n",
      "Trained batch 833 batch loss 6.06956959 epoch total loss 7.04043\n",
      "Trained batch 834 batch loss 6.46427488 epoch total loss 7.03973913\n",
      "Trained batch 835 batch loss 6.2755146 epoch total loss 7.03882408\n",
      "Trained batch 836 batch loss 6.33069801 epoch total loss 7.03797674\n",
      "Trained batch 837 batch loss 5.92047405 epoch total loss 7.0366416\n",
      "Trained batch 838 batch loss 5.92479515 epoch total loss 7.03531456\n",
      "Trained batch 839 batch loss 6.00422192 epoch total loss 7.03408575\n",
      "Trained batch 840 batch loss 6.39556217 epoch total loss 7.03332567\n",
      "Trained batch 841 batch loss 6.7593708 epoch total loss 7.033\n",
      "Trained batch 842 batch loss 7.00642824 epoch total loss 7.03296804\n",
      "Trained batch 843 batch loss 7.30058479 epoch total loss 7.03328609\n",
      "Trained batch 844 batch loss 7.52831602 epoch total loss 7.0338726\n",
      "Trained batch 845 batch loss 7.34565496 epoch total loss 7.03424168\n",
      "Trained batch 846 batch loss 7.28366089 epoch total loss 7.03453636\n",
      "Trained batch 847 batch loss 6.8651557 epoch total loss 7.03433657\n",
      "Trained batch 848 batch loss 6.64356565 epoch total loss 7.03387547\n",
      "Trained batch 849 batch loss 6.5559783 epoch total loss 7.0333128\n",
      "Trained batch 850 batch loss 6.22186327 epoch total loss 7.03235817\n",
      "Trained batch 851 batch loss 6.31432343 epoch total loss 7.03151464\n",
      "Trained batch 852 batch loss 6.48947477 epoch total loss 7.03087807\n",
      "Trained batch 853 batch loss 6.91760683 epoch total loss 7.03074503\n",
      "Trained batch 854 batch loss 7.20187855 epoch total loss 7.0309453\n",
      "Trained batch 855 batch loss 7.07749128 epoch total loss 7.03099966\n",
      "Trained batch 856 batch loss 7.22399807 epoch total loss 7.03122568\n",
      "Trained batch 857 batch loss 7.39951801 epoch total loss 7.03165531\n",
      "Trained batch 858 batch loss 7.35921097 epoch total loss 7.03203726\n",
      "Trained batch 859 batch loss 7.29867172 epoch total loss 7.03234768\n",
      "Trained batch 860 batch loss 6.74283028 epoch total loss 7.03201103\n",
      "Trained batch 861 batch loss 6.28903437 epoch total loss 7.03114796\n",
      "Trained batch 862 batch loss 6.85291386 epoch total loss 7.03094149\n",
      "Trained batch 863 batch loss 7.0895071 epoch total loss 7.0310092\n",
      "Trained batch 864 batch loss 7.11582899 epoch total loss 7.03110695\n",
      "Trained batch 865 batch loss 7.24770212 epoch total loss 7.03135729\n",
      "Trained batch 866 batch loss 7.21705866 epoch total loss 7.03157187\n",
      "Trained batch 867 batch loss 7.36491394 epoch total loss 7.0319562\n",
      "Trained batch 868 batch loss 7.00538731 epoch total loss 7.03192568\n",
      "Trained batch 869 batch loss 7.09218693 epoch total loss 7.0319953\n",
      "Trained batch 870 batch loss 6.62110519 epoch total loss 7.03152275\n",
      "Trained batch 871 batch loss 7.24748802 epoch total loss 7.03177071\n",
      "Trained batch 872 batch loss 7.41682577 epoch total loss 7.03221273\n",
      "Trained batch 873 batch loss 7.36289549 epoch total loss 7.03259134\n",
      "Trained batch 874 batch loss 7.44394 epoch total loss 7.03306198\n",
      "Trained batch 875 batch loss 6.63273191 epoch total loss 7.03260422\n",
      "Trained batch 876 batch loss 6.61803627 epoch total loss 7.0321312\n",
      "Trained batch 877 batch loss 6.62289858 epoch total loss 7.03166485\n",
      "Trained batch 878 batch loss 7.15187407 epoch total loss 7.0318017\n",
      "Trained batch 879 batch loss 7.06052 epoch total loss 7.0318346\n",
      "Trained batch 880 batch loss 7.35003281 epoch total loss 7.03219604\n",
      "Trained batch 881 batch loss 7.17578697 epoch total loss 7.03235912\n",
      "Trained batch 882 batch loss 7.36242 epoch total loss 7.03273296\n",
      "Trained batch 883 batch loss 7.33039856 epoch total loss 7.03307056\n",
      "Trained batch 884 batch loss 7.32506227 epoch total loss 7.03340101\n",
      "Trained batch 885 batch loss 7.31107235 epoch total loss 7.03371477\n",
      "Trained batch 886 batch loss 7.44074965 epoch total loss 7.03417397\n",
      "Trained batch 887 batch loss 7.32897425 epoch total loss 7.0345068\n",
      "Trained batch 888 batch loss 7.32241774 epoch total loss 7.03483057\n",
      "Trained batch 889 batch loss 7.18394613 epoch total loss 7.03499842\n",
      "Trained batch 890 batch loss 7.39156485 epoch total loss 7.03539944\n",
      "Trained batch 891 batch loss 6.50873184 epoch total loss 7.03480816\n",
      "Trained batch 892 batch loss 6.72393179 epoch total loss 7.03446\n",
      "Trained batch 893 batch loss 7.10800886 epoch total loss 7.03454208\n",
      "Trained batch 894 batch loss 6.96687126 epoch total loss 7.03446627\n",
      "Trained batch 895 batch loss 6.65999889 epoch total loss 7.03404808\n",
      "Trained batch 896 batch loss 6.91783142 epoch total loss 7.03391886\n",
      "Trained batch 897 batch loss 6.87586546 epoch total loss 7.03374243\n",
      "Trained batch 898 batch loss 7.16445303 epoch total loss 7.03388834\n",
      "Trained batch 899 batch loss 7.25966 epoch total loss 7.03413963\n",
      "Trained batch 900 batch loss 7.13029575 epoch total loss 7.03424644\n",
      "Trained batch 901 batch loss 7.01082039 epoch total loss 7.03422\n",
      "Trained batch 902 batch loss 6.74250889 epoch total loss 7.03389692\n",
      "Trained batch 903 batch loss 7.06157875 epoch total loss 7.03392792\n",
      "Trained batch 904 batch loss 7.03544903 epoch total loss 7.03393\n",
      "Trained batch 905 batch loss 7.13761854 epoch total loss 7.03404427\n",
      "Trained batch 906 batch loss 7.2681222 epoch total loss 7.03430271\n",
      "Trained batch 907 batch loss 7.50769615 epoch total loss 7.03482485\n",
      "Trained batch 908 batch loss 7.00364208 epoch total loss 7.03479\n",
      "Trained batch 909 batch loss 6.8020668 epoch total loss 7.03453398\n",
      "Trained batch 910 batch loss 6.63873816 epoch total loss 7.0340991\n",
      "Trained batch 911 batch loss 7.1442256 epoch total loss 7.03421974\n",
      "Trained batch 912 batch loss 7.35844374 epoch total loss 7.03457546\n",
      "Trained batch 913 batch loss 7.1465106 epoch total loss 7.03469801\n",
      "Trained batch 914 batch loss 6.71950388 epoch total loss 7.03435326\n",
      "Trained batch 915 batch loss 6.93294716 epoch total loss 7.03424263\n",
      "Trained batch 916 batch loss 7.08457279 epoch total loss 7.03429747\n",
      "Trained batch 917 batch loss 6.9629612 epoch total loss 7.03421974\n",
      "Trained batch 918 batch loss 6.98429775 epoch total loss 7.03416538\n",
      "Trained batch 919 batch loss 6.59538603 epoch total loss 7.03368759\n",
      "Trained batch 920 batch loss 6.68603611 epoch total loss 7.03331\n",
      "Trained batch 921 batch loss 6.94104099 epoch total loss 7.03320932\n",
      "Trained batch 922 batch loss 7.03671646 epoch total loss 7.03321314\n",
      "Trained batch 923 batch loss 7.38203096 epoch total loss 7.03359079\n",
      "Trained batch 924 batch loss 7.29597187 epoch total loss 7.03387499\n",
      "Trained batch 925 batch loss 7.49401426 epoch total loss 7.03437233\n",
      "Trained batch 926 batch loss 7.41339636 epoch total loss 7.03478193\n",
      "Trained batch 927 batch loss 7.1932745 epoch total loss 7.03495312\n",
      "Trained batch 928 batch loss 6.94912529 epoch total loss 7.03486061\n",
      "Trained batch 929 batch loss 6.51121759 epoch total loss 7.03429699\n",
      "Trained batch 930 batch loss 5.93714905 epoch total loss 7.03311682\n",
      "Trained batch 931 batch loss 5.88765383 epoch total loss 7.03188658\n",
      "Trained batch 932 batch loss 6.25462389 epoch total loss 7.03105259\n",
      "Trained batch 933 batch loss 6.54779577 epoch total loss 7.03053474\n",
      "Trained batch 934 batch loss 6.98090744 epoch total loss 7.03048134\n",
      "Trained batch 935 batch loss 7.22768402 epoch total loss 7.0306921\n",
      "Trained batch 936 batch loss 7.38955069 epoch total loss 7.03107595\n",
      "Trained batch 937 batch loss 7.20996428 epoch total loss 7.03126669\n",
      "Trained batch 938 batch loss 7.45054674 epoch total loss 7.03171396\n",
      "Trained batch 939 batch loss 7.29279327 epoch total loss 7.03199196\n",
      "Trained batch 940 batch loss 6.66712284 epoch total loss 7.03160381\n",
      "Trained batch 941 batch loss 6.98423719 epoch total loss 7.03155375\n",
      "Trained batch 942 batch loss 7.34687567 epoch total loss 7.03188801\n",
      "Trained batch 943 batch loss 7.4933548 epoch total loss 7.03237724\n",
      "Trained batch 944 batch loss 7.26943779 epoch total loss 7.03262854\n",
      "Trained batch 945 batch loss 7.31735134 epoch total loss 7.03293\n",
      "Trained batch 946 batch loss 7.30848122 epoch total loss 7.03322124\n",
      "Trained batch 947 batch loss 7.39063787 epoch total loss 7.03359842\n",
      "Trained batch 948 batch loss 7.03380394 epoch total loss 7.0335989\n",
      "Trained batch 949 batch loss 7.36739254 epoch total loss 7.03395033\n",
      "Trained batch 950 batch loss 6.95417356 epoch total loss 7.03386593\n",
      "Trained batch 951 batch loss 7.09254074 epoch total loss 7.03392792\n",
      "Trained batch 952 batch loss 7.18592072 epoch total loss 7.03408766\n",
      "Trained batch 953 batch loss 6.30312252 epoch total loss 7.0333209\n",
      "Trained batch 954 batch loss 6.18154812 epoch total loss 7.03242826\n",
      "Trained batch 955 batch loss 6.51948 epoch total loss 7.03189135\n",
      "Trained batch 956 batch loss 6.86566067 epoch total loss 7.0317173\n",
      "Trained batch 957 batch loss 7.42217731 epoch total loss 7.03212547\n",
      "Trained batch 958 batch loss 6.7878809 epoch total loss 7.03187084\n",
      "Trained batch 959 batch loss 6.94900846 epoch total loss 7.03178453\n",
      "Trained batch 960 batch loss 7.39204597 epoch total loss 7.03216\n",
      "Trained batch 961 batch loss 7.06346178 epoch total loss 7.03219271\n",
      "Trained batch 962 batch loss 6.65012312 epoch total loss 7.03179502\n",
      "Trained batch 963 batch loss 6.89158392 epoch total loss 7.03164959\n",
      "Trained batch 964 batch loss 6.69472599 epoch total loss 7.0313\n",
      "Trained batch 965 batch loss 7.15176487 epoch total loss 7.031425\n",
      "Trained batch 966 batch loss 7.26508427 epoch total loss 7.03166723\n",
      "Trained batch 967 batch loss 7.24709606 epoch total loss 7.03189\n",
      "Trained batch 968 batch loss 7.35274887 epoch total loss 7.03222084\n",
      "Trained batch 969 batch loss 7.4241991 epoch total loss 7.03262568\n",
      "Trained batch 970 batch loss 7.27416277 epoch total loss 7.03287458\n",
      "Trained batch 971 batch loss 7.22531509 epoch total loss 7.03307247\n",
      "Trained batch 972 batch loss 7.26895952 epoch total loss 7.03331518\n",
      "Trained batch 973 batch loss 7.2396245 epoch total loss 7.03352737\n",
      "Trained batch 974 batch loss 6.89569902 epoch total loss 7.03338575\n",
      "Trained batch 975 batch loss 7.02849579 epoch total loss 7.03338051\n",
      "Trained batch 976 batch loss 7.28059673 epoch total loss 7.03363371\n",
      "Trained batch 977 batch loss 7.14938974 epoch total loss 7.03375244\n",
      "Trained batch 978 batch loss 6.78808498 epoch total loss 7.03350115\n",
      "Trained batch 979 batch loss 7.21236801 epoch total loss 7.03368378\n",
      "Trained batch 980 batch loss 6.46666431 epoch total loss 7.03310537\n",
      "Trained batch 981 batch loss 7.26634645 epoch total loss 7.03334284\n",
      "Trained batch 982 batch loss 7.39328671 epoch total loss 7.03370953\n",
      "Trained batch 983 batch loss 7.07430029 epoch total loss 7.03375053\n",
      "Trained batch 984 batch loss 7.36029482 epoch total loss 7.03408241\n",
      "Trained batch 985 batch loss 7.00788784 epoch total loss 7.03405571\n",
      "Trained batch 986 batch loss 7.04949379 epoch total loss 7.03407145\n",
      "Trained batch 987 batch loss 7.09574604 epoch total loss 7.03413391\n",
      "Trained batch 988 batch loss 6.85674334 epoch total loss 7.03395414\n",
      "Trained batch 989 batch loss 7.06014156 epoch total loss 7.03398085\n",
      "Trained batch 990 batch loss 7.08865833 epoch total loss 7.03403616\n",
      "Trained batch 991 batch loss 7.09968662 epoch total loss 7.03410244\n",
      "Trained batch 992 batch loss 7.38402271 epoch total loss 7.03445482\n",
      "Trained batch 993 batch loss 7.30052 epoch total loss 7.03472233\n",
      "Trained batch 994 batch loss 7.36490822 epoch total loss 7.03505468\n",
      "Trained batch 995 batch loss 6.62338543 epoch total loss 7.03464079\n",
      "Trained batch 996 batch loss 6.94894028 epoch total loss 7.03455496\n",
      "Trained batch 997 batch loss 6.98572254 epoch total loss 7.03450584\n",
      "Trained batch 998 batch loss 6.33417511 epoch total loss 7.03380394\n",
      "Trained batch 999 batch loss 5.69838333 epoch total loss 7.03246689\n",
      "Trained batch 1000 batch loss 6.04295111 epoch total loss 7.03147745\n",
      "Trained batch 1001 batch loss 6.48020506 epoch total loss 7.0309267\n",
      "Trained batch 1002 batch loss 7.04093504 epoch total loss 7.03093672\n",
      "Trained batch 1003 batch loss 7.19252491 epoch total loss 7.03109741\n",
      "Trained batch 1004 batch loss 7.03929281 epoch total loss 7.03110552\n",
      "Trained batch 1005 batch loss 6.44707298 epoch total loss 7.03052473\n",
      "Trained batch 1006 batch loss 6.94468975 epoch total loss 7.03043938\n",
      "Trained batch 1007 batch loss 6.99110079 epoch total loss 7.03040028\n",
      "Trained batch 1008 batch loss 7.53581047 epoch total loss 7.03090191\n",
      "Trained batch 1009 batch loss 7.41947746 epoch total loss 7.03128672\n",
      "Trained batch 1010 batch loss 7.363976 epoch total loss 7.03161573\n",
      "Trained batch 1011 batch loss 7.50065184 epoch total loss 7.0320797\n",
      "Trained batch 1012 batch loss 7.11255741 epoch total loss 7.03215933\n",
      "Trained batch 1013 batch loss 7.32964563 epoch total loss 7.03245306\n",
      "Trained batch 1014 batch loss 7.38246822 epoch total loss 7.03279829\n",
      "Trained batch 1015 batch loss 7.10527372 epoch total loss 7.03287\n",
      "Trained batch 1016 batch loss 7.25741291 epoch total loss 7.03309059\n",
      "Trained batch 1017 batch loss 6.95426273 epoch total loss 7.03301287\n",
      "Trained batch 1018 batch loss 7.10583544 epoch total loss 7.03308487\n",
      "Trained batch 1019 batch loss 5.92217159 epoch total loss 7.03199482\n",
      "Trained batch 1020 batch loss 5.52065802 epoch total loss 7.03051281\n",
      "Trained batch 1021 batch loss 5.98311329 epoch total loss 7.02948666\n",
      "Trained batch 1022 batch loss 6.34205532 epoch total loss 7.02881432\n",
      "Trained batch 1023 batch loss 6.71858 epoch total loss 7.02851105\n",
      "Trained batch 1024 batch loss 7.17036867 epoch total loss 7.02865\n",
      "Trained batch 1025 batch loss 6.96205378 epoch total loss 7.02858448\n",
      "Trained batch 1026 batch loss 7.31213093 epoch total loss 7.02886105\n",
      "Trained batch 1027 batch loss 7.05602741 epoch total loss 7.02888775\n",
      "Trained batch 1028 batch loss 7.11748648 epoch total loss 7.02897406\n",
      "Trained batch 1029 batch loss 6.92622662 epoch total loss 7.02887392\n",
      "Trained batch 1030 batch loss 6.92363691 epoch total loss 7.02877188\n",
      "Trained batch 1031 batch loss 6.64314222 epoch total loss 7.02839804\n",
      "Trained batch 1032 batch loss 6.75630045 epoch total loss 7.02813435\n",
      "Trained batch 1033 batch loss 7.07377768 epoch total loss 7.02817869\n",
      "Trained batch 1034 batch loss 6.82352972 epoch total loss 7.0279808\n",
      "Trained batch 1035 batch loss 6.85225391 epoch total loss 7.02781057\n",
      "Trained batch 1036 batch loss 7.14558697 epoch total loss 7.02792454\n",
      "Trained batch 1037 batch loss 7.05047321 epoch total loss 7.027946\n",
      "Trained batch 1038 batch loss 6.85012722 epoch total loss 7.02777481\n",
      "Trained batch 1039 batch loss 6.48029613 epoch total loss 7.02724791\n",
      "Trained batch 1040 batch loss 6.27214384 epoch total loss 7.02652168\n",
      "Trained batch 1041 batch loss 6.71762133 epoch total loss 7.02622509\n",
      "Trained batch 1042 batch loss 6.87714291 epoch total loss 7.02608204\n",
      "Trained batch 1043 batch loss 7.00848198 epoch total loss 7.02606487\n",
      "Trained batch 1044 batch loss 6.95906544 epoch total loss 7.0260005\n",
      "Trained batch 1045 batch loss 7.06211281 epoch total loss 7.02603483\n",
      "Trained batch 1046 batch loss 6.89395714 epoch total loss 7.02590895\n",
      "Trained batch 1047 batch loss 6.41778326 epoch total loss 7.02532816\n",
      "Trained batch 1048 batch loss 6.73040104 epoch total loss 7.02504683\n",
      "Trained batch 1049 batch loss 7.03470182 epoch total loss 7.02505589\n",
      "Trained batch 1050 batch loss 7.01077175 epoch total loss 7.02504253\n",
      "Trained batch 1051 batch loss 6.98151255 epoch total loss 7.02500105\n",
      "Trained batch 1052 batch loss 7.18769693 epoch total loss 7.02515507\n",
      "Trained batch 1053 batch loss 7.26257086 epoch total loss 7.02538109\n",
      "Trained batch 1054 batch loss 7.14003372 epoch total loss 7.02549\n",
      "Trained batch 1055 batch loss 7.30546236 epoch total loss 7.02575541\n",
      "Trained batch 1056 batch loss 6.74327707 epoch total loss 7.0254879\n",
      "Trained batch 1057 batch loss 6.86628675 epoch total loss 7.02533722\n",
      "Trained batch 1058 batch loss 6.52719307 epoch total loss 7.02486658\n",
      "Trained batch 1059 batch loss 6.7271018 epoch total loss 7.02458525\n",
      "Trained batch 1060 batch loss 7.23966169 epoch total loss 7.0247879\n",
      "Trained batch 1061 batch loss 7.2781682 epoch total loss 7.02502728\n",
      "Trained batch 1062 batch loss 7.13093662 epoch total loss 7.02512693\n",
      "Trained batch 1063 batch loss 6.72549772 epoch total loss 7.02484512\n",
      "Trained batch 1064 batch loss 6.21237326 epoch total loss 7.02408123\n",
      "Trained batch 1065 batch loss 6.68114758 epoch total loss 7.02375937\n",
      "Trained batch 1066 batch loss 7.07391691 epoch total loss 7.0238061\n",
      "Trained batch 1067 batch loss 6.98080921 epoch total loss 7.02376604\n",
      "Trained batch 1068 batch loss 7.20352316 epoch total loss 7.02393436\n",
      "Trained batch 1069 batch loss 7.34324408 epoch total loss 7.02423334\n",
      "Trained batch 1070 batch loss 7.40859079 epoch total loss 7.0245924\n",
      "Trained batch 1071 batch loss 7.1292944 epoch total loss 7.02469\n",
      "Trained batch 1072 batch loss 7.23335934 epoch total loss 7.02488518\n",
      "Trained batch 1073 batch loss 7.30677938 epoch total loss 7.02514744\n",
      "Trained batch 1074 batch loss 7.33019352 epoch total loss 7.02543163\n",
      "Trained batch 1075 batch loss 7.23706293 epoch total loss 7.02562857\n",
      "Trained batch 1076 batch loss 7.3192749 epoch total loss 7.02590179\n",
      "Trained batch 1077 batch loss 7.38851118 epoch total loss 7.02623844\n",
      "Trained batch 1078 batch loss 7.35652399 epoch total loss 7.02654457\n",
      "Trained batch 1079 batch loss 7.30227661 epoch total loss 7.0268\n",
      "Trained batch 1080 batch loss 7.22466373 epoch total loss 7.02698326\n",
      "Trained batch 1081 batch loss 7.21445179 epoch total loss 7.02715683\n",
      "Trained batch 1082 batch loss 7.22068 epoch total loss 7.02733564\n",
      "Trained batch 1083 batch loss 7.39315796 epoch total loss 7.02767324\n",
      "Trained batch 1084 batch loss 7.19979191 epoch total loss 7.02783203\n",
      "Trained batch 1085 batch loss 6.72179699 epoch total loss 7.02754974\n",
      "Trained batch 1086 batch loss 6.09862661 epoch total loss 7.0266943\n",
      "Trained batch 1087 batch loss 6.99610519 epoch total loss 7.02666616\n",
      "Trained batch 1088 batch loss 7.08917522 epoch total loss 7.02672386\n",
      "Trained batch 1089 batch loss 7.24068117 epoch total loss 7.02692032\n",
      "Trained batch 1090 batch loss 7.19646358 epoch total loss 7.02707577\n",
      "Trained batch 1091 batch loss 7.33454609 epoch total loss 7.02735758\n",
      "Trained batch 1092 batch loss 7.42982721 epoch total loss 7.02772617\n",
      "Trained batch 1093 batch loss 7.2686615 epoch total loss 7.02794647\n",
      "Trained batch 1094 batch loss 7.35319901 epoch total loss 7.02824354\n",
      "Trained batch 1095 batch loss 7.46815348 epoch total loss 7.02864552\n",
      "Trained batch 1096 batch loss 7.46895695 epoch total loss 7.02904701\n",
      "Trained batch 1097 batch loss 7.08208084 epoch total loss 7.02909517\n",
      "Trained batch 1098 batch loss 7.31729317 epoch total loss 7.02935791\n",
      "Trained batch 1099 batch loss 6.41166 epoch total loss 7.02879572\n",
      "Trained batch 1100 batch loss 6.56318712 epoch total loss 7.02837229\n",
      "Trained batch 1101 batch loss 7.02342558 epoch total loss 7.028368\n",
      "Trained batch 1102 batch loss 7.0046196 epoch total loss 7.02834606\n",
      "Trained batch 1103 batch loss 6.66552305 epoch total loss 7.02801704\n",
      "Trained batch 1104 batch loss 6.86203432 epoch total loss 7.02786636\n",
      "Trained batch 1105 batch loss 6.91587925 epoch total loss 7.02776527\n",
      "Trained batch 1106 batch loss 6.80585575 epoch total loss 7.02756453\n",
      "Trained batch 1107 batch loss 7.13015223 epoch total loss 7.02765751\n",
      "Trained batch 1108 batch loss 6.794662 epoch total loss 7.02744675\n",
      "Trained batch 1109 batch loss 7.19113 epoch total loss 7.02759409\n",
      "Trained batch 1110 batch loss 6.57064438 epoch total loss 7.02718258\n",
      "Trained batch 1111 batch loss 7.20859623 epoch total loss 7.02734613\n",
      "Trained batch 1112 batch loss 7.24652243 epoch total loss 7.02754307\n",
      "Trained batch 1113 batch loss 6.72950029 epoch total loss 7.02727509\n",
      "Trained batch 1114 batch loss 6.65025759 epoch total loss 7.02693701\n",
      "Trained batch 1115 batch loss 7.06308 epoch total loss 7.02696943\n",
      "Trained batch 1116 batch loss 6.80721092 epoch total loss 7.0267725\n",
      "Trained batch 1117 batch loss 6.84686661 epoch total loss 7.02661133\n",
      "Trained batch 1118 batch loss 7.06492758 epoch total loss 7.02664518\n",
      "Trained batch 1119 batch loss 7.11863565 epoch total loss 7.02672768\n",
      "Trained batch 1120 batch loss 7.45232105 epoch total loss 7.02710724\n",
      "Trained batch 1121 batch loss 7.45137835 epoch total loss 7.02748585\n",
      "Trained batch 1122 batch loss 7.11610508 epoch total loss 7.027565\n",
      "Trained batch 1123 batch loss 7.34656525 epoch total loss 7.0278492\n",
      "Trained batch 1124 batch loss 6.89221239 epoch total loss 7.02772808\n",
      "Trained batch 1125 batch loss 7.28324127 epoch total loss 7.02795553\n",
      "Trained batch 1126 batch loss 6.78731823 epoch total loss 7.02774143\n",
      "Trained batch 1127 batch loss 6.69710541 epoch total loss 7.02744818\n",
      "Trained batch 1128 batch loss 6.88048077 epoch total loss 7.027318\n",
      "Trained batch 1129 batch loss 7.17963886 epoch total loss 7.02745295\n",
      "Trained batch 1130 batch loss 6.73045349 epoch total loss 7.02718973\n",
      "Trained batch 1131 batch loss 6.99774742 epoch total loss 7.02716351\n",
      "Trained batch 1132 batch loss 7.09537506 epoch total loss 7.02722406\n",
      "Trained batch 1133 batch loss 7.24217701 epoch total loss 7.02741337\n",
      "Trained batch 1134 batch loss 7.17624474 epoch total loss 7.02754498\n",
      "Trained batch 1135 batch loss 7.13551331 epoch total loss 7.02764034\n",
      "Trained batch 1136 batch loss 6.88665819 epoch total loss 7.02751589\n",
      "Trained batch 1137 batch loss 6.69345188 epoch total loss 7.02722216\n",
      "Trained batch 1138 batch loss 6.84873533 epoch total loss 7.02706528\n",
      "Trained batch 1139 batch loss 6.99866295 epoch total loss 7.02704\n",
      "Trained batch 1140 batch loss 7.05119038 epoch total loss 7.02706146\n",
      "Trained batch 1141 batch loss 6.79624033 epoch total loss 7.02685928\n",
      "Trained batch 1142 batch loss 6.96919155 epoch total loss 7.02680874\n",
      "Trained batch 1143 batch loss 7.16597366 epoch total loss 7.02693081\n",
      "Trained batch 1144 batch loss 6.98996925 epoch total loss 7.02689838\n",
      "Trained batch 1145 batch loss 6.56027555 epoch total loss 7.02649\n",
      "Trained batch 1146 batch loss 6.60754204 epoch total loss 7.02612495\n",
      "Trained batch 1147 batch loss 7.23817062 epoch total loss 7.02630949\n",
      "Trained batch 1148 batch loss 7.3798542 epoch total loss 7.02661753\n",
      "Trained batch 1149 batch loss 7.30419302 epoch total loss 7.02685928\n",
      "Trained batch 1150 batch loss 7.44999361 epoch total loss 7.0272274\n",
      "Trained batch 1151 batch loss 7.28741264 epoch total loss 7.02745342\n",
      "Trained batch 1152 batch loss 7.02381229 epoch total loss 7.02745056\n",
      "Trained batch 1153 batch loss 6.55194902 epoch total loss 7.0270381\n",
      "Trained batch 1154 batch loss 6.87674952 epoch total loss 7.02690792\n",
      "Trained batch 1155 batch loss 6.3370676 epoch total loss 7.02631044\n",
      "Trained batch 1156 batch loss 6.8377285 epoch total loss 7.02614737\n",
      "Trained batch 1157 batch loss 6.58910322 epoch total loss 7.02576971\n",
      "Trained batch 1158 batch loss 6.83106 epoch total loss 7.02560139\n",
      "Trained batch 1159 batch loss 6.62430906 epoch total loss 7.0252552\n",
      "Trained batch 1160 batch loss 6.27915907 epoch total loss 7.02461243\n",
      "Trained batch 1161 batch loss 6.62453222 epoch total loss 7.02426767\n",
      "Trained batch 1162 batch loss 6.3444972 epoch total loss 7.02368307\n",
      "Trained batch 1163 batch loss 6.71170378 epoch total loss 7.02341461\n",
      "Trained batch 1164 batch loss 6.93783379 epoch total loss 7.02334166\n",
      "Trained batch 1165 batch loss 7.1047163 epoch total loss 7.02341127\n",
      "Trained batch 1166 batch loss 6.95322466 epoch total loss 7.02335072\n",
      "Trained batch 1167 batch loss 7.33192635 epoch total loss 7.02361488\n",
      "Trained batch 1168 batch loss 7.35745335 epoch total loss 7.02390051\n",
      "Trained batch 1169 batch loss 7.52108431 epoch total loss 7.02432632\n",
      "Trained batch 1170 batch loss 7.42385912 epoch total loss 7.02466774\n",
      "Trained batch 1171 batch loss 6.84133244 epoch total loss 7.02451181\n",
      "Trained batch 1172 batch loss 7.11005783 epoch total loss 7.02458477\n",
      "Trained batch 1173 batch loss 6.32153559 epoch total loss 7.02398539\n",
      "Trained batch 1174 batch loss 7.47606373 epoch total loss 7.02437\n",
      "Trained batch 1175 batch loss 7.42578888 epoch total loss 7.02471161\n",
      "Trained batch 1176 batch loss 6.35222197 epoch total loss 7.02414\n",
      "Trained batch 1177 batch loss 6.39155149 epoch total loss 7.02360249\n",
      "Trained batch 1178 batch loss 6.60033131 epoch total loss 7.02324343\n",
      "Trained batch 1179 batch loss 7.31440639 epoch total loss 7.02349043\n",
      "Trained batch 1180 batch loss 7.38823271 epoch total loss 7.0238\n",
      "Trained batch 1181 batch loss 7.19585085 epoch total loss 7.02394581\n",
      "Trained batch 1182 batch loss 7.33855057 epoch total loss 7.02421236\n",
      "Trained batch 1183 batch loss 7.28280735 epoch total loss 7.02443123\n",
      "Trained batch 1184 batch loss 7.25302601 epoch total loss 7.02462435\n",
      "Trained batch 1185 batch loss 7.22165728 epoch total loss 7.02479076\n",
      "Trained batch 1186 batch loss 7.01540709 epoch total loss 7.02478313\n",
      "Trained batch 1187 batch loss 6.88974953 epoch total loss 7.02466917\n",
      "Trained batch 1188 batch loss 6.91266489 epoch total loss 7.02457523\n",
      "Trained batch 1189 batch loss 7.19221592 epoch total loss 7.02471638\n",
      "Trained batch 1190 batch loss 7.16465378 epoch total loss 7.02483416\n",
      "Trained batch 1191 batch loss 6.93131351 epoch total loss 7.02475595\n",
      "Trained batch 1192 batch loss 6.82369137 epoch total loss 7.02458715\n",
      "Trained batch 1193 batch loss 6.22839308 epoch total loss 7.02391958\n",
      "Trained batch 1194 batch loss 6.64304066 epoch total loss 7.0236\n",
      "Trained batch 1195 batch loss 7.29530478 epoch total loss 7.02382708\n",
      "Trained batch 1196 batch loss 6.65886259 epoch total loss 7.02352238\n",
      "Trained batch 1197 batch loss 6.96524143 epoch total loss 7.02347326\n",
      "Trained batch 1198 batch loss 7.14049673 epoch total loss 7.02357101\n",
      "Trained batch 1199 batch loss 7.36043215 epoch total loss 7.02385187\n",
      "Trained batch 1200 batch loss 6.91800642 epoch total loss 7.02376366\n",
      "Trained batch 1201 batch loss 7.45162725 epoch total loss 7.02412\n",
      "Trained batch 1202 batch loss 7.27603054 epoch total loss 7.02432966\n",
      "Trained batch 1203 batch loss 6.94796419 epoch total loss 7.02426624\n",
      "Trained batch 1204 batch loss 6.77890873 epoch total loss 7.02406263\n",
      "Trained batch 1205 batch loss 6.75579262 epoch total loss 7.02384043\n",
      "Trained batch 1206 batch loss 6.82106352 epoch total loss 7.0236721\n",
      "Trained batch 1207 batch loss 6.97791481 epoch total loss 7.02363396\n",
      "Trained batch 1208 batch loss 7.34777355 epoch total loss 7.02390242\n",
      "Trained batch 1209 batch loss 7.17914534 epoch total loss 7.02403\n",
      "Trained batch 1210 batch loss 6.70605135 epoch total loss 7.02376747\n",
      "Trained batch 1211 batch loss 7.1480279 epoch total loss 7.02387047\n",
      "Trained batch 1212 batch loss 6.92560244 epoch total loss 7.02378941\n",
      "Trained batch 1213 batch loss 7.45229244 epoch total loss 7.02414274\n",
      "Trained batch 1214 batch loss 7.03214073 epoch total loss 7.02414942\n",
      "Trained batch 1215 batch loss 6.42661572 epoch total loss 7.0236578\n",
      "Trained batch 1216 batch loss 6.84081268 epoch total loss 7.0235076\n",
      "Trained batch 1217 batch loss 6.32612658 epoch total loss 7.02293444\n",
      "Trained batch 1218 batch loss 6.07125044 epoch total loss 7.0221529\n",
      "Trained batch 1219 batch loss 5.99810266 epoch total loss 7.02131271\n",
      "Trained batch 1220 batch loss 6.44547129 epoch total loss 7.02084064\n",
      "Trained batch 1221 batch loss 6.6393609 epoch total loss 7.02052879\n",
      "Trained batch 1222 batch loss 6.91386414 epoch total loss 7.02044153\n",
      "Trained batch 1223 batch loss 7.54344082 epoch total loss 7.02086878\n",
      "Trained batch 1224 batch loss 7.22067595 epoch total loss 7.02103186\n",
      "Trained batch 1225 batch loss 7.29670906 epoch total loss 7.0212574\n",
      "Trained batch 1226 batch loss 7.39249563 epoch total loss 7.02156\n",
      "Trained batch 1227 batch loss 7.43149 epoch total loss 7.02189445\n",
      "Trained batch 1228 batch loss 7.3076849 epoch total loss 7.02212715\n",
      "Trained batch 1229 batch loss 7.44242859 epoch total loss 7.02246904\n",
      "Trained batch 1230 batch loss 7.41378689 epoch total loss 7.02278709\n",
      "Trained batch 1231 batch loss 7.25314951 epoch total loss 7.02297401\n",
      "Trained batch 1232 batch loss 7.08190298 epoch total loss 7.02302217\n",
      "Trained batch 1233 batch loss 7.15121937 epoch total loss 7.02312613\n",
      "Trained batch 1234 batch loss 6.83654 epoch total loss 7.02297544\n",
      "Trained batch 1235 batch loss 6.51546621 epoch total loss 7.02256441\n",
      "Trained batch 1236 batch loss 6.62029123 epoch total loss 7.02223873\n",
      "Trained batch 1237 batch loss 7.01970387 epoch total loss 7.02223682\n",
      "Trained batch 1238 batch loss 7.1548481 epoch total loss 7.02234411\n",
      "Trained batch 1239 batch loss 7.41813087 epoch total loss 7.02266359\n",
      "Trained batch 1240 batch loss 7.33967161 epoch total loss 7.02291918\n",
      "Trained batch 1241 batch loss 7.29456139 epoch total loss 7.02313852\n",
      "Trained batch 1242 batch loss 7.43741512 epoch total loss 7.02347231\n",
      "Trained batch 1243 batch loss 6.81381607 epoch total loss 7.02330303\n",
      "Trained batch 1244 batch loss 6.67912674 epoch total loss 7.02302599\n",
      "Trained batch 1245 batch loss 7.16588 epoch total loss 7.02314091\n",
      "Trained batch 1246 batch loss 7.06957769 epoch total loss 7.0231781\n",
      "Trained batch 1247 batch loss 6.86920071 epoch total loss 7.0230546\n",
      "Trained batch 1248 batch loss 6.78616142 epoch total loss 7.02286482\n",
      "Trained batch 1249 batch loss 6.84558487 epoch total loss 7.02272272\n",
      "Trained batch 1250 batch loss 7.14086485 epoch total loss 7.02281713\n",
      "Trained batch 1251 batch loss 7.0990386 epoch total loss 7.02287769\n",
      "Trained batch 1252 batch loss 7.28122234 epoch total loss 7.02308416\n",
      "Trained batch 1253 batch loss 7.21062851 epoch total loss 7.02323389\n",
      "Trained batch 1254 batch loss 6.61978579 epoch total loss 7.0229125\n",
      "Trained batch 1255 batch loss 7.26887894 epoch total loss 7.02310848\n",
      "Trained batch 1256 batch loss 7.10138369 epoch total loss 7.02317095\n",
      "Trained batch 1257 batch loss 6.99134111 epoch total loss 7.0231452\n",
      "Trained batch 1258 batch loss 7.02017975 epoch total loss 7.02314329\n",
      "Trained batch 1259 batch loss 6.88112307 epoch total loss 7.02303028\n",
      "Trained batch 1260 batch loss 7.00234032 epoch total loss 7.02301359\n",
      "Trained batch 1261 batch loss 7.03903818 epoch total loss 7.02302647\n",
      "Trained batch 1262 batch loss 7.41099882 epoch total loss 7.02333403\n",
      "Trained batch 1263 batch loss 7.23938704 epoch total loss 7.02350473\n",
      "Trained batch 1264 batch loss 6.9432106 epoch total loss 7.02344131\n",
      "Trained batch 1265 batch loss 7.21571589 epoch total loss 7.02359343\n",
      "Trained batch 1266 batch loss 7.16312504 epoch total loss 7.02370358\n",
      "Trained batch 1267 batch loss 6.80754423 epoch total loss 7.02353287\n",
      "Trained batch 1268 batch loss 7.32580423 epoch total loss 7.02377176\n",
      "Trained batch 1269 batch loss 7.38717079 epoch total loss 7.02405787\n",
      "Trained batch 1270 batch loss 6.66193 epoch total loss 7.02377272\n",
      "Trained batch 1271 batch loss 7.0677228 epoch total loss 7.02380705\n",
      "Trained batch 1272 batch loss 7.37420225 epoch total loss 7.02408218\n",
      "Trained batch 1273 batch loss 7.53421497 epoch total loss 7.0244832\n",
      "Trained batch 1274 batch loss 7.48871469 epoch total loss 7.02484703\n",
      "Trained batch 1275 batch loss 7.34635115 epoch total loss 7.02509975\n",
      "Trained batch 1276 batch loss 7.15763044 epoch total loss 7.02520323\n",
      "Trained batch 1277 batch loss 7.1119523 epoch total loss 7.02527142\n",
      "Trained batch 1278 batch loss 7.07804871 epoch total loss 7.0253129\n",
      "Trained batch 1279 batch loss 7.10237837 epoch total loss 7.02537298\n",
      "Trained batch 1280 batch loss 7.07997227 epoch total loss 7.0254159\n",
      "Trained batch 1281 batch loss 7.08386755 epoch total loss 7.02546167\n",
      "Trained batch 1282 batch loss 7.20136118 epoch total loss 7.02559853\n",
      "Trained batch 1283 batch loss 7.00522757 epoch total loss 7.02558231\n",
      "Trained batch 1284 batch loss 7.54549837 epoch total loss 7.02598763\n",
      "Trained batch 1285 batch loss 7.36523581 epoch total loss 7.02625179\n",
      "Trained batch 1286 batch loss 7.13325834 epoch total loss 7.02633476\n",
      "Trained batch 1287 batch loss 7.14993954 epoch total loss 7.02643108\n",
      "Trained batch 1288 batch loss 6.82070398 epoch total loss 7.02627087\n",
      "Trained batch 1289 batch loss 6.40098906 epoch total loss 7.02578592\n",
      "Trained batch 1290 batch loss 6.91413212 epoch total loss 7.02569962\n",
      "Trained batch 1291 batch loss 7.26597738 epoch total loss 7.02588511\n",
      "Trained batch 1292 batch loss 7.05737972 epoch total loss 7.02591\n",
      "Trained batch 1293 batch loss 6.81874132 epoch total loss 7.02574921\n",
      "Trained batch 1294 batch loss 6.863904 epoch total loss 7.02562475\n",
      "Trained batch 1295 batch loss 6.664 epoch total loss 7.02534533\n",
      "Trained batch 1296 batch loss 7.12787867 epoch total loss 7.02542448\n",
      "Trained batch 1297 batch loss 6.74821806 epoch total loss 7.02521086\n",
      "Trained batch 1298 batch loss 6.56492758 epoch total loss 7.02485561\n",
      "Trained batch 1299 batch loss 6.87120295 epoch total loss 7.02473736\n",
      "Trained batch 1300 batch loss 7.17487526 epoch total loss 7.02485275\n",
      "Trained batch 1301 batch loss 7.01321507 epoch total loss 7.02484417\n",
      "Trained batch 1302 batch loss 6.80409145 epoch total loss 7.02467442\n",
      "Trained batch 1303 batch loss 7.28729296 epoch total loss 7.02487564\n",
      "Trained batch 1304 batch loss 7.47720051 epoch total loss 7.02522278\n",
      "Trained batch 1305 batch loss 7.21095705 epoch total loss 7.02536535\n",
      "Trained batch 1306 batch loss 7.67633343 epoch total loss 7.02586412\n",
      "Trained batch 1307 batch loss 7.43993664 epoch total loss 7.02618027\n",
      "Trained batch 1308 batch loss 7.31557894 epoch total loss 7.02640152\n",
      "Trained batch 1309 batch loss 6.7960825 epoch total loss 7.02622557\n",
      "Trained batch 1310 batch loss 6.78717232 epoch total loss 7.02604294\n",
      "Trained batch 1311 batch loss 7.27143764 epoch total loss 7.02623034\n",
      "Trained batch 1312 batch loss 7.2116313 epoch total loss 7.02637148\n",
      "Trained batch 1313 batch loss 7.1484251 epoch total loss 7.02646446\n",
      "Trained batch 1314 batch loss 7.54910469 epoch total loss 7.02686214\n",
      "Trained batch 1315 batch loss 7.3868413 epoch total loss 7.02713585\n",
      "Trained batch 1316 batch loss 7.30635262 epoch total loss 7.02734804\n",
      "Trained batch 1317 batch loss 7.45802927 epoch total loss 7.02767515\n",
      "Trained batch 1318 batch loss 7.46333647 epoch total loss 7.0280056\n",
      "Trained batch 1319 batch loss 7.55060911 epoch total loss 7.02840185\n",
      "Trained batch 1320 batch loss 7.57492447 epoch total loss 7.02881622\n",
      "Trained batch 1321 batch loss 7.25847721 epoch total loss 7.02899027\n",
      "Trained batch 1322 batch loss 7.23366976 epoch total loss 7.02914476\n",
      "Trained batch 1323 batch loss 6.9954834 epoch total loss 7.02911901\n",
      "Trained batch 1324 batch loss 6.54486036 epoch total loss 7.02875328\n",
      "Trained batch 1325 batch loss 6.89535332 epoch total loss 7.02865267\n",
      "Trained batch 1326 batch loss 7.00348473 epoch total loss 7.02863407\n",
      "Trained batch 1327 batch loss 6.63933086 epoch total loss 7.02834082\n",
      "Trained batch 1328 batch loss 6.8170104 epoch total loss 7.02818203\n",
      "Trained batch 1329 batch loss 7.00810862 epoch total loss 7.02816677\n",
      "Trained batch 1330 batch loss 6.73252153 epoch total loss 7.02794456\n",
      "Trained batch 1331 batch loss 7.34544373 epoch total loss 7.02818298\n",
      "Trained batch 1332 batch loss 6.82122469 epoch total loss 7.02802801\n",
      "Trained batch 1333 batch loss 6.04451132 epoch total loss 7.02729034\n",
      "Trained batch 1334 batch loss 6.10273314 epoch total loss 7.02659702\n",
      "Trained batch 1335 batch loss 6.28696442 epoch total loss 7.02604294\n",
      "Trained batch 1336 batch loss 6.90913677 epoch total loss 7.02595568\n",
      "Trained batch 1337 batch loss 6.50116777 epoch total loss 7.02556324\n",
      "Trained batch 1338 batch loss 6.95947838 epoch total loss 7.02551413\n",
      "Trained batch 1339 batch loss 7.28858423 epoch total loss 7.02571058\n",
      "Trained batch 1340 batch loss 7.12630272 epoch total loss 7.02578545\n",
      "Trained batch 1341 batch loss 6.53732586 epoch total loss 7.02542114\n",
      "Trained batch 1342 batch loss 6.95742607 epoch total loss 7.02537\n",
      "Trained batch 1343 batch loss 7.16101456 epoch total loss 7.02547121\n",
      "Trained batch 1344 batch loss 7.36387682 epoch total loss 7.02572346\n",
      "Trained batch 1345 batch loss 7.06539059 epoch total loss 7.02575302\n",
      "Trained batch 1346 batch loss 6.91473579 epoch total loss 7.02567053\n",
      "Trained batch 1347 batch loss 7.07048082 epoch total loss 7.02570391\n",
      "Trained batch 1348 batch loss 7.09977436 epoch total loss 7.02575874\n",
      "Trained batch 1349 batch loss 6.77426815 epoch total loss 7.0255723\n",
      "Trained batch 1350 batch loss 6.83070612 epoch total loss 7.0254283\n",
      "Trained batch 1351 batch loss 7.01048136 epoch total loss 7.02541733\n",
      "Trained batch 1352 batch loss 6.48854637 epoch total loss 7.02502\n",
      "Trained batch 1353 batch loss 6.47228432 epoch total loss 7.02461195\n",
      "Trained batch 1354 batch loss 6.83781958 epoch total loss 7.02447414\n",
      "Trained batch 1355 batch loss 6.52219152 epoch total loss 7.02410364\n",
      "Trained batch 1356 batch loss 6.99045181 epoch total loss 7.02407837\n",
      "Trained batch 1357 batch loss 6.97847271 epoch total loss 7.02404499\n",
      "Trained batch 1358 batch loss 7.11700487 epoch total loss 7.02411366\n",
      "Trained batch 1359 batch loss 7.20834684 epoch total loss 7.0242486\n",
      "Trained batch 1360 batch loss 6.86452484 epoch total loss 7.0241313\n",
      "Trained batch 1361 batch loss 6.52827692 epoch total loss 7.02376699\n",
      "Trained batch 1362 batch loss 7.28478098 epoch total loss 7.02395868\n",
      "Trained batch 1363 batch loss 7.0929718 epoch total loss 7.02400923\n",
      "Trained batch 1364 batch loss 6.83087349 epoch total loss 7.02386761\n",
      "Trained batch 1365 batch loss 7.1629138 epoch total loss 7.02396965\n",
      "Trained batch 1366 batch loss 6.68255043 epoch total loss 7.02372\n",
      "Trained batch 1367 batch loss 7.38469791 epoch total loss 7.02398396\n",
      "Trained batch 1368 batch loss 7.16870165 epoch total loss 7.02409\n",
      "Trained batch 1369 batch loss 6.31871033 epoch total loss 7.02357435\n",
      "Trained batch 1370 batch loss 6.00980377 epoch total loss 7.0228343\n",
      "Trained batch 1371 batch loss 6.45729637 epoch total loss 7.02242184\n",
      "Trained batch 1372 batch loss 6.74946213 epoch total loss 7.02222252\n",
      "Trained batch 1373 batch loss 7.21845198 epoch total loss 7.02236557\n",
      "Trained batch 1374 batch loss 7.23027849 epoch total loss 7.0225172\n",
      "Trained batch 1375 batch loss 6.70621395 epoch total loss 7.02228689\n",
      "Trained batch 1376 batch loss 7.37843037 epoch total loss 7.02254629\n",
      "Trained batch 1377 batch loss 7.18784237 epoch total loss 7.02266598\n",
      "Trained batch 1378 batch loss 6.87611485 epoch total loss 7.02255964\n",
      "Trained batch 1379 batch loss 6.76714134 epoch total loss 7.02237463\n",
      "Trained batch 1380 batch loss 6.5660224 epoch total loss 7.02204418\n",
      "Trained batch 1381 batch loss 7.13697815 epoch total loss 7.02212715\n",
      "Trained batch 1382 batch loss 7.02048206 epoch total loss 7.0221262\n",
      "Trained batch 1383 batch loss 7.28185511 epoch total loss 7.02231407\n",
      "Trained batch 1384 batch loss 6.98805285 epoch total loss 7.02228928\n",
      "Trained batch 1385 batch loss 7.23601818 epoch total loss 7.02244425\n",
      "Trained batch 1386 batch loss 7.44938707 epoch total loss 7.02275181\n",
      "Trained batch 1387 batch loss 7.37473583 epoch total loss 7.02300596\n",
      "Trained batch 1388 batch loss 7.41898203 epoch total loss 7.02329111\n",
      "Epoch 3 train loss 7.023291110992432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 03:04:14.792788: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "2025-03-19 03:04:14.792843: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "\t [[RemoteCall]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 1 batch loss 6.85744953\n",
      "Validated batch 2 batch loss 7.00237703\n",
      "Validated batch 3 batch loss 6.67576456\n",
      "Validated batch 4 batch loss 6.78618908\n",
      "Validated batch 5 batch loss 6.79272\n",
      "Validated batch 6 batch loss 6.99570799\n",
      "Validated batch 7 batch loss 7.0516839\n",
      "Validated batch 8 batch loss 7.27992821\n",
      "Validated batch 9 batch loss 7.27300835\n",
      "Validated batch 10 batch loss 7.38191557\n",
      "Validated batch 11 batch loss 7.01873541\n",
      "Validated batch 12 batch loss 6.91456032\n",
      "Validated batch 13 batch loss 7.26107597\n",
      "Validated batch 14 batch loss 7.12843275\n",
      "Validated batch 15 batch loss 7.36888027\n",
      "Validated batch 16 batch loss 7.55051279\n",
      "Validated batch 17 batch loss 6.98566055\n",
      "Validated batch 18 batch loss 7.48176718\n",
      "Validated batch 19 batch loss 7.41653252\n",
      "Validated batch 20 batch loss 7.22574568\n",
      "Validated batch 21 batch loss 7.20149\n",
      "Validated batch 22 batch loss 7.4735136\n",
      "Validated batch 23 batch loss 7.44567728\n",
      "Validated batch 24 batch loss 7.31004286\n",
      "Validated batch 25 batch loss 7.22177124\n",
      "Validated batch 26 batch loss 6.99030209\n",
      "Validated batch 27 batch loss 6.82651615\n",
      "Validated batch 28 batch loss 6.74246264\n",
      "Validated batch 29 batch loss 7.25793362\n",
      "Validated batch 30 batch loss 6.95591307\n",
      "Validated batch 31 batch loss 7.31899881\n",
      "Validated batch 32 batch loss 7.29910851\n",
      "Validated batch 33 batch loss 7.0593729\n",
      "Validated batch 34 batch loss 7.16948414\n",
      "Validated batch 35 batch loss 6.5513258\n",
      "Validated batch 36 batch loss 7.12710905\n",
      "Validated batch 37 batch loss 6.86591244\n",
      "Validated batch 38 batch loss 7.50292826\n",
      "Validated batch 39 batch loss 7.21085787\n",
      "Validated batch 40 batch loss 7.10868263\n",
      "Validated batch 41 batch loss 6.56501818\n",
      "Validated batch 42 batch loss 6.73164749\n",
      "Validated batch 43 batch loss 7.26270294\n",
      "Validated batch 44 batch loss 6.82741499\n",
      "Validated batch 45 batch loss 6.8334465\n",
      "Validated batch 46 batch loss 6.96868706\n",
      "Validated batch 47 batch loss 7.40978479\n",
      "Validated batch 48 batch loss 7.30077887\n",
      "Validated batch 49 batch loss 7.16661596\n",
      "Validated batch 50 batch loss 6.54131079\n",
      "Validated batch 51 batch loss 7.16220379\n",
      "Validated batch 52 batch loss 6.91463184\n",
      "Validated batch 53 batch loss 6.51024437\n",
      "Validated batch 54 batch loss 7.06899118\n",
      "Validated batch 55 batch loss 6.69227362\n",
      "Validated batch 56 batch loss 7.21960497\n",
      "Validated batch 57 batch loss 6.82586288\n",
      "Validated batch 58 batch loss 6.69036674\n",
      "Validated batch 59 batch loss 6.9268117\n",
      "Validated batch 60 batch loss 7.07429361\n",
      "Validated batch 61 batch loss 7.05422258\n",
      "Validated batch 62 batch loss 7.27346945\n",
      "Validated batch 63 batch loss 7.00122929\n",
      "Validated batch 64 batch loss 7.35083914\n",
      "Validated batch 65 batch loss 7.21777296\n",
      "Validated batch 66 batch loss 6.7563262\n",
      "Validated batch 67 batch loss 7.24479628\n",
      "Validated batch 68 batch loss 6.12399626\n",
      "Validated batch 69 batch loss 7.44454193\n",
      "Validated batch 70 batch loss 7.13752174\n",
      "Validated batch 71 batch loss 7.22054\n",
      "Validated batch 72 batch loss 6.92312288\n",
      "Validated batch 73 batch loss 7.37420082\n",
      "Validated batch 74 batch loss 7.60190821\n",
      "Validated batch 75 batch loss 7.34465075\n",
      "Validated batch 76 batch loss 7.39258766\n",
      "Validated batch 77 batch loss 7.18926\n",
      "Validated batch 78 batch loss 7.49314\n",
      "Validated batch 79 batch loss 7.46071339\n",
      "Validated batch 80 batch loss 7.61478901\n",
      "Validated batch 81 batch loss 7.22967768\n",
      "Validated batch 82 batch loss 6.98281717\n",
      "Validated batch 83 batch loss 7.34869957\n",
      "Validated batch 84 batch loss 7.16983461\n",
      "Validated batch 85 batch loss 7.00289726\n",
      "Validated batch 86 batch loss 7.32892179\n",
      "Validated batch 87 batch loss 6.43055248\n",
      "Validated batch 88 batch loss 6.82793474\n",
      "Validated batch 89 batch loss 7.32843733\n",
      "Validated batch 90 batch loss 7.17988682\n",
      "Validated batch 91 batch loss 7.44966698\n",
      "Validated batch 92 batch loss 7.07421446\n",
      "Validated batch 93 batch loss 6.67670965\n",
      "Validated batch 94 batch loss 7.3283062\n",
      "Validated batch 95 batch loss 7.14649773\n",
      "Validated batch 96 batch loss 7.36121941\n",
      "Validated batch 97 batch loss 7.08733\n",
      "Validated batch 98 batch loss 7.51387691\n",
      "Validated batch 99 batch loss 6.9491806\n",
      "Validated batch 100 batch loss 7.45445395\n",
      "Validated batch 101 batch loss 6.92449522\n",
      "Validated batch 102 batch loss 7.08300638\n",
      "Validated batch 103 batch loss 7.17103338\n",
      "Validated batch 104 batch loss 6.759202\n",
      "Validated batch 105 batch loss 6.31220341\n",
      "Validated batch 106 batch loss 7.52122736\n",
      "Validated batch 107 batch loss 7.36300039\n",
      "Validated batch 108 batch loss 7.17095947\n",
      "Validated batch 109 batch loss 7.19509697\n",
      "Validated batch 110 batch loss 7.33614349\n",
      "Validated batch 111 batch loss 7.44139481\n",
      "Validated batch 112 batch loss 7.42144966\n",
      "Validated batch 113 batch loss 7.37686729\n",
      "Validated batch 114 batch loss 7.30201721\n",
      "Validated batch 115 batch loss 6.65545607\n",
      "Validated batch 116 batch loss 6.7038455\n",
      "Validated batch 117 batch loss 6.7747407\n",
      "Validated batch 118 batch loss 7.23587608\n",
      "Validated batch 119 batch loss 7.25395346\n",
      "Validated batch 120 batch loss 6.86776638\n",
      "Validated batch 121 batch loss 7.25507402\n",
      "Validated batch 122 batch loss 7.14007807\n",
      "Validated batch 123 batch loss 7.20244169\n",
      "Validated batch 124 batch loss 7.14216709\n",
      "Validated batch 125 batch loss 7.12457466\n",
      "Validated batch 126 batch loss 7.07718754\n",
      "Validated batch 127 batch loss 7.06434584\n",
      "Validated batch 128 batch loss 7.094594\n",
      "Validated batch 129 batch loss 7.32811975\n",
      "Validated batch 130 batch loss 7.48893738\n",
      "Validated batch 131 batch loss 7.19657516\n",
      "Validated batch 132 batch loss 7.14508343\n",
      "Validated batch 133 batch loss 6.64937973\n",
      "Validated batch 134 batch loss 7.3103261\n",
      "Validated batch 135 batch loss 7.59237671\n",
      "Validated batch 136 batch loss 7.36560249\n",
      "Validated batch 137 batch loss 7.3600297\n",
      "Validated batch 138 batch loss 6.92324114\n",
      "Validated batch 139 batch loss 7.28817081\n",
      "Validated batch 140 batch loss 6.90743303\n",
      "Validated batch 141 batch loss 7.08113384\n",
      "Validated batch 142 batch loss 7.04703665\n",
      "Validated batch 143 batch loss 6.9831686\n",
      "Validated batch 144 batch loss 7.36003447\n",
      "Validated batch 145 batch loss 7.01407766\n",
      "Validated batch 146 batch loss 7.36271143\n",
      "Validated batch 147 batch loss 7.24906635\n",
      "Validated batch 148 batch loss 7.19334412\n",
      "Validated batch 149 batch loss 7.52177811\n",
      "Validated batch 150 batch loss 7.35579967\n",
      "Validated batch 151 batch loss 6.91389799\n",
      "Validated batch 152 batch loss 7.03813028\n",
      "Validated batch 153 batch loss 7.58668852\n",
      "Validated batch 154 batch loss 6.85342789\n",
      "Validated batch 155 batch loss 7.41829252\n",
      "Validated batch 156 batch loss 6.8244729\n",
      "Validated batch 157 batch loss 7.52131224\n",
      "Validated batch 158 batch loss 6.87057495\n",
      "Validated batch 159 batch loss 7.05823183\n",
      "Validated batch 160 batch loss 7.10205936\n",
      "Validated batch 161 batch loss 7.09660196\n",
      "Validated batch 162 batch loss 7.51654053\n",
      "Validated batch 163 batch loss 7.17463589\n",
      "Validated batch 164 batch loss 7.33328152\n",
      "Validated batch 165 batch loss 7.25628901\n",
      "Validated batch 166 batch loss 7.36035585\n",
      "Validated batch 167 batch loss 7.46687317\n",
      "Validated batch 168 batch loss 7.32416773\n",
      "Validated batch 169 batch loss 6.89532852\n",
      "Validated batch 170 batch loss 6.83074808\n",
      "Validated batch 171 batch loss 7.02971745\n",
      "Validated batch 172 batch loss 7.14634323\n",
      "Validated batch 173 batch loss 7.19707441\n",
      "Validated batch 174 batch loss 6.85433388\n",
      "Validated batch 175 batch loss 6.89601803\n",
      "Validated batch 176 batch loss 7.09043598\n",
      "Validated batch 177 batch loss 7.21400309\n",
      "Validated batch 178 batch loss 7.65088844\n",
      "Validated batch 179 batch loss 7.28215933\n",
      "Validated batch 180 batch loss 7.49336243\n",
      "Validated batch 181 batch loss 7.59454346\n",
      "Validated batch 182 batch loss 7.46056604\n",
      "Validated batch 183 batch loss 7.17810488\n",
      "Validated batch 184 batch loss 6.62837887\n",
      "Validated batch 185 batch loss 3.51475358\n",
      "Epoch 3 val loss 7.114797115325928\n",
      "Model /home/minho/Desktop/aiffel/pose_estimation/models/model-epoch-3-loss-7.1148.weights.h5 saved.\n",
      "Start epoch 4 with learning rate 0.001\n",
      "Start distributed training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 03:04:23.261045: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "2025-03-19 03:04:23.261091: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "\t [[RemoteCall]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1 batch loss 7.26463652 epoch total loss 7.26463652\n",
      "Trained batch 2 batch loss 6.9796586 epoch total loss 7.12214756\n",
      "Trained batch 3 batch loss 6.80834723 epoch total loss 7.01754761\n",
      "Trained batch 4 batch loss 6.94139242 epoch total loss 6.99850893\n",
      "Trained batch 5 batch loss 7.26147175 epoch total loss 7.05110168\n",
      "Trained batch 6 batch loss 6.98057652 epoch total loss 7.03934717\n",
      "Trained batch 7 batch loss 7.13836 epoch total loss 7.05349207\n",
      "Trained batch 8 batch loss 6.23612309 epoch total loss 6.95132065\n",
      "Trained batch 9 batch loss 6.96742964 epoch total loss 6.95311069\n",
      "Trained batch 10 batch loss 6.89432 epoch total loss 6.94723129\n",
      "Trained batch 11 batch loss 6.8330121 epoch total loss 6.93684816\n",
      "Trained batch 12 batch loss 7.38903761 epoch total loss 6.9745307\n",
      "Trained batch 13 batch loss 7.46379805 epoch total loss 7.0121665\n",
      "Trained batch 14 batch loss 7.41892195 epoch total loss 7.04122066\n",
      "Trained batch 15 batch loss 7.46947193 epoch total loss 7.06977081\n",
      "Trained batch 16 batch loss 7.40878487 epoch total loss 7.09095907\n",
      "Trained batch 17 batch loss 6.83678293 epoch total loss 7.07600784\n",
      "Trained batch 18 batch loss 6.26128387 epoch total loss 7.03074503\n",
      "Trained batch 19 batch loss 6.31790113 epoch total loss 6.99322653\n",
      "Trained batch 20 batch loss 5.87470627 epoch total loss 6.93730068\n",
      "Trained batch 21 batch loss 6.12687826 epoch total loss 6.8987093\n",
      "Trained batch 22 batch loss 6.29208946 epoch total loss 6.87113523\n",
      "Trained batch 23 batch loss 6.62765694 epoch total loss 6.86054945\n",
      "Trained batch 24 batch loss 7.17504358 epoch total loss 6.87365341\n",
      "Trained batch 25 batch loss 7.15734482 epoch total loss 6.88500118\n",
      "Trained batch 26 batch loss 6.6790328 epoch total loss 6.87707949\n",
      "Trained batch 27 batch loss 6.34660387 epoch total loss 6.85743189\n",
      "Trained batch 28 batch loss 7.15228796 epoch total loss 6.86796236\n",
      "Trained batch 29 batch loss 6.89524174 epoch total loss 6.86890316\n",
      "Trained batch 30 batch loss 6.51360512 epoch total loss 6.85706043\n",
      "Trained batch 31 batch loss 6.88638544 epoch total loss 6.858006\n",
      "Trained batch 32 batch loss 6.32214212 epoch total loss 6.84126043\n",
      "Trained batch 33 batch loss 6.92364597 epoch total loss 6.84375715\n",
      "Trained batch 34 batch loss 7.05292559 epoch total loss 6.84990931\n",
      "Trained batch 35 batch loss 7.40009928 epoch total loss 6.86562872\n",
      "Trained batch 36 batch loss 6.96738195 epoch total loss 6.86845541\n",
      "Trained batch 37 batch loss 7.04817295 epoch total loss 6.87331247\n",
      "Trained batch 38 batch loss 6.68612814 epoch total loss 6.86838675\n",
      "Trained batch 39 batch loss 7.06860209 epoch total loss 6.87352037\n",
      "Trained batch 40 batch loss 7.13974333 epoch total loss 6.88017559\n",
      "Trained batch 41 batch loss 6.71552849 epoch total loss 6.87615967\n",
      "Trained batch 42 batch loss 6.73423576 epoch total loss 6.87278032\n",
      "Trained batch 43 batch loss 7.11584187 epoch total loss 6.87843275\n",
      "Trained batch 44 batch loss 6.7381897 epoch total loss 6.87524557\n",
      "Trained batch 45 batch loss 6.72459459 epoch total loss 6.87189817\n",
      "Trained batch 46 batch loss 6.57091522 epoch total loss 6.86535501\n",
      "Trained batch 47 batch loss 6.40835857 epoch total loss 6.85563183\n",
      "Trained batch 48 batch loss 6.20999098 epoch total loss 6.84218073\n",
      "Trained batch 49 batch loss 6.58784294 epoch total loss 6.83699\n",
      "Trained batch 50 batch loss 6.4180603 epoch total loss 6.82861137\n",
      "Trained batch 51 batch loss 6.72129679 epoch total loss 6.82650709\n",
      "Trained batch 52 batch loss 7.42520475 epoch total loss 6.83802032\n",
      "Trained batch 53 batch loss 7.29330349 epoch total loss 6.84661055\n",
      "Trained batch 54 batch loss 7.31207228 epoch total loss 6.85523033\n",
      "Trained batch 55 batch loss 7.36532211 epoch total loss 6.86450481\n",
      "Trained batch 56 batch loss 7.25275469 epoch total loss 6.87143755\n",
      "Trained batch 57 batch loss 6.91010475 epoch total loss 6.87211561\n",
      "Trained batch 58 batch loss 6.43809319 epoch total loss 6.86463261\n",
      "Trained batch 59 batch loss 6.40471792 epoch total loss 6.85683727\n",
      "Trained batch 60 batch loss 7.05806589 epoch total loss 6.86019135\n",
      "Trained batch 61 batch loss 7.21937084 epoch total loss 6.86607933\n",
      "Trained batch 62 batch loss 7.11832952 epoch total loss 6.87014771\n",
      "Trained batch 63 batch loss 7.23572111 epoch total loss 6.87595034\n",
      "Trained batch 64 batch loss 7.36001492 epoch total loss 6.88351393\n",
      "Trained batch 65 batch loss 7.17216301 epoch total loss 6.88795471\n",
      "Trained batch 66 batch loss 7.22801256 epoch total loss 6.89310694\n",
      "Trained batch 67 batch loss 7.1877718 epoch total loss 6.89750528\n",
      "Trained batch 68 batch loss 6.39426613 epoch total loss 6.89010429\n",
      "Trained batch 69 batch loss 6.1375556 epoch total loss 6.8791976\n",
      "Trained batch 70 batch loss 6.70547152 epoch total loss 6.87671614\n",
      "Trained batch 71 batch loss 6.58616638 epoch total loss 6.87262392\n",
      "Trained batch 72 batch loss 6.87787533 epoch total loss 6.87269688\n",
      "Trained batch 73 batch loss 6.99581861 epoch total loss 6.87438345\n",
      "Trained batch 74 batch loss 6.29277134 epoch total loss 6.86652374\n",
      "Trained batch 75 batch loss 6.89132881 epoch total loss 6.86685467\n",
      "Trained batch 76 batch loss 7.01572 epoch total loss 6.86881399\n",
      "Trained batch 77 batch loss 7.30088949 epoch total loss 6.87442541\n",
      "Trained batch 78 batch loss 7.19901752 epoch total loss 6.87858677\n",
      "Trained batch 79 batch loss 7.39572525 epoch total loss 6.88513327\n",
      "Trained batch 80 batch loss 6.07071829 epoch total loss 6.87495327\n",
      "Trained batch 81 batch loss 5.54992819 epoch total loss 6.85859489\n",
      "Trained batch 82 batch loss 5.6095686 epoch total loss 6.84336281\n",
      "Trained batch 83 batch loss 7.06798553 epoch total loss 6.84606934\n",
      "Trained batch 84 batch loss 7.17774391 epoch total loss 6.85001755\n",
      "Trained batch 85 batch loss 7.55117 epoch total loss 6.85826635\n",
      "Trained batch 86 batch loss 7.20339775 epoch total loss 6.86227894\n",
      "Trained batch 87 batch loss 6.59398842 epoch total loss 6.85919523\n",
      "Trained batch 88 batch loss 6.25634861 epoch total loss 6.85234499\n",
      "Trained batch 89 batch loss 6.63752937 epoch total loss 6.84993124\n",
      "Trained batch 90 batch loss 7.027771 epoch total loss 6.85190678\n",
      "Trained batch 91 batch loss 7.0410161 epoch total loss 6.85398531\n",
      "Trained batch 92 batch loss 7.43123531 epoch total loss 6.86025953\n",
      "Trained batch 93 batch loss 7.32661152 epoch total loss 6.86527395\n",
      "Trained batch 94 batch loss 7.35033798 epoch total loss 6.87043428\n",
      "Trained batch 95 batch loss 7.37844896 epoch total loss 6.87578201\n",
      "Trained batch 96 batch loss 7.35994101 epoch total loss 6.88082504\n",
      "Trained batch 97 batch loss 7.2761445 epoch total loss 6.8849\n",
      "Trained batch 98 batch loss 7.45255613 epoch total loss 6.89069271\n",
      "Trained batch 99 batch loss 7.11711359 epoch total loss 6.89298\n",
      "Trained batch 100 batch loss 7.37333965 epoch total loss 6.89778376\n",
      "Trained batch 101 batch loss 6.47621536 epoch total loss 6.89360952\n",
      "Trained batch 102 batch loss 6.58041 epoch total loss 6.89053869\n",
      "Trained batch 103 batch loss 6.45559168 epoch total loss 6.88631582\n",
      "Trained batch 104 batch loss 7.19534588 epoch total loss 6.88928747\n",
      "Trained batch 105 batch loss 6.98118496 epoch total loss 6.89016294\n",
      "Trained batch 106 batch loss 6.9724474 epoch total loss 6.89093924\n",
      "Trained batch 107 batch loss 6.90656 epoch total loss 6.89108515\n",
      "Trained batch 108 batch loss 6.60612106 epoch total loss 6.88844681\n",
      "Trained batch 109 batch loss 7.0677948 epoch total loss 6.89009237\n",
      "Trained batch 110 batch loss 6.53856325 epoch total loss 6.88689709\n",
      "Trained batch 111 batch loss 6.8920536 epoch total loss 6.88694286\n",
      "Trained batch 112 batch loss 6.54455471 epoch total loss 6.88388586\n",
      "Trained batch 113 batch loss 6.52895594 epoch total loss 6.88074493\n",
      "Trained batch 114 batch loss 6.04280615 epoch total loss 6.87339449\n",
      "Trained batch 115 batch loss 6.69131565 epoch total loss 6.87181139\n",
      "Trained batch 116 batch loss 6.21297312 epoch total loss 6.86613131\n",
      "Trained batch 117 batch loss 6.42301702 epoch total loss 6.86234426\n",
      "Trained batch 118 batch loss 7.12726402 epoch total loss 6.86458921\n",
      "Trained batch 119 batch loss 6.7788763 epoch total loss 6.86386919\n",
      "Trained batch 120 batch loss 7.38823557 epoch total loss 6.86823893\n",
      "Trained batch 121 batch loss 6.5565834 epoch total loss 6.86566305\n",
      "Trained batch 122 batch loss 6.54975367 epoch total loss 6.86307383\n",
      "Trained batch 123 batch loss 7.07146645 epoch total loss 6.86476803\n",
      "Trained batch 124 batch loss 7.19222212 epoch total loss 6.86740828\n",
      "Trained batch 125 batch loss 7.05628681 epoch total loss 6.86891937\n",
      "Trained batch 126 batch loss 6.66414595 epoch total loss 6.86729383\n",
      "Trained batch 127 batch loss 6.89120245 epoch total loss 6.86748219\n",
      "Trained batch 128 batch loss 7.23453903 epoch total loss 6.87035\n",
      "Trained batch 129 batch loss 7.13850594 epoch total loss 6.87242842\n",
      "Trained batch 130 batch loss 7.38955927 epoch total loss 6.87640667\n",
      "Trained batch 131 batch loss 7.34517479 epoch total loss 6.87998486\n",
      "Trained batch 132 batch loss 7.00709534 epoch total loss 6.88094759\n",
      "Trained batch 133 batch loss 7.073452 epoch total loss 6.88239479\n",
      "Trained batch 134 batch loss 7.39543867 epoch total loss 6.88622379\n",
      "Trained batch 135 batch loss 7.18951416 epoch total loss 6.88847\n",
      "Trained batch 136 batch loss 7.37264967 epoch total loss 6.89203024\n",
      "Trained batch 137 batch loss 6.99651814 epoch total loss 6.8927927\n",
      "Trained batch 138 batch loss 7.05313969 epoch total loss 6.89395475\n",
      "Trained batch 139 batch loss 7.21395826 epoch total loss 6.89625692\n",
      "Trained batch 140 batch loss 7.33757401 epoch total loss 6.89940929\n",
      "Trained batch 141 batch loss 7.2746563 epoch total loss 6.90207052\n",
      "Trained batch 142 batch loss 7.48782396 epoch total loss 6.90619564\n",
      "Trained batch 143 batch loss 7.4445 epoch total loss 6.90996027\n",
      "Trained batch 144 batch loss 7.4326849 epoch total loss 6.91359043\n",
      "Trained batch 145 batch loss 7.32197571 epoch total loss 6.91640663\n",
      "Trained batch 146 batch loss 7.29838657 epoch total loss 6.91902304\n",
      "Trained batch 147 batch loss 7.19291162 epoch total loss 6.92088652\n",
      "Trained batch 148 batch loss 7.1733036 epoch total loss 6.92259169\n",
      "Trained batch 149 batch loss 6.99751472 epoch total loss 6.92309475\n",
      "Trained batch 150 batch loss 7.38532734 epoch total loss 6.92617655\n",
      "Trained batch 151 batch loss 7.1372366 epoch total loss 6.92757416\n",
      "Trained batch 152 batch loss 7.22090626 epoch total loss 6.92950439\n",
      "Trained batch 153 batch loss 6.88244343 epoch total loss 6.92919683\n",
      "Trained batch 154 batch loss 7.01667309 epoch total loss 6.92976522\n",
      "Trained batch 155 batch loss 7.38574696 epoch total loss 6.93270683\n",
      "Trained batch 156 batch loss 7.09507275 epoch total loss 6.93374777\n",
      "Trained batch 157 batch loss 7.17673445 epoch total loss 6.93529558\n",
      "Trained batch 158 batch loss 7.22218657 epoch total loss 6.93711138\n",
      "Trained batch 159 batch loss 7.17978525 epoch total loss 6.93863773\n",
      "Trained batch 160 batch loss 7.11588526 epoch total loss 6.93974543\n",
      "Trained batch 161 batch loss 6.9060235 epoch total loss 6.93953562\n",
      "Trained batch 162 batch loss 7.18356752 epoch total loss 6.94104242\n",
      "Trained batch 163 batch loss 7.24918318 epoch total loss 6.94293261\n",
      "Trained batch 164 batch loss 7.03447962 epoch total loss 6.94349051\n",
      "Trained batch 165 batch loss 6.97952557 epoch total loss 6.94370842\n",
      "Trained batch 166 batch loss 7.290874 epoch total loss 6.9458\n",
      "Trained batch 167 batch loss 7.44363928 epoch total loss 6.94878101\n",
      "Trained batch 168 batch loss 7.44941139 epoch total loss 6.95176125\n",
      "Trained batch 169 batch loss 7.3840766 epoch total loss 6.954319\n",
      "Trained batch 170 batch loss 7.20464611 epoch total loss 6.955791\n",
      "Trained batch 171 batch loss 6.84058 epoch total loss 6.95511723\n",
      "Trained batch 172 batch loss 7.02591562 epoch total loss 6.95552874\n",
      "Trained batch 173 batch loss 7.14123917 epoch total loss 6.9566021\n",
      "Trained batch 174 batch loss 6.94129276 epoch total loss 6.95651436\n",
      "Trained batch 175 batch loss 6.74753761 epoch total loss 6.95532036\n",
      "Trained batch 176 batch loss 7.23069668 epoch total loss 6.95688486\n",
      "Trained batch 177 batch loss 6.79675674 epoch total loss 6.9559803\n",
      "Trained batch 178 batch loss 7.23239708 epoch total loss 6.95753336\n",
      "Trained batch 179 batch loss 7.19185591 epoch total loss 6.95884275\n",
      "Trained batch 180 batch loss 7.15646553 epoch total loss 6.95994043\n",
      "Trained batch 181 batch loss 6.81555462 epoch total loss 6.95914268\n",
      "Trained batch 182 batch loss 6.8248229 epoch total loss 6.95840502\n",
      "Trained batch 183 batch loss 6.54706 epoch total loss 6.95615721\n",
      "Trained batch 184 batch loss 7.2332387 epoch total loss 6.95766354\n",
      "Trained batch 185 batch loss 7.26756907 epoch total loss 6.95933867\n",
      "Trained batch 186 batch loss 7.1475296 epoch total loss 6.96035099\n",
      "Trained batch 187 batch loss 7.51872349 epoch total loss 6.96333647\n",
      "Trained batch 188 batch loss 7.33177614 epoch total loss 6.96529627\n",
      "Trained batch 189 batch loss 7.3567338 epoch total loss 6.96736717\n",
      "Trained batch 190 batch loss 7.37250042 epoch total loss 6.96949959\n",
      "Trained batch 191 batch loss 7.27993345 epoch total loss 6.97112513\n",
      "Trained batch 192 batch loss 7.09437609 epoch total loss 6.97176695\n",
      "Trained batch 193 batch loss 6.91272354 epoch total loss 6.97146082\n",
      "Trained batch 194 batch loss 7.17968225 epoch total loss 6.97253418\n",
      "Trained batch 195 batch loss 6.77755499 epoch total loss 6.97153425\n",
      "Trained batch 196 batch loss 7.11712217 epoch total loss 6.97227716\n",
      "Trained batch 197 batch loss 7.19188452 epoch total loss 6.97339153\n",
      "Trained batch 198 batch loss 7.15520477 epoch total loss 6.97431\n",
      "Trained batch 199 batch loss 7.40344048 epoch total loss 6.97646618\n",
      "Trained batch 200 batch loss 7.3634553 epoch total loss 6.97840071\n",
      "Trained batch 201 batch loss 7.34078693 epoch total loss 6.98020411\n",
      "Trained batch 202 batch loss 7.35325861 epoch total loss 6.9820509\n",
      "Trained batch 203 batch loss 7.22337437 epoch total loss 6.98323965\n",
      "Trained batch 204 batch loss 7.15708685 epoch total loss 6.98409176\n",
      "Trained batch 205 batch loss 7.46606779 epoch total loss 6.98644304\n",
      "Trained batch 206 batch loss 7.38993931 epoch total loss 6.98840141\n",
      "Trained batch 207 batch loss 7.04242182 epoch total loss 6.98866272\n",
      "Trained batch 208 batch loss 6.93962097 epoch total loss 6.98842669\n",
      "Trained batch 209 batch loss 7.09652805 epoch total loss 6.98894405\n",
      "Trained batch 210 batch loss 6.4730258 epoch total loss 6.98648739\n",
      "Trained batch 211 batch loss 6.82837105 epoch total loss 6.9857378\n",
      "Trained batch 212 batch loss 7.11851883 epoch total loss 6.98636436\n",
      "Trained batch 213 batch loss 7.20934868 epoch total loss 6.9874115\n",
      "Trained batch 214 batch loss 7.11889 epoch total loss 6.98802567\n",
      "Trained batch 215 batch loss 7.06678963 epoch total loss 6.98839188\n",
      "Trained batch 216 batch loss 6.46542263 epoch total loss 6.98597097\n",
      "Trained batch 217 batch loss 6.66871166 epoch total loss 6.98450899\n",
      "Trained batch 218 batch loss 6.94824505 epoch total loss 6.98434258\n",
      "Trained batch 219 batch loss 7.10120773 epoch total loss 6.98487616\n",
      "Trained batch 220 batch loss 7.17593718 epoch total loss 6.98574448\n",
      "Trained batch 221 batch loss 7.3400836 epoch total loss 6.9873476\n",
      "Trained batch 222 batch loss 7.22955513 epoch total loss 6.98843908\n",
      "Trained batch 223 batch loss 7.13135433 epoch total loss 6.98908\n",
      "Trained batch 224 batch loss 7.49142 epoch total loss 6.99132252\n",
      "Trained batch 225 batch loss 7.06203604 epoch total loss 6.99163675\n",
      "Trained batch 226 batch loss 7.45561647 epoch total loss 6.99368954\n",
      "Trained batch 227 batch loss 7.33195066 epoch total loss 6.99517965\n",
      "Trained batch 228 batch loss 7.11640215 epoch total loss 6.99571133\n",
      "Trained batch 229 batch loss 6.51405525 epoch total loss 6.993608\n",
      "Trained batch 230 batch loss 6.35976744 epoch total loss 6.99085236\n",
      "Trained batch 231 batch loss 5.96095753 epoch total loss 6.98639345\n",
      "Trained batch 232 batch loss 6.61927557 epoch total loss 6.98481131\n",
      "Trained batch 233 batch loss 6.68298626 epoch total loss 6.98351574\n",
      "Trained batch 234 batch loss 6.85714912 epoch total loss 6.98297596\n",
      "Trained batch 235 batch loss 6.39098835 epoch total loss 6.98045683\n",
      "Trained batch 236 batch loss 6.4883461 epoch total loss 6.97837162\n",
      "Trained batch 237 batch loss 6.64404964 epoch total loss 6.97696114\n",
      "Trained batch 238 batch loss 6.6093936 epoch total loss 6.97541666\n",
      "Trained batch 239 batch loss 7.11206055 epoch total loss 6.97598839\n",
      "Trained batch 240 batch loss 7.05492783 epoch total loss 6.97631741\n",
      "Trained batch 241 batch loss 7.30456352 epoch total loss 6.97767925\n",
      "Trained batch 242 batch loss 7.3452673 epoch total loss 6.97919798\n",
      "Trained batch 243 batch loss 6.9739027 epoch total loss 6.97917604\n",
      "Trained batch 244 batch loss 6.98527241 epoch total loss 6.97920084\n",
      "Trained batch 245 batch loss 6.89506388 epoch total loss 6.97885752\n",
      "Trained batch 246 batch loss 7.35468578 epoch total loss 6.9803853\n",
      "Trained batch 247 batch loss 7.11881733 epoch total loss 6.98094559\n",
      "Trained batch 248 batch loss 7.22022152 epoch total loss 6.98191\n",
      "Trained batch 249 batch loss 7.36089468 epoch total loss 6.98343229\n",
      "Trained batch 250 batch loss 7.31715584 epoch total loss 6.98476696\n",
      "Trained batch 251 batch loss 7.38421535 epoch total loss 6.98635817\n",
      "Trained batch 252 batch loss 7.21460485 epoch total loss 6.98726416\n",
      "Trained batch 253 batch loss 7.17775822 epoch total loss 6.98801708\n",
      "Trained batch 254 batch loss 7.23455954 epoch total loss 6.98898792\n",
      "Trained batch 255 batch loss 6.79712725 epoch total loss 6.98823547\n",
      "Trained batch 256 batch loss 6.91776514 epoch total loss 6.98796\n",
      "Trained batch 257 batch loss 6.89064884 epoch total loss 6.98758125\n",
      "Trained batch 258 batch loss 6.85118055 epoch total loss 6.98705244\n",
      "Trained batch 259 batch loss 6.28066587 epoch total loss 6.98432493\n",
      "Trained batch 260 batch loss 6.04033804 epoch total loss 6.98069429\n",
      "Trained batch 261 batch loss 6.85441875 epoch total loss 6.9802103\n",
      "Trained batch 262 batch loss 7.00453329 epoch total loss 6.98030281\n",
      "Trained batch 263 batch loss 7.4807682 epoch total loss 6.98220539\n",
      "Trained batch 264 batch loss 6.72835159 epoch total loss 6.98124409\n",
      "Trained batch 265 batch loss 6.33540249 epoch total loss 6.97880745\n",
      "Trained batch 266 batch loss 6.22175026 epoch total loss 6.97596121\n",
      "Trained batch 267 batch loss 6.97678947 epoch total loss 6.97596455\n",
      "Trained batch 268 batch loss 7.29797459 epoch total loss 6.97716618\n",
      "Trained batch 269 batch loss 7.4388175 epoch total loss 6.97888231\n",
      "Trained batch 270 batch loss 7.08735514 epoch total loss 6.97928429\n",
      "Trained batch 271 batch loss 7.46730947 epoch total loss 6.98108482\n",
      "Trained batch 272 batch loss 7.42997074 epoch total loss 6.98273516\n",
      "Trained batch 273 batch loss 7.43005323 epoch total loss 6.98437357\n",
      "Trained batch 274 batch loss 7.46105528 epoch total loss 6.98611355\n",
      "Trained batch 275 batch loss 7.23769665 epoch total loss 6.98702812\n",
      "Trained batch 276 batch loss 7.06026125 epoch total loss 6.98729372\n",
      "Trained batch 277 batch loss 7.26426792 epoch total loss 6.98829365\n",
      "Trained batch 278 batch loss 7.0217948 epoch total loss 6.98841429\n",
      "Trained batch 279 batch loss 7.51009417 epoch total loss 6.99028444\n",
      "Trained batch 280 batch loss 7.19387722 epoch total loss 6.99101114\n",
      "Trained batch 281 batch loss 7.22675323 epoch total loss 6.99185038\n",
      "Trained batch 282 batch loss 6.99264669 epoch total loss 6.99185324\n",
      "Trained batch 283 batch loss 7.23360348 epoch total loss 6.99270773\n",
      "Trained batch 284 batch loss 7.36811495 epoch total loss 6.99402952\n",
      "Trained batch 285 batch loss 7.30870533 epoch total loss 6.99513388\n",
      "Trained batch 286 batch loss 7.41482639 epoch total loss 6.9966011\n",
      "Trained batch 287 batch loss 7.3061471 epoch total loss 6.99767971\n",
      "Trained batch 288 batch loss 7.40330362 epoch total loss 6.99908829\n",
      "Trained batch 289 batch loss 7.29664755 epoch total loss 7.00011778\n",
      "Trained batch 290 batch loss 7.29441 epoch total loss 7.00113297\n",
      "Trained batch 291 batch loss 7.4517355 epoch total loss 7.00268126\n",
      "Trained batch 292 batch loss 7.44964743 epoch total loss 7.00421238\n",
      "Trained batch 293 batch loss 7.32061911 epoch total loss 7.00529194\n",
      "Trained batch 294 batch loss 7.25496626 epoch total loss 7.00614071\n",
      "Trained batch 295 batch loss 7.37864828 epoch total loss 7.00740385\n",
      "Trained batch 296 batch loss 7.35771704 epoch total loss 7.00858688\n",
      "Trained batch 297 batch loss 7.07933712 epoch total loss 7.0088253\n",
      "Trained batch 298 batch loss 7.02524233 epoch total loss 7.00888\n",
      "Trained batch 299 batch loss 7.20250225 epoch total loss 7.00952721\n",
      "Trained batch 300 batch loss 7.02333593 epoch total loss 7.00957346\n",
      "Trained batch 301 batch loss 7.16929388 epoch total loss 7.0101037\n",
      "Trained batch 302 batch loss 6.94312382 epoch total loss 7.00988197\n",
      "Trained batch 303 batch loss 6.90026188 epoch total loss 7.00952\n",
      "Trained batch 304 batch loss 7.2449708 epoch total loss 7.01029396\n",
      "Trained batch 305 batch loss 7.62423563 epoch total loss 7.01230717\n",
      "Trained batch 306 batch loss 7.41764116 epoch total loss 7.01363182\n",
      "Trained batch 307 batch loss 7.41108704 epoch total loss 7.01492691\n",
      "Trained batch 308 batch loss 6.96746111 epoch total loss 7.01477289\n",
      "Trained batch 309 batch loss 6.42991972 epoch total loss 7.01288033\n",
      "Trained batch 310 batch loss 6.89450026 epoch total loss 7.01249838\n",
      "Trained batch 311 batch loss 7.38354731 epoch total loss 7.01369143\n",
      "Trained batch 312 batch loss 7.17453814 epoch total loss 7.01420689\n",
      "Trained batch 313 batch loss 7.28312492 epoch total loss 7.01506662\n",
      "Trained batch 314 batch loss 7.0745883 epoch total loss 7.0152564\n",
      "Trained batch 315 batch loss 6.61005449 epoch total loss 7.01397038\n",
      "Trained batch 316 batch loss 7.10237408 epoch total loss 7.01425\n",
      "Trained batch 317 batch loss 7.08614779 epoch total loss 7.01447678\n",
      "Trained batch 318 batch loss 7.38911486 epoch total loss 7.01565504\n",
      "Trained batch 319 batch loss 7.3060503 epoch total loss 7.0165658\n",
      "Trained batch 320 batch loss 7.42337418 epoch total loss 7.01783657\n",
      "Trained batch 321 batch loss 7.31735849 epoch total loss 7.01876974\n",
      "Trained batch 322 batch loss 7.08337784 epoch total loss 7.01897097\n",
      "Trained batch 323 batch loss 7.18823814 epoch total loss 7.01949501\n",
      "Trained batch 324 batch loss 7.35564184 epoch total loss 7.02053261\n",
      "Trained batch 325 batch loss 7.4500308 epoch total loss 7.02185392\n",
      "Trained batch 326 batch loss 7.27597 epoch total loss 7.02263308\n",
      "Trained batch 327 batch loss 7.08016634 epoch total loss 7.02280903\n",
      "Trained batch 328 batch loss 7.05546093 epoch total loss 7.02290821\n",
      "Trained batch 329 batch loss 6.91149187 epoch total loss 7.02256918\n",
      "Trained batch 330 batch loss 6.91603804 epoch total loss 7.02224636\n",
      "Trained batch 331 batch loss 7.1319418 epoch total loss 7.02257729\n",
      "Trained batch 332 batch loss 6.78263903 epoch total loss 7.02185488\n",
      "Trained batch 333 batch loss 6.91591501 epoch total loss 7.0215373\n",
      "Trained batch 334 batch loss 6.88554764 epoch total loss 7.02113\n",
      "Trained batch 335 batch loss 7.00697279 epoch total loss 7.02108812\n",
      "Trained batch 336 batch loss 6.87873316 epoch total loss 7.02066422\n",
      "Trained batch 337 batch loss 7.11523771 epoch total loss 7.0209446\n",
      "Trained batch 338 batch loss 7.0758 epoch total loss 7.02110672\n",
      "Trained batch 339 batch loss 7.24671698 epoch total loss 7.02177238\n",
      "Trained batch 340 batch loss 7.27704906 epoch total loss 7.0225234\n",
      "Trained batch 341 batch loss 7.43929958 epoch total loss 7.02374554\n",
      "Trained batch 342 batch loss 7.14349747 epoch total loss 7.02409554\n",
      "Trained batch 343 batch loss 6.98858 epoch total loss 7.02399206\n",
      "Trained batch 344 batch loss 7.30126762 epoch total loss 7.02479792\n",
      "Trained batch 345 batch loss 6.76950836 epoch total loss 7.02405834\n",
      "Trained batch 346 batch loss 6.8082757 epoch total loss 7.02343464\n",
      "Trained batch 347 batch loss 6.6402936 epoch total loss 7.02233076\n",
      "Trained batch 348 batch loss 6.90885735 epoch total loss 7.02200508\n",
      "Trained batch 349 batch loss 6.98826647 epoch total loss 7.02190828\n",
      "Trained batch 350 batch loss 7.15820169 epoch total loss 7.02229786\n",
      "Trained batch 351 batch loss 7.30819178 epoch total loss 7.02311182\n",
      "Trained batch 352 batch loss 7.27955437 epoch total loss 7.02384043\n",
      "Trained batch 353 batch loss 7.5031867 epoch total loss 7.02519846\n",
      "Trained batch 354 batch loss 7.14096737 epoch total loss 7.02552509\n",
      "Trained batch 355 batch loss 6.69574642 epoch total loss 7.02459621\n",
      "Trained batch 356 batch loss 6.96877575 epoch total loss 7.02443933\n",
      "Trained batch 357 batch loss 6.8850503 epoch total loss 7.02404881\n",
      "Trained batch 358 batch loss 7.08509588 epoch total loss 7.02421951\n",
      "Trained batch 359 batch loss 7.3673687 epoch total loss 7.02517557\n",
      "Trained batch 360 batch loss 7.33524084 epoch total loss 7.02603674\n",
      "Trained batch 361 batch loss 7.48642635 epoch total loss 7.0273118\n",
      "Trained batch 362 batch loss 7.4762764 epoch total loss 7.02855253\n",
      "Trained batch 363 batch loss 7.34609222 epoch total loss 7.02942753\n",
      "Trained batch 364 batch loss 7.3429575 epoch total loss 7.0302887\n",
      "Trained batch 365 batch loss 7.47507906 epoch total loss 7.03150749\n",
      "Trained batch 366 batch loss 7.53014326 epoch total loss 7.03287\n",
      "Trained batch 367 batch loss 7.36769247 epoch total loss 7.03378201\n",
      "Trained batch 368 batch loss 7.01685762 epoch total loss 7.03373575\n",
      "Trained batch 369 batch loss 7.3803668 epoch total loss 7.03467512\n",
      "Trained batch 370 batch loss 7.41980553 epoch total loss 7.03571653\n",
      "Trained batch 371 batch loss 7.20074844 epoch total loss 7.03616095\n",
      "Trained batch 372 batch loss 6.80431318 epoch total loss 7.03553772\n",
      "Trained batch 373 batch loss 7.3820219 epoch total loss 7.0364666\n",
      "Trained batch 374 batch loss 7.33417225 epoch total loss 7.03726292\n",
      "Trained batch 375 batch loss 7.27379036 epoch total loss 7.0378933\n",
      "Trained batch 376 batch loss 7.18026 epoch total loss 7.03827143\n",
      "Trained batch 377 batch loss 6.99918509 epoch total loss 7.03816795\n",
      "Trained batch 378 batch loss 6.5756731 epoch total loss 7.03694487\n",
      "Trained batch 379 batch loss 7.18289232 epoch total loss 7.03732967\n",
      "Trained batch 380 batch loss 7.31001711 epoch total loss 7.03804731\n",
      "Trained batch 381 batch loss 7.37693119 epoch total loss 7.03893709\n",
      "Trained batch 382 batch loss 7.39471769 epoch total loss 7.03986835\n",
      "Trained batch 383 batch loss 7.21386862 epoch total loss 7.04032278\n",
      "Trained batch 384 batch loss 7.34095573 epoch total loss 7.04110575\n",
      "Trained batch 385 batch loss 6.40950489 epoch total loss 7.03946495\n",
      "Trained batch 386 batch loss 6.05038834 epoch total loss 7.03690243\n",
      "Trained batch 387 batch loss 6.62192774 epoch total loss 7.03583\n",
      "Trained batch 388 batch loss 7.31323099 epoch total loss 7.0365448\n",
      "Trained batch 389 batch loss 7.13273811 epoch total loss 7.03679228\n",
      "Trained batch 390 batch loss 7.16248655 epoch total loss 7.0371151\n",
      "Trained batch 391 batch loss 7.44048595 epoch total loss 7.0381465\n",
      "Trained batch 392 batch loss 7.38393354 epoch total loss 7.03902864\n",
      "Trained batch 393 batch loss 7.29167747 epoch total loss 7.0396719\n",
      "Trained batch 394 batch loss 7.43013477 epoch total loss 7.04066324\n",
      "Trained batch 395 batch loss 7.06900787 epoch total loss 7.04073524\n",
      "Trained batch 396 batch loss 7.16594458 epoch total loss 7.04105139\n",
      "Trained batch 397 batch loss 6.71092176 epoch total loss 7.04022\n",
      "Trained batch 398 batch loss 6.88941956 epoch total loss 7.0398407\n",
      "Trained batch 399 batch loss 6.93319511 epoch total loss 7.03957319\n",
      "Trained batch 400 batch loss 6.66202402 epoch total loss 7.03862953\n",
      "Trained batch 401 batch loss 7.14947319 epoch total loss 7.0389061\n",
      "Trained batch 402 batch loss 6.95861435 epoch total loss 7.03870583\n",
      "Trained batch 403 batch loss 6.95162773 epoch total loss 7.03849\n",
      "Trained batch 404 batch loss 6.70935726 epoch total loss 7.03767538\n",
      "Trained batch 405 batch loss 6.72370052 epoch total loss 7.0369\n",
      "Trained batch 406 batch loss 6.9764514 epoch total loss 7.03675175\n",
      "Trained batch 407 batch loss 6.80368567 epoch total loss 7.03617907\n",
      "Trained batch 408 batch loss 7.03257179 epoch total loss 7.03617\n",
      "Trained batch 409 batch loss 6.63419628 epoch total loss 7.03518724\n",
      "Trained batch 410 batch loss 6.66758108 epoch total loss 7.03429031\n",
      "Trained batch 411 batch loss 6.68802309 epoch total loss 7.03344774\n",
      "Trained batch 412 batch loss 6.31789589 epoch total loss 7.0317111\n",
      "Trained batch 413 batch loss 7.19038057 epoch total loss 7.03209543\n",
      "Trained batch 414 batch loss 7.13263512 epoch total loss 7.03233814\n",
      "Trained batch 415 batch loss 7.3841033 epoch total loss 7.03318548\n",
      "Trained batch 416 batch loss 7.26269293 epoch total loss 7.03373718\n",
      "Trained batch 417 batch loss 7.31165218 epoch total loss 7.0344038\n",
      "Trained batch 418 batch loss 7.31108046 epoch total loss 7.03506565\n",
      "Trained batch 419 batch loss 6.7903614 epoch total loss 7.03448153\n",
      "Trained batch 420 batch loss 6.94931 epoch total loss 7.03427839\n",
      "Trained batch 421 batch loss 6.91187143 epoch total loss 7.03398752\n",
      "Trained batch 422 batch loss 7.01651 epoch total loss 7.03394651\n",
      "Trained batch 423 batch loss 7.22432899 epoch total loss 7.03439665\n",
      "Trained batch 424 batch loss 6.62547445 epoch total loss 7.03343248\n",
      "Trained batch 425 batch loss 6.90024471 epoch total loss 7.03311872\n",
      "Trained batch 426 batch loss 6.69963408 epoch total loss 7.03233624\n",
      "Trained batch 427 batch loss 7.22306347 epoch total loss 7.03278303\n",
      "Trained batch 428 batch loss 7.11136389 epoch total loss 7.03296661\n",
      "Trained batch 429 batch loss 7.14823961 epoch total loss 7.03323507\n",
      "Trained batch 430 batch loss 6.96132851 epoch total loss 7.03306818\n",
      "Trained batch 431 batch loss 7.00727654 epoch total loss 7.0330081\n",
      "Trained batch 432 batch loss 7.21717358 epoch total loss 7.03343487\n",
      "Trained batch 433 batch loss 6.93938684 epoch total loss 7.03321791\n",
      "Trained batch 434 batch loss 7.04576874 epoch total loss 7.03324652\n",
      "Trained batch 435 batch loss 7.26796532 epoch total loss 7.0337863\n",
      "Trained batch 436 batch loss 7.3386178 epoch total loss 7.03448534\n",
      "Trained batch 437 batch loss 6.92657757 epoch total loss 7.03423834\n",
      "Trained batch 438 batch loss 6.88365698 epoch total loss 7.03389406\n",
      "Trained batch 439 batch loss 7.02677107 epoch total loss 7.03387833\n",
      "Trained batch 440 batch loss 6.68697262 epoch total loss 7.03309\n",
      "Trained batch 441 batch loss 6.78007412 epoch total loss 7.032516\n",
      "Trained batch 442 batch loss 6.70434666 epoch total loss 7.03177357\n",
      "Trained batch 443 batch loss 6.63378 epoch total loss 7.03087521\n",
      "Trained batch 444 batch loss 7.35471058 epoch total loss 7.03160477\n",
      "Trained batch 445 batch loss 7.26976061 epoch total loss 7.03214\n",
      "Trained batch 446 batch loss 7.35092115 epoch total loss 7.03285456\n",
      "Trained batch 447 batch loss 7.30476809 epoch total loss 7.03346252\n",
      "Trained batch 448 batch loss 7.00516891 epoch total loss 7.03339911\n",
      "Trained batch 449 batch loss 7.22257185 epoch total loss 7.03382063\n",
      "Trained batch 450 batch loss 7.26524544 epoch total loss 7.03433466\n",
      "Trained batch 451 batch loss 7.26076031 epoch total loss 7.03483677\n",
      "Trained batch 452 batch loss 7.02579451 epoch total loss 7.03481722\n",
      "Trained batch 453 batch loss 7.06866026 epoch total loss 7.03489161\n",
      "Trained batch 454 batch loss 6.92396259 epoch total loss 7.03464746\n",
      "Trained batch 455 batch loss 6.49389076 epoch total loss 7.03345919\n",
      "Trained batch 456 batch loss 6.56275129 epoch total loss 7.03242683\n",
      "Trained batch 457 batch loss 6.87145805 epoch total loss 7.03207445\n",
      "Trained batch 458 batch loss 6.8892746 epoch total loss 7.03176212\n",
      "Trained batch 459 batch loss 6.76769 epoch total loss 7.03118658\n",
      "Trained batch 460 batch loss 6.80296755 epoch total loss 7.03069067\n",
      "Trained batch 461 batch loss 6.94084406 epoch total loss 7.03049564\n",
      "Trained batch 462 batch loss 6.60740089 epoch total loss 7.02958\n",
      "Trained batch 463 batch loss 6.76892233 epoch total loss 7.02901745\n",
      "Trained batch 464 batch loss 6.95652676 epoch total loss 7.02886105\n",
      "Trained batch 465 batch loss 6.90358877 epoch total loss 7.02859163\n",
      "Trained batch 466 batch loss 7.04117918 epoch total loss 7.02861881\n",
      "Trained batch 467 batch loss 6.8181119 epoch total loss 7.0281682\n",
      "Trained batch 468 batch loss 6.56098747 epoch total loss 7.02717\n",
      "Trained batch 469 batch loss 7.41296196 epoch total loss 7.02799225\n",
      "Trained batch 470 batch loss 7.26090813 epoch total loss 7.02848816\n",
      "Trained batch 471 batch loss 7.21022558 epoch total loss 7.02887392\n",
      "Trained batch 472 batch loss 7.34925556 epoch total loss 7.02955294\n",
      "Trained batch 473 batch loss 6.96536255 epoch total loss 7.02941704\n",
      "Trained batch 474 batch loss 6.88729143 epoch total loss 7.02911711\n",
      "Trained batch 475 batch loss 6.75642109 epoch total loss 7.028543\n",
      "Trained batch 476 batch loss 7.17289686 epoch total loss 7.02884626\n",
      "Trained batch 477 batch loss 6.66047478 epoch total loss 7.02807379\n",
      "Trained batch 478 batch loss 6.98541355 epoch total loss 7.02798414\n",
      "Trained batch 479 batch loss 7.15929747 epoch total loss 7.02825832\n",
      "Trained batch 480 batch loss 7.23586273 epoch total loss 7.02869081\n",
      "Trained batch 481 batch loss 7.20057631 epoch total loss 7.02904797\n",
      "Trained batch 482 batch loss 7.54570723 epoch total loss 7.03012\n",
      "Trained batch 483 batch loss 7.27603102 epoch total loss 7.03062916\n",
      "Trained batch 484 batch loss 6.85923815 epoch total loss 7.03027487\n",
      "Trained batch 485 batch loss 6.51935 epoch total loss 7.02922153\n",
      "Trained batch 486 batch loss 6.58086681 epoch total loss 7.02829885\n",
      "Trained batch 487 batch loss 6.14454269 epoch total loss 7.02648401\n",
      "Trained batch 488 batch loss 6.5069685 epoch total loss 7.02541971\n",
      "Trained batch 489 batch loss 6.20165825 epoch total loss 7.02373505\n",
      "Trained batch 490 batch loss 5.71649265 epoch total loss 7.02106714\n",
      "Trained batch 491 batch loss 5.71220303 epoch total loss 7.01840162\n",
      "Trained batch 492 batch loss 6.11265659 epoch total loss 7.01656055\n",
      "Trained batch 493 batch loss 6.25016546 epoch total loss 7.01500607\n",
      "Trained batch 494 batch loss 6.83980274 epoch total loss 7.0146513\n",
      "Trained batch 495 batch loss 7.09196281 epoch total loss 7.0148077\n",
      "Trained batch 496 batch loss 7.25539398 epoch total loss 7.01529264\n",
      "Trained batch 497 batch loss 7.43324852 epoch total loss 7.01613379\n",
      "Trained batch 498 batch loss 7.34289694 epoch total loss 7.01679039\n",
      "Trained batch 499 batch loss 7.01049709 epoch total loss 7.01677752\n",
      "Trained batch 500 batch loss 7.46049929 epoch total loss 7.01766491\n",
      "Trained batch 501 batch loss 7.16541433 epoch total loss 7.01796\n",
      "Trained batch 502 batch loss 7.2624712 epoch total loss 7.0184474\n",
      "Trained batch 503 batch loss 7.16011047 epoch total loss 7.01872873\n",
      "Trained batch 504 batch loss 6.9647584 epoch total loss 7.01862192\n",
      "Trained batch 505 batch loss 7.29726791 epoch total loss 7.0191741\n",
      "Trained batch 506 batch loss 7.00044727 epoch total loss 7.01913691\n",
      "Trained batch 507 batch loss 6.28962803 epoch total loss 7.01769781\n",
      "Trained batch 508 batch loss 6.40648603 epoch total loss 7.01649475\n",
      "Trained batch 509 batch loss 6.71537733 epoch total loss 7.015903\n",
      "Trained batch 510 batch loss 7.0445075 epoch total loss 7.01595926\n",
      "Trained batch 511 batch loss 7.16359758 epoch total loss 7.01624823\n",
      "Trained batch 512 batch loss 7.21948671 epoch total loss 7.01664495\n",
      "Trained batch 513 batch loss 7.23797178 epoch total loss 7.01707649\n",
      "Trained batch 514 batch loss 7.4039135 epoch total loss 7.01782894\n",
      "Trained batch 515 batch loss 6.88895369 epoch total loss 7.0175786\n",
      "Trained batch 516 batch loss 6.95939445 epoch total loss 7.01746607\n",
      "Trained batch 517 batch loss 7.0437088 epoch total loss 7.01751661\n",
      "Trained batch 518 batch loss 7.21247387 epoch total loss 7.01789284\n",
      "Trained batch 519 batch loss 7.17053795 epoch total loss 7.01818752\n",
      "Trained batch 520 batch loss 7.18459415 epoch total loss 7.01850748\n",
      "Trained batch 521 batch loss 7.42637777 epoch total loss 7.01929\n",
      "Trained batch 522 batch loss 6.8272233 epoch total loss 7.01892185\n",
      "Trained batch 523 batch loss 7.1459527 epoch total loss 7.01916504\n",
      "Trained batch 524 batch loss 7.10744095 epoch total loss 7.01933336\n",
      "Trained batch 525 batch loss 7.38467503 epoch total loss 7.02002907\n",
      "Trained batch 526 batch loss 7.40808964 epoch total loss 7.02076721\n",
      "Trained batch 527 batch loss 7.24290276 epoch total loss 7.02118874\n",
      "Trained batch 528 batch loss 7.18606472 epoch total loss 7.02150106\n",
      "Trained batch 529 batch loss 6.62672806 epoch total loss 7.02075481\n",
      "Trained batch 530 batch loss 6.40504885 epoch total loss 7.01959276\n",
      "Trained batch 531 batch loss 6.38921452 epoch total loss 7.01840591\n",
      "Trained batch 532 batch loss 6.33522844 epoch total loss 7.01712132\n",
      "Trained batch 533 batch loss 6.1588912 epoch total loss 7.01551151\n",
      "Trained batch 534 batch loss 6.00073671 epoch total loss 7.01361084\n",
      "Trained batch 535 batch loss 5.84629679 epoch total loss 7.01142883\n",
      "Trained batch 536 batch loss 5.61372614 epoch total loss 7.00882149\n",
      "Trained batch 537 batch loss 6.52042866 epoch total loss 7.00791216\n",
      "Trained batch 538 batch loss 6.96568871 epoch total loss 7.00783348\n",
      "Trained batch 539 batch loss 7.08009624 epoch total loss 7.00796747\n",
      "Trained batch 540 batch loss 7.15689945 epoch total loss 7.00824356\n",
      "Trained batch 541 batch loss 6.9483242 epoch total loss 7.00813246\n",
      "Trained batch 542 batch loss 7.07442904 epoch total loss 7.008255\n",
      "Trained batch 543 batch loss 6.53074121 epoch total loss 7.00737524\n",
      "Trained batch 544 batch loss 6.38288403 epoch total loss 7.00622749\n",
      "Trained batch 545 batch loss 7.04248142 epoch total loss 7.00629377\n",
      "Trained batch 546 batch loss 7.14520216 epoch total loss 7.0065484\n",
      "Trained batch 547 batch loss 7.33319044 epoch total loss 7.00714588\n",
      "Trained batch 548 batch loss 7.10820484 epoch total loss 7.00733\n",
      "Trained batch 549 batch loss 7.26282454 epoch total loss 7.00779581\n",
      "Trained batch 550 batch loss 7.09301424 epoch total loss 7.00795078\n",
      "Trained batch 551 batch loss 7.32501554 epoch total loss 7.00852585\n",
      "Trained batch 552 batch loss 7.38954544 epoch total loss 7.00921631\n",
      "Trained batch 553 batch loss 7.41789055 epoch total loss 7.00995541\n",
      "Trained batch 554 batch loss 7.20181847 epoch total loss 7.01030207\n",
      "Trained batch 555 batch loss 7.35176182 epoch total loss 7.01091719\n",
      "Trained batch 556 batch loss 7.36644936 epoch total loss 7.01155663\n",
      "Trained batch 557 batch loss 7.36289263 epoch total loss 7.01218748\n",
      "Trained batch 558 batch loss 7.18164968 epoch total loss 7.01249075\n",
      "Trained batch 559 batch loss 7.33464813 epoch total loss 7.01306725\n",
      "Trained batch 560 batch loss 7.24983644 epoch total loss 7.01349\n",
      "Trained batch 561 batch loss 7.30500126 epoch total loss 7.01400948\n",
      "Trained batch 562 batch loss 7.22588301 epoch total loss 7.01438665\n",
      "Trained batch 563 batch loss 7.43306923 epoch total loss 7.01513\n",
      "Trained batch 564 batch loss 7.47222 epoch total loss 7.01594067\n",
      "Trained batch 565 batch loss 7.3635025 epoch total loss 7.01655579\n",
      "Trained batch 566 batch loss 7.08427382 epoch total loss 7.01667547\n",
      "Trained batch 567 batch loss 7.00345516 epoch total loss 7.01665211\n",
      "Trained batch 568 batch loss 7.1168642 epoch total loss 7.01682854\n",
      "Trained batch 569 batch loss 6.82729 epoch total loss 7.0164957\n",
      "Trained batch 570 batch loss 6.44754267 epoch total loss 7.01549721\n",
      "Trained batch 571 batch loss 7.13313 epoch total loss 7.0157032\n",
      "Trained batch 572 batch loss 6.96593857 epoch total loss 7.01561594\n",
      "Trained batch 573 batch loss 7.1271925 epoch total loss 7.01581097\n",
      "Trained batch 574 batch loss 6.74831152 epoch total loss 7.01534462\n",
      "Trained batch 575 batch loss 7.27708197 epoch total loss 7.0158\n",
      "Trained batch 576 batch loss 7.20555544 epoch total loss 7.01612949\n",
      "Trained batch 577 batch loss 6.81741238 epoch total loss 7.01578474\n",
      "Trained batch 578 batch loss 7.1079073 epoch total loss 7.01594448\n",
      "Trained batch 579 batch loss 6.96645784 epoch total loss 7.01585913\n",
      "Trained batch 580 batch loss 6.54270506 epoch total loss 7.01504326\n",
      "Trained batch 581 batch loss 6.64304924 epoch total loss 7.01440287\n",
      "Trained batch 582 batch loss 6.84788132 epoch total loss 7.01411676\n",
      "Trained batch 583 batch loss 7.22668743 epoch total loss 7.01448154\n",
      "Trained batch 584 batch loss 7.40784 epoch total loss 7.01515532\n",
      "Trained batch 585 batch loss 7.14573669 epoch total loss 7.015378\n",
      "Trained batch 586 batch loss 6.95229149 epoch total loss 7.01527\n",
      "Trained batch 587 batch loss 6.82824326 epoch total loss 7.01495123\n",
      "Trained batch 588 batch loss 7.29883289 epoch total loss 7.01543379\n",
      "Trained batch 589 batch loss 7.41855383 epoch total loss 7.01611805\n",
      "Trained batch 590 batch loss 7.41255951 epoch total loss 7.01679039\n",
      "Trained batch 591 batch loss 7.45495033 epoch total loss 7.01753187\n",
      "Trained batch 592 batch loss 7.05696774 epoch total loss 7.01759863\n",
      "Trained batch 593 batch loss 7.20992565 epoch total loss 7.01792336\n",
      "Trained batch 594 batch loss 7.10957909 epoch total loss 7.0180769\n",
      "Trained batch 595 batch loss 6.35761595 epoch total loss 7.01696682\n",
      "Trained batch 596 batch loss 6.68219185 epoch total loss 7.01640511\n",
      "Trained batch 597 batch loss 7.20175362 epoch total loss 7.01671505\n",
      "Trained batch 598 batch loss 7.14935589 epoch total loss 7.01693726\n",
      "Trained batch 599 batch loss 6.59145784 epoch total loss 7.01622677\n",
      "Trained batch 600 batch loss 6.67369747 epoch total loss 7.01565599\n",
      "Trained batch 601 batch loss 6.6889987 epoch total loss 7.0151124\n",
      "Trained batch 602 batch loss 7.42762852 epoch total loss 7.01579762\n",
      "Trained batch 603 batch loss 6.65182 epoch total loss 7.01519442\n",
      "Trained batch 604 batch loss 6.86793661 epoch total loss 7.01495075\n",
      "Trained batch 605 batch loss 6.6786871 epoch total loss 7.01439524\n",
      "Trained batch 606 batch loss 7.01992321 epoch total loss 7.0144043\n",
      "Trained batch 607 batch loss 7.29116774 epoch total loss 7.01486\n",
      "Trained batch 608 batch loss 7.00985146 epoch total loss 7.01485157\n",
      "Trained batch 609 batch loss 7.13404274 epoch total loss 7.01504755\n",
      "Trained batch 610 batch loss 7.46948195 epoch total loss 7.01579237\n",
      "Trained batch 611 batch loss 7.19688559 epoch total loss 7.01608849\n",
      "Trained batch 612 batch loss 7.17290688 epoch total loss 7.01634455\n",
      "Trained batch 613 batch loss 7.46448755 epoch total loss 7.01707554\n",
      "Trained batch 614 batch loss 7.37303495 epoch total loss 7.01765537\n",
      "Trained batch 615 batch loss 7.32757521 epoch total loss 7.01815939\n",
      "Trained batch 616 batch loss 7.18578 epoch total loss 7.01843119\n",
      "Trained batch 617 batch loss 7.16372347 epoch total loss 7.01866627\n",
      "Trained batch 618 batch loss 7.38508177 epoch total loss 7.01925945\n",
      "Trained batch 619 batch loss 7.52012253 epoch total loss 7.02006865\n",
      "Trained batch 620 batch loss 6.63008642 epoch total loss 7.01943922\n",
      "Trained batch 621 batch loss 7.25129938 epoch total loss 7.01981258\n",
      "Trained batch 622 batch loss 7.38144636 epoch total loss 7.02039385\n",
      "Trained batch 623 batch loss 7.37975788 epoch total loss 7.02097082\n",
      "Trained batch 624 batch loss 7.30741501 epoch total loss 7.02143049\n",
      "Trained batch 625 batch loss 7.44361734 epoch total loss 7.02210617\n",
      "Trained batch 626 batch loss 7.48951244 epoch total loss 7.02285337\n",
      "Trained batch 627 batch loss 7.52657843 epoch total loss 7.02365637\n",
      "Trained batch 628 batch loss 6.98437 epoch total loss 7.0235939\n",
      "Trained batch 629 batch loss 6.91121769 epoch total loss 7.02341509\n",
      "Trained batch 630 batch loss 7.18683195 epoch total loss 7.02367449\n",
      "Trained batch 631 batch loss 6.65755749 epoch total loss 7.02309465\n",
      "Trained batch 632 batch loss 6.75053263 epoch total loss 7.02266359\n",
      "Trained batch 633 batch loss 6.72820234 epoch total loss 7.02219772\n",
      "Trained batch 634 batch loss 6.80208445 epoch total loss 7.02185106\n",
      "Trained batch 635 batch loss 6.78288507 epoch total loss 7.02147436\n",
      "Trained batch 636 batch loss 7.13641787 epoch total loss 7.02165461\n",
      "Trained batch 637 batch loss 6.66351128 epoch total loss 7.02109289\n",
      "Trained batch 638 batch loss 7.09455824 epoch total loss 7.02120829\n",
      "Trained batch 639 batch loss 7.24529743 epoch total loss 7.02155828\n",
      "Trained batch 640 batch loss 6.98085785 epoch total loss 7.02149487\n",
      "Trained batch 641 batch loss 6.95987225 epoch total loss 7.02139902\n",
      "Trained batch 642 batch loss 7.08006287 epoch total loss 7.02149057\n",
      "Trained batch 643 batch loss 6.91396427 epoch total loss 7.0213232\n",
      "Trained batch 644 batch loss 7.12029648 epoch total loss 7.02147675\n",
      "Trained batch 645 batch loss 7.1561923 epoch total loss 7.0216856\n",
      "Trained batch 646 batch loss 7.34730625 epoch total loss 7.02218962\n",
      "Trained batch 647 batch loss 7.28580141 epoch total loss 7.02259684\n",
      "Trained batch 648 batch loss 7.04659414 epoch total loss 7.02263355\n",
      "Trained batch 649 batch loss 6.18806267 epoch total loss 7.02134752\n",
      "Trained batch 650 batch loss 6.92535448 epoch total loss 7.0211997\n",
      "Trained batch 651 batch loss 7.15451336 epoch total loss 7.02140427\n",
      "Trained batch 652 batch loss 7.17141628 epoch total loss 7.0216341\n",
      "Trained batch 653 batch loss 7.09432077 epoch total loss 7.0217452\n",
      "Trained batch 654 batch loss 7.06459236 epoch total loss 7.02181053\n",
      "Trained batch 655 batch loss 7.19095755 epoch total loss 7.02206898\n",
      "Trained batch 656 batch loss 7.07625294 epoch total loss 7.02215147\n",
      "Trained batch 657 batch loss 6.98517132 epoch total loss 7.0220952\n",
      "Trained batch 658 batch loss 6.47859764 epoch total loss 7.02126932\n",
      "Trained batch 659 batch loss 5.91971874 epoch total loss 7.01959801\n",
      "Trained batch 660 batch loss 5.75042868 epoch total loss 7.01767492\n",
      "Trained batch 661 batch loss 5.30513334 epoch total loss 7.01508427\n",
      "Trained batch 662 batch loss 6.81597567 epoch total loss 7.01478338\n",
      "Trained batch 663 batch loss 7.44354248 epoch total loss 7.01543\n",
      "Trained batch 664 batch loss 7.22690153 epoch total loss 7.0157485\n",
      "Trained batch 665 batch loss 6.83893728 epoch total loss 7.01548243\n",
      "Trained batch 666 batch loss 6.87107134 epoch total loss 7.01526594\n",
      "Trained batch 667 batch loss 7.21159697 epoch total loss 7.01555967\n",
      "Trained batch 668 batch loss 6.55295372 epoch total loss 7.01486683\n",
      "Trained batch 669 batch loss 6.32590961 epoch total loss 7.01383686\n",
      "Trained batch 670 batch loss 6.54946 epoch total loss 7.01314354\n",
      "Trained batch 671 batch loss 6.5164814 epoch total loss 7.01240349\n",
      "Trained batch 672 batch loss 7.03512383 epoch total loss 7.01243734\n",
      "Trained batch 673 batch loss 6.96584511 epoch total loss 7.0123682\n",
      "Trained batch 674 batch loss 7.14318562 epoch total loss 7.0125618\n",
      "Trained batch 675 batch loss 7.273839 epoch total loss 7.01294899\n",
      "Trained batch 676 batch loss 6.60094452 epoch total loss 7.01234\n",
      "Trained batch 677 batch loss 6.41557264 epoch total loss 7.0114584\n",
      "Trained batch 678 batch loss 6.39419174 epoch total loss 7.01054764\n",
      "Trained batch 679 batch loss 7.00471783 epoch total loss 7.01053953\n",
      "Trained batch 680 batch loss 6.6774292 epoch total loss 7.01004934\n",
      "Trained batch 681 batch loss 6.61120081 epoch total loss 7.00946379\n",
      "Trained batch 682 batch loss 6.33915663 epoch total loss 7.00848103\n",
      "Trained batch 683 batch loss 6.48214149 epoch total loss 7.00771046\n",
      "Trained batch 684 batch loss 6.61204147 epoch total loss 7.00713158\n",
      "Trained batch 685 batch loss 6.35128069 epoch total loss 7.00617361\n",
      "Trained batch 686 batch loss 6.81784821 epoch total loss 7.00589943\n",
      "Trained batch 687 batch loss 7.11842442 epoch total loss 7.00606346\n",
      "Trained batch 688 batch loss 7.09018087 epoch total loss 7.00618601\n",
      "Trained batch 689 batch loss 7.08945799 epoch total loss 7.00630665\n",
      "Trained batch 690 batch loss 7.21523046 epoch total loss 7.00660944\n",
      "Trained batch 691 batch loss 7.49664307 epoch total loss 7.0073185\n",
      "Trained batch 692 batch loss 7.48550606 epoch total loss 7.00800943\n",
      "Trained batch 693 batch loss 7.41588879 epoch total loss 7.00859833\n",
      "Trained batch 694 batch loss 6.89498854 epoch total loss 7.0084343\n",
      "Trained batch 695 batch loss 6.8444 epoch total loss 7.00819826\n",
      "Trained batch 696 batch loss 7.41777754 epoch total loss 7.00878716\n",
      "Trained batch 697 batch loss 7.41736889 epoch total loss 7.00937319\n",
      "Trained batch 698 batch loss 7.41610098 epoch total loss 7.00995588\n",
      "Trained batch 699 batch loss 7.31429195 epoch total loss 7.01039171\n",
      "Trained batch 700 batch loss 7.44542551 epoch total loss 7.01101303\n",
      "Trained batch 701 batch loss 7.16692781 epoch total loss 7.01123524\n",
      "Trained batch 702 batch loss 6.95896673 epoch total loss 7.01116085\n",
      "Trained batch 703 batch loss 6.93789196 epoch total loss 7.0110569\n",
      "Trained batch 704 batch loss 7.19754553 epoch total loss 7.01132202\n",
      "Trained batch 705 batch loss 7.18236208 epoch total loss 7.01156425\n",
      "Trained batch 706 batch loss 6.97430897 epoch total loss 7.01151133\n",
      "Trained batch 707 batch loss 7.06876 epoch total loss 7.01159239\n",
      "Trained batch 708 batch loss 6.80157 epoch total loss 7.0112958\n",
      "Trained batch 709 batch loss 6.84254932 epoch total loss 7.01105833\n",
      "Trained batch 710 batch loss 7.06621265 epoch total loss 7.01113605\n",
      "Trained batch 711 batch loss 7.09838438 epoch total loss 7.0112586\n",
      "Trained batch 712 batch loss 7.29755545 epoch total loss 7.01166058\n",
      "Trained batch 713 batch loss 7.30759907 epoch total loss 7.01207542\n",
      "Trained batch 714 batch loss 7.40601873 epoch total loss 7.0126276\n",
      "Trained batch 715 batch loss 7.52836132 epoch total loss 7.01334906\n",
      "Trained batch 716 batch loss 7.41532946 epoch total loss 7.01391077\n",
      "Trained batch 717 batch loss 7.39677906 epoch total loss 7.01444483\n",
      "Trained batch 718 batch loss 6.53024626 epoch total loss 7.01377058\n",
      "Trained batch 719 batch loss 6.0695858 epoch total loss 7.01245785\n",
      "Trained batch 720 batch loss 6.21008825 epoch total loss 7.011343\n",
      "Trained batch 721 batch loss 6.87698889 epoch total loss 7.01115656\n",
      "Trained batch 722 batch loss 7.0177927 epoch total loss 7.01116562\n",
      "Trained batch 723 batch loss 6.79045 epoch total loss 7.01086044\n",
      "Trained batch 724 batch loss 7.33309746 epoch total loss 7.01130533\n",
      "Trained batch 725 batch loss 7.3233757 epoch total loss 7.01173544\n",
      "Trained batch 726 batch loss 7.28906584 epoch total loss 7.01211739\n",
      "Trained batch 727 batch loss 7.16076756 epoch total loss 7.01232195\n",
      "Trained batch 728 batch loss 6.60797358 epoch total loss 7.01176643\n",
      "Trained batch 729 batch loss 6.40737867 epoch total loss 7.01093721\n",
      "Trained batch 730 batch loss 7.20377636 epoch total loss 7.0112009\n",
      "Trained batch 731 batch loss 7.17226076 epoch total loss 7.01142168\n",
      "Trained batch 732 batch loss 7.0504117 epoch total loss 7.01147461\n",
      "Trained batch 733 batch loss 7.33408451 epoch total loss 7.01191473\n",
      "Trained batch 734 batch loss 7.12984276 epoch total loss 7.01207542\n",
      "Trained batch 735 batch loss 7.38683891 epoch total loss 7.01258516\n",
      "Trained batch 736 batch loss 7.39382935 epoch total loss 7.01310349\n",
      "Trained batch 737 batch loss 7.31651831 epoch total loss 7.013515\n",
      "Trained batch 738 batch loss 7.08510494 epoch total loss 7.01361179\n",
      "Trained batch 739 batch loss 6.90848684 epoch total loss 7.0134697\n",
      "Trained batch 740 batch loss 7.02293205 epoch total loss 7.01348257\n",
      "Trained batch 741 batch loss 7.29530764 epoch total loss 7.01386309\n",
      "Trained batch 742 batch loss 7.19337 epoch total loss 7.01410484\n",
      "Trained batch 743 batch loss 7.18670559 epoch total loss 7.01433706\n",
      "Trained batch 744 batch loss 7.06382465 epoch total loss 7.01440382\n",
      "Trained batch 745 batch loss 6.69725132 epoch total loss 7.013978\n",
      "Trained batch 746 batch loss 6.60559082 epoch total loss 7.01343\n",
      "Trained batch 747 batch loss 6.99357653 epoch total loss 7.01340389\n",
      "Trained batch 748 batch loss 7.13925076 epoch total loss 7.01357222\n",
      "Trained batch 749 batch loss 7.36544609 epoch total loss 7.01404142\n",
      "Trained batch 750 batch loss 7.32853794 epoch total loss 7.01446104\n",
      "Trained batch 751 batch loss 7.2166996 epoch total loss 7.01473045\n",
      "Trained batch 752 batch loss 7.14135885 epoch total loss 7.01489925\n",
      "Trained batch 753 batch loss 6.94647264 epoch total loss 7.01480818\n",
      "Trained batch 754 batch loss 7.09976816 epoch total loss 7.01492\n",
      "Trained batch 755 batch loss 7.12510872 epoch total loss 7.01506615\n",
      "Trained batch 756 batch loss 7.15680647 epoch total loss 7.01525354\n",
      "Trained batch 757 batch loss 6.57760429 epoch total loss 7.01467562\n",
      "Trained batch 758 batch loss 7.04212904 epoch total loss 7.01471138\n",
      "Trained batch 759 batch loss 6.51766157 epoch total loss 7.01405668\n",
      "Trained batch 760 batch loss 7.29816723 epoch total loss 7.01443052\n",
      "Trained batch 761 batch loss 7.09931803 epoch total loss 7.0145421\n",
      "Trained batch 762 batch loss 7.25980663 epoch total loss 7.01486397\n",
      "Trained batch 763 batch loss 7.2647171 epoch total loss 7.01519108\n",
      "Trained batch 764 batch loss 7.25663233 epoch total loss 7.01550722\n",
      "Trained batch 765 batch loss 7.17404318 epoch total loss 7.01571417\n",
      "Trained batch 766 batch loss 7.29165125 epoch total loss 7.01607418\n",
      "Trained batch 767 batch loss 7.3622303 epoch total loss 7.01652575\n",
      "Trained batch 768 batch loss 7.22161055 epoch total loss 7.01679277\n",
      "Trained batch 769 batch loss 7.25126076 epoch total loss 7.01709795\n",
      "Trained batch 770 batch loss 7.11236668 epoch total loss 7.01722193\n",
      "Trained batch 771 batch loss 6.63873816 epoch total loss 7.01673079\n",
      "Trained batch 772 batch loss 6.59977531 epoch total loss 7.01619053\n",
      "Trained batch 773 batch loss 7.24615526 epoch total loss 7.01648808\n",
      "Trained batch 774 batch loss 7.18360567 epoch total loss 7.01670361\n",
      "Trained batch 775 batch loss 6.80021238 epoch total loss 7.01642466\n",
      "Trained batch 776 batch loss 7.06815243 epoch total loss 7.01649141\n",
      "Trained batch 777 batch loss 7.30784512 epoch total loss 7.01686621\n",
      "Trained batch 778 batch loss 7.29805708 epoch total loss 7.01722717\n",
      "Trained batch 779 batch loss 7.10949135 epoch total loss 7.01734543\n",
      "Trained batch 780 batch loss 7.30281 epoch total loss 7.01771164\n",
      "Trained batch 781 batch loss 6.12599182 epoch total loss 7.01656961\n",
      "Trained batch 782 batch loss 6.79426193 epoch total loss 7.01628542\n",
      "Trained batch 783 batch loss 7.16741228 epoch total loss 7.01647854\n",
      "Trained batch 784 batch loss 7.08871222 epoch total loss 7.01657104\n",
      "Trained batch 785 batch loss 7.10217667 epoch total loss 7.01668\n",
      "Trained batch 786 batch loss 7.29770374 epoch total loss 7.01703787\n",
      "Trained batch 787 batch loss 7.20332956 epoch total loss 7.0172739\n",
      "Trained batch 788 batch loss 6.86465693 epoch total loss 7.01708078\n",
      "Trained batch 789 batch loss 7.35932636 epoch total loss 7.01751423\n",
      "Trained batch 790 batch loss 7.23274183 epoch total loss 7.01778698\n",
      "Trained batch 791 batch loss 7.0111 epoch total loss 7.01777887\n",
      "Trained batch 792 batch loss 7.13279343 epoch total loss 7.01792383\n",
      "Trained batch 793 batch loss 7.35747099 epoch total loss 7.01835203\n",
      "Trained batch 794 batch loss 6.76487398 epoch total loss 7.01803255\n",
      "Trained batch 795 batch loss 6.44200706 epoch total loss 7.01730776\n",
      "Trained batch 796 batch loss 6.46568108 epoch total loss 7.01661491\n",
      "Trained batch 797 batch loss 6.18948841 epoch total loss 7.01557732\n",
      "Trained batch 798 batch loss 6.28592062 epoch total loss 7.01466322\n",
      "Trained batch 799 batch loss 7.0193 epoch total loss 7.01466942\n",
      "Trained batch 800 batch loss 7.19786358 epoch total loss 7.0148983\n",
      "Trained batch 801 batch loss 7.32469749 epoch total loss 7.01528502\n",
      "Trained batch 802 batch loss 6.93893528 epoch total loss 7.01518965\n",
      "Trained batch 803 batch loss 6.06328 epoch total loss 7.01400471\n",
      "Trained batch 804 batch loss 5.6296196 epoch total loss 7.01228237\n",
      "Trained batch 805 batch loss 6.10805321 epoch total loss 7.01115894\n",
      "Trained batch 806 batch loss 6.71983099 epoch total loss 7.0107975\n",
      "Trained batch 807 batch loss 7.28793812 epoch total loss 7.01114082\n",
      "Trained batch 808 batch loss 7.5948596 epoch total loss 7.01186323\n",
      "Trained batch 809 batch loss 6.59847832 epoch total loss 7.01135254\n",
      "Trained batch 810 batch loss 6.2981391 epoch total loss 7.0104723\n",
      "Trained batch 811 batch loss 7.13977385 epoch total loss 7.01063156\n",
      "Trained batch 812 batch loss 7.1819315 epoch total loss 7.0108428\n",
      "Trained batch 813 batch loss 7.32716417 epoch total loss 7.0112319\n",
      "Trained batch 814 batch loss 7.46730375 epoch total loss 7.01179171\n",
      "Trained batch 815 batch loss 7.34319448 epoch total loss 7.01219845\n",
      "Trained batch 816 batch loss 7.4168973 epoch total loss 7.01269484\n",
      "Trained batch 817 batch loss 7.18144512 epoch total loss 7.01290131\n",
      "Trained batch 818 batch loss 7.33862114 epoch total loss 7.01329947\n",
      "Trained batch 819 batch loss 7.06127119 epoch total loss 7.01335764\n",
      "Trained batch 820 batch loss 6.95290518 epoch total loss 7.01328421\n",
      "Trained batch 821 batch loss 7.4502182 epoch total loss 7.01381636\n",
      "Trained batch 822 batch loss 7.17230749 epoch total loss 7.01400948\n",
      "Trained batch 823 batch loss 6.22510767 epoch total loss 7.01305056\n",
      "Trained batch 824 batch loss 6.57013798 epoch total loss 7.01251316\n",
      "Trained batch 825 batch loss 6.71083546 epoch total loss 7.0121479\n",
      "Trained batch 826 batch loss 6.46892452 epoch total loss 7.01149\n",
      "Trained batch 827 batch loss 5.82834673 epoch total loss 7.01005888\n",
      "Trained batch 828 batch loss 6.17044735 epoch total loss 7.00904512\n",
      "Trained batch 829 batch loss 6.22518873 epoch total loss 7.00809956\n",
      "Trained batch 830 batch loss 6.66025591 epoch total loss 7.00768\n",
      "Trained batch 831 batch loss 7.0388236 epoch total loss 7.00771809\n",
      "Trained batch 832 batch loss 6.9712739 epoch total loss 7.00767422\n",
      "Trained batch 833 batch loss 7.3664422 epoch total loss 7.00810432\n",
      "Trained batch 834 batch loss 7.01591969 epoch total loss 7.00811386\n",
      "Trained batch 835 batch loss 7.26950455 epoch total loss 7.00842714\n",
      "Trained batch 836 batch loss 7.26503611 epoch total loss 7.00873423\n",
      "Trained batch 837 batch loss 7.41087532 epoch total loss 7.0092144\n",
      "Trained batch 838 batch loss 7.52182961 epoch total loss 7.00982618\n",
      "Trained batch 839 batch loss 7.21801376 epoch total loss 7.01007414\n",
      "Trained batch 840 batch loss 7.32389593 epoch total loss 7.0104475\n",
      "Trained batch 841 batch loss 7.25809765 epoch total loss 7.01074219\n",
      "Trained batch 842 batch loss 7.07217693 epoch total loss 7.01081514\n",
      "Trained batch 843 batch loss 6.82696819 epoch total loss 7.01059723\n",
      "Trained batch 844 batch loss 6.83968687 epoch total loss 7.01039505\n",
      "Trained batch 845 batch loss 6.48050308 epoch total loss 7.00976801\n",
      "Trained batch 846 batch loss 6.96051884 epoch total loss 7.00971\n",
      "Trained batch 847 batch loss 6.70730877 epoch total loss 7.00935268\n",
      "Trained batch 848 batch loss 6.51705265 epoch total loss 7.00877237\n",
      "Trained batch 849 batch loss 6.413414 epoch total loss 7.00807142\n",
      "Trained batch 850 batch loss 6.65579414 epoch total loss 7.00765705\n",
      "Trained batch 851 batch loss 6.47901535 epoch total loss 7.00703573\n",
      "Trained batch 852 batch loss 7.30059481 epoch total loss 7.00738049\n",
      "Trained batch 853 batch loss 7.12054396 epoch total loss 7.00751305\n",
      "Trained batch 854 batch loss 7.44077635 epoch total loss 7.0080204\n",
      "Trained batch 855 batch loss 7.38482571 epoch total loss 7.00846148\n",
      "Trained batch 856 batch loss 7.35560465 epoch total loss 7.00886679\n",
      "Trained batch 857 batch loss 7.38510513 epoch total loss 7.00930595\n",
      "Trained batch 858 batch loss 7.2640028 epoch total loss 7.00960302\n",
      "Trained batch 859 batch loss 7.0857234 epoch total loss 7.00969172\n",
      "Trained batch 860 batch loss 6.85938 epoch total loss 7.00951672\n",
      "Trained batch 861 batch loss 7.18233442 epoch total loss 7.00971746\n",
      "Trained batch 862 batch loss 7.26509714 epoch total loss 7.01001358\n",
      "Trained batch 863 batch loss 7.34024048 epoch total loss 7.01039648\n",
      "Trained batch 864 batch loss 7.38211203 epoch total loss 7.01082706\n",
      "Trained batch 865 batch loss 7.01925 epoch total loss 7.0108366\n",
      "Trained batch 866 batch loss 7.34603691 epoch total loss 7.01122379\n",
      "Trained batch 867 batch loss 7.13643837 epoch total loss 7.0113678\n",
      "Trained batch 868 batch loss 7.21683598 epoch total loss 7.01160479\n",
      "Trained batch 869 batch loss 7.19797 epoch total loss 7.01181889\n",
      "Trained batch 870 batch loss 6.85564423 epoch total loss 7.01163912\n",
      "Trained batch 871 batch loss 6.8136735 epoch total loss 7.01141167\n",
      "Trained batch 872 batch loss 7.07454443 epoch total loss 7.01148415\n",
      "Trained batch 873 batch loss 7.37049723 epoch total loss 7.01189566\n",
      "Trained batch 874 batch loss 7.0982933 epoch total loss 7.01199436\n",
      "Trained batch 875 batch loss 7.1079917 epoch total loss 7.01210356\n",
      "Trained batch 876 batch loss 7.04158115 epoch total loss 7.01213741\n",
      "Trained batch 877 batch loss 6.54932785 epoch total loss 7.01160955\n",
      "Trained batch 878 batch loss 6.89364433 epoch total loss 7.01147509\n",
      "Trained batch 879 batch loss 7.17006826 epoch total loss 7.01165533\n",
      "Trained batch 880 batch loss 6.45137119 epoch total loss 7.01101828\n",
      "Trained batch 881 batch loss 7.07670784 epoch total loss 7.01109314\n",
      "Trained batch 882 batch loss 7.20174932 epoch total loss 7.01130915\n",
      "Trained batch 883 batch loss 7.07353115 epoch total loss 7.01137972\n",
      "Trained batch 884 batch loss 7.10313654 epoch total loss 7.01148367\n",
      "Trained batch 885 batch loss 6.02208424 epoch total loss 7.01036549\n",
      "Trained batch 886 batch loss 6.80159283 epoch total loss 7.01013\n",
      "Trained batch 887 batch loss 7.23136568 epoch total loss 7.01037931\n",
      "Trained batch 888 batch loss 7.36759806 epoch total loss 7.01078176\n",
      "Trained batch 889 batch loss 7.40115166 epoch total loss 7.01122093\n",
      "Trained batch 890 batch loss 7.44219589 epoch total loss 7.0117054\n",
      "Trained batch 891 batch loss 7.33626461 epoch total loss 7.01207\n",
      "Trained batch 892 batch loss 7.36217976 epoch total loss 7.01246262\n",
      "Trained batch 893 batch loss 7.32665586 epoch total loss 7.01281452\n",
      "Trained batch 894 batch loss 7.30800343 epoch total loss 7.01314497\n",
      "Trained batch 895 batch loss 7.24524 epoch total loss 7.01340389\n",
      "Trained batch 896 batch loss 7.37597799 epoch total loss 7.01380873\n",
      "Trained batch 897 batch loss 7.14639807 epoch total loss 7.01395655\n",
      "Trained batch 898 batch loss 7.09540367 epoch total loss 7.01404715\n",
      "Trained batch 899 batch loss 7.18150949 epoch total loss 7.01423359\n",
      "Trained batch 900 batch loss 7.45764399 epoch total loss 7.01472616\n",
      "Trained batch 901 batch loss 7.09692764 epoch total loss 7.01481771\n",
      "Trained batch 902 batch loss 7.25442219 epoch total loss 7.01508331\n",
      "Trained batch 903 batch loss 7.41241121 epoch total loss 7.01552343\n",
      "Trained batch 904 batch loss 7.08849716 epoch total loss 7.01560402\n",
      "Trained batch 905 batch loss 7.25241947 epoch total loss 7.0158658\n",
      "Trained batch 906 batch loss 7.0392766 epoch total loss 7.01589108\n",
      "Trained batch 907 batch loss 6.63741827 epoch total loss 7.01547384\n",
      "Trained batch 908 batch loss 7.07856 epoch total loss 7.01554346\n",
      "Trained batch 909 batch loss 7.1243124 epoch total loss 7.01566315\n",
      "Trained batch 910 batch loss 7.38269329 epoch total loss 7.01606655\n",
      "Trained batch 911 batch loss 7.111413 epoch total loss 7.01617098\n",
      "Trained batch 912 batch loss 6.69949532 epoch total loss 7.01582432\n",
      "Trained batch 913 batch loss 6.63062048 epoch total loss 7.01540232\n",
      "Trained batch 914 batch loss 6.94631052 epoch total loss 7.01532698\n",
      "Trained batch 915 batch loss 7.14217758 epoch total loss 7.01546526\n",
      "Trained batch 916 batch loss 7.14411592 epoch total loss 7.01560593\n",
      "Trained batch 917 batch loss 6.72005939 epoch total loss 7.01528358\n",
      "Trained batch 918 batch loss 6.62923956 epoch total loss 7.01486349\n",
      "Trained batch 919 batch loss 7.09126568 epoch total loss 7.01494646\n",
      "Trained batch 920 batch loss 6.97781134 epoch total loss 7.01490641\n",
      "Trained batch 921 batch loss 6.64186764 epoch total loss 7.01450157\n",
      "Trained batch 922 batch loss 6.69215393 epoch total loss 7.01415205\n",
      "Trained batch 923 batch loss 6.64751577 epoch total loss 7.01375484\n",
      "Trained batch 924 batch loss 6.43792057 epoch total loss 7.01313162\n",
      "Trained batch 925 batch loss 6.37629175 epoch total loss 7.01244354\n",
      "Trained batch 926 batch loss 6.91591835 epoch total loss 7.01233959\n",
      "Trained batch 927 batch loss 7.04323959 epoch total loss 7.01237297\n",
      "Trained batch 928 batch loss 6.43237352 epoch total loss 7.01174831\n",
      "Trained batch 929 batch loss 6.97050953 epoch total loss 7.01170397\n",
      "Trained batch 930 batch loss 7.3133688 epoch total loss 7.01202869\n",
      "Trained batch 931 batch loss 7.35679102 epoch total loss 7.0123992\n",
      "Trained batch 932 batch loss 7.41408205 epoch total loss 7.01283\n",
      "Trained batch 933 batch loss 6.98063278 epoch total loss 7.01279545\n",
      "Trained batch 934 batch loss 6.7975769 epoch total loss 7.01256466\n",
      "Trained batch 935 batch loss 6.92159939 epoch total loss 7.01246691\n",
      "Trained batch 936 batch loss 6.99041939 epoch total loss 7.01244354\n",
      "Trained batch 937 batch loss 7.11936426 epoch total loss 7.01255703\n",
      "Trained batch 938 batch loss 7.42089462 epoch total loss 7.01299238\n",
      "Trained batch 939 batch loss 7.48061371 epoch total loss 7.01349\n",
      "Trained batch 940 batch loss 6.94200754 epoch total loss 7.01341438\n",
      "Trained batch 941 batch loss 6.01380777 epoch total loss 7.01235199\n",
      "Trained batch 942 batch loss 6.64602089 epoch total loss 7.01196289\n",
      "Trained batch 943 batch loss 7.13026428 epoch total loss 7.0120883\n",
      "Trained batch 944 batch loss 6.96888781 epoch total loss 7.01204252\n",
      "Trained batch 945 batch loss 7.43365812 epoch total loss 7.01248884\n",
      "Trained batch 946 batch loss 7.38319 epoch total loss 7.0128808\n",
      "Trained batch 947 batch loss 7.36771393 epoch total loss 7.01325512\n",
      "Trained batch 948 batch loss 7.31125498 epoch total loss 7.01356936\n",
      "Trained batch 949 batch loss 7.31447506 epoch total loss 7.01388645\n",
      "Trained batch 950 batch loss 7.3117156 epoch total loss 7.01419973\n",
      "Trained batch 951 batch loss 7.43620062 epoch total loss 7.01464319\n",
      "Trained batch 952 batch loss 7.38810968 epoch total loss 7.01503563\n",
      "Trained batch 953 batch loss 7.37199211 epoch total loss 7.01541042\n",
      "Trained batch 954 batch loss 7.45227861 epoch total loss 7.01586819\n",
      "Trained batch 955 batch loss 7.07937479 epoch total loss 7.01593494\n",
      "Trained batch 956 batch loss 6.97888803 epoch total loss 7.01589632\n",
      "Trained batch 957 batch loss 7.096 epoch total loss 7.01598024\n",
      "Trained batch 958 batch loss 6.654 epoch total loss 7.01560211\n",
      "Trained batch 959 batch loss 6.84493208 epoch total loss 7.01542377\n",
      "Trained batch 960 batch loss 7.56738758 epoch total loss 7.01599884\n",
      "Trained batch 961 batch loss 7.39630032 epoch total loss 7.01639462\n",
      "Trained batch 962 batch loss 7.18382502 epoch total loss 7.01656866\n",
      "Trained batch 963 batch loss 7.287817 epoch total loss 7.01685\n",
      "Trained batch 964 batch loss 6.72036 epoch total loss 7.01654243\n",
      "Trained batch 965 batch loss 7.06802559 epoch total loss 7.01659536\n",
      "Trained batch 966 batch loss 6.9934659 epoch total loss 7.01657152\n",
      "Trained batch 967 batch loss 6.86401653 epoch total loss 7.01641417\n",
      "Trained batch 968 batch loss 6.97534 epoch total loss 7.01637173\n",
      "Trained batch 969 batch loss 7.13527107 epoch total loss 7.01649427\n",
      "Trained batch 970 batch loss 7.20716476 epoch total loss 7.01669073\n",
      "Trained batch 971 batch loss 7.42118025 epoch total loss 7.01710749\n",
      "Trained batch 972 batch loss 7.38754559 epoch total loss 7.01748848\n",
      "Trained batch 973 batch loss 6.96337843 epoch total loss 7.01743317\n",
      "Trained batch 974 batch loss 6.68634605 epoch total loss 7.01709318\n",
      "Trained batch 975 batch loss 7.12341404 epoch total loss 7.01720238\n",
      "Trained batch 976 batch loss 7.0631566 epoch total loss 7.01724958\n",
      "Trained batch 977 batch loss 6.73972845 epoch total loss 7.01696539\n",
      "Trained batch 978 batch loss 7.13384056 epoch total loss 7.01708508\n",
      "Trained batch 979 batch loss 6.90113 epoch total loss 7.01696682\n",
      "Trained batch 980 batch loss 6.67004 epoch total loss 7.01661253\n",
      "Trained batch 981 batch loss 7.24503851 epoch total loss 7.01684523\n",
      "Trained batch 982 batch loss 7.52295589 epoch total loss 7.01736069\n",
      "Trained batch 983 batch loss 7.29453516 epoch total loss 7.0176425\n",
      "Trained batch 984 batch loss 7.22178078 epoch total loss 7.01785\n",
      "Trained batch 985 batch loss 7.033566 epoch total loss 7.01786613\n",
      "Trained batch 986 batch loss 6.94727135 epoch total loss 7.01779461\n",
      "Trained batch 987 batch loss 7.20420361 epoch total loss 7.01798344\n",
      "Trained batch 988 batch loss 6.50738382 epoch total loss 7.01746655\n",
      "Trained batch 989 batch loss 6.23949814 epoch total loss 7.01668\n",
      "Trained batch 990 batch loss 6.1942029 epoch total loss 7.01584911\n",
      "Trained batch 991 batch loss 6.67950201 epoch total loss 7.01550961\n",
      "Trained batch 992 batch loss 6.67799187 epoch total loss 7.01516962\n",
      "Trained batch 993 batch loss 7.18015432 epoch total loss 7.01533604\n",
      "Trained batch 994 batch loss 6.78980589 epoch total loss 7.01510906\n",
      "Trained batch 995 batch loss 6.8257432 epoch total loss 7.0149188\n",
      "Trained batch 996 batch loss 6.24560404 epoch total loss 7.01414633\n",
      "Trained batch 997 batch loss 7.17908335 epoch total loss 7.01431179\n",
      "Trained batch 998 batch loss 7.02139139 epoch total loss 7.01431894\n",
      "Trained batch 999 batch loss 6.84421349 epoch total loss 7.01414871\n",
      "Trained batch 1000 batch loss 6.64710665 epoch total loss 7.01378155\n",
      "Trained batch 1001 batch loss 7.10595369 epoch total loss 7.01387405\n",
      "Trained batch 1002 batch loss 7.33176327 epoch total loss 7.01419067\n",
      "Trained batch 1003 batch loss 7.42903662 epoch total loss 7.01460457\n",
      "Trained batch 1004 batch loss 7.41802 epoch total loss 7.01500654\n",
      "Trained batch 1005 batch loss 7.1037879 epoch total loss 7.01509476\n",
      "Trained batch 1006 batch loss 6.60714102 epoch total loss 7.01468945\n",
      "Trained batch 1007 batch loss 6.96129799 epoch total loss 7.01463652\n",
      "Trained batch 1008 batch loss 7.0989337 epoch total loss 7.01472\n",
      "Trained batch 1009 batch loss 6.99887037 epoch total loss 7.0147047\n",
      "Trained batch 1010 batch loss 7.06569719 epoch total loss 7.01475525\n",
      "Trained batch 1011 batch loss 6.24125624 epoch total loss 7.01399\n",
      "Trained batch 1012 batch loss 6.40270424 epoch total loss 7.01338625\n",
      "Trained batch 1013 batch loss 7.10433388 epoch total loss 7.01347637\n",
      "Trained batch 1014 batch loss 7.02139854 epoch total loss 7.013484\n",
      "Trained batch 1015 batch loss 7.35460901 epoch total loss 7.01382\n",
      "Trained batch 1016 batch loss 7.45413971 epoch total loss 7.01425362\n",
      "Trained batch 1017 batch loss 7.23661661 epoch total loss 7.01447201\n",
      "Trained batch 1018 batch loss 6.61629486 epoch total loss 7.014081\n",
      "Trained batch 1019 batch loss 6.48823357 epoch total loss 7.01356506\n",
      "Trained batch 1020 batch loss 6.80910492 epoch total loss 7.01336432\n",
      "Trained batch 1021 batch loss 6.84483337 epoch total loss 7.01319933\n",
      "Trained batch 1022 batch loss 6.86233044 epoch total loss 7.01305151\n",
      "Trained batch 1023 batch loss 6.69785 epoch total loss 7.01274347\n",
      "Trained batch 1024 batch loss 7.22755957 epoch total loss 7.01295328\n",
      "Trained batch 1025 batch loss 6.8134613 epoch total loss 7.01275873\n",
      "Trained batch 1026 batch loss 6.91716194 epoch total loss 7.01266527\n",
      "Trained batch 1027 batch loss 6.76440477 epoch total loss 7.01242399\n",
      "Trained batch 1028 batch loss 7.11719608 epoch total loss 7.01252556\n",
      "Trained batch 1029 batch loss 6.92228746 epoch total loss 7.0124383\n",
      "Trained batch 1030 batch loss 6.85832834 epoch total loss 7.01228857\n",
      "Trained batch 1031 batch loss 6.93742 epoch total loss 7.01221609\n",
      "Trained batch 1032 batch loss 6.81751108 epoch total loss 7.01202726\n",
      "Trained batch 1033 batch loss 6.67452383 epoch total loss 7.0117\n",
      "Trained batch 1034 batch loss 6.97208738 epoch total loss 7.01166201\n",
      "Trained batch 1035 batch loss 7.05747604 epoch total loss 7.01170635\n",
      "Trained batch 1036 batch loss 6.70774078 epoch total loss 7.0114131\n",
      "Trained batch 1037 batch loss 6.97731876 epoch total loss 7.01138\n",
      "Trained batch 1038 batch loss 7.23250294 epoch total loss 7.01159334\n",
      "Trained batch 1039 batch loss 7.06843615 epoch total loss 7.0116477\n",
      "Trained batch 1040 batch loss 6.55361176 epoch total loss 7.01120758\n",
      "Trained batch 1041 batch loss 6.52995205 epoch total loss 7.01074505\n",
      "Trained batch 1042 batch loss 6.98932362 epoch total loss 7.01072454\n",
      "Trained batch 1043 batch loss 7.18159819 epoch total loss 7.0108881\n",
      "Trained batch 1044 batch loss 6.88652086 epoch total loss 7.01076937\n",
      "Trained batch 1045 batch loss 6.10364294 epoch total loss 7.00990105\n",
      "Trained batch 1046 batch loss 6.39545345 epoch total loss 7.00931358\n",
      "Trained batch 1047 batch loss 6.26579332 epoch total loss 7.00860357\n",
      "Trained batch 1048 batch loss 6.78689957 epoch total loss 7.00839233\n",
      "Trained batch 1049 batch loss 6.84589338 epoch total loss 7.00823689\n",
      "Trained batch 1050 batch loss 7.09437323 epoch total loss 7.0083189\n",
      "Trained batch 1051 batch loss 7.10570669 epoch total loss 7.00841141\n",
      "Trained batch 1052 batch loss 7.17430878 epoch total loss 7.00856924\n",
      "Trained batch 1053 batch loss 7.29391 epoch total loss 7.00884\n",
      "Trained batch 1054 batch loss 7.14275742 epoch total loss 7.00896692\n",
      "Trained batch 1055 batch loss 6.90975285 epoch total loss 7.00887299\n",
      "Trained batch 1056 batch loss 7.15937805 epoch total loss 7.00901508\n",
      "Trained batch 1057 batch loss 6.91764164 epoch total loss 7.00892878\n",
      "Trained batch 1058 batch loss 6.82341194 epoch total loss 7.0087533\n",
      "Trained batch 1059 batch loss 6.92200375 epoch total loss 7.00867081\n",
      "Trained batch 1060 batch loss 6.81294584 epoch total loss 7.00848627\n",
      "Trained batch 1061 batch loss 6.927104 epoch total loss 7.00841\n",
      "Trained batch 1062 batch loss 6.50789213 epoch total loss 7.00793839\n",
      "Trained batch 1063 batch loss 6.23965073 epoch total loss 7.00721598\n",
      "Trained batch 1064 batch loss 7.12311935 epoch total loss 7.0073247\n",
      "Trained batch 1065 batch loss 6.6435647 epoch total loss 7.00698328\n",
      "Trained batch 1066 batch loss 7.11035538 epoch total loss 7.00708\n",
      "Trained batch 1067 batch loss 7.13860035 epoch total loss 7.00720358\n",
      "Trained batch 1068 batch loss 5.94805241 epoch total loss 7.00621176\n",
      "Trained batch 1069 batch loss 6.88144445 epoch total loss 7.00609493\n",
      "Trained batch 1070 batch loss 7.17871523 epoch total loss 7.00625658\n",
      "Trained batch 1071 batch loss 7.02773857 epoch total loss 7.00627661\n",
      "Trained batch 1072 batch loss 7.40474033 epoch total loss 7.00664806\n",
      "Trained batch 1073 batch loss 7.24364328 epoch total loss 7.00686932\n",
      "Trained batch 1074 batch loss 7.30797958 epoch total loss 7.0071497\n",
      "Trained batch 1075 batch loss 7.34030676 epoch total loss 7.00745964\n",
      "Trained batch 1076 batch loss 7.25832558 epoch total loss 7.00769281\n",
      "Trained batch 1077 batch loss 6.75608301 epoch total loss 7.00745869\n",
      "Trained batch 1078 batch loss 6.93956089 epoch total loss 7.00739574\n",
      "Trained batch 1079 batch loss 7.02628088 epoch total loss 7.00741339\n",
      "Trained batch 1080 batch loss 7.15292454 epoch total loss 7.00754786\n",
      "Trained batch 1081 batch loss 7.05077 epoch total loss 7.00758791\n",
      "Trained batch 1082 batch loss 7.13158226 epoch total loss 7.00770235\n",
      "Trained batch 1083 batch loss 6.93528 epoch total loss 7.00763512\n",
      "Trained batch 1084 batch loss 6.07116795 epoch total loss 7.00677156\n",
      "Trained batch 1085 batch loss 6.10049629 epoch total loss 7.00593615\n",
      "Trained batch 1086 batch loss 6.26081276 epoch total loss 7.00525\n",
      "Trained batch 1087 batch loss 7.39209461 epoch total loss 7.00560617\n",
      "Trained batch 1088 batch loss 7.26051188 epoch total loss 7.0058403\n",
      "Trained batch 1089 batch loss 7.00945139 epoch total loss 7.00584364\n",
      "Trained batch 1090 batch loss 6.94363785 epoch total loss 7.0057869\n",
      "Trained batch 1091 batch loss 7.13234043 epoch total loss 7.00590277\n",
      "Trained batch 1092 batch loss 7.36870241 epoch total loss 7.00623512\n",
      "Trained batch 1093 batch loss 7.31101322 epoch total loss 7.00651407\n",
      "Trained batch 1094 batch loss 7.41314411 epoch total loss 7.00688553\n",
      "Trained batch 1095 batch loss 7.18582773 epoch total loss 7.00704908\n",
      "Trained batch 1096 batch loss 7.34534931 epoch total loss 7.0073576\n",
      "Trained batch 1097 batch loss 7.14836884 epoch total loss 7.00748634\n",
      "Trained batch 1098 batch loss 6.44405699 epoch total loss 7.00697279\n",
      "Trained batch 1099 batch loss 6.8735342 epoch total loss 7.00685167\n",
      "Trained batch 1100 batch loss 7.34834337 epoch total loss 7.00716162\n",
      "Trained batch 1101 batch loss 7.3644042 epoch total loss 7.00748587\n",
      "Trained batch 1102 batch loss 7.14122772 epoch total loss 7.00760746\n",
      "Trained batch 1103 batch loss 6.16975832 epoch total loss 7.00684786\n",
      "Trained batch 1104 batch loss 6.49454355 epoch total loss 7.0063839\n",
      "Trained batch 1105 batch loss 6.6686883 epoch total loss 7.00607824\n",
      "Trained batch 1106 batch loss 7.14411545 epoch total loss 7.0062027\n",
      "Trained batch 1107 batch loss 7.20466375 epoch total loss 7.00638199\n",
      "Trained batch 1108 batch loss 7.19688463 epoch total loss 7.00655413\n",
      "Trained batch 1109 batch loss 7.42952633 epoch total loss 7.0069356\n",
      "Trained batch 1110 batch loss 7.38705254 epoch total loss 7.00727797\n",
      "Trained batch 1111 batch loss 7.27368879 epoch total loss 7.00751781\n",
      "Trained batch 1112 batch loss 7.03400469 epoch total loss 7.00754213\n",
      "Trained batch 1113 batch loss 7.04842472 epoch total loss 7.00757885\n",
      "Trained batch 1114 batch loss 6.82427692 epoch total loss 7.00741386\n",
      "Trained batch 1115 batch loss 7.29351664 epoch total loss 7.0076704\n",
      "Trained batch 1116 batch loss 7.35351896 epoch total loss 7.00798035\n",
      "Trained batch 1117 batch loss 7.36029959 epoch total loss 7.00829601\n",
      "Trained batch 1118 batch loss 7.06855869 epoch total loss 7.00835\n",
      "Trained batch 1119 batch loss 7.27100515 epoch total loss 7.0085845\n",
      "Trained batch 1120 batch loss 7.30801153 epoch total loss 7.00885201\n",
      "Trained batch 1121 batch loss 7.19812298 epoch total loss 7.00902081\n",
      "Trained batch 1122 batch loss 7.36323452 epoch total loss 7.00933647\n",
      "Trained batch 1123 batch loss 7.38963604 epoch total loss 7.00967503\n",
      "Trained batch 1124 batch loss 7.20627356 epoch total loss 7.00985\n",
      "Trained batch 1125 batch loss 7.01627636 epoch total loss 7.00985527\n",
      "Trained batch 1126 batch loss 7.17203236 epoch total loss 7.00999928\n",
      "Trained batch 1127 batch loss 7.09110928 epoch total loss 7.01007128\n",
      "Trained batch 1128 batch loss 6.62859821 epoch total loss 7.0097332\n",
      "Trained batch 1129 batch loss 6.20033741 epoch total loss 7.00901604\n",
      "Trained batch 1130 batch loss 6.5153904 epoch total loss 7.00857925\n",
      "Trained batch 1131 batch loss 7.01554966 epoch total loss 7.00858593\n",
      "Trained batch 1132 batch loss 7.15225744 epoch total loss 7.00871277\n",
      "Trained batch 1133 batch loss 7.4186635 epoch total loss 7.00907421\n",
      "Trained batch 1134 batch loss 6.95739269 epoch total loss 7.00902891\n",
      "Trained batch 1135 batch loss 6.1641531 epoch total loss 7.00828457\n",
      "Trained batch 1136 batch loss 6.97288942 epoch total loss 7.0082531\n",
      "Trained batch 1137 batch loss 7.03158951 epoch total loss 7.0082736\n",
      "Trained batch 1138 batch loss 7.36013317 epoch total loss 7.00858307\n",
      "Trained batch 1139 batch loss 7.27937555 epoch total loss 7.00882101\n",
      "Trained batch 1140 batch loss 6.86744881 epoch total loss 7.00869703\n",
      "Trained batch 1141 batch loss 7.09170818 epoch total loss 7.00877\n",
      "Trained batch 1142 batch loss 6.52600479 epoch total loss 7.00834703\n",
      "Trained batch 1143 batch loss 7.18821812 epoch total loss 7.00850391\n",
      "Trained batch 1144 batch loss 7.00660086 epoch total loss 7.00850248\n",
      "Trained batch 1145 batch loss 7.31204319 epoch total loss 7.0087676\n",
      "Trained batch 1146 batch loss 7.000247 epoch total loss 7.00876045\n",
      "Trained batch 1147 batch loss 7.13740206 epoch total loss 7.00887251\n",
      "Trained batch 1148 batch loss 7.02775955 epoch total loss 7.0088892\n",
      "Trained batch 1149 batch loss 7.08060122 epoch total loss 7.00895119\n",
      "Trained batch 1150 batch loss 7.30213165 epoch total loss 7.0092063\n",
      "Trained batch 1151 batch loss 7.08184147 epoch total loss 7.00926971\n",
      "Trained batch 1152 batch loss 7.23586273 epoch total loss 7.00946617\n",
      "Trained batch 1153 batch loss 7.20482206 epoch total loss 7.00963545\n",
      "Trained batch 1154 batch loss 7.50732613 epoch total loss 7.01006699\n",
      "Trained batch 1155 batch loss 7.27817392 epoch total loss 7.01029921\n",
      "Trained batch 1156 batch loss 7.16886 epoch total loss 7.01043653\n",
      "Trained batch 1157 batch loss 7.04205513 epoch total loss 7.01046371\n",
      "Trained batch 1158 batch loss 7.2915554 epoch total loss 7.01070642\n",
      "Trained batch 1159 batch loss 7.126966 epoch total loss 7.01080656\n",
      "Trained batch 1160 batch loss 7.39740372 epoch total loss 7.01114\n",
      "Trained batch 1161 batch loss 7.30895424 epoch total loss 7.01139641\n",
      "Trained batch 1162 batch loss 7.28128338 epoch total loss 7.01162863\n",
      "Trained batch 1163 batch loss 7.31308031 epoch total loss 7.01188803\n",
      "Trained batch 1164 batch loss 7.27272463 epoch total loss 7.01211214\n",
      "Trained batch 1165 batch loss 7.06913805 epoch total loss 7.01216125\n",
      "Trained batch 1166 batch loss 7.41763639 epoch total loss 7.01250887\n",
      "Trained batch 1167 batch loss 6.91893387 epoch total loss 7.01242876\n",
      "Trained batch 1168 batch loss 6.96493578 epoch total loss 7.01238823\n",
      "Trained batch 1169 batch loss 6.95994759 epoch total loss 7.01234293\n",
      "Trained batch 1170 batch loss 6.20195436 epoch total loss 7.01165\n",
      "Trained batch 1171 batch loss 6.51583576 epoch total loss 7.01122665\n",
      "Trained batch 1172 batch loss 7.13924122 epoch total loss 7.01133633\n",
      "Trained batch 1173 batch loss 6.35141754 epoch total loss 7.01077366\n",
      "Trained batch 1174 batch loss 7.11935 epoch total loss 7.01086617\n",
      "Trained batch 1175 batch loss 6.84970617 epoch total loss 7.01072884\n",
      "Trained batch 1176 batch loss 6.33795404 epoch total loss 7.01015663\n",
      "Trained batch 1177 batch loss 6.91464567 epoch total loss 7.01007605\n",
      "Trained batch 1178 batch loss 7.38952112 epoch total loss 7.01039839\n",
      "Trained batch 1179 batch loss 7.01509619 epoch total loss 7.01040173\n",
      "Trained batch 1180 batch loss 7.24240208 epoch total loss 7.01059818\n",
      "Trained batch 1181 batch loss 7.17728186 epoch total loss 7.01074\n",
      "Trained batch 1182 batch loss 6.79551029 epoch total loss 7.01055813\n",
      "Trained batch 1183 batch loss 7.00159836 epoch total loss 7.0105505\n",
      "Trained batch 1184 batch loss 6.58316326 epoch total loss 7.01018953\n",
      "Trained batch 1185 batch loss 6.90461063 epoch total loss 7.01010036\n",
      "Trained batch 1186 batch loss 6.99019766 epoch total loss 7.01008368\n",
      "Trained batch 1187 batch loss 7.23159075 epoch total loss 7.01027\n",
      "Trained batch 1188 batch loss 7.20412779 epoch total loss 7.0104332\n",
      "Trained batch 1189 batch loss 7.46532249 epoch total loss 7.01081514\n",
      "Trained batch 1190 batch loss 7.46067524 epoch total loss 7.01119375\n",
      "Trained batch 1191 batch loss 7.43560934 epoch total loss 7.01155\n",
      "Trained batch 1192 batch loss 7.09895563 epoch total loss 7.01162291\n",
      "Trained batch 1193 batch loss 7.32353 epoch total loss 7.01188421\n",
      "Trained batch 1194 batch loss 7.12051105 epoch total loss 7.01197481\n",
      "Trained batch 1195 batch loss 7.40265 epoch total loss 7.01230145\n",
      "Trained batch 1196 batch loss 7.43976831 epoch total loss 7.0126586\n",
      "Trained batch 1197 batch loss 7.21446466 epoch total loss 7.0128274\n",
      "Trained batch 1198 batch loss 7.13297653 epoch total loss 7.01292753\n",
      "Trained batch 1199 batch loss 7.00828218 epoch total loss 7.01292324\n",
      "Trained batch 1200 batch loss 7.28509378 epoch total loss 7.01315\n",
      "Trained batch 1201 batch loss 7.42213058 epoch total loss 7.01349068\n",
      "Trained batch 1202 batch loss 7.24407482 epoch total loss 7.01368237\n",
      "Trained batch 1203 batch loss 7.39456892 epoch total loss 7.01399899\n",
      "Trained batch 1204 batch loss 7.45772 epoch total loss 7.01436758\n",
      "Trained batch 1205 batch loss 7.30192614 epoch total loss 7.01460648\n",
      "Trained batch 1206 batch loss 7.27629089 epoch total loss 7.01482344\n",
      "Trained batch 1207 batch loss 7.08734226 epoch total loss 7.01488304\n",
      "Trained batch 1208 batch loss 6.96394 epoch total loss 7.0148406\n",
      "Trained batch 1209 batch loss 7.02465725 epoch total loss 7.01484871\n",
      "Trained batch 1210 batch loss 7.44004 epoch total loss 7.01520061\n",
      "Trained batch 1211 batch loss 7.30798912 epoch total loss 7.01544189\n",
      "Trained batch 1212 batch loss 7.24726868 epoch total loss 7.01563311\n",
      "Trained batch 1213 batch loss 7.15779924 epoch total loss 7.01575041\n",
      "Trained batch 1214 batch loss 7.17147827 epoch total loss 7.01587915\n",
      "Trained batch 1215 batch loss 6.80493212 epoch total loss 7.01570559\n",
      "Trained batch 1216 batch loss 6.52531862 epoch total loss 7.01530218\n",
      "Trained batch 1217 batch loss 6.41556168 epoch total loss 7.01480961\n",
      "Trained batch 1218 batch loss 7.0922513 epoch total loss 7.01487303\n",
      "Trained batch 1219 batch loss 6.84831381 epoch total loss 7.01473665\n",
      "Trained batch 1220 batch loss 6.97744894 epoch total loss 7.01470613\n",
      "Trained batch 1221 batch loss 6.77844715 epoch total loss 7.01451254\n",
      "Trained batch 1222 batch loss 7.13666773 epoch total loss 7.01461267\n",
      "Trained batch 1223 batch loss 6.99530649 epoch total loss 7.01459646\n",
      "Trained batch 1224 batch loss 6.59638596 epoch total loss 7.01425505\n",
      "Trained batch 1225 batch loss 6.12989902 epoch total loss 7.01353312\n",
      "Trained batch 1226 batch loss 6.08837652 epoch total loss 7.01277828\n",
      "Trained batch 1227 batch loss 6.4984436 epoch total loss 7.01235867\n",
      "Trained batch 1228 batch loss 6.49394608 epoch total loss 7.01193666\n",
      "Trained batch 1229 batch loss 7.20492554 epoch total loss 7.01209402\n",
      "Trained batch 1230 batch loss 7.27893 epoch total loss 7.01231098\n",
      "Trained batch 1231 batch loss 6.59587 epoch total loss 7.01197243\n",
      "Trained batch 1232 batch loss 7.06338739 epoch total loss 7.01201439\n",
      "Trained batch 1233 batch loss 7.01888657 epoch total loss 7.01201963\n",
      "Trained batch 1234 batch loss 6.82518578 epoch total loss 7.01186848\n",
      "Trained batch 1235 batch loss 7.27480364 epoch total loss 7.01208115\n",
      "Trained batch 1236 batch loss 7.44342422 epoch total loss 7.01242971\n",
      "Trained batch 1237 batch loss 7.33583832 epoch total loss 7.0126915\n",
      "Trained batch 1238 batch loss 7.08448219 epoch total loss 7.01274967\n",
      "Trained batch 1239 batch loss 6.94949532 epoch total loss 7.01269865\n",
      "Trained batch 1240 batch loss 6.95743847 epoch total loss 7.01265335\n",
      "Trained batch 1241 batch loss 6.83551502 epoch total loss 7.01251125\n",
      "Trained batch 1242 batch loss 6.86554527 epoch total loss 7.01239252\n",
      "Trained batch 1243 batch loss 6.96838808 epoch total loss 7.01235771\n",
      "Trained batch 1244 batch loss 6.74676847 epoch total loss 7.01214409\n",
      "Trained batch 1245 batch loss 6.66889191 epoch total loss 7.01186848\n",
      "Trained batch 1246 batch loss 7.22421312 epoch total loss 7.01203918\n",
      "Trained batch 1247 batch loss 7.33711052 epoch total loss 7.0123\n",
      "Trained batch 1248 batch loss 7.38994169 epoch total loss 7.01260233\n",
      "Trained batch 1249 batch loss 7.30177546 epoch total loss 7.0128336\n",
      "Trained batch 1250 batch loss 7.44764328 epoch total loss 7.01318121\n",
      "Trained batch 1251 batch loss 7.51247883 epoch total loss 7.01358032\n",
      "Trained batch 1252 batch loss 7.20390654 epoch total loss 7.01373291\n",
      "Trained batch 1253 batch loss 7.10979 epoch total loss 7.0138092\n",
      "Trained batch 1254 batch loss 7.21709585 epoch total loss 7.01397085\n",
      "Trained batch 1255 batch loss 7.30059862 epoch total loss 7.01419926\n",
      "Trained batch 1256 batch loss 7.3560276 epoch total loss 7.01447201\n",
      "Trained batch 1257 batch loss 7.32504463 epoch total loss 7.01471901\n",
      "Trained batch 1258 batch loss 7.42604446 epoch total loss 7.01504612\n",
      "Trained batch 1259 batch loss 7.35355663 epoch total loss 7.01531458\n",
      "Trained batch 1260 batch loss 7.3066678 epoch total loss 7.01554585\n",
      "Trained batch 1261 batch loss 6.77114105 epoch total loss 7.01535225\n",
      "Trained batch 1262 batch loss 7.18482399 epoch total loss 7.01548672\n",
      "Trained batch 1263 batch loss 7.42597342 epoch total loss 7.01581144\n",
      "Trained batch 1264 batch loss 6.8331933 epoch total loss 7.01566648\n",
      "Trained batch 1265 batch loss 7.12704277 epoch total loss 7.0157547\n",
      "Trained batch 1266 batch loss 6.91286469 epoch total loss 7.01567364\n",
      "Trained batch 1267 batch loss 6.80511665 epoch total loss 7.01550722\n",
      "Trained batch 1268 batch loss 6.96769762 epoch total loss 7.01546955\n",
      "Trained batch 1269 batch loss 7.30289 epoch total loss 7.01569557\n",
      "Trained batch 1270 batch loss 7.33029 epoch total loss 7.01594353\n",
      "Trained batch 1271 batch loss 7.47280216 epoch total loss 7.01630259\n",
      "Trained batch 1272 batch loss 7.12260866 epoch total loss 7.01638651\n",
      "Trained batch 1273 batch loss 7.23816204 epoch total loss 7.01656103\n",
      "Trained batch 1274 batch loss 7.33371067 epoch total loss 7.01681\n",
      "Trained batch 1275 batch loss 7.18889093 epoch total loss 7.01694489\n",
      "Trained batch 1276 batch loss 6.71820354 epoch total loss 7.01671028\n",
      "Trained batch 1277 batch loss 6.4153614 epoch total loss 7.01623917\n",
      "Trained batch 1278 batch loss 6.90736389 epoch total loss 7.01615381\n",
      "Trained batch 1279 batch loss 6.90451431 epoch total loss 7.01606655\n",
      "Trained batch 1280 batch loss 7.16345215 epoch total loss 7.01618099\n",
      "Trained batch 1281 batch loss 7.37231255 epoch total loss 7.01645899\n",
      "Trained batch 1282 batch loss 7.42574787 epoch total loss 7.01677847\n",
      "Trained batch 1283 batch loss 7.33703 epoch total loss 7.01702785\n",
      "Trained batch 1284 batch loss 7.2540226 epoch total loss 7.01721239\n",
      "Trained batch 1285 batch loss 7.18062305 epoch total loss 7.01733971\n",
      "Trained batch 1286 batch loss 7.27154064 epoch total loss 7.01753712\n",
      "Trained batch 1287 batch loss 6.50794077 epoch total loss 7.01714087\n",
      "Trained batch 1288 batch loss 7.01816893 epoch total loss 7.0171423\n",
      "Trained batch 1289 batch loss 6.94658 epoch total loss 7.01708698\n",
      "Trained batch 1290 batch loss 6.62877607 epoch total loss 7.0167861\n",
      "Trained batch 1291 batch loss 6.87817287 epoch total loss 7.01667881\n",
      "Trained batch 1292 batch loss 7.18855 epoch total loss 7.01681185\n",
      "Trained batch 1293 batch loss 6.65547228 epoch total loss 7.01653194\n",
      "Trained batch 1294 batch loss 7.06487274 epoch total loss 7.01656914\n",
      "Trained batch 1295 batch loss 6.79004145 epoch total loss 7.01639414\n",
      "Trained batch 1296 batch loss 6.73355675 epoch total loss 7.01617575\n",
      "Trained batch 1297 batch loss 6.73114061 epoch total loss 7.0159564\n",
      "Trained batch 1298 batch loss 7.12258148 epoch total loss 7.01603889\n",
      "Trained batch 1299 batch loss 6.94071817 epoch total loss 7.01598072\n",
      "Trained batch 1300 batch loss 7.01078081 epoch total loss 7.01597643\n",
      "Trained batch 1301 batch loss 7.2922616 epoch total loss 7.01618862\n",
      "Trained batch 1302 batch loss 7.21297455 epoch total loss 7.01634\n",
      "Trained batch 1303 batch loss 6.92242098 epoch total loss 7.01626825\n",
      "Trained batch 1304 batch loss 6.78381157 epoch total loss 7.01609\n",
      "Trained batch 1305 batch loss 6.46727562 epoch total loss 7.01566935\n",
      "Trained batch 1306 batch loss 6.65429974 epoch total loss 7.0153923\n",
      "Trained batch 1307 batch loss 6.87422228 epoch total loss 7.01528406\n",
      "Trained batch 1308 batch loss 6.65472651 epoch total loss 7.01500845\n",
      "Trained batch 1309 batch loss 5.97736502 epoch total loss 7.01421595\n",
      "Trained batch 1310 batch loss 6.23928833 epoch total loss 7.01362419\n",
      "Trained batch 1311 batch loss 5.7776475 epoch total loss 7.01268101\n",
      "Trained batch 1312 batch loss 6.25846577 epoch total loss 7.01210642\n",
      "Trained batch 1313 batch loss 6.53143263 epoch total loss 7.01174\n",
      "Trained batch 1314 batch loss 6.4652667 epoch total loss 7.01132393\n",
      "Trained batch 1315 batch loss 6.87726355 epoch total loss 7.01122189\n",
      "Trained batch 1316 batch loss 6.836308 epoch total loss 7.01108885\n",
      "Trained batch 1317 batch loss 6.74640942 epoch total loss 7.01088762\n",
      "Trained batch 1318 batch loss 6.66156435 epoch total loss 7.01062202\n",
      "Trained batch 1319 batch loss 6.81039333 epoch total loss 7.01047039\n",
      "Trained batch 1320 batch loss 7.10409117 epoch total loss 7.01054192\n",
      "Trained batch 1321 batch loss 7.0214982 epoch total loss 7.01055\n",
      "Trained batch 1322 batch loss 7.54283476 epoch total loss 7.01095295\n",
      "Trained batch 1323 batch loss 6.41216278 epoch total loss 7.0105\n",
      "Trained batch 1324 batch loss 5.70940399 epoch total loss 7.00951719\n",
      "Trained batch 1325 batch loss 6.24529171 epoch total loss 7.00894\n",
      "Trained batch 1326 batch loss 6.88686943 epoch total loss 7.00884819\n",
      "Trained batch 1327 batch loss 6.47138 epoch total loss 7.00844336\n",
      "Trained batch 1328 batch loss 7.31278563 epoch total loss 7.00867224\n",
      "Trained batch 1329 batch loss 7.19000053 epoch total loss 7.00880909\n",
      "Trained batch 1330 batch loss 6.9238987 epoch total loss 7.00874519\n",
      "Trained batch 1331 batch loss 7.16914415 epoch total loss 7.00886536\n",
      "Trained batch 1332 batch loss 6.87484646 epoch total loss 7.00876474\n",
      "Trained batch 1333 batch loss 6.84523296 epoch total loss 7.00864267\n",
      "Trained batch 1334 batch loss 6.71740532 epoch total loss 7.00842428\n",
      "Trained batch 1335 batch loss 6.71477365 epoch total loss 7.00820446\n",
      "Trained batch 1336 batch loss 7.07469082 epoch total loss 7.00825405\n",
      "Trained batch 1337 batch loss 6.97715902 epoch total loss 7.00823116\n",
      "Trained batch 1338 batch loss 7.04116774 epoch total loss 7.00825548\n",
      "Trained batch 1339 batch loss 6.87139606 epoch total loss 7.00815296\n",
      "Trained batch 1340 batch loss 7.36926556 epoch total loss 7.00842237\n",
      "Trained batch 1341 batch loss 6.98675585 epoch total loss 7.00840616\n",
      "Trained batch 1342 batch loss 6.69077349 epoch total loss 7.00816917\n",
      "Trained batch 1343 batch loss 6.37415266 epoch total loss 7.00769711\n",
      "Trained batch 1344 batch loss 6.32731867 epoch total loss 7.0071907\n",
      "Trained batch 1345 batch loss 6.74581242 epoch total loss 7.00699663\n",
      "Trained batch 1346 batch loss 7.12977886 epoch total loss 7.00708771\n",
      "Trained batch 1347 batch loss 6.55286884 epoch total loss 7.00675058\n",
      "Trained batch 1348 batch loss 6.96244907 epoch total loss 7.00671768\n",
      "Trained batch 1349 batch loss 7.02213717 epoch total loss 7.0067296\n",
      "Trained batch 1350 batch loss 6.9985342 epoch total loss 7.00672293\n",
      "Trained batch 1351 batch loss 7.1954174 epoch total loss 7.00686264\n",
      "Trained batch 1352 batch loss 6.50949287 epoch total loss 7.006495\n",
      "Trained batch 1353 batch loss 7.35703182 epoch total loss 7.0067544\n",
      "Trained batch 1354 batch loss 7.36719847 epoch total loss 7.00702047\n",
      "Trained batch 1355 batch loss 7.24594593 epoch total loss 7.0071969\n",
      "Trained batch 1356 batch loss 6.91565037 epoch total loss 7.00712967\n",
      "Trained batch 1357 batch loss 6.91732121 epoch total loss 7.00706339\n",
      "Trained batch 1358 batch loss 6.6983614 epoch total loss 7.00683594\n",
      "Trained batch 1359 batch loss 6.59860849 epoch total loss 7.00653553\n",
      "Trained batch 1360 batch loss 6.84453392 epoch total loss 7.0064168\n",
      "Trained batch 1361 batch loss 6.77152252 epoch total loss 7.00624418\n",
      "Trained batch 1362 batch loss 7.0825572 epoch total loss 7.00630045\n",
      "Trained batch 1363 batch loss 6.83184862 epoch total loss 7.00617266\n",
      "Trained batch 1364 batch loss 6.94129181 epoch total loss 7.00612497\n",
      "Trained batch 1365 batch loss 7.12083864 epoch total loss 7.00620937\n",
      "Trained batch 1366 batch loss 6.95299387 epoch total loss 7.00617027\n",
      "Trained batch 1367 batch loss 6.42809582 epoch total loss 7.00574732\n",
      "Trained batch 1368 batch loss 6.73939276 epoch total loss 7.00555229\n",
      "Trained batch 1369 batch loss 7.25448704 epoch total loss 7.00573444\n",
      "Trained batch 1370 batch loss 6.85519361 epoch total loss 7.00562477\n",
      "Trained batch 1371 batch loss 6.92523432 epoch total loss 7.00556612\n",
      "Trained batch 1372 batch loss 7.30355358 epoch total loss 7.00578308\n",
      "Trained batch 1373 batch loss 7.15473938 epoch total loss 7.00589132\n",
      "Trained batch 1374 batch loss 7.0729804 epoch total loss 7.00594044\n",
      "Trained batch 1375 batch loss 7.35838509 epoch total loss 7.0061965\n",
      "Trained batch 1376 batch loss 7.31301689 epoch total loss 7.00642\n",
      "Trained batch 1377 batch loss 6.74505615 epoch total loss 7.00623035\n",
      "Trained batch 1378 batch loss 6.41987467 epoch total loss 7.00580502\n",
      "Trained batch 1379 batch loss 7.04383612 epoch total loss 7.00583267\n",
      "Trained batch 1380 batch loss 6.70758772 epoch total loss 7.00561666\n",
      "Trained batch 1381 batch loss 7.22508764 epoch total loss 7.00577545\n",
      "Trained batch 1382 batch loss 7.3023119 epoch total loss 7.00599\n",
      "Trained batch 1383 batch loss 7.00900936 epoch total loss 7.00599194\n",
      "Trained batch 1384 batch loss 6.8280735 epoch total loss 7.00586367\n",
      "Trained batch 1385 batch loss 6.69896412 epoch total loss 7.00564241\n",
      "Trained batch 1386 batch loss 7.39092731 epoch total loss 7.00592\n",
      "Trained batch 1387 batch loss 6.96284485 epoch total loss 7.00588894\n",
      "Trained batch 1388 batch loss 7.01257753 epoch total loss 7.00589371\n",
      "Epoch 4 train loss 7.005893707275391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 03:05:22.406038: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "2025-03-19 03:05:22.406092: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "\t [[RemoteCall]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 1 batch loss 6.75132608\n",
      "Validated batch 2 batch loss 6.99262476\n",
      "Validated batch 3 batch loss 6.5229\n",
      "Validated batch 4 batch loss 6.71027184\n",
      "Validated batch 5 batch loss 6.70087528\n",
      "Validated batch 6 batch loss 6.84916449\n",
      "Validated batch 7 batch loss 7.02511406\n",
      "Validated batch 8 batch loss 7.12363148\n",
      "Validated batch 9 batch loss 7.3163352\n",
      "Validated batch 10 batch loss 7.22867775\n",
      "Validated batch 11 batch loss 7.00222445\n",
      "Validated batch 12 batch loss 6.78747368\n",
      "Validated batch 13 batch loss 7.15210724\n",
      "Validated batch 14 batch loss 6.92025137\n",
      "Validated batch 15 batch loss 7.25980091\n",
      "Validated batch 16 batch loss 7.36382723\n",
      "Validated batch 17 batch loss 6.91594\n",
      "Validated batch 18 batch loss 7.29190826\n",
      "Validated batch 19 batch loss 7.17239189\n",
      "Validated batch 20 batch loss 7.24812222\n",
      "Validated batch 21 batch loss 7.02885532\n",
      "Validated batch 22 batch loss 7.25212479\n",
      "Validated batch 23 batch loss 7.30797243\n",
      "Validated batch 24 batch loss 7.30862379\n",
      "Validated batch 25 batch loss 7.22864485\n",
      "Validated batch 26 batch loss 7.02334452\n",
      "Validated batch 27 batch loss 6.22958469\n",
      "Validated batch 28 batch loss 6.867239\n",
      "Validated batch 29 batch loss 6.90742779\n",
      "Validated batch 30 batch loss 6.40317106\n",
      "Validated batch 31 batch loss 6.83789682\n",
      "Validated batch 32 batch loss 6.76439667\n",
      "Validated batch 33 batch loss 7.0261631\n",
      "Validated batch 34 batch loss 6.73403168\n",
      "Validated batch 35 batch loss 6.59807205\n",
      "Validated batch 36 batch loss 6.78214025\n",
      "Validated batch 37 batch loss 6.74676704\n",
      "Validated batch 38 batch loss 7.26747894\n",
      "Validated batch 39 batch loss 7.0961113\n",
      "Validated batch 40 batch loss 7.01829195\n",
      "Validated batch 41 batch loss 7.14207172\n",
      "Validated batch 42 batch loss 7.04333067\n",
      "Validated batch 43 batch loss 6.51875734\n",
      "Validated batch 44 batch loss 7.24246168\n",
      "Validated batch 45 batch loss 6.03046417\n",
      "Validated batch 46 batch loss 7.45129824\n",
      "Validated batch 47 batch loss 7.37333393\n",
      "Validated batch 48 batch loss 7.04960108\n",
      "Validated batch 49 batch loss 6.87689734\n",
      "Validated batch 50 batch loss 6.72545433\n",
      "Validated batch 51 batch loss 6.67087221\n",
      "Validated batch 52 batch loss 7.06582546\n",
      "Validated batch 53 batch loss 6.81352186\n",
      "Validated batch 54 batch loss 7.20782709\n",
      "Validated batch 55 batch loss 7.30118799\n",
      "Validated batch 56 batch loss 6.92510605\n",
      "Validated batch 57 batch loss 6.91634035\n",
      "Validated batch 58 batch loss 6.57966471\n",
      "Validated batch 59 batch loss 6.99310207\n",
      "Validated batch 60 batch loss 6.87605572\n",
      "Validated batch 61 batch loss 7.28049\n",
      "Validated batch 62 batch loss 6.90860701\n",
      "Validated batch 63 batch loss 6.96056\n",
      "Validated batch 64 batch loss 6.31019449\n",
      "Validated batch 65 batch loss 6.75332165\n",
      "Validated batch 66 batch loss 7.12741566\n",
      "Validated batch 67 batch loss 6.71560574\n",
      "Validated batch 68 batch loss 6.80788136\n",
      "Validated batch 69 batch loss 6.84791946\n",
      "Validated batch 70 batch loss 6.94608736\n",
      "Validated batch 71 batch loss 7.12440586\n",
      "Validated batch 72 batch loss 6.83366585\n",
      "Validated batch 73 batch loss 7.28449726\n",
      "Validated batch 74 batch loss 7.40503168\n",
      "Validated batch 75 batch loss 7.20032644\n",
      "Validated batch 76 batch loss 7.25970316\n",
      "Validated batch 77 batch loss 7.06112814\n",
      "Validated batch 78 batch loss 7.20277452\n",
      "Validated batch 79 batch loss 7.42577171\n",
      "Validated batch 80 batch loss 7.46557093\n",
      "Validated batch 81 batch loss 7.15302467\n",
      "Validated batch 82 batch loss 6.75290155\n",
      "Validated batch 83 batch loss 7.26297045\n",
      "Validated batch 84 batch loss 6.97992802\n",
      "Validated batch 85 batch loss 6.90618706\n",
      "Validated batch 86 batch loss 7.19098473\n",
      "Validated batch 87 batch loss 6.33713341\n",
      "Validated batch 88 batch loss 6.64885187\n",
      "Validated batch 89 batch loss 7.36761618\n",
      "Validated batch 90 batch loss 6.92914486\n",
      "Validated batch 91 batch loss 7.37633181\n",
      "Validated batch 92 batch loss 7.0558424\n",
      "Validated batch 93 batch loss 7.32673836\n",
      "Validated batch 94 batch loss 6.83787\n",
      "Validated batch 95 batch loss 6.84034204\n",
      "Validated batch 96 batch loss 6.72877264\n",
      "Validated batch 97 batch loss 6.99571609\n",
      "Validated batch 98 batch loss 7.1509\n",
      "Validated batch 99 batch loss 6.91251326\n",
      "Validated batch 100 batch loss 7.2051034\n",
      "Validated batch 101 batch loss 6.82526588\n",
      "Validated batch 102 batch loss 7.24550772\n",
      "Validated batch 103 batch loss 7.39471436\n",
      "Validated batch 104 batch loss 7.11504555\n",
      "Validated batch 105 batch loss 7.06133461\n",
      "Validated batch 106 batch loss 6.901052\n",
      "Validated batch 107 batch loss 7.2843132\n",
      "Validated batch 108 batch loss 6.8508029\n",
      "Validated batch 109 batch loss 7.25366259\n",
      "Validated batch 110 batch loss 6.77536964\n",
      "Validated batch 111 batch loss 7.08166265\n",
      "Validated batch 112 batch loss 6.92917919\n",
      "Validated batch 113 batch loss 6.92841625\n",
      "Validated batch 114 batch loss 6.97109938\n",
      "Validated batch 115 batch loss 7.06887\n",
      "Validated batch 116 batch loss 7.42997742\n",
      "Validated batch 117 batch loss 7.0607996\n",
      "Validated batch 118 batch loss 7.15248156\n",
      "Validated batch 119 batch loss 7.00949955\n",
      "Validated batch 120 batch loss 7.16476\n",
      "Validated batch 121 batch loss 7.31922817\n",
      "Validated batch 122 batch loss 7.07146883\n",
      "Validated batch 123 batch loss 6.84097195\n",
      "Validated batch 124 batch loss 6.79184961\n",
      "Validated batch 125 batch loss 6.84957552\n",
      "Validated batch 126 batch loss 7.17032099\n",
      "Validated batch 127 batch loss 7.05237627\n",
      "Validated batch 128 batch loss 6.74229383\n",
      "Validated batch 129 batch loss 6.70229912\n",
      "Validated batch 130 batch loss 6.87089491\n",
      "Validated batch 131 batch loss 7.21849\n",
      "Validated batch 132 batch loss 7.26456785\n",
      "Validated batch 133 batch loss 7.17102242\n",
      "Validated batch 134 batch loss 7.50172472\n",
      "Validated batch 135 batch loss 7.5221777\n",
      "Validated batch 136 batch loss 7.32982206\n",
      "Validated batch 137 batch loss 6.91859341\n",
      "Validated batch 138 batch loss 6.66733122\n",
      "Validated batch 139 batch loss 6.65322\n",
      "Validated batch 140 batch loss 6.80549622\n",
      "Validated batch 141 batch loss 7.03359604\n",
      "Validated batch 142 batch loss 7.08828735\n",
      "Validated batch 143 batch loss 6.52165127\n",
      "Validated batch 144 batch loss 7.144207\n",
      "Validated batch 145 batch loss 7.11485529\n",
      "Validated batch 146 batch loss 7.10468912\n",
      "Validated batch 147 batch loss 6.95998478\n",
      "Validated batch 148 batch loss 7.1242218\n",
      "Validated batch 149 batch loss 7.05374813\n",
      "Validated batch 150 batch loss 6.87845182\n",
      "Validated batch 151 batch loss 7.08049488\n",
      "Validated batch 152 batch loss 7.11312437\n",
      "Validated batch 153 batch loss 7.25336647\n",
      "Validated batch 154 batch loss 7.16216946\n",
      "Validated batch 155 batch loss 6.84399414\n",
      "Validated batch 156 batch loss 6.54252291\n",
      "Validated batch 157 batch loss 7.19194078\n",
      "Validated batch 158 batch loss 7.36494446\n",
      "Validated batch 159 batch loss 7.15206718\n",
      "Validated batch 160 batch loss 7.33729601\n",
      "Validated batch 161 batch loss 6.99270487\n",
      "Validated batch 162 batch loss 6.73127174\n",
      "Validated batch 163 batch loss 7.08370638\n",
      "Validated batch 164 batch loss 7.10807037\n",
      "Validated batch 165 batch loss 7.2951622\n",
      "Validated batch 166 batch loss 6.9778161\n",
      "Validated batch 167 batch loss 7.14700651\n",
      "Validated batch 168 batch loss 6.94293213\n",
      "Validated batch 169 batch loss 7.28850412\n",
      "Validated batch 170 batch loss 7.07894039\n",
      "Validated batch 171 batch loss 6.77011585\n",
      "Validated batch 172 batch loss 6.97732306\n",
      "Validated batch 173 batch loss 6.89752293\n",
      "Validated batch 174 batch loss 5.92261791\n",
      "Validated batch 175 batch loss 7.29784155\n",
      "Validated batch 176 batch loss 7.34832096\n",
      "Validated batch 177 batch loss 7.10592\n",
      "Validated batch 178 batch loss 6.95381355\n",
      "Validated batch 179 batch loss 7.10380554\n",
      "Validated batch 180 batch loss 7.29567385\n",
      "Validated batch 181 batch loss 7.33461332\n",
      "Validated batch 182 batch loss 7.26175\n",
      "Validated batch 183 batch loss 7.2523222\n",
      "Validated batch 184 batch loss 6.31993675\n",
      "Validated batch 185 batch loss 3.51023817\n",
      "Epoch 4 val loss 6.991769313812256\n",
      "Model /home/minho/Desktop/aiffel/pose_estimation/models/model-epoch-4-loss-6.9918.weights.h5 saved.\n",
      "Start epoch 5 with learning rate 0.001\n",
      "Start distributed training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 03:05:30.817032: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "2025-03-19 03:05:30.817079: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "\t [[RemoteCall]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1 batch loss 7.48300838 epoch total loss 7.48300838\n",
      "Trained batch 2 batch loss 7.14967251 epoch total loss 7.31634045\n",
      "Trained batch 3 batch loss 7.26144743 epoch total loss 7.29804277\n",
      "Trained batch 4 batch loss 7.29014587 epoch total loss 7.29606867\n",
      "Trained batch 5 batch loss 7.27447224 epoch total loss 7.29174948\n",
      "Trained batch 6 batch loss 7.30554533 epoch total loss 7.29404879\n",
      "Trained batch 7 batch loss 7.33497143 epoch total loss 7.29989529\n",
      "Trained batch 8 batch loss 7.50388288 epoch total loss 7.32539368\n",
      "Trained batch 9 batch loss 7.26409149 epoch total loss 7.31858253\n",
      "Trained batch 10 batch loss 7.44104195 epoch total loss 7.33082819\n",
      "Trained batch 11 batch loss 7.280509 epoch total loss 7.32625389\n",
      "Trained batch 12 batch loss 7.26610136 epoch total loss 7.3212409\n",
      "Trained batch 13 batch loss 7.4332037 epoch total loss 7.32985353\n",
      "Trained batch 14 batch loss 7.15671539 epoch total loss 7.31748629\n",
      "Trained batch 15 batch loss 7.25187111 epoch total loss 7.31311178\n",
      "Trained batch 16 batch loss 7.00175858 epoch total loss 7.29365253\n",
      "Trained batch 17 batch loss 7.08076239 epoch total loss 7.28113\n",
      "Trained batch 18 batch loss 7.03093576 epoch total loss 7.26722956\n",
      "Trained batch 19 batch loss 7.34720087 epoch total loss 7.2714386\n",
      "Trained batch 20 batch loss 6.83360863 epoch total loss 7.249547\n",
      "Trained batch 21 batch loss 7.23279142 epoch total loss 7.24874878\n",
      "Trained batch 22 batch loss 6.84125948 epoch total loss 7.23022652\n",
      "Trained batch 23 batch loss 7.09654665 epoch total loss 7.22441435\n",
      "Trained batch 24 batch loss 7.32175112 epoch total loss 7.22847\n",
      "Trained batch 25 batch loss 7.38419533 epoch total loss 7.23469925\n",
      "Trained batch 26 batch loss 7.23127794 epoch total loss 7.23456764\n",
      "Trained batch 27 batch loss 7.1551795 epoch total loss 7.23162746\n",
      "Trained batch 28 batch loss 7.4447937 epoch total loss 7.23924\n",
      "Trained batch 29 batch loss 7.15479136 epoch total loss 7.23632812\n",
      "Trained batch 30 batch loss 7.17202806 epoch total loss 7.23418474\n",
      "Trained batch 31 batch loss 7.40534544 epoch total loss 7.23970604\n",
      "Trained batch 32 batch loss 7.37738609 epoch total loss 7.24400854\n",
      "Trained batch 33 batch loss 6.97557354 epoch total loss 7.23587418\n",
      "Trained batch 34 batch loss 6.45172739 epoch total loss 7.21281052\n",
      "Trained batch 35 batch loss 6.09581566 epoch total loss 7.18089628\n",
      "Trained batch 36 batch loss 6.25115347 epoch total loss 7.15507\n",
      "Trained batch 37 batch loss 6.69513416 epoch total loss 7.14263916\n",
      "Trained batch 38 batch loss 6.78239059 epoch total loss 7.13315868\n",
      "Trained batch 39 batch loss 6.63740158 epoch total loss 7.12044668\n",
      "Trained batch 40 batch loss 6.5250659 epoch total loss 7.10556173\n",
      "Trained batch 41 batch loss 6.452 epoch total loss 7.08962107\n",
      "Trained batch 42 batch loss 6.67303085 epoch total loss 7.07970238\n",
      "Trained batch 43 batch loss 6.52769089 epoch total loss 7.06686449\n",
      "Trained batch 44 batch loss 6.94070625 epoch total loss 7.06399727\n",
      "Trained batch 45 batch loss 6.38882113 epoch total loss 7.04899359\n",
      "Trained batch 46 batch loss 6.98784924 epoch total loss 7.04766464\n",
      "Trained batch 47 batch loss 6.17559052 epoch total loss 7.02911\n",
      "Trained batch 48 batch loss 6.0921154 epoch total loss 7.00958872\n",
      "Trained batch 49 batch loss 6.3142066 epoch total loss 6.99539757\n",
      "Trained batch 50 batch loss 6.65876579 epoch total loss 6.98866463\n",
      "Trained batch 51 batch loss 6.82798052 epoch total loss 6.98551369\n",
      "Trained batch 52 batch loss 7.22723436 epoch total loss 6.99016237\n",
      "Trained batch 53 batch loss 7.23589611 epoch total loss 6.99479866\n",
      "Trained batch 54 batch loss 7.41466 epoch total loss 7.00257444\n",
      "Trained batch 55 batch loss 7.36620331 epoch total loss 7.00918579\n",
      "Trained batch 56 batch loss 7.29073334 epoch total loss 7.01421356\n",
      "Trained batch 57 batch loss 7.26376534 epoch total loss 7.0185914\n",
      "Trained batch 58 batch loss 6.4943943 epoch total loss 7.00955343\n",
      "Trained batch 59 batch loss 6.29029799 epoch total loss 6.99736261\n",
      "Trained batch 60 batch loss 7.01185036 epoch total loss 6.99760389\n",
      "Trained batch 61 batch loss 7.28088903 epoch total loss 7.00224781\n",
      "Trained batch 62 batch loss 6.95783091 epoch total loss 7.00153112\n",
      "Trained batch 63 batch loss 7.34821129 epoch total loss 7.00703382\n",
      "Trained batch 64 batch loss 7.35613918 epoch total loss 7.01248884\n",
      "Trained batch 65 batch loss 6.4701314 epoch total loss 7.00414467\n",
      "Trained batch 66 batch loss 6.10417128 epoch total loss 6.99050856\n",
      "Trained batch 67 batch loss 5.73529387 epoch total loss 6.9717741\n",
      "Trained batch 68 batch loss 6.80714035 epoch total loss 6.96935272\n",
      "Trained batch 69 batch loss 6.8732338 epoch total loss 6.9679594\n",
      "Trained batch 70 batch loss 7.07607937 epoch total loss 6.96950436\n",
      "Trained batch 71 batch loss 7.32439804 epoch total loss 6.97450256\n",
      "Trained batch 72 batch loss 7.18902636 epoch total loss 6.97748232\n",
      "Trained batch 73 batch loss 6.74905729 epoch total loss 6.97435331\n",
      "Trained batch 74 batch loss 7.26298141 epoch total loss 6.97825336\n",
      "Trained batch 75 batch loss 7.07996655 epoch total loss 6.97960949\n",
      "Trained batch 76 batch loss 6.73801804 epoch total loss 6.97643089\n",
      "Trained batch 77 batch loss 6.75191689 epoch total loss 6.97351456\n",
      "Trained batch 78 batch loss 7.07537317 epoch total loss 6.97482061\n",
      "Trained batch 79 batch loss 6.97070742 epoch total loss 6.97476864\n",
      "Trained batch 80 batch loss 6.62391567 epoch total loss 6.97038269\n",
      "Trained batch 81 batch loss 7.02894926 epoch total loss 6.97110558\n",
      "Trained batch 82 batch loss 7.22705269 epoch total loss 6.97422695\n",
      "Trained batch 83 batch loss 6.96998882 epoch total loss 6.97417545\n",
      "Trained batch 84 batch loss 6.5069809 epoch total loss 6.96861362\n",
      "Trained batch 85 batch loss 6.455369 epoch total loss 6.96257544\n",
      "Trained batch 86 batch loss 6.37667179 epoch total loss 6.95576239\n",
      "Trained batch 87 batch loss 6.54346 epoch total loss 6.9510231\n",
      "Trained batch 88 batch loss 7.31716394 epoch total loss 6.95518351\n",
      "Trained batch 89 batch loss 7.10886431 epoch total loss 6.95691061\n",
      "Trained batch 90 batch loss 7.22955179 epoch total loss 6.95994\n",
      "Trained batch 91 batch loss 7.25769949 epoch total loss 6.96321201\n",
      "Trained batch 92 batch loss 7.33595371 epoch total loss 6.96726322\n",
      "Trained batch 93 batch loss 7.2360239 epoch total loss 6.97015333\n",
      "Trained batch 94 batch loss 7.28100586 epoch total loss 6.97346\n",
      "Trained batch 95 batch loss 7.33214378 epoch total loss 6.97723579\n",
      "Trained batch 96 batch loss 7.38172674 epoch total loss 6.98144913\n",
      "Trained batch 97 batch loss 7.2581377 epoch total loss 6.98430157\n",
      "Trained batch 98 batch loss 6.66021299 epoch total loss 6.98099422\n",
      "Trained batch 99 batch loss 6.70783043 epoch total loss 6.97823524\n",
      "Trained batch 100 batch loss 6.9982419 epoch total loss 6.97843504\n",
      "Trained batch 101 batch loss 7.29366302 epoch total loss 6.98155594\n",
      "Trained batch 102 batch loss 7.02300358 epoch total loss 6.9819622\n",
      "Trained batch 103 batch loss 7.22704601 epoch total loss 6.98434162\n",
      "Trained batch 104 batch loss 6.90490246 epoch total loss 6.98357821\n",
      "Trained batch 105 batch loss 7.4028945 epoch total loss 6.98757172\n",
      "Trained batch 106 batch loss 7.27873421 epoch total loss 6.9903183\n",
      "Trained batch 107 batch loss 7.18507719 epoch total loss 6.99213839\n",
      "Trained batch 108 batch loss 6.95179796 epoch total loss 6.99176455\n",
      "Trained batch 109 batch loss 6.41043282 epoch total loss 6.9864316\n",
      "Trained batch 110 batch loss 6.6940608 epoch total loss 6.98377371\n",
      "Trained batch 111 batch loss 7.09462404 epoch total loss 6.98477221\n",
      "Trained batch 112 batch loss 7.38518238 epoch total loss 6.98834705\n",
      "Trained batch 113 batch loss 7.12918949 epoch total loss 6.98959398\n",
      "Trained batch 114 batch loss 7.10109234 epoch total loss 6.9905715\n",
      "Trained batch 115 batch loss 7.12340307 epoch total loss 6.99172688\n",
      "Trained batch 116 batch loss 7.17395401 epoch total loss 6.99329758\n",
      "Trained batch 117 batch loss 7.05259323 epoch total loss 6.99380445\n",
      "Trained batch 118 batch loss 6.95732641 epoch total loss 6.99349546\n",
      "Trained batch 119 batch loss 7.26756716 epoch total loss 6.99579906\n",
      "Trained batch 120 batch loss 7.26923656 epoch total loss 6.99807739\n",
      "Trained batch 121 batch loss 6.9996686 epoch total loss 6.99809074\n",
      "Trained batch 122 batch loss 6.43264246 epoch total loss 6.99345589\n",
      "Trained batch 123 batch loss 6.66537666 epoch total loss 6.99078846\n",
      "Trained batch 124 batch loss 6.10117292 epoch total loss 6.98361444\n",
      "Trained batch 125 batch loss 6.50105286 epoch total loss 6.97975397\n",
      "Trained batch 126 batch loss 6.6833787 epoch total loss 6.97740126\n",
      "Trained batch 127 batch loss 6.93651342 epoch total loss 6.97707939\n",
      "Trained batch 128 batch loss 6.99586916 epoch total loss 6.97722626\n",
      "Trained batch 129 batch loss 7.34400177 epoch total loss 6.98006964\n",
      "Trained batch 130 batch loss 6.87963915 epoch total loss 6.97929668\n",
      "Trained batch 131 batch loss 6.44551229 epoch total loss 6.97522211\n",
      "Trained batch 132 batch loss 6.33937836 epoch total loss 6.9704051\n",
      "Trained batch 133 batch loss 6.98614645 epoch total loss 6.97052336\n",
      "Trained batch 134 batch loss 7.27169847 epoch total loss 6.97277117\n",
      "Trained batch 135 batch loss 7.09872055 epoch total loss 6.97370386\n",
      "Trained batch 136 batch loss 7.2802515 epoch total loss 6.97595787\n",
      "Trained batch 137 batch loss 7.39057207 epoch total loss 6.97898436\n",
      "Trained batch 138 batch loss 7.02104568 epoch total loss 6.97928905\n",
      "Trained batch 139 batch loss 7.21627903 epoch total loss 6.98099422\n",
      "Trained batch 140 batch loss 7.44007063 epoch total loss 6.98427343\n",
      "Trained batch 141 batch loss 7.21298122 epoch total loss 6.98589516\n",
      "Trained batch 142 batch loss 7.25766182 epoch total loss 6.98780918\n",
      "Trained batch 143 batch loss 7.28473711 epoch total loss 6.98988581\n",
      "Trained batch 144 batch loss 7.38268137 epoch total loss 6.99261332\n",
      "Trained batch 145 batch loss 7.29118824 epoch total loss 6.99467278\n",
      "Trained batch 146 batch loss 7.31294966 epoch total loss 6.9968524\n",
      "Trained batch 147 batch loss 7.13004684 epoch total loss 6.99775887\n",
      "Trained batch 148 batch loss 7.28695536 epoch total loss 6.99971294\n",
      "Trained batch 149 batch loss 7.28236437 epoch total loss 7.00161\n",
      "Trained batch 150 batch loss 7.21592188 epoch total loss 7.00303888\n",
      "Trained batch 151 batch loss 7.46766663 epoch total loss 7.00611544\n",
      "Trained batch 152 batch loss 7.45111227 epoch total loss 7.00904369\n",
      "Trained batch 153 batch loss 7.51007938 epoch total loss 7.01231861\n",
      "Trained batch 154 batch loss 7.45263481 epoch total loss 7.01517773\n",
      "Trained batch 155 batch loss 7.20074511 epoch total loss 7.01637554\n",
      "Trained batch 156 batch loss 6.90499496 epoch total loss 7.01566172\n",
      "Trained batch 157 batch loss 7.06500578 epoch total loss 7.01597643\n",
      "Trained batch 158 batch loss 6.910779 epoch total loss 7.01531076\n",
      "Trained batch 159 batch loss 7.2652421 epoch total loss 7.01688242\n",
      "Trained batch 160 batch loss 7.1903739 epoch total loss 7.01796722\n",
      "Trained batch 161 batch loss 7.47992897 epoch total loss 7.02083683\n",
      "Trained batch 162 batch loss 7.1806879 epoch total loss 7.02182341\n",
      "Trained batch 163 batch loss 7.46358061 epoch total loss 7.02453375\n",
      "Trained batch 164 batch loss 7.23952341 epoch total loss 7.02584457\n",
      "Trained batch 165 batch loss 7.18588066 epoch total loss 7.02681494\n",
      "Trained batch 166 batch loss 7.12471485 epoch total loss 7.02740479\n",
      "Trained batch 167 batch loss 7.21322632 epoch total loss 7.02851772\n",
      "Trained batch 168 batch loss 6.83024597 epoch total loss 7.02733707\n",
      "Trained batch 169 batch loss 7.09587669 epoch total loss 7.02774239\n",
      "Trained batch 170 batch loss 6.83343315 epoch total loss 7.02659893\n",
      "Trained batch 171 batch loss 6.77741385 epoch total loss 7.02514219\n",
      "Trained batch 172 batch loss 6.546978 epoch total loss 7.02236223\n",
      "Trained batch 173 batch loss 6.35267401 epoch total loss 7.01849127\n",
      "Trained batch 174 batch loss 7.18410206 epoch total loss 7.01944304\n",
      "Trained batch 175 batch loss 7.15634155 epoch total loss 7.02022552\n",
      "Trained batch 176 batch loss 7.14085579 epoch total loss 7.02091074\n",
      "Trained batch 177 batch loss 7.24935675 epoch total loss 7.02220154\n",
      "Trained batch 178 batch loss 6.99710178 epoch total loss 7.02206039\n",
      "Trained batch 179 batch loss 7.32528305 epoch total loss 7.0237546\n",
      "Trained batch 180 batch loss 7.21713734 epoch total loss 7.02482891\n",
      "Trained batch 181 batch loss 6.86342049 epoch total loss 7.02393723\n",
      "Trained batch 182 batch loss 7.30673265 epoch total loss 7.02549124\n",
      "Trained batch 183 batch loss 7.22985935 epoch total loss 7.02660799\n",
      "Trained batch 184 batch loss 6.9380579 epoch total loss 7.02612686\n",
      "Trained batch 185 batch loss 6.79875708 epoch total loss 7.02489758\n",
      "Trained batch 186 batch loss 7.42684507 epoch total loss 7.02705908\n",
      "Trained batch 187 batch loss 7.43231964 epoch total loss 7.0292263\n",
      "Trained batch 188 batch loss 7.55265379 epoch total loss 7.03201056\n",
      "Trained batch 189 batch loss 7.06627703 epoch total loss 7.03219175\n",
      "Trained batch 190 batch loss 6.50705 epoch total loss 7.02942801\n",
      "Trained batch 191 batch loss 6.20107222 epoch total loss 7.02509069\n",
      "Trained batch 192 batch loss 6.56427956 epoch total loss 7.02269125\n",
      "Trained batch 193 batch loss 7.0316186 epoch total loss 7.0227375\n",
      "Trained batch 194 batch loss 7.11730766 epoch total loss 7.02322483\n",
      "Trained batch 195 batch loss 7.40737247 epoch total loss 7.02519464\n",
      "Trained batch 196 batch loss 7.16486168 epoch total loss 7.02590752\n",
      "Trained batch 197 batch loss 7.41442394 epoch total loss 7.02787971\n",
      "Trained batch 198 batch loss 6.94444656 epoch total loss 7.02745819\n",
      "Trained batch 199 batch loss 6.5439496 epoch total loss 7.02502871\n",
      "Trained batch 200 batch loss 6.65956974 epoch total loss 7.02320147\n",
      "Trained batch 201 batch loss 6.86631203 epoch total loss 7.02242088\n",
      "Trained batch 202 batch loss 6.6752677 epoch total loss 7.02070236\n",
      "Trained batch 203 batch loss 6.41852 epoch total loss 7.01773643\n",
      "Trained batch 204 batch loss 6.73335552 epoch total loss 7.01634264\n",
      "Trained batch 205 batch loss 6.65976572 epoch total loss 7.01460314\n",
      "Trained batch 206 batch loss 6.69682455 epoch total loss 7.01306057\n",
      "Trained batch 207 batch loss 6.47100496 epoch total loss 7.0104413\n",
      "Trained batch 208 batch loss 6.58114624 epoch total loss 7.00837755\n",
      "Trained batch 209 batch loss 6.62080908 epoch total loss 7.00652361\n",
      "Trained batch 210 batch loss 6.74235582 epoch total loss 7.00526524\n",
      "Trained batch 211 batch loss 7.18749952 epoch total loss 7.00612879\n",
      "Trained batch 212 batch loss 6.96295929 epoch total loss 7.00592566\n",
      "Trained batch 213 batch loss 7.44400883 epoch total loss 7.00798225\n",
      "Trained batch 214 batch loss 7.47715712 epoch total loss 7.01017475\n",
      "Trained batch 215 batch loss 7.50603199 epoch total loss 7.01248074\n",
      "Trained batch 216 batch loss 7.271626 epoch total loss 7.01368046\n",
      "Trained batch 217 batch loss 6.52866411 epoch total loss 7.01144552\n",
      "Trained batch 218 batch loss 6.34591246 epoch total loss 7.00839281\n",
      "Trained batch 219 batch loss 6.31679535 epoch total loss 7.00523472\n",
      "Trained batch 220 batch loss 7.18402147 epoch total loss 7.00604725\n",
      "Trained batch 221 batch loss 7.17011547 epoch total loss 7.00679\n",
      "Trained batch 222 batch loss 7.29773283 epoch total loss 7.00810051\n",
      "Trained batch 223 batch loss 6.85942936 epoch total loss 7.00743389\n",
      "Trained batch 224 batch loss 6.63880491 epoch total loss 7.00578785\n",
      "Trained batch 225 batch loss 6.75292635 epoch total loss 7.00466394\n",
      "Trained batch 226 batch loss 7.00497437 epoch total loss 7.00466585\n",
      "Trained batch 227 batch loss 7.23595047 epoch total loss 7.00568438\n",
      "Trained batch 228 batch loss 7.22344637 epoch total loss 7.00663948\n",
      "Trained batch 229 batch loss 6.93147898 epoch total loss 7.00631142\n",
      "Trained batch 230 batch loss 6.88641119 epoch total loss 7.00578976\n",
      "Trained batch 231 batch loss 7.08069849 epoch total loss 7.00611401\n",
      "Trained batch 232 batch loss 7.1069479 epoch total loss 7.00654888\n",
      "Trained batch 233 batch loss 6.93944883 epoch total loss 7.00626087\n",
      "Trained batch 234 batch loss 7.07228613 epoch total loss 7.00654268\n",
      "Trained batch 235 batch loss 7.06137609 epoch total loss 7.00677633\n",
      "Trained batch 236 batch loss 7.23497486 epoch total loss 7.00774336\n",
      "Trained batch 237 batch loss 7.19934654 epoch total loss 7.0085516\n",
      "Trained batch 238 batch loss 7.15960455 epoch total loss 7.00918627\n",
      "Trained batch 239 batch loss 7.04507589 epoch total loss 7.00933599\n",
      "Trained batch 240 batch loss 6.65254831 epoch total loss 7.00784969\n",
      "Trained batch 241 batch loss 6.20995 epoch total loss 7.00453901\n",
      "Trained batch 242 batch loss 5.98855114 epoch total loss 7.00034046\n",
      "Trained batch 243 batch loss 6.14589882 epoch total loss 6.99682426\n",
      "Trained batch 244 batch loss 6.55748 epoch total loss 6.99502373\n",
      "Trained batch 245 batch loss 6.84920216 epoch total loss 6.99442863\n",
      "Trained batch 246 batch loss 6.88806677 epoch total loss 6.99399614\n",
      "Trained batch 247 batch loss 7.19810295 epoch total loss 6.9948225\n",
      "Trained batch 248 batch loss 7.28044462 epoch total loss 6.99597406\n",
      "Trained batch 249 batch loss 7.50073 epoch total loss 6.9980011\n",
      "Trained batch 250 batch loss 7.18452263 epoch total loss 6.99874735\n",
      "Trained batch 251 batch loss 7.39864159 epoch total loss 7.00034094\n",
      "Trained batch 252 batch loss 7.18521929 epoch total loss 7.00107431\n",
      "Trained batch 253 batch loss 7.4360671 epoch total loss 7.00279379\n",
      "Trained batch 254 batch loss 7.42585564 epoch total loss 7.00445938\n",
      "Trained batch 255 batch loss 7.20798922 epoch total loss 7.00525761\n",
      "Trained batch 256 batch loss 7.13725519 epoch total loss 7.00577307\n",
      "Trained batch 257 batch loss 6.97009802 epoch total loss 7.00563431\n",
      "Trained batch 258 batch loss 7.04247093 epoch total loss 7.00577688\n",
      "Trained batch 259 batch loss 6.38658571 epoch total loss 7.0033865\n",
      "Trained batch 260 batch loss 6.57049656 epoch total loss 7.00172186\n",
      "Trained batch 261 batch loss 6.49541 epoch total loss 6.99978161\n",
      "Trained batch 262 batch loss 6.18518162 epoch total loss 6.99667263\n",
      "Trained batch 263 batch loss 7.1118927 epoch total loss 6.99711084\n",
      "Trained batch 264 batch loss 7.06370687 epoch total loss 6.99736309\n",
      "Trained batch 265 batch loss 7.36566734 epoch total loss 6.99875307\n",
      "Trained batch 266 batch loss 7.05251789 epoch total loss 6.99895525\n",
      "Trained batch 267 batch loss 6.60309458 epoch total loss 6.99747276\n",
      "Trained batch 268 batch loss 6.85208035 epoch total loss 6.99693\n",
      "Trained batch 269 batch loss 6.92971373 epoch total loss 6.99668026\n",
      "Trained batch 270 batch loss 6.76893139 epoch total loss 6.99583673\n",
      "Trained batch 271 batch loss 6.85514498 epoch total loss 6.99531698\n",
      "Trained batch 272 batch loss 6.45289183 epoch total loss 6.99332285\n",
      "Trained batch 273 batch loss 6.6282444 epoch total loss 6.9919858\n",
      "Trained batch 274 batch loss 6.28802347 epoch total loss 6.9894166\n",
      "Trained batch 275 batch loss 6.46334505 epoch total loss 6.98750353\n",
      "Trained batch 276 batch loss 6.82301235 epoch total loss 6.98690748\n",
      "Trained batch 277 batch loss 6.79211283 epoch total loss 6.98620415\n",
      "Trained batch 278 batch loss 6.91360855 epoch total loss 6.98594284\n",
      "Trained batch 279 batch loss 7.12550735 epoch total loss 6.98644304\n",
      "Trained batch 280 batch loss 7.37506294 epoch total loss 6.98783112\n",
      "Trained batch 281 batch loss 7.19733763 epoch total loss 6.98857689\n",
      "Trained batch 282 batch loss 6.4280057 epoch total loss 6.98658895\n",
      "Trained batch 283 batch loss 6.81822824 epoch total loss 6.98599434\n",
      "Trained batch 284 batch loss 6.3618927 epoch total loss 6.98379707\n",
      "Trained batch 285 batch loss 7.03665972 epoch total loss 6.98398209\n",
      "Trained batch 286 batch loss 6.93247509 epoch total loss 6.98380232\n",
      "Trained batch 287 batch loss 7.07031298 epoch total loss 6.98410368\n",
      "Trained batch 288 batch loss 7.11245966 epoch total loss 6.98454905\n",
      "Trained batch 289 batch loss 7.39291954 epoch total loss 6.98596239\n",
      "Trained batch 290 batch loss 7.25843906 epoch total loss 6.98690176\n",
      "Trained batch 291 batch loss 7.3698411 epoch total loss 6.98821783\n",
      "Trained batch 292 batch loss 7.37686682 epoch total loss 6.98954868\n",
      "Trained batch 293 batch loss 7.36985493 epoch total loss 6.99084663\n",
      "Trained batch 294 batch loss 7.31113052 epoch total loss 6.99193573\n",
      "Trained batch 295 batch loss 6.74418974 epoch total loss 6.99109602\n",
      "Trained batch 296 batch loss 6.65672684 epoch total loss 6.98996639\n",
      "Trained batch 297 batch loss 6.74127293 epoch total loss 6.98912859\n",
      "Trained batch 298 batch loss 7.53551197 epoch total loss 6.99096203\n",
      "Trained batch 299 batch loss 7.2814765 epoch total loss 6.99193335\n",
      "Trained batch 300 batch loss 6.8732338 epoch total loss 6.99153805\n",
      "Trained batch 301 batch loss 6.49387026 epoch total loss 6.98988485\n",
      "Trained batch 302 batch loss 6.33299589 epoch total loss 6.98770952\n",
      "Trained batch 303 batch loss 6.6801734 epoch total loss 6.98669481\n",
      "Trained batch 304 batch loss 7.12951136 epoch total loss 6.98716402\n",
      "Trained batch 305 batch loss 7.43508053 epoch total loss 6.98863268\n",
      "Trained batch 306 batch loss 7.2577858 epoch total loss 6.98951244\n",
      "Trained batch 307 batch loss 7.32810736 epoch total loss 6.99061537\n",
      "Trained batch 308 batch loss 7.37751436 epoch total loss 6.99187136\n",
      "Trained batch 309 batch loss 7.34034204 epoch total loss 6.99299908\n",
      "Trained batch 310 batch loss 7.2695179 epoch total loss 6.99389076\n",
      "Trained batch 311 batch loss 7.39696932 epoch total loss 6.99518728\n",
      "Trained batch 312 batch loss 6.96141624 epoch total loss 6.99507904\n",
      "Trained batch 313 batch loss 7.27462244 epoch total loss 6.99597216\n",
      "Trained batch 314 batch loss 7.24796 epoch total loss 6.99677467\n",
      "Trained batch 315 batch loss 7.38023853 epoch total loss 6.99799204\n",
      "Trained batch 316 batch loss 7.15233374 epoch total loss 6.99848032\n",
      "Trained batch 317 batch loss 7.2464757 epoch total loss 6.99926281\n",
      "Trained batch 318 batch loss 6.87513304 epoch total loss 6.99887276\n",
      "Trained batch 319 batch loss 7.2081933 epoch total loss 6.99952936\n",
      "Trained batch 320 batch loss 7.32868338 epoch total loss 7.0005579\n",
      "Trained batch 321 batch loss 7.0526576 epoch total loss 7.00072\n",
      "Trained batch 322 batch loss 6.45829725 epoch total loss 6.99903536\n",
      "Trained batch 323 batch loss 7.16635227 epoch total loss 6.9995532\n",
      "Trained batch 324 batch loss 7.03879404 epoch total loss 6.99967432\n",
      "Trained batch 325 batch loss 7.35978603 epoch total loss 7.00078297\n",
      "Trained batch 326 batch loss 7.20659256 epoch total loss 7.00141382\n",
      "Trained batch 327 batch loss 7.27170753 epoch total loss 7.00224066\n",
      "Trained batch 328 batch loss 7.11996031 epoch total loss 7.00259924\n",
      "Trained batch 329 batch loss 6.4705 epoch total loss 7.00098181\n",
      "Trained batch 330 batch loss 7.24874115 epoch total loss 7.00173283\n",
      "Trained batch 331 batch loss 7.26022387 epoch total loss 7.00251389\n",
      "Trained batch 332 batch loss 6.63043404 epoch total loss 7.00139284\n",
      "Trained batch 333 batch loss 6.88134146 epoch total loss 7.00103235\n",
      "Trained batch 334 batch loss 6.70653725 epoch total loss 7.00015068\n",
      "Trained batch 335 batch loss 6.81476927 epoch total loss 6.99959707\n",
      "Trained batch 336 batch loss 7.0017314 epoch total loss 6.99960327\n",
      "Trained batch 337 batch loss 6.95120287 epoch total loss 6.99945974\n",
      "Trained batch 338 batch loss 7.47714043 epoch total loss 7.00087261\n",
      "Trained batch 339 batch loss 7.32056 epoch total loss 7.0018158\n",
      "Trained batch 340 batch loss 7.4211812 epoch total loss 7.0030489\n",
      "Trained batch 341 batch loss 7.4071331 epoch total loss 7.00423431\n",
      "Trained batch 342 batch loss 7.1990962 epoch total loss 7.00480366\n",
      "Trained batch 343 batch loss 6.47504473 epoch total loss 7.00325918\n",
      "Trained batch 344 batch loss 6.11945152 epoch total loss 7.00069\n",
      "Trained batch 345 batch loss 5.85659313 epoch total loss 6.99737406\n",
      "Trained batch 346 batch loss 6.03671789 epoch total loss 6.99459696\n",
      "Trained batch 347 batch loss 6.28539753 epoch total loss 6.99255323\n",
      "Trained batch 348 batch loss 6.54735661 epoch total loss 6.99127388\n",
      "Trained batch 349 batch loss 6.41773 epoch total loss 6.9896307\n",
      "Trained batch 350 batch loss 6.7925868 epoch total loss 6.98906755\n",
      "Trained batch 351 batch loss 6.94862795 epoch total loss 6.98895264\n",
      "Trained batch 352 batch loss 6.58433628 epoch total loss 6.98780251\n",
      "Trained batch 353 batch loss 6.80616951 epoch total loss 6.987288\n",
      "Trained batch 354 batch loss 7.04289055 epoch total loss 6.98744535\n",
      "Trained batch 355 batch loss 7.1852107 epoch total loss 6.98800278\n",
      "Trained batch 356 batch loss 6.53131151 epoch total loss 6.98671961\n",
      "Trained batch 357 batch loss 6.3630271 epoch total loss 6.98497248\n",
      "Trained batch 358 batch loss 5.88257551 epoch total loss 6.98189354\n",
      "Trained batch 359 batch loss 6.46741 epoch total loss 6.98046064\n",
      "Trained batch 360 batch loss 6.37211132 epoch total loss 6.97877073\n",
      "Trained batch 361 batch loss 6.92848635 epoch total loss 6.9786315\n",
      "Trained batch 362 batch loss 6.95301437 epoch total loss 6.97856092\n",
      "Trained batch 363 batch loss 7.22619486 epoch total loss 6.9792428\n",
      "Trained batch 364 batch loss 6.89883661 epoch total loss 6.97902203\n",
      "Trained batch 365 batch loss 6.97381163 epoch total loss 6.97900772\n",
      "Trained batch 366 batch loss 6.76340818 epoch total loss 6.97841883\n",
      "Trained batch 367 batch loss 7.24769688 epoch total loss 6.97915316\n",
      "Trained batch 368 batch loss 7.2904458 epoch total loss 6.97999907\n",
      "Trained batch 369 batch loss 7.05893755 epoch total loss 6.98021269\n",
      "Trained batch 370 batch loss 7.33981848 epoch total loss 6.98118448\n",
      "Trained batch 371 batch loss 6.66480827 epoch total loss 6.9803319\n",
      "Trained batch 372 batch loss 7.33786774 epoch total loss 6.9812932\n",
      "Trained batch 373 batch loss 7.07866478 epoch total loss 6.98155403\n",
      "Trained batch 374 batch loss 7.11830807 epoch total loss 6.98192\n",
      "Trained batch 375 batch loss 7.26051426 epoch total loss 6.98266268\n",
      "Trained batch 376 batch loss 7.02169228 epoch total loss 6.98276663\n",
      "Trained batch 377 batch loss 6.66490555 epoch total loss 6.9819231\n",
      "Trained batch 378 batch loss 6.66155767 epoch total loss 6.98107576\n",
      "Trained batch 379 batch loss 7.04038048 epoch total loss 6.98123217\n",
      "Trained batch 380 batch loss 7.367383 epoch total loss 6.98224831\n",
      "Trained batch 381 batch loss 7.02775335 epoch total loss 6.98236799\n",
      "Trained batch 382 batch loss 7.32290268 epoch total loss 6.98325968\n",
      "Trained batch 383 batch loss 7.05948877 epoch total loss 6.983459\n",
      "Trained batch 384 batch loss 6.84842825 epoch total loss 6.98310709\n",
      "Trained batch 385 batch loss 7.30004644 epoch total loss 6.98393059\n",
      "Trained batch 386 batch loss 7.01828814 epoch total loss 6.98401976\n",
      "Trained batch 387 batch loss 7.30755854 epoch total loss 6.98485565\n",
      "Trained batch 388 batch loss 6.81385899 epoch total loss 6.98441505\n",
      "Trained batch 389 batch loss 6.86395216 epoch total loss 6.98410559\n",
      "Trained batch 390 batch loss 6.66070127 epoch total loss 6.98327637\n",
      "Trained batch 391 batch loss 6.83690262 epoch total loss 6.98290205\n",
      "Trained batch 392 batch loss 7.21985149 epoch total loss 6.98350668\n",
      "Trained batch 393 batch loss 7.38215 epoch total loss 6.98452091\n",
      "Trained batch 394 batch loss 7.29905939 epoch total loss 6.98531914\n",
      "Trained batch 395 batch loss 7.33639669 epoch total loss 6.98620844\n",
      "Trained batch 396 batch loss 7.27215624 epoch total loss 6.98693037\n",
      "Trained batch 397 batch loss 7.08429193 epoch total loss 6.98717546\n",
      "Trained batch 398 batch loss 6.42564678 epoch total loss 6.9857645\n",
      "Trained batch 399 batch loss 7.06507111 epoch total loss 6.98596334\n",
      "Trained batch 400 batch loss 7.35416794 epoch total loss 6.98688412\n",
      "Trained batch 401 batch loss 7.20533323 epoch total loss 6.98742867\n",
      "Trained batch 402 batch loss 6.78800583 epoch total loss 6.98693323\n",
      "Trained batch 403 batch loss 6.7621212 epoch total loss 6.98637533\n",
      "Trained batch 404 batch loss 6.61985874 epoch total loss 6.98546839\n",
      "Trained batch 405 batch loss 7.13000059 epoch total loss 6.98582458\n",
      "Trained batch 406 batch loss 7.19928455 epoch total loss 6.98635054\n",
      "Trained batch 407 batch loss 7.08743334 epoch total loss 6.98659849\n",
      "Trained batch 408 batch loss 6.63613319 epoch total loss 6.98573971\n",
      "Trained batch 409 batch loss 7.12587118 epoch total loss 6.98608255\n",
      "Trained batch 410 batch loss 7.20775938 epoch total loss 6.98662329\n",
      "Trained batch 411 batch loss 6.7159543 epoch total loss 6.98596525\n",
      "Trained batch 412 batch loss 7.07317305 epoch total loss 6.98617697\n",
      "Trained batch 413 batch loss 7.21680784 epoch total loss 6.98673534\n",
      "Trained batch 414 batch loss 7.20141268 epoch total loss 6.98725414\n",
      "Trained batch 415 batch loss 7.3737545 epoch total loss 6.98818541\n",
      "Trained batch 416 batch loss 7.41628313 epoch total loss 6.98921442\n",
      "Trained batch 417 batch loss 7.12755966 epoch total loss 6.98954582\n",
      "Trained batch 418 batch loss 7.20714521 epoch total loss 6.99006605\n",
      "Trained batch 419 batch loss 6.94958305 epoch total loss 6.98996925\n",
      "Trained batch 420 batch loss 6.95317173 epoch total loss 6.98988152\n",
      "Trained batch 421 batch loss 7.06566668 epoch total loss 6.99006176\n",
      "Trained batch 422 batch loss 6.52962446 epoch total loss 6.98897028\n",
      "Trained batch 423 batch loss 6.79136038 epoch total loss 6.98850298\n",
      "Trained batch 424 batch loss 6.89610147 epoch total loss 6.98828459\n",
      "Trained batch 425 batch loss 7.0718751 epoch total loss 6.98848104\n",
      "Trained batch 426 batch loss 7.17435408 epoch total loss 6.98891735\n",
      "Trained batch 427 batch loss 6.70109606 epoch total loss 6.98824358\n",
      "Trained batch 428 batch loss 6.60650778 epoch total loss 6.98735142\n",
      "Trained batch 429 batch loss 6.61528683 epoch total loss 6.98648405\n",
      "Trained batch 430 batch loss 7.24333382 epoch total loss 6.98708153\n",
      "Trained batch 431 batch loss 7.09123898 epoch total loss 6.98732328\n",
      "Trained batch 432 batch loss 6.79916048 epoch total loss 6.98688745\n",
      "Trained batch 433 batch loss 6.74615049 epoch total loss 6.98633146\n",
      "Trained batch 434 batch loss 6.44720364 epoch total loss 6.9850893\n",
      "Trained batch 435 batch loss 6.48819113 epoch total loss 6.98394728\n",
      "Trained batch 436 batch loss 6.77944946 epoch total loss 6.98347855\n",
      "Trained batch 437 batch loss 6.91052341 epoch total loss 6.98331165\n",
      "Trained batch 438 batch loss 6.89346027 epoch total loss 6.98310709\n",
      "Trained batch 439 batch loss 7.23976851 epoch total loss 6.98369169\n",
      "Trained batch 440 batch loss 7.36865044 epoch total loss 6.98456621\n",
      "Trained batch 441 batch loss 7.44144773 epoch total loss 6.98560238\n",
      "Trained batch 442 batch loss 7.28137302 epoch total loss 6.98627186\n",
      "Trained batch 443 batch loss 7.23232412 epoch total loss 6.98682737\n",
      "Trained batch 444 batch loss 7.51803732 epoch total loss 6.98802376\n",
      "Trained batch 445 batch loss 7.51534081 epoch total loss 6.98920918\n",
      "Trained batch 446 batch loss 7.21777248 epoch total loss 6.9897213\n",
      "Trained batch 447 batch loss 7.15473223 epoch total loss 6.99009085\n",
      "Trained batch 448 batch loss 6.67192554 epoch total loss 6.98938036\n",
      "Trained batch 449 batch loss 6.02921486 epoch total loss 6.98724222\n",
      "Trained batch 450 batch loss 7.22446537 epoch total loss 6.98776913\n",
      "Trained batch 451 batch loss 7.01458549 epoch total loss 6.98782873\n",
      "Trained batch 452 batch loss 6.59737301 epoch total loss 6.98696518\n",
      "Trained batch 453 batch loss 6.73932362 epoch total loss 6.98641825\n",
      "Trained batch 454 batch loss 6.85504293 epoch total loss 6.98612881\n",
      "Trained batch 455 batch loss 7.01767445 epoch total loss 6.98619795\n",
      "Trained batch 456 batch loss 7.25657 epoch total loss 6.98679066\n",
      "Trained batch 457 batch loss 7.23632908 epoch total loss 6.98733664\n",
      "Trained batch 458 batch loss 7.10908747 epoch total loss 6.98760271\n",
      "Trained batch 459 batch loss 7.04187679 epoch total loss 6.98772097\n",
      "Trained batch 460 batch loss 7.06519461 epoch total loss 6.98789\n",
      "Trained batch 461 batch loss 7.0812664 epoch total loss 6.98809242\n",
      "Trained batch 462 batch loss 7.33274555 epoch total loss 6.9888382\n",
      "Trained batch 463 batch loss 7.10950708 epoch total loss 6.98909903\n",
      "Trained batch 464 batch loss 6.81847811 epoch total loss 6.98873091\n",
      "Trained batch 465 batch loss 6.96060848 epoch total loss 6.98867083\n",
      "Trained batch 466 batch loss 6.73544216 epoch total loss 6.98812723\n",
      "Trained batch 467 batch loss 7.43606281 epoch total loss 6.98908615\n",
      "Trained batch 468 batch loss 7.39419889 epoch total loss 6.98995209\n",
      "Trained batch 469 batch loss 7.37163258 epoch total loss 6.99076605\n",
      "Trained batch 470 batch loss 7.35438 epoch total loss 6.99154\n",
      "Trained batch 471 batch loss 7.16776085 epoch total loss 6.9919138\n",
      "Trained batch 472 batch loss 7.42627335 epoch total loss 6.99283409\n",
      "Trained batch 473 batch loss 7.02683783 epoch total loss 6.99290609\n",
      "Trained batch 474 batch loss 6.63873482 epoch total loss 6.99215889\n",
      "Trained batch 475 batch loss 6.57681465 epoch total loss 6.99128437\n",
      "Trained batch 476 batch loss 6.74290514 epoch total loss 6.99076271\n",
      "Trained batch 477 batch loss 7.04488134 epoch total loss 6.9908762\n",
      "Trained batch 478 batch loss 7.23312616 epoch total loss 6.99138308\n",
      "Trained batch 479 batch loss 7.2025528 epoch total loss 6.99182415\n",
      "Trained batch 480 batch loss 6.89416265 epoch total loss 6.99162054\n",
      "Trained batch 481 batch loss 6.15643 epoch total loss 6.98988438\n",
      "Trained batch 482 batch loss 6.11377239 epoch total loss 6.98806667\n",
      "Trained batch 483 batch loss 6.77612114 epoch total loss 6.98762751\n",
      "Trained batch 484 batch loss 6.4343524 epoch total loss 6.98648453\n",
      "Trained batch 485 batch loss 6.99535513 epoch total loss 6.98650265\n",
      "Trained batch 486 batch loss 7.02345419 epoch total loss 6.98657894\n",
      "Trained batch 487 batch loss 7.2204 epoch total loss 6.98705912\n",
      "Trained batch 488 batch loss 6.93231392 epoch total loss 6.98694706\n",
      "Trained batch 489 batch loss 7.351408 epoch total loss 6.98769188\n",
      "Trained batch 490 batch loss 7.20998144 epoch total loss 6.98814583\n",
      "Trained batch 491 batch loss 7.00214434 epoch total loss 6.98817444\n",
      "Trained batch 492 batch loss 7.07535696 epoch total loss 6.98835182\n",
      "Trained batch 493 batch loss 6.9849596 epoch total loss 6.98834467\n",
      "Trained batch 494 batch loss 6.96074772 epoch total loss 6.98828888\n",
      "Trained batch 495 batch loss 6.63527966 epoch total loss 6.98757553\n",
      "Trained batch 496 batch loss 6.84033203 epoch total loss 6.98727846\n",
      "Trained batch 497 batch loss 6.65432692 epoch total loss 6.98660851\n",
      "Trained batch 498 batch loss 6.51968384 epoch total loss 6.98567104\n",
      "Trained batch 499 batch loss 7.0688405 epoch total loss 6.98583794\n",
      "Trained batch 500 batch loss 7.19214249 epoch total loss 6.9862504\n",
      "Trained batch 501 batch loss 7.31477737 epoch total loss 6.98690605\n",
      "Trained batch 502 batch loss 7.38200092 epoch total loss 6.98769331\n",
      "Trained batch 503 batch loss 7.36348248 epoch total loss 6.98844051\n",
      "Trained batch 504 batch loss 7.4725275 epoch total loss 6.98940086\n",
      "Trained batch 505 batch loss 7.09915638 epoch total loss 6.98961782\n",
      "Trained batch 506 batch loss 7.12513304 epoch total loss 6.98988581\n",
      "Trained batch 507 batch loss 6.78626156 epoch total loss 6.98948479\n",
      "Trained batch 508 batch loss 7.20621395 epoch total loss 6.98991156\n",
      "Trained batch 509 batch loss 7.23559475 epoch total loss 6.99039412\n",
      "Trained batch 510 batch loss 7.4282794 epoch total loss 6.99125242\n",
      "Trained batch 511 batch loss 7.00676537 epoch total loss 6.99128294\n",
      "Trained batch 512 batch loss 7.18332243 epoch total loss 6.99165821\n",
      "Trained batch 513 batch loss 7.26773071 epoch total loss 6.99219656\n",
      "Trained batch 514 batch loss 7.39326191 epoch total loss 6.99297714\n",
      "Trained batch 515 batch loss 7.26050711 epoch total loss 6.99349642\n",
      "Trained batch 516 batch loss 7.25406408 epoch total loss 6.99400139\n",
      "Trained batch 517 batch loss 7.17204428 epoch total loss 6.99434614\n",
      "Trained batch 518 batch loss 6.89089 epoch total loss 6.99414635\n",
      "Trained batch 519 batch loss 7.21317959 epoch total loss 6.99456835\n",
      "Trained batch 520 batch loss 7.26651859 epoch total loss 6.99509144\n",
      "Trained batch 521 batch loss 7.17817926 epoch total loss 6.99544287\n",
      "Trained batch 522 batch loss 6.93958235 epoch total loss 6.99533606\n",
      "Trained batch 523 batch loss 7.09043455 epoch total loss 6.99551773\n",
      "Trained batch 524 batch loss 6.74244118 epoch total loss 6.99503469\n",
      "Trained batch 525 batch loss 7.09990883 epoch total loss 6.99523449\n",
      "Trained batch 526 batch loss 7.19996 epoch total loss 6.99562359\n",
      "Trained batch 527 batch loss 7.36222363 epoch total loss 6.99631929\n",
      "Trained batch 528 batch loss 7.21860313 epoch total loss 6.99674034\n",
      "Trained batch 529 batch loss 7.14221096 epoch total loss 6.997015\n",
      "Trained batch 530 batch loss 7.09962082 epoch total loss 6.9972086\n",
      "Trained batch 531 batch loss 6.85943317 epoch total loss 6.99694872\n",
      "Trained batch 532 batch loss 6.88120747 epoch total loss 6.99673128\n",
      "Trained batch 533 batch loss 6.31664324 epoch total loss 6.99545527\n",
      "Trained batch 534 batch loss 6.35029173 epoch total loss 6.99424696\n",
      "Trained batch 535 batch loss 6.77423859 epoch total loss 6.99383593\n",
      "Trained batch 536 batch loss 6.56115723 epoch total loss 6.99302816\n",
      "Trained batch 537 batch loss 6.78089285 epoch total loss 6.99263334\n",
      "Trained batch 538 batch loss 7.06058168 epoch total loss 6.9927597\n",
      "Trained batch 539 batch loss 6.72320271 epoch total loss 6.9922595\n",
      "Trained batch 540 batch loss 6.55854082 epoch total loss 6.99145651\n",
      "Trained batch 541 batch loss 6.55551863 epoch total loss 6.99065065\n",
      "Trained batch 542 batch loss 6.7000494 epoch total loss 6.99011421\n",
      "Trained batch 543 batch loss 7.02197456 epoch total loss 6.99017286\n",
      "Trained batch 544 batch loss 7.1820364 epoch total loss 6.99052572\n",
      "Trained batch 545 batch loss 7.56031132 epoch total loss 6.99157095\n",
      "Trained batch 546 batch loss 7.26198101 epoch total loss 6.99206638\n",
      "Trained batch 547 batch loss 7.33628941 epoch total loss 6.99269533\n",
      "Trained batch 548 batch loss 7.39588308 epoch total loss 6.99343157\n",
      "Trained batch 549 batch loss 7.23817539 epoch total loss 6.99387741\n",
      "Trained batch 550 batch loss 7.2404809 epoch total loss 6.99432564\n",
      "Trained batch 551 batch loss 6.52570629 epoch total loss 6.99347496\n",
      "Trained batch 552 batch loss 7.23209715 epoch total loss 6.99390745\n",
      "Trained batch 553 batch loss 7.17653418 epoch total loss 6.9942379\n",
      "Trained batch 554 batch loss 6.63300371 epoch total loss 6.99358559\n",
      "Trained batch 555 batch loss 7.356668 epoch total loss 6.99424\n",
      "Trained batch 556 batch loss 7.17873335 epoch total loss 6.99457169\n",
      "Trained batch 557 batch loss 6.92807913 epoch total loss 6.99445248\n",
      "Trained batch 558 batch loss 6.50938702 epoch total loss 6.99358273\n",
      "Trained batch 559 batch loss 7.31536198 epoch total loss 6.99415874\n",
      "Trained batch 560 batch loss 7.22128916 epoch total loss 6.99456406\n",
      "Trained batch 561 batch loss 7.09269142 epoch total loss 6.99473906\n",
      "Trained batch 562 batch loss 7.43366671 epoch total loss 6.99552\n",
      "Trained batch 563 batch loss 7.50164366 epoch total loss 6.99641895\n",
      "Trained batch 564 batch loss 7.31459808 epoch total loss 6.99698353\n",
      "Trained batch 565 batch loss 7.35998726 epoch total loss 6.99762583\n",
      "Trained batch 566 batch loss 6.96791792 epoch total loss 6.99757385\n",
      "Trained batch 567 batch loss 6.33505726 epoch total loss 6.99640512\n",
      "Trained batch 568 batch loss 6.31278181 epoch total loss 6.99520159\n",
      "Trained batch 569 batch loss 6.72700167 epoch total loss 6.99473\n",
      "Trained batch 570 batch loss 6.50875664 epoch total loss 6.99387741\n",
      "Trained batch 571 batch loss 6.64816761 epoch total loss 6.9932723\n",
      "Trained batch 572 batch loss 6.99758434 epoch total loss 6.99328\n",
      "Trained batch 573 batch loss 6.54456472 epoch total loss 6.99249697\n",
      "Trained batch 574 batch loss 6.87257814 epoch total loss 6.99228811\n",
      "Trained batch 575 batch loss 7.42232132 epoch total loss 6.99303579\n",
      "Trained batch 576 batch loss 6.7150054 epoch total loss 6.99255323\n",
      "Trained batch 577 batch loss 7.34175777 epoch total loss 6.99315834\n",
      "Trained batch 578 batch loss 7.24720669 epoch total loss 6.99359846\n",
      "Trained batch 579 batch loss 5.8852787 epoch total loss 6.99168396\n",
      "Trained batch 580 batch loss 5.90043163 epoch total loss 6.98980236\n",
      "Trained batch 581 batch loss 5.52015209 epoch total loss 6.98727322\n",
      "Trained batch 582 batch loss 7.03044224 epoch total loss 6.9873476\n",
      "Trained batch 583 batch loss 7.17949915 epoch total loss 6.9876771\n",
      "Trained batch 584 batch loss 7.47506285 epoch total loss 6.98851156\n",
      "Trained batch 585 batch loss 7.4314003 epoch total loss 6.98926878\n",
      "Trained batch 586 batch loss 7.51943684 epoch total loss 6.99017334\n",
      "Trained batch 587 batch loss 7.3206048 epoch total loss 6.99073696\n",
      "Trained batch 588 batch loss 7.12811 epoch total loss 6.99097\n",
      "Trained batch 589 batch loss 6.98588181 epoch total loss 6.99096155\n",
      "Trained batch 590 batch loss 7.06973791 epoch total loss 6.99109507\n",
      "Trained batch 591 batch loss 6.88364649 epoch total loss 6.99091339\n",
      "Trained batch 592 batch loss 6.9664793 epoch total loss 6.99087191\n",
      "Trained batch 593 batch loss 6.41740799 epoch total loss 6.98990488\n",
      "Trained batch 594 batch loss 6.8536725 epoch total loss 6.98967552\n",
      "Trained batch 595 batch loss 6.77707672 epoch total loss 6.98931789\n",
      "Trained batch 596 batch loss 7.21929884 epoch total loss 6.98970366\n",
      "Trained batch 597 batch loss 7.39201546 epoch total loss 6.99037743\n",
      "Trained batch 598 batch loss 7.28431845 epoch total loss 6.99086905\n",
      "Trained batch 599 batch loss 7.31143856 epoch total loss 6.99140406\n",
      "Trained batch 600 batch loss 7.16127586 epoch total loss 6.99168682\n",
      "Trained batch 601 batch loss 7.20849562 epoch total loss 6.99204779\n",
      "Trained batch 602 batch loss 7.22862959 epoch total loss 6.9924407\n",
      "Trained batch 603 batch loss 7.17371845 epoch total loss 6.99274158\n",
      "Trained batch 604 batch loss 7.00673962 epoch total loss 6.99276447\n",
      "Trained batch 605 batch loss 7.21216154 epoch total loss 6.99312782\n",
      "Trained batch 606 batch loss 6.70208549 epoch total loss 6.99264765\n",
      "Trained batch 607 batch loss 6.5789032 epoch total loss 6.99196625\n",
      "Trained batch 608 batch loss 7.29204416 epoch total loss 6.99246\n",
      "Trained batch 609 batch loss 7.21879625 epoch total loss 6.99283123\n",
      "Trained batch 610 batch loss 7.27171326 epoch total loss 6.99328804\n",
      "Trained batch 611 batch loss 7.21314764 epoch total loss 6.99364853\n",
      "Trained batch 612 batch loss 7.43624544 epoch total loss 6.99437141\n",
      "Trained batch 613 batch loss 7.36867952 epoch total loss 6.99498177\n",
      "Trained batch 614 batch loss 6.37789154 epoch total loss 6.99397659\n",
      "Trained batch 615 batch loss 7.11245298 epoch total loss 6.99416924\n",
      "Trained batch 616 batch loss 6.38350344 epoch total loss 6.99317741\n",
      "Trained batch 617 batch loss 6.01328325 epoch total loss 6.99158907\n",
      "Trained batch 618 batch loss 6.38618898 epoch total loss 6.99060965\n",
      "Trained batch 619 batch loss 6.31050777 epoch total loss 6.98951101\n",
      "Trained batch 620 batch loss 5.91401148 epoch total loss 6.98777628\n",
      "Trained batch 621 batch loss 5.76502943 epoch total loss 6.98580742\n",
      "Trained batch 622 batch loss 6.15609121 epoch total loss 6.98447371\n",
      "Trained batch 623 batch loss 6.39016962 epoch total loss 6.98352\n",
      "Trained batch 624 batch loss 6.75001287 epoch total loss 6.98314571\n",
      "Trained batch 625 batch loss 6.78342056 epoch total loss 6.98282576\n",
      "Trained batch 626 batch loss 7.39147043 epoch total loss 6.98347855\n",
      "Trained batch 627 batch loss 7.34141636 epoch total loss 6.98404932\n",
      "Trained batch 628 batch loss 7.31931973 epoch total loss 6.98458338\n",
      "Trained batch 629 batch loss 7.44878626 epoch total loss 6.98532152\n",
      "Trained batch 630 batch loss 7.50288296 epoch total loss 6.98614311\n",
      "Trained batch 631 batch loss 7.09329939 epoch total loss 6.98631287\n",
      "Trained batch 632 batch loss 7.08626 epoch total loss 6.98647118\n",
      "Trained batch 633 batch loss 7.25614882 epoch total loss 6.98689747\n",
      "Trained batch 634 batch loss 7.05836058 epoch total loss 6.98701048\n",
      "Trained batch 635 batch loss 7.26311827 epoch total loss 6.98744535\n",
      "Trained batch 636 batch loss 7.21335602 epoch total loss 6.9878006\n",
      "Trained batch 637 batch loss 7.37378502 epoch total loss 6.98840714\n",
      "Trained batch 638 batch loss 7.00848293 epoch total loss 6.98843813\n",
      "Trained batch 639 batch loss 7.32050753 epoch total loss 6.98895741\n",
      "Trained batch 640 batch loss 7.44000101 epoch total loss 6.98966217\n",
      "Trained batch 641 batch loss 7.48549414 epoch total loss 6.9904356\n",
      "Trained batch 642 batch loss 7.34356642 epoch total loss 6.99098587\n",
      "Trained batch 643 batch loss 7.3381815 epoch total loss 6.99152613\n",
      "Trained batch 644 batch loss 7.35591793 epoch total loss 6.99209213\n",
      "Trained batch 645 batch loss 7.37875748 epoch total loss 6.99269152\n",
      "Trained batch 646 batch loss 7.17634344 epoch total loss 6.99297571\n",
      "Trained batch 647 batch loss 7.02540922 epoch total loss 6.99302578\n",
      "Trained batch 648 batch loss 7.1944418 epoch total loss 6.99333668\n",
      "Trained batch 649 batch loss 7.1805582 epoch total loss 6.99362516\n",
      "Trained batch 650 batch loss 7.10379 epoch total loss 6.99379492\n",
      "Trained batch 651 batch loss 7.43488216 epoch total loss 6.99447298\n",
      "Trained batch 652 batch loss 7.33432 epoch total loss 6.99499416\n",
      "Trained batch 653 batch loss 6.67956161 epoch total loss 6.9945116\n",
      "Trained batch 654 batch loss 7.29438877 epoch total loss 6.99497032\n",
      "Trained batch 655 batch loss 6.92801046 epoch total loss 6.99486828\n",
      "Trained batch 656 batch loss 7.33720636 epoch total loss 6.99539042\n",
      "Trained batch 657 batch loss 7.2404356 epoch total loss 6.99576283\n",
      "Trained batch 658 batch loss 7.10562086 epoch total loss 6.99592972\n",
      "Trained batch 659 batch loss 6.74937344 epoch total loss 6.99555588\n",
      "Trained batch 660 batch loss 6.34167 epoch total loss 6.99456549\n",
      "Trained batch 661 batch loss 6.28121281 epoch total loss 6.99348593\n",
      "Trained batch 662 batch loss 6.31876516 epoch total loss 6.99246693\n",
      "Trained batch 663 batch loss 6.27853489 epoch total loss 6.99138975\n",
      "Trained batch 664 batch loss 6.08000517 epoch total loss 6.99001741\n",
      "Trained batch 665 batch loss 5.78633213 epoch total loss 6.98820686\n",
      "Trained batch 666 batch loss 5.70754528 epoch total loss 6.98628426\n",
      "Trained batch 667 batch loss 6.16636419 epoch total loss 6.98505497\n",
      "Trained batch 668 batch loss 6.74153662 epoch total loss 6.98469067\n",
      "Trained batch 669 batch loss 7.37689447 epoch total loss 6.98527718\n",
      "Trained batch 670 batch loss 7.0680213 epoch total loss 6.9854\n",
      "Trained batch 671 batch loss 7.06440115 epoch total loss 6.98551798\n",
      "Trained batch 672 batch loss 7.35487795 epoch total loss 6.98606777\n",
      "Trained batch 673 batch loss 7.08197498 epoch total loss 6.98621082\n",
      "Trained batch 674 batch loss 7.02108765 epoch total loss 6.98626232\n",
      "Trained batch 675 batch loss 6.96251202 epoch total loss 6.98622704\n",
      "Trained batch 676 batch loss 6.41285181 epoch total loss 6.98537922\n",
      "Trained batch 677 batch loss 6.4526763 epoch total loss 6.98459196\n",
      "Trained batch 678 batch loss 6.84350872 epoch total loss 6.98438454\n",
      "Trained batch 679 batch loss 6.58516693 epoch total loss 6.98379612\n",
      "Trained batch 680 batch loss 6.35000849 epoch total loss 6.98286438\n",
      "Trained batch 681 batch loss 5.99654627 epoch total loss 6.98141575\n",
      "Trained batch 682 batch loss 5.99291086 epoch total loss 6.97996616\n",
      "Trained batch 683 batch loss 6.60499907 epoch total loss 6.97941732\n",
      "Trained batch 684 batch loss 6.03746367 epoch total loss 6.97804\n",
      "Trained batch 685 batch loss 6.48490715 epoch total loss 6.97732\n",
      "Trained batch 686 batch loss 6.91627026 epoch total loss 6.9772315\n",
      "Trained batch 687 batch loss 6.79170322 epoch total loss 6.97696114\n",
      "Trained batch 688 batch loss 6.90376043 epoch total loss 6.9768548\n",
      "Trained batch 689 batch loss 6.70532751 epoch total loss 6.97646093\n",
      "Trained batch 690 batch loss 6.4407258 epoch total loss 6.97568512\n",
      "Trained batch 691 batch loss 7.23385715 epoch total loss 6.97605848\n",
      "Trained batch 692 batch loss 7.2611351 epoch total loss 6.97647095\n",
      "Trained batch 693 batch loss 7.31415701 epoch total loss 6.9769578\n",
      "Trained batch 694 batch loss 7.35762501 epoch total loss 6.97750616\n",
      "Trained batch 695 batch loss 6.89808178 epoch total loss 6.97739172\n",
      "Trained batch 696 batch loss 6.90199947 epoch total loss 6.977283\n",
      "Trained batch 697 batch loss 7.15969372 epoch total loss 6.97754478\n",
      "Trained batch 698 batch loss 6.58685827 epoch total loss 6.97698498\n",
      "Trained batch 699 batch loss 6.47018528 epoch total loss 6.97626\n",
      "Trained batch 700 batch loss 7.02360868 epoch total loss 6.97632742\n",
      "Trained batch 701 batch loss 7.14533567 epoch total loss 6.9765687\n",
      "Trained batch 702 batch loss 7.1772418 epoch total loss 6.9768548\n",
      "Trained batch 703 batch loss 7.40958691 epoch total loss 6.9774704\n",
      "Trained batch 704 batch loss 6.95964479 epoch total loss 6.97744465\n",
      "Trained batch 705 batch loss 7.0098815 epoch total loss 6.97749043\n",
      "Trained batch 706 batch loss 6.91568041 epoch total loss 6.97740269\n",
      "Trained batch 707 batch loss 6.94521475 epoch total loss 6.97735739\n",
      "Trained batch 708 batch loss 6.72936344 epoch total loss 6.97700739\n",
      "Trained batch 709 batch loss 6.92559767 epoch total loss 6.97693491\n",
      "Trained batch 710 batch loss 7.28573847 epoch total loss 6.97737\n",
      "Trained batch 711 batch loss 7.2366128 epoch total loss 6.97773457\n",
      "Trained batch 712 batch loss 7.36502028 epoch total loss 6.97827911\n",
      "Trained batch 713 batch loss 7.57609367 epoch total loss 6.97911739\n",
      "Trained batch 714 batch loss 7.41389656 epoch total loss 6.97972679\n",
      "Trained batch 715 batch loss 6.93905 epoch total loss 6.97966957\n",
      "Trained batch 716 batch loss 6.83328772 epoch total loss 6.97946548\n",
      "Trained batch 717 batch loss 7.10985374 epoch total loss 6.97964764\n",
      "Trained batch 718 batch loss 6.85811901 epoch total loss 6.97947788\n",
      "Trained batch 719 batch loss 7.00286722 epoch total loss 6.97951031\n",
      "Trained batch 720 batch loss 7.32602215 epoch total loss 6.97999191\n",
      "Trained batch 721 batch loss 7.08017159 epoch total loss 6.98013067\n",
      "Trained batch 722 batch loss 7.13209248 epoch total loss 6.98034143\n",
      "Trained batch 723 batch loss 7.30380964 epoch total loss 6.98078871\n",
      "Trained batch 724 batch loss 7.04305792 epoch total loss 6.98087454\n",
      "Trained batch 725 batch loss 6.81319761 epoch total loss 6.98064327\n",
      "Trained batch 726 batch loss 6.63472509 epoch total loss 6.98016691\n",
      "Trained batch 727 batch loss 6.92414093 epoch total loss 6.98009\n",
      "Trained batch 728 batch loss 6.70197201 epoch total loss 6.97970819\n",
      "Trained batch 729 batch loss 6.80827522 epoch total loss 6.97947264\n",
      "Trained batch 730 batch loss 7.10627699 epoch total loss 6.97964668\n",
      "Trained batch 731 batch loss 7.17280388 epoch total loss 6.97991085\n",
      "Trained batch 732 batch loss 7.02601719 epoch total loss 6.97997379\n",
      "Trained batch 733 batch loss 6.48751688 epoch total loss 6.97930145\n",
      "Trained batch 734 batch loss 6.46841049 epoch total loss 6.97860527\n",
      "Trained batch 735 batch loss 7.14478445 epoch total loss 6.97883177\n",
      "Trained batch 736 batch loss 7.44438 epoch total loss 6.97946453\n",
      "Trained batch 737 batch loss 7.39452171 epoch total loss 6.98002768\n",
      "Trained batch 738 batch loss 7.11064482 epoch total loss 6.98020458\n",
      "Trained batch 739 batch loss 6.8540535 epoch total loss 6.98003387\n",
      "Trained batch 740 batch loss 7.23910618 epoch total loss 6.98038435\n",
      "Trained batch 741 batch loss 6.8600049 epoch total loss 6.98022175\n",
      "Trained batch 742 batch loss 7.08463812 epoch total loss 6.98036194\n",
      "Trained batch 743 batch loss 7.23889208 epoch total loss 6.98071\n",
      "Trained batch 744 batch loss 7.00630569 epoch total loss 6.98074436\n",
      "Trained batch 745 batch loss 6.80058289 epoch total loss 6.98050261\n",
      "Trained batch 746 batch loss 6.93777084 epoch total loss 6.98044586\n",
      "Trained batch 747 batch loss 6.54419565 epoch total loss 6.97986221\n",
      "Trained batch 748 batch loss 7.10620975 epoch total loss 6.98003149\n",
      "Trained batch 749 batch loss 7.27055836 epoch total loss 6.98041916\n",
      "Trained batch 750 batch loss 7.26071644 epoch total loss 6.980793\n",
      "Trained batch 751 batch loss 7.22100306 epoch total loss 6.98111296\n",
      "Trained batch 752 batch loss 7.48668623 epoch total loss 6.98178577\n",
      "Trained batch 753 batch loss 7.22099495 epoch total loss 6.98210335\n",
      "Trained batch 754 batch loss 7.48494673 epoch total loss 6.98277044\n",
      "Trained batch 755 batch loss 7.11704493 epoch total loss 6.9829483\n",
      "Trained batch 756 batch loss 7.26413631 epoch total loss 6.98332\n",
      "Trained batch 757 batch loss 6.84903097 epoch total loss 6.98314285\n",
      "Trained batch 758 batch loss 7.0414772 epoch total loss 6.98322\n",
      "Trained batch 759 batch loss 7.20121956 epoch total loss 6.98350716\n",
      "Trained batch 760 batch loss 7.47541523 epoch total loss 6.9841547\n",
      "Trained batch 761 batch loss 7.25382805 epoch total loss 6.98450899\n",
      "Trained batch 762 batch loss 7.4351449 epoch total loss 6.98510027\n",
      "Trained batch 763 batch loss 7.42254734 epoch total loss 6.98567343\n",
      "Trained batch 764 batch loss 6.9834013 epoch total loss 6.98567057\n",
      "Trained batch 765 batch loss 7.21187162 epoch total loss 6.98596621\n",
      "Trained batch 766 batch loss 6.71565676 epoch total loss 6.98561335\n",
      "Trained batch 767 batch loss 6.8974905 epoch total loss 6.98549843\n",
      "Trained batch 768 batch loss 6.93801546 epoch total loss 6.98543692\n",
      "Trained batch 769 batch loss 6.62491 epoch total loss 6.98496819\n",
      "Trained batch 770 batch loss 7.14316273 epoch total loss 6.98517323\n",
      "Trained batch 771 batch loss 6.90567303 epoch total loss 6.98507\n",
      "Trained batch 772 batch loss 6.93733549 epoch total loss 6.98500872\n",
      "Trained batch 773 batch loss 6.68089676 epoch total loss 6.98461485\n",
      "Trained batch 774 batch loss 6.8550849 epoch total loss 6.98444748\n",
      "Trained batch 775 batch loss 6.99885559 epoch total loss 6.98446655\n",
      "Trained batch 776 batch loss 6.82947445 epoch total loss 6.98426676\n",
      "Trained batch 777 batch loss 6.89158 epoch total loss 6.98414755\n",
      "Trained batch 778 batch loss 6.7748909 epoch total loss 6.98387861\n",
      "Trained batch 779 batch loss 6.55399227 epoch total loss 6.98332691\n",
      "Trained batch 780 batch loss 6.67038631 epoch total loss 6.98292589\n",
      "Trained batch 781 batch loss 6.7907877 epoch total loss 6.98268032\n",
      "Trained batch 782 batch loss 6.74003124 epoch total loss 6.98237\n",
      "Trained batch 783 batch loss 6.75282 epoch total loss 6.98207712\n",
      "Trained batch 784 batch loss 6.89636135 epoch total loss 6.98196793\n",
      "Trained batch 785 batch loss 7.36041403 epoch total loss 6.98245\n",
      "Trained batch 786 batch loss 7.41899 epoch total loss 6.98300505\n",
      "Trained batch 787 batch loss 7.39601851 epoch total loss 6.98353\n",
      "Trained batch 788 batch loss 7.20731497 epoch total loss 6.98381424\n",
      "Trained batch 789 batch loss 6.40884829 epoch total loss 6.98308516\n",
      "Trained batch 790 batch loss 6.71065378 epoch total loss 6.9827404\n",
      "Trained batch 791 batch loss 7.2458415 epoch total loss 6.98307228\n",
      "Trained batch 792 batch loss 6.90050697 epoch total loss 6.98296833\n",
      "Trained batch 793 batch loss 6.831357 epoch total loss 6.98277712\n",
      "Trained batch 794 batch loss 6.76757574 epoch total loss 6.98250628\n",
      "Trained batch 795 batch loss 6.61808491 epoch total loss 6.98204803\n",
      "Trained batch 796 batch loss 6.82988644 epoch total loss 6.98185682\n",
      "Trained batch 797 batch loss 7.07157755 epoch total loss 6.98197\n",
      "Trained batch 798 batch loss 7.3707242 epoch total loss 6.98245668\n",
      "Trained batch 799 batch loss 7.30731 epoch total loss 6.98286295\n",
      "Trained batch 800 batch loss 7.35818338 epoch total loss 6.98333263\n",
      "Trained batch 801 batch loss 6.65190315 epoch total loss 6.98291874\n",
      "Trained batch 802 batch loss 6.31219816 epoch total loss 6.98208237\n",
      "Trained batch 803 batch loss 7.36048174 epoch total loss 6.98255301\n",
      "Trained batch 804 batch loss 7.2159214 epoch total loss 6.9828434\n",
      "Trained batch 805 batch loss 6.38219929 epoch total loss 6.98209715\n",
      "Trained batch 806 batch loss 5.58684444 epoch total loss 6.98036623\n",
      "Trained batch 807 batch loss 6.25054693 epoch total loss 6.97946215\n",
      "Trained batch 808 batch loss 7.21922302 epoch total loss 6.97975874\n",
      "Trained batch 809 batch loss 7.26125193 epoch total loss 6.98010683\n",
      "Trained batch 810 batch loss 6.90811825 epoch total loss 6.98001766\n",
      "Trained batch 811 batch loss 6.8958106 epoch total loss 6.97991419\n",
      "Trained batch 812 batch loss 7.23167276 epoch total loss 6.98022413\n",
      "Trained batch 813 batch loss 7.33107901 epoch total loss 6.98065567\n",
      "Trained batch 814 batch loss 6.83795214 epoch total loss 6.98048\n",
      "Trained batch 815 batch loss 6.55101538 epoch total loss 6.97995281\n",
      "Trained batch 816 batch loss 7.03760242 epoch total loss 6.98002338\n",
      "Trained batch 817 batch loss 6.95266438 epoch total loss 6.97999\n",
      "Trained batch 818 batch loss 7.01539612 epoch total loss 6.9800334\n",
      "Trained batch 819 batch loss 7.14499044 epoch total loss 6.9802351\n",
      "Trained batch 820 batch loss 7.12956095 epoch total loss 6.98041677\n",
      "Trained batch 821 batch loss 7.12495 epoch total loss 6.9805932\n",
      "Trained batch 822 batch loss 7.38613892 epoch total loss 6.98108673\n",
      "Trained batch 823 batch loss 7.27394485 epoch total loss 6.98144245\n",
      "Trained batch 824 batch loss 7.3209877 epoch total loss 6.98185396\n",
      "Trained batch 825 batch loss 7.05085564 epoch total loss 6.98193789\n",
      "Trained batch 826 batch loss 6.88357878 epoch total loss 6.98181868\n",
      "Trained batch 827 batch loss 6.55888 epoch total loss 6.98130751\n",
      "Trained batch 828 batch loss 7.25638199 epoch total loss 6.98164\n",
      "Trained batch 829 batch loss 7.47633553 epoch total loss 6.98223686\n",
      "Trained batch 830 batch loss 7.39102125 epoch total loss 6.98272943\n",
      "Trained batch 831 batch loss 7.47038412 epoch total loss 6.98331594\n",
      "Trained batch 832 batch loss 6.21477938 epoch total loss 6.98239231\n",
      "Trained batch 833 batch loss 6.41263723 epoch total loss 6.98170853\n",
      "Trained batch 834 batch loss 7.06162739 epoch total loss 6.98180437\n",
      "Trained batch 835 batch loss 7.12395859 epoch total loss 6.9819746\n",
      "Trained batch 836 batch loss 7.23749733 epoch total loss 6.98228\n",
      "Trained batch 837 batch loss 7.08690691 epoch total loss 6.98240519\n",
      "Trained batch 838 batch loss 7.27594948 epoch total loss 6.98275518\n",
      "Trained batch 839 batch loss 7.31696272 epoch total loss 6.98315334\n",
      "Trained batch 840 batch loss 7.22238 epoch total loss 6.98343801\n",
      "Trained batch 841 batch loss 7.22322941 epoch total loss 6.98372316\n",
      "Trained batch 842 batch loss 7.32623434 epoch total loss 6.98413\n",
      "Trained batch 843 batch loss 7.45464468 epoch total loss 6.98468781\n",
      "Trained batch 844 batch loss 7.35721159 epoch total loss 6.98512936\n",
      "Trained batch 845 batch loss 7.2436738 epoch total loss 6.98543549\n",
      "Trained batch 846 batch loss 7.03684711 epoch total loss 6.98549604\n",
      "Trained batch 847 batch loss 7.1925621 epoch total loss 6.98574\n",
      "Trained batch 848 batch loss 7.2397933 epoch total loss 6.98603964\n",
      "Trained batch 849 batch loss 7.17075777 epoch total loss 6.98625755\n",
      "Trained batch 850 batch loss 6.93045616 epoch total loss 6.98619175\n",
      "Trained batch 851 batch loss 7.17987204 epoch total loss 6.9864192\n",
      "Trained batch 852 batch loss 7.3127532 epoch total loss 6.98680258\n",
      "Trained batch 853 batch loss 7.01119852 epoch total loss 6.98683119\n",
      "Trained batch 854 batch loss 6.27571583 epoch total loss 6.98599863\n",
      "Trained batch 855 batch loss 6.47907495 epoch total loss 6.98540592\n",
      "Trained batch 856 batch loss 6.6989789 epoch total loss 6.98507166\n",
      "Trained batch 857 batch loss 7.12197256 epoch total loss 6.9852314\n",
      "Trained batch 858 batch loss 6.94742966 epoch total loss 6.98518705\n",
      "Trained batch 859 batch loss 7.13525534 epoch total loss 6.98536158\n",
      "Trained batch 860 batch loss 7.42041874 epoch total loss 6.9858675\n",
      "Trained batch 861 batch loss 7.21404743 epoch total loss 6.98613262\n",
      "Trained batch 862 batch loss 7.06976938 epoch total loss 6.98622942\n",
      "Trained batch 863 batch loss 6.99367046 epoch total loss 6.986238\n",
      "Trained batch 864 batch loss 7.15265512 epoch total loss 6.98643112\n",
      "Trained batch 865 batch loss 6.8983326 epoch total loss 6.98632908\n",
      "Trained batch 866 batch loss 7.10766888 epoch total loss 6.98646975\n",
      "Trained batch 867 batch loss 6.93125629 epoch total loss 6.98640585\n",
      "Trained batch 868 batch loss 7.07229233 epoch total loss 6.98650455\n",
      "Trained batch 869 batch loss 6.73767138 epoch total loss 6.98621845\n",
      "Trained batch 870 batch loss 7.33563805 epoch total loss 6.98662\n",
      "Trained batch 871 batch loss 6.63083887 epoch total loss 6.9862113\n",
      "Trained batch 872 batch loss 7.07911253 epoch total loss 6.98631811\n",
      "Trained batch 873 batch loss 7.07003 epoch total loss 6.98641348\n",
      "Trained batch 874 batch loss 7.36283684 epoch total loss 6.98684454\n",
      "Trained batch 875 batch loss 7.13624763 epoch total loss 6.98701525\n",
      "Trained batch 876 batch loss 7.16507864 epoch total loss 6.98721838\n",
      "Trained batch 877 batch loss 7.30313921 epoch total loss 6.98757839\n",
      "Trained batch 878 batch loss 6.7944212 epoch total loss 6.98735857\n",
      "Trained batch 879 batch loss 6.58413076 epoch total loss 6.9869\n",
      "Trained batch 880 batch loss 6.73600101 epoch total loss 6.98661423\n",
      "Trained batch 881 batch loss 6.96744299 epoch total loss 6.98659229\n",
      "Trained batch 882 batch loss 7.31106043 epoch total loss 6.98696041\n",
      "Trained batch 883 batch loss 7.07662392 epoch total loss 6.98706198\n",
      "Trained batch 884 batch loss 7.01545811 epoch total loss 6.9870944\n",
      "Trained batch 885 batch loss 6.87823534 epoch total loss 6.98697138\n",
      "Trained batch 886 batch loss 6.83823442 epoch total loss 6.98680353\n",
      "Trained batch 887 batch loss 7.11291647 epoch total loss 6.98694563\n",
      "Trained batch 888 batch loss 7.43368053 epoch total loss 6.98744869\n",
      "Trained batch 889 batch loss 6.926373 epoch total loss 6.98738\n",
      "Trained batch 890 batch loss 7.13917732 epoch total loss 6.98755026\n",
      "Trained batch 891 batch loss 6.93669796 epoch total loss 6.98749304\n",
      "Trained batch 892 batch loss 6.18197 epoch total loss 6.98659039\n",
      "Trained batch 893 batch loss 6.53258133 epoch total loss 6.98608208\n",
      "Trained batch 894 batch loss 6.40224695 epoch total loss 6.98542929\n",
      "Trained batch 895 batch loss 6.36585617 epoch total loss 6.98473692\n",
      "Trained batch 896 batch loss 6.43467236 epoch total loss 6.98412275\n",
      "Trained batch 897 batch loss 6.46254921 epoch total loss 6.98354101\n",
      "Trained batch 898 batch loss 6.29795122 epoch total loss 6.9827776\n",
      "Trained batch 899 batch loss 7.11187935 epoch total loss 6.98292112\n",
      "Trained batch 900 batch loss 7.19536829 epoch total loss 6.98315716\n",
      "Trained batch 901 batch loss 7.19951916 epoch total loss 6.98339748\n",
      "Trained batch 902 batch loss 6.42742777 epoch total loss 6.98278093\n",
      "Trained batch 903 batch loss 6.57879114 epoch total loss 6.98233318\n",
      "Trained batch 904 batch loss 6.87899733 epoch total loss 6.98221874\n",
      "Trained batch 905 batch loss 7.16062117 epoch total loss 6.98241615\n",
      "Trained batch 906 batch loss 6.95983171 epoch total loss 6.98239136\n",
      "Trained batch 907 batch loss 6.8711524 epoch total loss 6.98226833\n",
      "Trained batch 908 batch loss 6.88550377 epoch total loss 6.982162\n",
      "Trained batch 909 batch loss 7.11955452 epoch total loss 6.98231316\n",
      "Trained batch 910 batch loss 7.24787617 epoch total loss 6.98260546\n",
      "Trained batch 911 batch loss 7.32165909 epoch total loss 6.98297787\n",
      "Trained batch 912 batch loss 6.87910318 epoch total loss 6.98286343\n",
      "Trained batch 913 batch loss 7.02214432 epoch total loss 6.98290634\n",
      "Trained batch 914 batch loss 7.2167635 epoch total loss 6.9831624\n",
      "Trained batch 915 batch loss 7.14056396 epoch total loss 6.98333454\n",
      "Trained batch 916 batch loss 7.26114511 epoch total loss 6.98363781\n",
      "Trained batch 917 batch loss 7.03178692 epoch total loss 6.98369026\n",
      "Trained batch 918 batch loss 7.35828114 epoch total loss 6.98409843\n",
      "Trained batch 919 batch loss 7.17854 epoch total loss 6.98431\n",
      "Trained batch 920 batch loss 7.42946911 epoch total loss 6.98479414\n",
      "Trained batch 921 batch loss 7.24622154 epoch total loss 6.98507786\n",
      "Trained batch 922 batch loss 7.33076477 epoch total loss 6.98545265\n",
      "Trained batch 923 batch loss 6.86033344 epoch total loss 6.98531723\n",
      "Trained batch 924 batch loss 6.98525858 epoch total loss 6.98531723\n",
      "Trained batch 925 batch loss 6.96245956 epoch total loss 6.98529243\n",
      "Trained batch 926 batch loss 7.30294657 epoch total loss 6.98563528\n",
      "Trained batch 927 batch loss 7.4138217 epoch total loss 6.98609734\n",
      "Trained batch 928 batch loss 7.5381093 epoch total loss 6.98669243\n",
      "Trained batch 929 batch loss 7.48745871 epoch total loss 6.98723125\n",
      "Trained batch 930 batch loss 7.38357782 epoch total loss 6.98765755\n",
      "Trained batch 931 batch loss 7.43603706 epoch total loss 6.98813915\n",
      "Trained batch 932 batch loss 6.84888458 epoch total loss 6.98799\n",
      "Trained batch 933 batch loss 7.06918621 epoch total loss 6.98807716\n",
      "Trained batch 934 batch loss 6.8950057 epoch total loss 6.9879775\n",
      "Trained batch 935 batch loss 6.70757532 epoch total loss 6.98767757\n",
      "Trained batch 936 batch loss 6.99389553 epoch total loss 6.98768377\n",
      "Trained batch 937 batch loss 6.79090786 epoch total loss 6.98747396\n",
      "Trained batch 938 batch loss 6.7191391 epoch total loss 6.98718786\n",
      "Trained batch 939 batch loss 6.65334415 epoch total loss 6.98683262\n",
      "Trained batch 940 batch loss 6.78402328 epoch total loss 6.98661709\n",
      "Trained batch 941 batch loss 7.00841904 epoch total loss 6.98664\n",
      "Trained batch 942 batch loss 7.05777836 epoch total loss 6.98671532\n",
      "Trained batch 943 batch loss 7.04661512 epoch total loss 6.98677874\n",
      "Trained batch 944 batch loss 7.1636076 epoch total loss 6.98696566\n",
      "Trained batch 945 batch loss 6.79218912 epoch total loss 6.98675966\n",
      "Trained batch 946 batch loss 7.13185358 epoch total loss 6.98691273\n",
      "Trained batch 947 batch loss 6.95510244 epoch total loss 6.98687935\n",
      "Trained batch 948 batch loss 7.14457846 epoch total loss 6.98704576\n",
      "Trained batch 949 batch loss 7.10840082 epoch total loss 6.98717356\n",
      "Trained batch 950 batch loss 7.47702408 epoch total loss 6.98768902\n",
      "Trained batch 951 batch loss 7.28153515 epoch total loss 6.98799849\n",
      "Trained batch 952 batch loss 6.78258419 epoch total loss 6.98778248\n",
      "Trained batch 953 batch loss 6.52385521 epoch total loss 6.9872961\n",
      "Trained batch 954 batch loss 7.12128496 epoch total loss 6.98743629\n",
      "Trained batch 955 batch loss 7.10757542 epoch total loss 6.9875617\n",
      "Trained batch 956 batch loss 6.83886194 epoch total loss 6.98740625\n",
      "Trained batch 957 batch loss 7.32905245 epoch total loss 6.9877634\n",
      "Trained batch 958 batch loss 7.20924807 epoch total loss 6.98799467\n",
      "Trained batch 959 batch loss 7.0124011 epoch total loss 6.98802\n",
      "Trained batch 960 batch loss 7.05801773 epoch total loss 6.9880929\n",
      "Trained batch 961 batch loss 7.16832638 epoch total loss 6.98828077\n",
      "Trained batch 962 batch loss 6.9375124 epoch total loss 6.98822784\n",
      "Trained batch 963 batch loss 7.17485142 epoch total loss 6.98842192\n",
      "Trained batch 964 batch loss 7.20984459 epoch total loss 6.98865128\n",
      "Trained batch 965 batch loss 7.36123848 epoch total loss 6.98903751\n",
      "Trained batch 966 batch loss 6.80462551 epoch total loss 6.98884678\n",
      "Trained batch 967 batch loss 6.945467 epoch total loss 6.98880196\n",
      "Trained batch 968 batch loss 7.20262051 epoch total loss 6.98902273\n",
      "Trained batch 969 batch loss 7.36664486 epoch total loss 6.98941231\n",
      "Trained batch 970 batch loss 7.42223215 epoch total loss 6.98985863\n",
      "Trained batch 971 batch loss 7.48552656 epoch total loss 6.99036932\n",
      "Trained batch 972 batch loss 7.11406088 epoch total loss 6.99049664\n",
      "Trained batch 973 batch loss 6.83829594 epoch total loss 6.99034\n",
      "Trained batch 974 batch loss 7.07661676 epoch total loss 6.99042892\n",
      "Trained batch 975 batch loss 7.34874105 epoch total loss 6.99079609\n",
      "Trained batch 976 batch loss 7.41961718 epoch total loss 6.99123526\n",
      "Trained batch 977 batch loss 7.24721718 epoch total loss 6.99149752\n",
      "Trained batch 978 batch loss 7.32453966 epoch total loss 6.99183798\n",
      "Trained batch 979 batch loss 7.45501232 epoch total loss 6.992311\n",
      "Trained batch 980 batch loss 7.14386272 epoch total loss 6.99246597\n",
      "Trained batch 981 batch loss 7.20112181 epoch total loss 6.99267864\n",
      "Trained batch 982 batch loss 7.00439787 epoch total loss 6.99269056\n",
      "Trained batch 983 batch loss 7.34473038 epoch total loss 6.99304867\n",
      "Trained batch 984 batch loss 7.34235096 epoch total loss 6.99340391\n",
      "Trained batch 985 batch loss 7.40701294 epoch total loss 6.99382401\n",
      "Trained batch 986 batch loss 7.42436075 epoch total loss 6.99426031\n",
      "Trained batch 987 batch loss 7.19170427 epoch total loss 6.99446058\n",
      "Trained batch 988 batch loss 7.34232521 epoch total loss 6.99481297\n",
      "Trained batch 989 batch loss 7.2582984 epoch total loss 6.99507904\n",
      "Trained batch 990 batch loss 7.11056948 epoch total loss 6.99519539\n",
      "Trained batch 991 batch loss 6.90516472 epoch total loss 6.99510479\n",
      "Trained batch 992 batch loss 7.27807951 epoch total loss 6.99539042\n",
      "Trained batch 993 batch loss 7.01325655 epoch total loss 6.99540806\n",
      "Trained batch 994 batch loss 7.41964817 epoch total loss 6.99583483\n",
      "Trained batch 995 batch loss 7.33401 epoch total loss 6.99617481\n",
      "Trained batch 996 batch loss 7.09046268 epoch total loss 6.99626923\n",
      "Trained batch 997 batch loss 7.06446648 epoch total loss 6.99633741\n",
      "Trained batch 998 batch loss 7.36055088 epoch total loss 6.99670219\n",
      "Trained batch 999 batch loss 6.89890194 epoch total loss 6.99660444\n",
      "Trained batch 1000 batch loss 7.35264921 epoch total loss 6.99696064\n",
      "Trained batch 1001 batch loss 6.7368803 epoch total loss 6.99670076\n",
      "Trained batch 1002 batch loss 6.8572011 epoch total loss 6.99656153\n",
      "Trained batch 1003 batch loss 7.13070774 epoch total loss 6.99669552\n",
      "Trained batch 1004 batch loss 7.00366259 epoch total loss 6.99670267\n",
      "Trained batch 1005 batch loss 6.78318596 epoch total loss 6.99649\n",
      "Trained batch 1006 batch loss 6.80453444 epoch total loss 6.99629974\n",
      "Trained batch 1007 batch loss 7.09618425 epoch total loss 6.99639893\n",
      "Trained batch 1008 batch loss 6.59994316 epoch total loss 6.99600554\n",
      "Trained batch 1009 batch loss 7.00864649 epoch total loss 6.99601841\n",
      "Trained batch 1010 batch loss 7.16415 epoch total loss 6.99618483\n",
      "Trained batch 1011 batch loss 7.2891078 epoch total loss 6.99647427\n",
      "Trained batch 1012 batch loss 7.28181553 epoch total loss 6.99675608\n",
      "Trained batch 1013 batch loss 7.14478207 epoch total loss 6.99690247\n",
      "Trained batch 1014 batch loss 7.15488768 epoch total loss 6.99705839\n",
      "Trained batch 1015 batch loss 7.12801552 epoch total loss 6.99718714\n",
      "Trained batch 1016 batch loss 7.08644342 epoch total loss 6.99727488\n",
      "Trained batch 1017 batch loss 7.12240791 epoch total loss 6.99739838\n",
      "Trained batch 1018 batch loss 6.5557313 epoch total loss 6.99696445\n",
      "Trained batch 1019 batch loss 7.10077095 epoch total loss 6.99706602\n",
      "Trained batch 1020 batch loss 6.38759232 epoch total loss 6.99646854\n",
      "Trained batch 1021 batch loss 6.68234253 epoch total loss 6.99616051\n",
      "Trained batch 1022 batch loss 7.04581308 epoch total loss 6.99620914\n",
      "Trained batch 1023 batch loss 7.0572443 epoch total loss 6.99626875\n",
      "Trained batch 1024 batch loss 6.59689426 epoch total loss 6.9958787\n",
      "Trained batch 1025 batch loss 6.48845243 epoch total loss 6.99538326\n",
      "Trained batch 1026 batch loss 6.90412 epoch total loss 6.99529457\n",
      "Trained batch 1027 batch loss 6.90365314 epoch total loss 6.9952054\n",
      "Trained batch 1028 batch loss 7.00464106 epoch total loss 6.99521494\n",
      "Trained batch 1029 batch loss 6.96000195 epoch total loss 6.99518061\n",
      "Trained batch 1030 batch loss 7.03526 epoch total loss 6.99521971\n",
      "Trained batch 1031 batch loss 6.70365 epoch total loss 6.99493694\n",
      "Trained batch 1032 batch loss 6.65685797 epoch total loss 6.99460888\n",
      "Trained batch 1033 batch loss 7.26002121 epoch total loss 6.99486637\n",
      "Trained batch 1034 batch loss 7.23283672 epoch total loss 6.99509621\n",
      "Trained batch 1035 batch loss 7.22569323 epoch total loss 6.99531889\n",
      "Trained batch 1036 batch loss 7.38905191 epoch total loss 6.99569941\n",
      "Trained batch 1037 batch loss 6.97615433 epoch total loss 6.99568033\n",
      "Trained batch 1038 batch loss 6.97020245 epoch total loss 6.99565601\n",
      "Trained batch 1039 batch loss 6.78454685 epoch total loss 6.99545288\n",
      "Trained batch 1040 batch loss 6.72813177 epoch total loss 6.99519539\n",
      "Trained batch 1041 batch loss 6.7353487 epoch total loss 6.994946\n",
      "Trained batch 1042 batch loss 7.11912537 epoch total loss 6.99506521\n",
      "Trained batch 1043 batch loss 6.86092186 epoch total loss 6.99493647\n",
      "Trained batch 1044 batch loss 6.81185532 epoch total loss 6.99476147\n",
      "Trained batch 1045 batch loss 7.15280867 epoch total loss 6.99491262\n",
      "Trained batch 1046 batch loss 7.01787233 epoch total loss 6.99493456\n",
      "Trained batch 1047 batch loss 6.34959316 epoch total loss 6.99431849\n",
      "Trained batch 1048 batch loss 6.79828167 epoch total loss 6.99413109\n",
      "Trained batch 1049 batch loss 6.8218112 epoch total loss 6.99396706\n",
      "Trained batch 1050 batch loss 6.77324486 epoch total loss 6.99375677\n",
      "Trained batch 1051 batch loss 7.0699687 epoch total loss 6.99382925\n",
      "Trained batch 1052 batch loss 7.34013414 epoch total loss 6.99415874\n",
      "Trained batch 1053 batch loss 7.33309174 epoch total loss 6.99448061\n",
      "Trained batch 1054 batch loss 6.95105505 epoch total loss 6.9944396\n",
      "Trained batch 1055 batch loss 7.0943 epoch total loss 6.99453402\n",
      "Trained batch 1056 batch loss 6.91517115 epoch total loss 6.99445868\n",
      "Trained batch 1057 batch loss 6.16122818 epoch total loss 6.99367046\n",
      "Trained batch 1058 batch loss 7.10936689 epoch total loss 6.99377966\n",
      "Trained batch 1059 batch loss 7.0127883 epoch total loss 6.99379778\n",
      "Trained batch 1060 batch loss 7.23545027 epoch total loss 6.99402523\n",
      "Trained batch 1061 batch loss 7.03317165 epoch total loss 6.99406242\n",
      "Trained batch 1062 batch loss 7.20512724 epoch total loss 6.99426126\n",
      "Trained batch 1063 batch loss 7.06105518 epoch total loss 6.99432373\n",
      "Trained batch 1064 batch loss 7.15351963 epoch total loss 6.99447346\n",
      "Trained batch 1065 batch loss 7.10264063 epoch total loss 6.99457502\n",
      "Trained batch 1066 batch loss 7.34067154 epoch total loss 6.99489975\n",
      "Trained batch 1067 batch loss 7.13575315 epoch total loss 6.99503183\n",
      "Trained batch 1068 batch loss 7.25243187 epoch total loss 6.99527264\n",
      "Trained batch 1069 batch loss 7.32670212 epoch total loss 6.99558258\n",
      "Trained batch 1070 batch loss 7.07242298 epoch total loss 6.99565411\n",
      "Trained batch 1071 batch loss 6.90174437 epoch total loss 6.99556684\n",
      "Trained batch 1072 batch loss 7.18798637 epoch total loss 6.99574614\n",
      "Trained batch 1073 batch loss 7.48020029 epoch total loss 6.9961977\n",
      "Trained batch 1074 batch loss 7.39538097 epoch total loss 6.99656916\n",
      "Trained batch 1075 batch loss 7.33294582 epoch total loss 6.99688244\n",
      "Trained batch 1076 batch loss 7.25428677 epoch total loss 6.99712181\n",
      "Trained batch 1077 batch loss 7.34487438 epoch total loss 6.99744415\n",
      "Trained batch 1078 batch loss 6.94074869 epoch total loss 6.9973917\n",
      "Trained batch 1079 batch loss 7.26193619 epoch total loss 6.9976368\n",
      "Trained batch 1080 batch loss 6.88958073 epoch total loss 6.99753666\n",
      "Trained batch 1081 batch loss 7.1723547 epoch total loss 6.99769878\n",
      "Trained batch 1082 batch loss 7.13048935 epoch total loss 6.99782133\n",
      "Trained batch 1083 batch loss 6.25661 epoch total loss 6.99713707\n",
      "Trained batch 1084 batch loss 6.41487646 epoch total loss 6.9966\n",
      "Trained batch 1085 batch loss 6.4690547 epoch total loss 6.99611378\n",
      "Trained batch 1086 batch loss 7.25981379 epoch total loss 6.99635696\n",
      "Trained batch 1087 batch loss 7.24194479 epoch total loss 6.99658298\n",
      "Trained batch 1088 batch loss 6.73478556 epoch total loss 6.99634218\n",
      "Trained batch 1089 batch loss 6.5830369 epoch total loss 6.99596262\n",
      "Trained batch 1090 batch loss 6.0566206 epoch total loss 6.99510098\n",
      "Trained batch 1091 batch loss 6.62908077 epoch total loss 6.99476528\n",
      "Trained batch 1092 batch loss 7.24438047 epoch total loss 6.99499369\n",
      "Trained batch 1093 batch loss 7.48091936 epoch total loss 6.99543858\n",
      "Trained batch 1094 batch loss 7.43268 epoch total loss 6.99583817\n",
      "Trained batch 1095 batch loss 7.44264364 epoch total loss 6.99624634\n",
      "Trained batch 1096 batch loss 7.46951675 epoch total loss 6.99667835\n",
      "Trained batch 1097 batch loss 7.3774848 epoch total loss 6.99702549\n",
      "Trained batch 1098 batch loss 7.41949224 epoch total loss 6.9974103\n",
      "Trained batch 1099 batch loss 7.41605282 epoch total loss 6.99779081\n",
      "Trained batch 1100 batch loss 6.95036316 epoch total loss 6.9977479\n",
      "Trained batch 1101 batch loss 7.26280975 epoch total loss 6.99798822\n",
      "Trained batch 1102 batch loss 7.27937031 epoch total loss 6.99824381\n",
      "Trained batch 1103 batch loss 7.23083496 epoch total loss 6.99845457\n",
      "Trained batch 1104 batch loss 7.30100155 epoch total loss 6.99872828\n",
      "Trained batch 1105 batch loss 7.17539215 epoch total loss 6.99888802\n",
      "Trained batch 1106 batch loss 7.09266949 epoch total loss 6.99897289\n",
      "Trained batch 1107 batch loss 6.89594841 epoch total loss 6.99888\n",
      "Trained batch 1108 batch loss 7.02006149 epoch total loss 6.99889898\n",
      "Trained batch 1109 batch loss 7.36274529 epoch total loss 6.99922752\n",
      "Trained batch 1110 batch loss 7.19409561 epoch total loss 6.999403\n",
      "Trained batch 1111 batch loss 7.49274397 epoch total loss 6.99984694\n",
      "Trained batch 1112 batch loss 7.25323772 epoch total loss 7.00007486\n",
      "Trained batch 1113 batch loss 7.37215757 epoch total loss 7.00040913\n",
      "Trained batch 1114 batch loss 6.99581432 epoch total loss 7.00040483\n",
      "Trained batch 1115 batch loss 6.37726402 epoch total loss 6.99984646\n",
      "Trained batch 1116 batch loss 6.73424053 epoch total loss 6.99960852\n",
      "Trained batch 1117 batch loss 7.15535116 epoch total loss 6.99974775\n",
      "Trained batch 1118 batch loss 7.12067509 epoch total loss 6.999856\n",
      "Trained batch 1119 batch loss 7.19076443 epoch total loss 7.0000267\n",
      "Trained batch 1120 batch loss 6.5187273 epoch total loss 6.9995966\n",
      "Trained batch 1121 batch loss 6.47619104 epoch total loss 6.99913\n",
      "Trained batch 1122 batch loss 7.22560692 epoch total loss 6.99933147\n",
      "Trained batch 1123 batch loss 6.93878508 epoch total loss 6.99927759\n",
      "Trained batch 1124 batch loss 6.73390675 epoch total loss 6.99904156\n",
      "Trained batch 1125 batch loss 6.88051891 epoch total loss 6.99893618\n",
      "Trained batch 1126 batch loss 6.70802593 epoch total loss 6.99867773\n",
      "Trained batch 1127 batch loss 7.25256634 epoch total loss 6.9989028\n",
      "Trained batch 1128 batch loss 7.44496727 epoch total loss 6.9992981\n",
      "Trained batch 1129 batch loss 7.15672445 epoch total loss 6.99943781\n",
      "Trained batch 1130 batch loss 7.65029907 epoch total loss 7.00001383\n",
      "Trained batch 1131 batch loss 7.27606106 epoch total loss 7.00025797\n",
      "Trained batch 1132 batch loss 6.75241947 epoch total loss 7.00003862\n",
      "Trained batch 1133 batch loss 6.71284628 epoch total loss 6.99978542\n",
      "Trained batch 1134 batch loss 7.16860676 epoch total loss 6.9999342\n",
      "Trained batch 1135 batch loss 7.20019865 epoch total loss 7.00011063\n",
      "Trained batch 1136 batch loss 6.99853659 epoch total loss 7.0001092\n",
      "Trained batch 1137 batch loss 7.22462368 epoch total loss 7.00030661\n",
      "Trained batch 1138 batch loss 6.93104887 epoch total loss 7.00024605\n",
      "Trained batch 1139 batch loss 6.52745342 epoch total loss 6.99983072\n",
      "Trained batch 1140 batch loss 7.18277407 epoch total loss 6.99999094\n",
      "Trained batch 1141 batch loss 7.42303276 epoch total loss 7.00036144\n",
      "Trained batch 1142 batch loss 7.36454344 epoch total loss 7.00068092\n",
      "Trained batch 1143 batch loss 7.44450331 epoch total loss 7.00106907\n",
      "Trained batch 1144 batch loss 7.26831865 epoch total loss 7.00130272\n",
      "Trained batch 1145 batch loss 7.33618546 epoch total loss 7.0015955\n",
      "Trained batch 1146 batch loss 7.04972219 epoch total loss 7.00163746\n",
      "Trained batch 1147 batch loss 7.23470926 epoch total loss 7.00184059\n",
      "Trained batch 1148 batch loss 7.25989151 epoch total loss 7.00206518\n",
      "Trained batch 1149 batch loss 7.16926813 epoch total loss 7.00221109\n",
      "Trained batch 1150 batch loss 7.08935261 epoch total loss 7.00228691\n",
      "Trained batch 1151 batch loss 6.87328768 epoch total loss 7.00217438\n",
      "Trained batch 1152 batch loss 7.24776 epoch total loss 7.00238752\n",
      "Trained batch 1153 batch loss 6.99216938 epoch total loss 7.00237894\n",
      "Trained batch 1154 batch loss 7.02649307 epoch total loss 7.00239944\n",
      "Trained batch 1155 batch loss 6.65495396 epoch total loss 7.00209856\n",
      "Trained batch 1156 batch loss 6.8684783 epoch total loss 7.00198317\n",
      "Trained batch 1157 batch loss 6.78354645 epoch total loss 7.00179434\n",
      "Trained batch 1158 batch loss 6.6212554 epoch total loss 7.0014658\n",
      "Trained batch 1159 batch loss 6.59745789 epoch total loss 7.00111723\n",
      "Trained batch 1160 batch loss 6.80459547 epoch total loss 7.00094795\n",
      "Trained batch 1161 batch loss 6.59657526 epoch total loss 7.0006\n",
      "Trained batch 1162 batch loss 7.1991806 epoch total loss 7.00077057\n",
      "Trained batch 1163 batch loss 7.07486725 epoch total loss 7.00083447\n",
      "Trained batch 1164 batch loss 7.2423296 epoch total loss 7.00104141\n",
      "Trained batch 1165 batch loss 6.84494543 epoch total loss 7.00090742\n",
      "Trained batch 1166 batch loss 6.73904657 epoch total loss 7.00068283\n",
      "Trained batch 1167 batch loss 7.25816488 epoch total loss 7.00090361\n",
      "Trained batch 1168 batch loss 6.93397903 epoch total loss 7.00084639\n",
      "Trained batch 1169 batch loss 6.71283245 epoch total loss 7.00060034\n",
      "Trained batch 1170 batch loss 6.58156633 epoch total loss 7.00024223\n",
      "Trained batch 1171 batch loss 7.2008462 epoch total loss 7.00041342\n",
      "Trained batch 1172 batch loss 7.41346741 epoch total loss 7.0007658\n",
      "Trained batch 1173 batch loss 6.55592 epoch total loss 7.00038624\n",
      "Trained batch 1174 batch loss 6.64579725 epoch total loss 7.00008392\n",
      "Trained batch 1175 batch loss 7.13131094 epoch total loss 7.0001955\n",
      "Trained batch 1176 batch loss 7.16705132 epoch total loss 7.00033712\n",
      "Trained batch 1177 batch loss 6.84782314 epoch total loss 7.00020742\n",
      "Trained batch 1178 batch loss 6.9005084 epoch total loss 7.00012255\n",
      "Trained batch 1179 batch loss 7.15573549 epoch total loss 7.00025415\n",
      "Trained batch 1180 batch loss 7.04157257 epoch total loss 7.00028944\n",
      "Trained batch 1181 batch loss 7.40988684 epoch total loss 7.00063658\n",
      "Trained batch 1182 batch loss 7.03621387 epoch total loss 7.00066662\n",
      "Trained batch 1183 batch loss 6.9617939 epoch total loss 7.00063419\n",
      "Trained batch 1184 batch loss 7.00686836 epoch total loss 7.00063944\n",
      "Trained batch 1185 batch loss 6.75160789 epoch total loss 7.00042915\n",
      "Trained batch 1186 batch loss 6.46501589 epoch total loss 6.99997759\n",
      "Trained batch 1187 batch loss 6.66835976 epoch total loss 6.99969816\n",
      "Trained batch 1188 batch loss 6.70122623 epoch total loss 6.99944687\n",
      "Trained batch 1189 batch loss 7.06660509 epoch total loss 6.99950314\n",
      "Trained batch 1190 batch loss 6.78880596 epoch total loss 6.99932623\n",
      "Trained batch 1191 batch loss 7.00191069 epoch total loss 6.99932861\n",
      "Trained batch 1192 batch loss 7.15680027 epoch total loss 6.9994607\n",
      "Trained batch 1193 batch loss 7.32355309 epoch total loss 6.99973249\n",
      "Trained batch 1194 batch loss 7.10087299 epoch total loss 6.99981689\n",
      "Trained batch 1195 batch loss 6.83330679 epoch total loss 6.99967718\n",
      "Trained batch 1196 batch loss 7.18550444 epoch total loss 6.99983263\n",
      "Trained batch 1197 batch loss 6.28258896 epoch total loss 6.99923325\n",
      "Trained batch 1198 batch loss 7.13422632 epoch total loss 6.9993453\n",
      "Trained batch 1199 batch loss 7.3633213 epoch total loss 6.99964905\n",
      "Trained batch 1200 batch loss 7.21227884 epoch total loss 6.99982595\n",
      "Trained batch 1201 batch loss 7.09586334 epoch total loss 6.99990559\n",
      "Trained batch 1202 batch loss 7.07411051 epoch total loss 6.99996758\n",
      "Trained batch 1203 batch loss 6.93538857 epoch total loss 6.99991417\n",
      "Trained batch 1204 batch loss 6.90058327 epoch total loss 6.9998312\n",
      "Trained batch 1205 batch loss 7.15893555 epoch total loss 6.99996376\n",
      "Trained batch 1206 batch loss 6.94925833 epoch total loss 6.99992132\n",
      "Trained batch 1207 batch loss 6.85842514 epoch total loss 6.99980402\n",
      "Trained batch 1208 batch loss 7.15464735 epoch total loss 6.99993229\n",
      "Trained batch 1209 batch loss 7.0833683 epoch total loss 7.00000095\n",
      "Trained batch 1210 batch loss 7.40374804 epoch total loss 7.00033426\n",
      "Trained batch 1211 batch loss 7.26905727 epoch total loss 7.00055647\n",
      "Trained batch 1212 batch loss 6.78115702 epoch total loss 7.00037527\n",
      "Trained batch 1213 batch loss 6.78831768 epoch total loss 7.00020027\n",
      "Trained batch 1214 batch loss 6.90712643 epoch total loss 7.00012398\n",
      "Trained batch 1215 batch loss 7.08950853 epoch total loss 7.00019789\n",
      "Trained batch 1216 batch loss 7.18198967 epoch total loss 7.00034714\n",
      "Trained batch 1217 batch loss 7.03230524 epoch total loss 7.00037336\n",
      "Trained batch 1218 batch loss 6.72566 epoch total loss 7.00014734\n",
      "Trained batch 1219 batch loss 7.05086 epoch total loss 7.00018883\n",
      "Trained batch 1220 batch loss 7.05804491 epoch total loss 7.00023603\n",
      "Trained batch 1221 batch loss 6.71788073 epoch total loss 7.00000477\n",
      "Trained batch 1222 batch loss 6.77016449 epoch total loss 6.99981689\n",
      "Trained batch 1223 batch loss 6.83568764 epoch total loss 6.9996829\n",
      "Trained batch 1224 batch loss 7.05530071 epoch total loss 6.99972868\n",
      "Trained batch 1225 batch loss 7.26147842 epoch total loss 6.99994278\n",
      "Trained batch 1226 batch loss 7.30108404 epoch total loss 7.00018787\n",
      "Trained batch 1227 batch loss 7.32776594 epoch total loss 7.00045538\n",
      "Trained batch 1228 batch loss 7.43165731 epoch total loss 7.00080633\n",
      "Trained batch 1229 batch loss 7.45736647 epoch total loss 7.00117779\n",
      "Trained batch 1230 batch loss 7.32620144 epoch total loss 7.00144196\n",
      "Trained batch 1231 batch loss 7.19113541 epoch total loss 7.00159597\n",
      "Trained batch 1232 batch loss 7.04060316 epoch total loss 7.00162792\n",
      "Trained batch 1233 batch loss 7.38854456 epoch total loss 7.00194216\n",
      "Trained batch 1234 batch loss 7.32336092 epoch total loss 7.00220251\n",
      "Trained batch 1235 batch loss 7.38333511 epoch total loss 7.0025115\n",
      "Trained batch 1236 batch loss 7.00758696 epoch total loss 7.00251579\n",
      "Trained batch 1237 batch loss 6.97205734 epoch total loss 7.00249052\n",
      "Trained batch 1238 batch loss 7.15825748 epoch total loss 7.00261641\n",
      "Trained batch 1239 batch loss 7.12816477 epoch total loss 7.00271749\n",
      "Trained batch 1240 batch loss 7.29470825 epoch total loss 7.00295353\n",
      "Trained batch 1241 batch loss 7.41894197 epoch total loss 7.00328875\n",
      "Trained batch 1242 batch loss 7.40319538 epoch total loss 7.00361061\n",
      "Trained batch 1243 batch loss 7.16862106 epoch total loss 7.00374365\n",
      "Trained batch 1244 batch loss 7.10123158 epoch total loss 7.00382233\n",
      "Trained batch 1245 batch loss 7.13989544 epoch total loss 7.00393152\n",
      "Trained batch 1246 batch loss 7.4268465 epoch total loss 7.00427055\n",
      "Trained batch 1247 batch loss 7.3720932 epoch total loss 7.00456572\n",
      "Trained batch 1248 batch loss 6.98897076 epoch total loss 7.00455332\n",
      "Trained batch 1249 batch loss 6.9906044 epoch total loss 7.00454187\n",
      "Trained batch 1250 batch loss 6.95913029 epoch total loss 7.00450563\n",
      "Trained batch 1251 batch loss 6.67014408 epoch total loss 7.00423813\n",
      "Trained batch 1252 batch loss 6.37661505 epoch total loss 7.00373697\n",
      "Trained batch 1253 batch loss 7.2308321 epoch total loss 7.00391817\n",
      "Trained batch 1254 batch loss 7.17489 epoch total loss 7.00405407\n",
      "Trained batch 1255 batch loss 7.35697269 epoch total loss 7.00433588\n",
      "Trained batch 1256 batch loss 6.89628792 epoch total loss 7.00425\n",
      "Trained batch 1257 batch loss 6.77343416 epoch total loss 7.00406647\n",
      "Trained batch 1258 batch loss 6.246418 epoch total loss 7.00346375\n",
      "Trained batch 1259 batch loss 5.63544559 epoch total loss 7.00237751\n",
      "Trained batch 1260 batch loss 5.71629906 epoch total loss 7.00135612\n",
      "Trained batch 1261 batch loss 6.37217855 epoch total loss 7.00085735\n",
      "Trained batch 1262 batch loss 7.10875034 epoch total loss 7.00094271\n",
      "Trained batch 1263 batch loss 7.46518564 epoch total loss 7.00131\n",
      "Trained batch 1264 batch loss 6.71097088 epoch total loss 7.00108\n",
      "Trained batch 1265 batch loss 6.59379053 epoch total loss 7.00075817\n",
      "Trained batch 1266 batch loss 6.49096966 epoch total loss 7.00035572\n",
      "Trained batch 1267 batch loss 7.27108097 epoch total loss 7.00057\n",
      "Trained batch 1268 batch loss 7.34062958 epoch total loss 7.0008378\n",
      "Trained batch 1269 batch loss 7.42533779 epoch total loss 7.00117302\n",
      "Trained batch 1270 batch loss 7.40366077 epoch total loss 7.00148964\n",
      "Trained batch 1271 batch loss 7.34791946 epoch total loss 7.00176191\n",
      "Trained batch 1272 batch loss 7.24181461 epoch total loss 7.00195074\n",
      "Trained batch 1273 batch loss 7.07297468 epoch total loss 7.00200701\n",
      "Trained batch 1274 batch loss 7.19699478 epoch total loss 7.00216\n",
      "Trained batch 1275 batch loss 6.95427132 epoch total loss 7.0021224\n",
      "Trained batch 1276 batch loss 7.19148159 epoch total loss 7.0022707\n",
      "Trained batch 1277 batch loss 7.33805 epoch total loss 7.00253344\n",
      "Trained batch 1278 batch loss 7.12200069 epoch total loss 7.0026269\n",
      "Trained batch 1279 batch loss 5.80181265 epoch total loss 7.001688\n",
      "Trained batch 1280 batch loss 6.8146081 epoch total loss 7.00154209\n",
      "Trained batch 1281 batch loss 6.71240377 epoch total loss 7.00131655\n",
      "Trained batch 1282 batch loss 6.61954355 epoch total loss 7.00101852\n",
      "Trained batch 1283 batch loss 6.33627796 epoch total loss 7.0005\n",
      "Trained batch 1284 batch loss 6.67523098 epoch total loss 7.00024652\n",
      "Trained batch 1285 batch loss 6.88259888 epoch total loss 7.00015497\n",
      "Trained batch 1286 batch loss 7.09468889 epoch total loss 7.0002284\n",
      "Trained batch 1287 batch loss 7.18872929 epoch total loss 7.00037479\n",
      "Trained batch 1288 batch loss 7.31908846 epoch total loss 7.00062227\n",
      "Trained batch 1289 batch loss 7.29426765 epoch total loss 7.00085\n",
      "Trained batch 1290 batch loss 7.42777777 epoch total loss 7.00118113\n",
      "Trained batch 1291 batch loss 7.16150475 epoch total loss 7.00130463\n",
      "Trained batch 1292 batch loss 7.02963829 epoch total loss 7.00132656\n",
      "Trained batch 1293 batch loss 7.1275239 epoch total loss 7.00142431\n",
      "Trained batch 1294 batch loss 7.34107 epoch total loss 7.00168657\n",
      "Trained batch 1295 batch loss 7.16939 epoch total loss 7.0018158\n",
      "Trained batch 1296 batch loss 7.08410549 epoch total loss 7.00187922\n",
      "Trained batch 1297 batch loss 7.30745649 epoch total loss 7.00211477\n",
      "Trained batch 1298 batch loss 7.36193752 epoch total loss 7.00239229\n",
      "Trained batch 1299 batch loss 7.06598 epoch total loss 7.00244188\n",
      "Trained batch 1300 batch loss 7.13947487 epoch total loss 7.00254726\n",
      "Trained batch 1301 batch loss 7.25986862 epoch total loss 7.00274515\n",
      "Trained batch 1302 batch loss 6.9848218 epoch total loss 7.00273085\n",
      "Trained batch 1303 batch loss 7.13285255 epoch total loss 7.00283098\n",
      "Trained batch 1304 batch loss 6.81887102 epoch total loss 7.00269\n",
      "Trained batch 1305 batch loss 6.44769955 epoch total loss 7.0022645\n",
      "Trained batch 1306 batch loss 7.08892584 epoch total loss 7.00233078\n",
      "Trained batch 1307 batch loss 6.76943874 epoch total loss 7.00215244\n",
      "Trained batch 1308 batch loss 7.10507393 epoch total loss 7.0022316\n",
      "Trained batch 1309 batch loss 7.15270185 epoch total loss 7.00234652\n",
      "Trained batch 1310 batch loss 6.86136341 epoch total loss 7.00223875\n",
      "Trained batch 1311 batch loss 6.44726276 epoch total loss 7.00181532\n",
      "Trained batch 1312 batch loss 7.1461134 epoch total loss 7.00192547\n",
      "Trained batch 1313 batch loss 7.00698853 epoch total loss 7.00192928\n",
      "Trained batch 1314 batch loss 6.89321947 epoch total loss 7.00184679\n",
      "Trained batch 1315 batch loss 6.10764885 epoch total loss 7.00116682\n",
      "Trained batch 1316 batch loss 7.17210579 epoch total loss 7.00129652\n",
      "Trained batch 1317 batch loss 6.89985514 epoch total loss 7.00121927\n",
      "Trained batch 1318 batch loss 7.18778 epoch total loss 7.00136042\n",
      "Trained batch 1319 batch loss 6.70045567 epoch total loss 7.00113201\n",
      "Trained batch 1320 batch loss 6.75842 epoch total loss 7.00094843\n",
      "Trained batch 1321 batch loss 7.1203866 epoch total loss 7.00103855\n",
      "Trained batch 1322 batch loss 6.59055805 epoch total loss 7.00072813\n",
      "Trained batch 1323 batch loss 6.53651857 epoch total loss 7.00037718\n",
      "Trained batch 1324 batch loss 6.59818935 epoch total loss 7.00007391\n",
      "Trained batch 1325 batch loss 6.62276077 epoch total loss 6.99978924\n",
      "Trained batch 1326 batch loss 7.2513032 epoch total loss 6.99997854\n",
      "Trained batch 1327 batch loss 7.00835 epoch total loss 6.99998522\n",
      "Trained batch 1328 batch loss 6.60022163 epoch total loss 6.99968433\n",
      "Trained batch 1329 batch loss 6.53087282 epoch total loss 6.99933195\n",
      "Trained batch 1330 batch loss 7.02527332 epoch total loss 6.9993515\n",
      "Trained batch 1331 batch loss 6.89580202 epoch total loss 6.99927378\n",
      "Trained batch 1332 batch loss 7.20170546 epoch total loss 6.99942589\n",
      "Trained batch 1333 batch loss 7.2541 epoch total loss 6.99961662\n",
      "Trained batch 1334 batch loss 7.26697111 epoch total loss 6.99981689\n",
      "Trained batch 1335 batch loss 7.35023832 epoch total loss 7.00007963\n",
      "Trained batch 1336 batch loss 7.27980089 epoch total loss 7.00028944\n",
      "Trained batch 1337 batch loss 7.18114328 epoch total loss 7.00042439\n",
      "Trained batch 1338 batch loss 6.83909082 epoch total loss 7.00030375\n",
      "Trained batch 1339 batch loss 6.99684715 epoch total loss 7.00030136\n",
      "Trained batch 1340 batch loss 6.70397806 epoch total loss 7.00008\n",
      "Trained batch 1341 batch loss 6.92954826 epoch total loss 7.00002766\n",
      "Trained batch 1342 batch loss 7.25962 epoch total loss 7.00022125\n",
      "Trained batch 1343 batch loss 6.95459509 epoch total loss 7.0001874\n",
      "Trained batch 1344 batch loss 6.69017076 epoch total loss 6.99995708\n",
      "Trained batch 1345 batch loss 7.01585531 epoch total loss 6.99996901\n",
      "Trained batch 1346 batch loss 6.99165726 epoch total loss 6.99996233\n",
      "Trained batch 1347 batch loss 6.87207079 epoch total loss 6.99986744\n",
      "Trained batch 1348 batch loss 7.25830889 epoch total loss 7.0000596\n",
      "Trained batch 1349 batch loss 7.14591265 epoch total loss 7.00016737\n",
      "Trained batch 1350 batch loss 7.0778532 epoch total loss 7.00022507\n",
      "Trained batch 1351 batch loss 7.12815189 epoch total loss 7.00031948\n",
      "Trained batch 1352 batch loss 6.33178 epoch total loss 6.999825\n",
      "Trained batch 1353 batch loss 5.7158494 epoch total loss 6.99887609\n",
      "Trained batch 1354 batch loss 5.79922438 epoch total loss 6.99799\n",
      "Trained batch 1355 batch loss 5.34863567 epoch total loss 6.99677277\n",
      "Trained batch 1356 batch loss 6.90431547 epoch total loss 6.99670458\n",
      "Trained batch 1357 batch loss 7.28880405 epoch total loss 6.99692\n",
      "Trained batch 1358 batch loss 7.33628511 epoch total loss 6.99716949\n",
      "Trained batch 1359 batch loss 7.02197218 epoch total loss 6.99718761\n",
      "Trained batch 1360 batch loss 6.71165657 epoch total loss 6.99697781\n",
      "Trained batch 1361 batch loss 6.99206209 epoch total loss 6.99697399\n",
      "Trained batch 1362 batch loss 6.27482319 epoch total loss 6.99644375\n",
      "Trained batch 1363 batch loss 6.54469585 epoch total loss 6.99611235\n",
      "Trained batch 1364 batch loss 6.78271 epoch total loss 6.99595547\n",
      "Trained batch 1365 batch loss 6.57066727 epoch total loss 6.99564362\n",
      "Trained batch 1366 batch loss 6.66427 epoch total loss 6.99540091\n",
      "Trained batch 1367 batch loss 6.55597067 epoch total loss 6.99507952\n",
      "Trained batch 1368 batch loss 7.11685801 epoch total loss 6.99516869\n",
      "Trained batch 1369 batch loss 7.123703 epoch total loss 6.99526262\n",
      "Trained batch 1370 batch loss 7.19010735 epoch total loss 6.9954052\n",
      "Trained batch 1371 batch loss 7.35657692 epoch total loss 6.99566841\n",
      "Trained batch 1372 batch loss 7.40393 epoch total loss 6.99596643\n",
      "Trained batch 1373 batch loss 7.29502296 epoch total loss 6.99618387\n",
      "Trained batch 1374 batch loss 6.94480228 epoch total loss 6.9961462\n",
      "Trained batch 1375 batch loss 6.7939105 epoch total loss 6.99599934\n",
      "Trained batch 1376 batch loss 6.74474144 epoch total loss 6.99581718\n",
      "Trained batch 1377 batch loss 7.09474945 epoch total loss 6.99588871\n",
      "Trained batch 1378 batch loss 7.41586208 epoch total loss 6.99619389\n",
      "Trained batch 1379 batch loss 7.42585659 epoch total loss 6.99650526\n",
      "Trained batch 1380 batch loss 7.34363842 epoch total loss 6.99675703\n",
      "Trained batch 1381 batch loss 7.33205414 epoch total loss 6.99699974\n",
      "Trained batch 1382 batch loss 7.34605932 epoch total loss 6.99725199\n",
      "Trained batch 1383 batch loss 7.30388498 epoch total loss 6.99747372\n",
      "Trained batch 1384 batch loss 7.52026176 epoch total loss 6.99785137\n",
      "Trained batch 1385 batch loss 7.48259974 epoch total loss 6.99820137\n",
      "Trained batch 1386 batch loss 7.42788076 epoch total loss 6.99851131\n",
      "Trained batch 1387 batch loss 7.03153515 epoch total loss 6.99853468\n",
      "Trained batch 1388 batch loss 7.21836805 epoch total loss 6.99869347\n",
      "Epoch 5 train loss 6.998693466186523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 03:06:30.084082: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "2025-03-19 03:06:30.084143: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "\t [[RemoteCall]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 1 batch loss 6.88131523\n",
      "Validated batch 2 batch loss 6.48756361\n",
      "Validated batch 3 batch loss 7.13311434\n",
      "Validated batch 4 batch loss 7.112216\n",
      "Validated batch 5 batch loss 6.83808041\n",
      "Validated batch 6 batch loss 7.12859583\n",
      "Validated batch 7 batch loss 7.04813623\n",
      "Validated batch 8 batch loss 7.22015\n",
      "Validated batch 9 batch loss 6.95184469\n",
      "Validated batch 10 batch loss 7.08744192\n",
      "Validated batch 11 batch loss 6.74885082\n",
      "Validated batch 12 batch loss 6.96820116\n",
      "Validated batch 13 batch loss 6.98752499\n",
      "Validated batch 14 batch loss 7.31614065\n",
      "Validated batch 15 batch loss 7.1287303\n",
      "Validated batch 16 batch loss 6.9567194\n",
      "Validated batch 17 batch loss 7.25005198\n",
      "Validated batch 18 batch loss 6.49308872\n",
      "Validated batch 19 batch loss 7.30410051\n",
      "Validated batch 20 batch loss 7.41420937\n",
      "Validated batch 21 batch loss 7.15915346\n",
      "Validated batch 22 batch loss 7.31647158\n",
      "Validated batch 23 batch loss 6.8234663\n",
      "Validated batch 24 batch loss 7.24180841\n",
      "Validated batch 25 batch loss 7.11450291\n",
      "Validated batch 26 batch loss 6.91138697\n",
      "Validated batch 27 batch loss 6.75651\n",
      "Validated batch 28 batch loss 6.63630772\n",
      "Validated batch 29 batch loss 7.15586185\n",
      "Validated batch 30 batch loss 6.82037687\n",
      "Validated batch 31 batch loss 7.21248817\n",
      "Validated batch 32 batch loss 7.27227736\n",
      "Validated batch 33 batch loss 6.93678856\n",
      "Validated batch 34 batch loss 6.98295689\n",
      "Validated batch 35 batch loss 6.50631094\n",
      "Validated batch 36 batch loss 7.09459257\n",
      "Validated batch 37 batch loss 6.82374907\n",
      "Validated batch 38 batch loss 7.25660229\n",
      "Validated batch 39 batch loss 7.04826117\n",
      "Validated batch 40 batch loss 6.96264029\n",
      "Validated batch 41 batch loss 6.31651211\n",
      "Validated batch 42 batch loss 6.6598053\n",
      "Validated batch 43 batch loss 7.23936653\n",
      "Validated batch 44 batch loss 6.75232315\n",
      "Validated batch 45 batch loss 6.79011917\n",
      "Validated batch 46 batch loss 6.85663366\n",
      "Validated batch 47 batch loss 6.93191433\n",
      "Validated batch 48 batch loss 6.99460888\n",
      "Validated batch 49 batch loss 6.57716465\n",
      "Validated batch 50 batch loss 6.54104\n",
      "Validated batch 51 batch loss 6.67428493\n",
      "Validated batch 52 batch loss 7.04510307\n",
      "Validated batch 53 batch loss 6.87816715\n",
      "Validated batch 54 batch loss 7.07907295\n",
      "Validated batch 55 batch loss 7.27490139\n",
      "Validated batch 56 batch loss 7.25704241\n",
      "Validated batch 57 batch loss 7.17562485\n",
      "Validated batch 58 batch loss 6.64668703\n",
      "Validated batch 59 batch loss 7.09847736\n",
      "Validated batch 60 batch loss 7.03659058\n",
      "Validated batch 61 batch loss 7.16466331\n",
      "Validated batch 62 batch loss 7.36603069\n",
      "Validated batch 63 batch loss 6.8931818\n",
      "Validated batch 64 batch loss 7.31460476\n",
      "Validated batch 65 batch loss 7.21292782\n",
      "Validated batch 66 batch loss 7.16070318\n",
      "Validated batch 67 batch loss 7.0648737\n",
      "Validated batch 68 batch loss 7.22817898\n",
      "Validated batch 69 batch loss 7.32507563\n",
      "Validated batch 70 batch loss 7.49127388\n",
      "Validated batch 71 batch loss 7.05769491\n",
      "Validated batch 72 batch loss 7.15786\n",
      "Validated batch 73 batch loss 7.14835072\n",
      "Validated batch 74 batch loss 7.18783569\n",
      "Validated batch 75 batch loss 7.33199167\n",
      "Validated batch 76 batch loss 7.06549454\n",
      "Validated batch 77 batch loss 6.72400141\n",
      "Validated batch 78 batch loss 6.91976404\n",
      "Validated batch 79 batch loss 6.84299278\n",
      "Validated batch 80 batch loss 7.23966074\n",
      "Validated batch 81 batch loss 6.89516687\n",
      "Validated batch 82 batch loss 6.74619484\n",
      "Validated batch 83 batch loss 6.61051798\n",
      "Validated batch 84 batch loss 7.00954056\n",
      "Validated batch 85 batch loss 7.33762407\n",
      "Validated batch 86 batch loss 7.30181265\n",
      "Validated batch 87 batch loss 7.07441759\n",
      "Validated batch 88 batch loss 7.53041553\n",
      "Validated batch 89 batch loss 7.41784143\n",
      "Validated batch 90 batch loss 7.40970707\n",
      "Validated batch 91 batch loss 6.89358282\n",
      "Validated batch 92 batch loss 6.6897912\n",
      "Validated batch 93 batch loss 6.36864805\n",
      "Validated batch 94 batch loss 7.27583075\n",
      "Validated batch 95 batch loss 7.06976128\n",
      "Validated batch 96 batch loss 7.21786976\n",
      "Validated batch 97 batch loss 7.0182066\n",
      "Validated batch 98 batch loss 7.32732105\n",
      "Validated batch 99 batch loss 6.85352373\n",
      "Validated batch 100 batch loss 7.3875494\n",
      "Validated batch 101 batch loss 6.81130028\n",
      "Validated batch 102 batch loss 6.92420149\n",
      "Validated batch 103 batch loss 7.04543304\n",
      "Validated batch 104 batch loss 6.68895864\n",
      "Validated batch 105 batch loss 6.22972775\n",
      "Validated batch 106 batch loss 7.33465433\n",
      "Validated batch 107 batch loss 7.24279308\n",
      "Validated batch 108 batch loss 6.99786711\n",
      "Validated batch 109 batch loss 7.06145096\n",
      "Validated batch 110 batch loss 7.19562054\n",
      "Validated batch 111 batch loss 7.29132557\n",
      "Validated batch 112 batch loss 7.28877878\n",
      "Validated batch 113 batch loss 7.25993872\n",
      "Validated batch 114 batch loss 7.1294508\n",
      "Validated batch 115 batch loss 6.5150218\n",
      "Validated batch 116 batch loss 7.10501671\n",
      "Validated batch 117 batch loss 6.84428406\n",
      "Validated batch 118 batch loss 6.82939672\n",
      "Validated batch 119 batch loss 6.82790184\n",
      "Validated batch 120 batch loss 6.91023684\n",
      "Validated batch 121 batch loss 7.18553972\n",
      "Validated batch 122 batch loss 6.93100595\n",
      "Validated batch 123 batch loss 7.22225094\n",
      "Validated batch 124 batch loss 6.96463633\n",
      "Validated batch 125 batch loss 7.13206816\n",
      "Validated batch 126 batch loss 7.38939047\n",
      "Validated batch 127 batch loss 7.27601385\n",
      "Validated batch 128 batch loss 6.88892651\n",
      "Validated batch 129 batch loss 6.93156576\n",
      "Validated batch 130 batch loss 7.30103064\n",
      "Validated batch 131 batch loss 6.8422184\n",
      "Validated batch 132 batch loss 7.26632786\n",
      "Validated batch 133 batch loss 6.78636265\n",
      "Validated batch 134 batch loss 7.1979208\n",
      "Validated batch 135 batch loss 6.86026239\n",
      "Validated batch 136 batch loss 6.96717215\n",
      "Validated batch 137 batch loss 6.96296\n",
      "Validated batch 138 batch loss 7.07319164\n",
      "Validated batch 139 batch loss 6.9024744\n",
      "Validated batch 140 batch loss 7.08784294\n",
      "Validated batch 141 batch loss 7.08227491\n",
      "Validated batch 142 batch loss 6.95257378\n",
      "Validated batch 143 batch loss 7.41011381\n",
      "Validated batch 144 batch loss 7.28279305\n",
      "Validated batch 145 batch loss 7.30647802\n",
      "Validated batch 146 batch loss 6.99749422\n",
      "Validated batch 147 batch loss 7.28875732\n",
      "Validated batch 148 batch loss 7.25993347\n",
      "Validated batch 149 batch loss 7.44827461\n",
      "Validated batch 150 batch loss 7.41956282\n",
      "Validated batch 151 batch loss 6.52027559\n",
      "Validated batch 152 batch loss 7.27201223\n",
      "Validated batch 153 batch loss 7.15497112\n",
      "Validated batch 154 batch loss 6.89511919\n",
      "Validated batch 155 batch loss 7.13067389\n",
      "Validated batch 156 batch loss 6.57555\n",
      "Validated batch 157 batch loss 6.46600628\n",
      "Validated batch 158 batch loss 7.36615896\n",
      "Validated batch 159 batch loss 7.05410194\n",
      "Validated batch 160 batch loss 7.2984128\n",
      "Validated batch 161 batch loss 7.05130863\n",
      "Validated batch 162 batch loss 7.22510815\n",
      "Validated batch 163 batch loss 7.38762474\n",
      "Validated batch 164 batch loss 7.0543313\n",
      "Validated batch 165 batch loss 6.39710808\n",
      "Validated batch 166 batch loss 6.92184067\n",
      "Validated batch 167 batch loss 6.75591135\n",
      "Validated batch 168 batch loss 6.65825796\n",
      "Validated batch 169 batch loss 6.75873089\n",
      "Validated batch 170 batch loss 6.43566704\n",
      "Validated batch 171 batch loss 7.22856617\n",
      "Validated batch 172 batch loss 7.0217042\n",
      "Validated batch 173 batch loss 6.5237751\n",
      "Validated batch 174 batch loss 6.42194223\n",
      "Validated batch 175 batch loss 7.25434542\n",
      "Validated batch 176 batch loss 6.72529411\n",
      "Validated batch 177 batch loss 7.11606073\n",
      "Validated batch 178 batch loss 6.84587765\n",
      "Validated batch 179 batch loss 7.36046171\n",
      "Validated batch 180 batch loss 6.88740396\n",
      "Validated batch 181 batch loss 6.93869352\n",
      "Validated batch 182 batch loss 7.08588171\n",
      "Validated batch 183 batch loss 6.26779318\n",
      "Validated batch 184 batch loss 6.93945885\n",
      "Validated batch 185 batch loss 3.66968083\n",
      "Epoch 5 val loss 6.997101306915283\n",
      "Start epoch 6 with learning rate 0.001\n",
      "Start distributed training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 03:06:38.567877: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "2025-03-19 03:06:38.567925: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "\t [[RemoteCall]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1 batch loss 7.03334713 epoch total loss 7.03334713\n",
      "Trained batch 2 batch loss 7.1148138 epoch total loss 7.07408047\n",
      "Trained batch 3 batch loss 6.9783864 epoch total loss 7.04218245\n",
      "Trained batch 4 batch loss 7.23069286 epoch total loss 7.08930969\n",
      "Trained batch 5 batch loss 7.11393356 epoch total loss 7.09423447\n",
      "Trained batch 6 batch loss 6.73253345 epoch total loss 7.03395081\n",
      "Trained batch 7 batch loss 7.18942356 epoch total loss 7.05616093\n",
      "Trained batch 8 batch loss 5.76908731 epoch total loss 6.89527702\n",
      "Trained batch 9 batch loss 5.99309587 epoch total loss 6.79503441\n",
      "Trained batch 10 batch loss 5.71065474 epoch total loss 6.68659687\n",
      "Trained batch 11 batch loss 6.68308973 epoch total loss 6.68627787\n",
      "Trained batch 12 batch loss 6.44563675 epoch total loss 6.666224\n",
      "Trained batch 13 batch loss 7.38916731 epoch total loss 6.72183514\n",
      "Trained batch 14 batch loss 6.88072824 epoch total loss 6.73318481\n",
      "Trained batch 15 batch loss 6.61623287 epoch total loss 6.72538805\n",
      "Trained batch 16 batch loss 7.26284504 epoch total loss 6.75897932\n",
      "Trained batch 17 batch loss 6.78273296 epoch total loss 6.76037645\n",
      "Trained batch 18 batch loss 6.08876944 epoch total loss 6.7230649\n",
      "Trained batch 19 batch loss 6.6860075 epoch total loss 6.72111416\n",
      "Trained batch 20 batch loss 6.76943588 epoch total loss 6.72353077\n",
      "Trained batch 21 batch loss 6.54555607 epoch total loss 6.71505594\n",
      "Trained batch 22 batch loss 7.07808447 epoch total loss 6.73155689\n",
      "Trained batch 23 batch loss 7.35043144 epoch total loss 6.75846481\n",
      "Trained batch 24 batch loss 6.57516336 epoch total loss 6.75082731\n",
      "Trained batch 25 batch loss 7.03353453 epoch total loss 6.76213551\n",
      "Trained batch 26 batch loss 6.70810032 epoch total loss 6.76005745\n",
      "Trained batch 27 batch loss 6.28703117 epoch total loss 6.74253798\n",
      "Trained batch 28 batch loss 6.84282923 epoch total loss 6.74612\n",
      "Trained batch 29 batch loss 6.81300354 epoch total loss 6.74842644\n",
      "Trained batch 30 batch loss 6.91422796 epoch total loss 6.75395298\n",
      "Trained batch 31 batch loss 6.57174349 epoch total loss 6.74807549\n",
      "Trained batch 32 batch loss 6.18165779 epoch total loss 6.73037481\n",
      "Trained batch 33 batch loss 6.54316902 epoch total loss 6.72470188\n",
      "Trained batch 34 batch loss 6.11910439 epoch total loss 6.70689\n",
      "Trained batch 35 batch loss 6.60579109 epoch total loss 6.7040019\n",
      "Trained batch 36 batch loss 7.09474707 epoch total loss 6.71485567\n",
      "Trained batch 37 batch loss 6.90928125 epoch total loss 6.72011042\n",
      "Trained batch 38 batch loss 7.0195365 epoch total loss 6.72799\n",
      "Trained batch 39 batch loss 7.41193962 epoch total loss 6.74552727\n",
      "Trained batch 40 batch loss 7.24540329 epoch total loss 6.75802374\n",
      "Trained batch 41 batch loss 7.393013 epoch total loss 6.77351141\n",
      "Trained batch 42 batch loss 7.43877172 epoch total loss 6.78935099\n",
      "Trained batch 43 batch loss 7.18087101 epoch total loss 6.79845619\n",
      "Trained batch 44 batch loss 7.32069 epoch total loss 6.81032515\n",
      "Trained batch 45 batch loss 7.18929863 epoch total loss 6.81874657\n",
      "Trained batch 46 batch loss 7.08472729 epoch total loss 6.82452869\n",
      "Trained batch 47 batch loss 7.2660675 epoch total loss 6.83392334\n",
      "Trained batch 48 batch loss 7.12868166 epoch total loss 6.84006453\n",
      "Trained batch 49 batch loss 7.18094635 epoch total loss 6.8470211\n",
      "Trained batch 50 batch loss 6.97338629 epoch total loss 6.84954834\n",
      "Trained batch 51 batch loss 7.21686459 epoch total loss 6.85675049\n",
      "Trained batch 52 batch loss 7.27680874 epoch total loss 6.86482811\n",
      "Trained batch 53 batch loss 7.17705345 epoch total loss 6.87071943\n",
      "Trained batch 54 batch loss 7.46992493 epoch total loss 6.88181543\n",
      "Trained batch 55 batch loss 7.37541866 epoch total loss 6.89079046\n",
      "Trained batch 56 batch loss 7.40512466 epoch total loss 6.89997482\n",
      "Trained batch 57 batch loss 7.52746964 epoch total loss 6.91098356\n",
      "Trained batch 58 batch loss 7.26330471 epoch total loss 6.91705799\n",
      "Trained batch 59 batch loss 7.43026686 epoch total loss 6.92575645\n",
      "Trained batch 60 batch loss 7.22071648 epoch total loss 6.93067217\n",
      "Trained batch 61 batch loss 7.310009 epoch total loss 6.9368906\n",
      "Trained batch 62 batch loss 7.13409948 epoch total loss 6.94007158\n",
      "Trained batch 63 batch loss 7.17506266 epoch total loss 6.94380093\n",
      "Trained batch 64 batch loss 7.0808053 epoch total loss 6.94594193\n",
      "Trained batch 65 batch loss 7.16063166 epoch total loss 6.94924498\n",
      "Trained batch 66 batch loss 7.17450285 epoch total loss 6.95265818\n",
      "Trained batch 67 batch loss 7.38394499 epoch total loss 6.959095\n",
      "Trained batch 68 batch loss 7.23378563 epoch total loss 6.96313477\n",
      "Trained batch 69 batch loss 6.94150782 epoch total loss 6.96282101\n",
      "Trained batch 70 batch loss 7.29312515 epoch total loss 6.96754\n",
      "Trained batch 71 batch loss 6.96118164 epoch total loss 6.96745\n",
      "Trained batch 72 batch loss 7.00616646 epoch total loss 6.96798801\n",
      "Trained batch 73 batch loss 7.29828739 epoch total loss 6.97251225\n",
      "Trained batch 74 batch loss 7.12780142 epoch total loss 6.97461081\n",
      "Trained batch 75 batch loss 7.35484552 epoch total loss 6.97968102\n",
      "Trained batch 76 batch loss 7.21738958 epoch total loss 6.98280907\n",
      "Trained batch 77 batch loss 7.33901 epoch total loss 6.98743486\n",
      "Trained batch 78 batch loss 7.06972 epoch total loss 6.98848963\n",
      "Trained batch 79 batch loss 7.42124224 epoch total loss 6.99396753\n",
      "Trained batch 80 batch loss 7.13144112 epoch total loss 6.99568653\n",
      "Trained batch 81 batch loss 7.08714104 epoch total loss 6.99681568\n",
      "Trained batch 82 batch loss 6.8625617 epoch total loss 6.99517822\n",
      "Trained batch 83 batch loss 6.67938614 epoch total loss 6.99137354\n",
      "Trained batch 84 batch loss 6.76593 epoch total loss 6.98868942\n",
      "Trained batch 85 batch loss 7.01912642 epoch total loss 6.98904753\n",
      "Trained batch 86 batch loss 6.20084238 epoch total loss 6.97988272\n",
      "Trained batch 87 batch loss 6.51471 epoch total loss 6.97453594\n",
      "Trained batch 88 batch loss 6.83593464 epoch total loss 6.97296095\n",
      "Trained batch 89 batch loss 6.41518354 epoch total loss 6.9666934\n",
      "Trained batch 90 batch loss 6.83061409 epoch total loss 6.96518135\n",
      "Trained batch 91 batch loss 7.13065624 epoch total loss 6.967\n",
      "Trained batch 92 batch loss 6.84633684 epoch total loss 6.96568823\n",
      "Trained batch 93 batch loss 7.04093647 epoch total loss 6.96649742\n",
      "Trained batch 94 batch loss 7.05219555 epoch total loss 6.96740913\n",
      "Trained batch 95 batch loss 7.05843067 epoch total loss 6.9683671\n",
      "Trained batch 96 batch loss 7.20948648 epoch total loss 6.9708786\n",
      "Trained batch 97 batch loss 7.28741503 epoch total loss 6.97414207\n",
      "Trained batch 98 batch loss 6.96080303 epoch total loss 6.9740057\n",
      "Trained batch 99 batch loss 6.70496178 epoch total loss 6.9712882\n",
      "Trained batch 100 batch loss 6.86155653 epoch total loss 6.970191\n",
      "Trained batch 101 batch loss 6.72755432 epoch total loss 6.9677887\n",
      "Trained batch 102 batch loss 6.86094856 epoch total loss 6.96674109\n",
      "Trained batch 103 batch loss 6.65024948 epoch total loss 6.96366882\n",
      "Trained batch 104 batch loss 6.66906118 epoch total loss 6.96083593\n",
      "Trained batch 105 batch loss 6.99830246 epoch total loss 6.96119261\n",
      "Trained batch 106 batch loss 7.09186697 epoch total loss 6.96242523\n",
      "Trained batch 107 batch loss 7.00261211 epoch total loss 6.96280098\n",
      "Trained batch 108 batch loss 6.95084906 epoch total loss 6.96269035\n",
      "Trained batch 109 batch loss 7.00041819 epoch total loss 6.96303654\n",
      "Trained batch 110 batch loss 6.2010088 epoch total loss 6.95610905\n",
      "Trained batch 111 batch loss 6.71298695 epoch total loss 6.95391893\n",
      "Trained batch 112 batch loss 6.91534567 epoch total loss 6.95357466\n",
      "Trained batch 113 batch loss 6.66739845 epoch total loss 6.95104218\n",
      "Trained batch 114 batch loss 6.97579288 epoch total loss 6.95125914\n",
      "Trained batch 115 batch loss 6.928267 epoch total loss 6.95105934\n",
      "Trained batch 116 batch loss 7.15690851 epoch total loss 6.95283413\n",
      "Trained batch 117 batch loss 7.05213 epoch total loss 6.9536829\n",
      "Trained batch 118 batch loss 6.29437447 epoch total loss 6.94809532\n",
      "Trained batch 119 batch loss 5.99741507 epoch total loss 6.94010639\n",
      "Trained batch 120 batch loss 6.18316698 epoch total loss 6.93379879\n",
      "Trained batch 121 batch loss 6.05754757 epoch total loss 6.92655706\n",
      "Trained batch 122 batch loss 6.89910126 epoch total loss 6.926332\n",
      "Trained batch 123 batch loss 6.88563538 epoch total loss 6.92600107\n",
      "Trained batch 124 batch loss 6.98732519 epoch total loss 6.92649555\n",
      "Trained batch 125 batch loss 6.98248959 epoch total loss 6.9269433\n",
      "Trained batch 126 batch loss 6.82430172 epoch total loss 6.92612839\n",
      "Trained batch 127 batch loss 6.88100386 epoch total loss 6.92577314\n",
      "Trained batch 128 batch loss 6.95739841 epoch total loss 6.92602\n",
      "Trained batch 129 batch loss 7.33208466 epoch total loss 6.92916822\n",
      "Trained batch 130 batch loss 7.35433197 epoch total loss 6.93243837\n",
      "Trained batch 131 batch loss 6.74409199 epoch total loss 6.931\n",
      "Trained batch 132 batch loss 6.9943471 epoch total loss 6.93148041\n",
      "Trained batch 133 batch loss 6.82877064 epoch total loss 6.93070793\n",
      "Trained batch 134 batch loss 6.88356209 epoch total loss 6.93035603\n",
      "Trained batch 135 batch loss 6.58614683 epoch total loss 6.92780638\n",
      "Trained batch 136 batch loss 6.72127771 epoch total loss 6.92628765\n",
      "Trained batch 137 batch loss 6.44697666 epoch total loss 6.92278862\n",
      "Trained batch 138 batch loss 7.32906199 epoch total loss 6.92573261\n",
      "Trained batch 139 batch loss 6.87494659 epoch total loss 6.92536736\n",
      "Trained batch 140 batch loss 7.08437824 epoch total loss 6.9265027\n",
      "Trained batch 141 batch loss 6.90471935 epoch total loss 6.92634821\n",
      "Trained batch 142 batch loss 6.34459114 epoch total loss 6.9222517\n",
      "Trained batch 143 batch loss 7.15222502 epoch total loss 6.9238596\n",
      "Trained batch 144 batch loss 6.97650528 epoch total loss 6.92422533\n",
      "Trained batch 145 batch loss 7.36803102 epoch total loss 6.92728615\n",
      "Trained batch 146 batch loss 7.3400588 epoch total loss 6.93011332\n",
      "Trained batch 147 batch loss 7.19175386 epoch total loss 6.93189335\n",
      "Trained batch 148 batch loss 6.9906745 epoch total loss 6.93229055\n",
      "Trained batch 149 batch loss 6.79743576 epoch total loss 6.93138599\n",
      "Trained batch 150 batch loss 6.73867846 epoch total loss 6.93010092\n",
      "Trained batch 151 batch loss 6.88797569 epoch total loss 6.92982149\n",
      "Trained batch 152 batch loss 5.92804098 epoch total loss 6.9232316\n",
      "Trained batch 153 batch loss 6.45976782 epoch total loss 6.92020178\n",
      "Trained batch 154 batch loss 6.9341259 epoch total loss 6.9202919\n",
      "Trained batch 155 batch loss 7.33398199 epoch total loss 6.92296124\n",
      "Trained batch 156 batch loss 7.14031315 epoch total loss 6.92435408\n",
      "Trained batch 157 batch loss 6.94858122 epoch total loss 6.92450857\n",
      "Trained batch 158 batch loss 7.11221123 epoch total loss 6.92569637\n",
      "Trained batch 159 batch loss 6.8718996 epoch total loss 6.9253583\n",
      "Trained batch 160 batch loss 6.91588 epoch total loss 6.92529917\n",
      "Trained batch 161 batch loss 7.32525396 epoch total loss 6.92778301\n",
      "Trained batch 162 batch loss 7.02828169 epoch total loss 6.92840338\n",
      "Trained batch 163 batch loss 7.18134451 epoch total loss 6.92995548\n",
      "Trained batch 164 batch loss 7.27882481 epoch total loss 6.93208265\n",
      "Trained batch 165 batch loss 6.47689724 epoch total loss 6.92932415\n",
      "Trained batch 166 batch loss 6.43362808 epoch total loss 6.92633772\n",
      "Trained batch 167 batch loss 6.59429169 epoch total loss 6.92434931\n",
      "Trained batch 168 batch loss 6.04928446 epoch total loss 6.91914082\n",
      "Trained batch 169 batch loss 6.5971036 epoch total loss 6.9172349\n",
      "Trained batch 170 batch loss 7.01301098 epoch total loss 6.91779852\n",
      "Trained batch 171 batch loss 6.9087472 epoch total loss 6.91774511\n",
      "Trained batch 172 batch loss 7.33694029 epoch total loss 6.92018223\n",
      "Trained batch 173 batch loss 7.09363 epoch total loss 6.92118502\n",
      "Trained batch 174 batch loss 7.10360718 epoch total loss 6.92223358\n",
      "Trained batch 175 batch loss 7.23831511 epoch total loss 6.92403936\n",
      "Trained batch 176 batch loss 7.18704891 epoch total loss 6.92553377\n",
      "Trained batch 177 batch loss 7.46147108 epoch total loss 6.92856121\n",
      "Trained batch 178 batch loss 7.42107582 epoch total loss 6.93132782\n",
      "Trained batch 179 batch loss 7.33986378 epoch total loss 6.93361\n",
      "Trained batch 180 batch loss 7.32538795 epoch total loss 6.93578672\n",
      "Trained batch 181 batch loss 7.38505125 epoch total loss 6.93826866\n",
      "Trained batch 182 batch loss 7.39282513 epoch total loss 6.94076633\n",
      "Trained batch 183 batch loss 7.42196178 epoch total loss 6.94339609\n",
      "Trained batch 184 batch loss 7.29889297 epoch total loss 6.94532824\n",
      "Trained batch 185 batch loss 7.32742 epoch total loss 6.94739342\n",
      "Trained batch 186 batch loss 7.33796501 epoch total loss 6.94949389\n",
      "Trained batch 187 batch loss 7.15964079 epoch total loss 6.95061779\n",
      "Trained batch 188 batch loss 7.26844931 epoch total loss 6.95230818\n",
      "Trained batch 189 batch loss 7.09793854 epoch total loss 6.95307827\n",
      "Trained batch 190 batch loss 7.05135059 epoch total loss 6.95359612\n",
      "Trained batch 191 batch loss 7.29804182 epoch total loss 6.95539951\n",
      "Trained batch 192 batch loss 7.28687382 epoch total loss 6.95712614\n",
      "Trained batch 193 batch loss 6.68711329 epoch total loss 6.9557271\n",
      "Trained batch 194 batch loss 6.85109425 epoch total loss 6.9551878\n",
      "Trained batch 195 batch loss 7.05798626 epoch total loss 6.9557147\n",
      "Trained batch 196 batch loss 7.26779222 epoch total loss 6.95730734\n",
      "Trained batch 197 batch loss 7.26903 epoch total loss 6.95888948\n",
      "Trained batch 198 batch loss 7.0860815 epoch total loss 6.95953178\n",
      "Trained batch 199 batch loss 6.20590115 epoch total loss 6.95574474\n",
      "Trained batch 200 batch loss 6.15068722 epoch total loss 6.95171928\n",
      "Trained batch 201 batch loss 6.7188406 epoch total loss 6.95056105\n",
      "Trained batch 202 batch loss 6.63568 epoch total loss 6.94900179\n",
      "Trained batch 203 batch loss 6.86921215 epoch total loss 6.94860888\n",
      "Trained batch 204 batch loss 7.0418582 epoch total loss 6.94906616\n",
      "Trained batch 205 batch loss 7.30313349 epoch total loss 6.95079327\n",
      "Trained batch 206 batch loss 6.82144499 epoch total loss 6.95016527\n",
      "Trained batch 207 batch loss 7.35718966 epoch total loss 6.95213127\n",
      "Trained batch 208 batch loss 7.32650709 epoch total loss 6.95393133\n",
      "Trained batch 209 batch loss 7.24077797 epoch total loss 6.95530367\n",
      "Trained batch 210 batch loss 6.73992443 epoch total loss 6.95427752\n",
      "Trained batch 211 batch loss 6.87844563 epoch total loss 6.95391798\n",
      "Trained batch 212 batch loss 7.09170055 epoch total loss 6.95456791\n",
      "Trained batch 213 batch loss 6.8312192 epoch total loss 6.95398855\n",
      "Trained batch 214 batch loss 6.93438196 epoch total loss 6.953897\n",
      "Trained batch 215 batch loss 6.62159157 epoch total loss 6.95235109\n",
      "Trained batch 216 batch loss 6.52698898 epoch total loss 6.95038176\n",
      "Trained batch 217 batch loss 6.94457483 epoch total loss 6.95035505\n",
      "Trained batch 218 batch loss 7.3478961 epoch total loss 6.95217848\n",
      "Trained batch 219 batch loss 7.05964756 epoch total loss 6.95266962\n",
      "Trained batch 220 batch loss 7.10128832 epoch total loss 6.9533453\n",
      "Trained batch 221 batch loss 7.02770233 epoch total loss 6.95368195\n",
      "Trained batch 222 batch loss 7.2624073 epoch total loss 6.9550724\n",
      "Trained batch 223 batch loss 7.10416222 epoch total loss 6.95574093\n",
      "Trained batch 224 batch loss 7.05811691 epoch total loss 6.95619822\n",
      "Trained batch 225 batch loss 6.60442829 epoch total loss 6.95463419\n",
      "Trained batch 226 batch loss 7.19491053 epoch total loss 6.95569754\n",
      "Trained batch 227 batch loss 6.8889904 epoch total loss 6.9554038\n",
      "Trained batch 228 batch loss 7.18957663 epoch total loss 6.95643091\n",
      "Trained batch 229 batch loss 7.45981216 epoch total loss 6.95862913\n",
      "Trained batch 230 batch loss 7.43029356 epoch total loss 6.96068\n",
      "Trained batch 231 batch loss 7.17680788 epoch total loss 6.96161556\n",
      "Trained batch 232 batch loss 7.21691036 epoch total loss 6.9627161\n",
      "Trained batch 233 batch loss 7.27205753 epoch total loss 6.96404362\n",
      "Trained batch 234 batch loss 7.28535175 epoch total loss 6.96541691\n",
      "Trained batch 235 batch loss 6.90717888 epoch total loss 6.96516943\n",
      "Trained batch 236 batch loss 6.36966181 epoch total loss 6.96264601\n",
      "Trained batch 237 batch loss 6.71792698 epoch total loss 6.96161318\n",
      "Trained batch 238 batch loss 6.76431704 epoch total loss 6.96078396\n",
      "Trained batch 239 batch loss 7.06860733 epoch total loss 6.96123505\n",
      "Trained batch 240 batch loss 6.90944242 epoch total loss 6.96101952\n",
      "Trained batch 241 batch loss 7.14112806 epoch total loss 6.96176672\n",
      "Trained batch 242 batch loss 7.04950905 epoch total loss 6.96212959\n",
      "Trained batch 243 batch loss 7.43035507 epoch total loss 6.96405602\n",
      "Trained batch 244 batch loss 7.20894146 epoch total loss 6.96505976\n",
      "Trained batch 245 batch loss 7.39160919 epoch total loss 6.96680069\n",
      "Trained batch 246 batch loss 6.88204336 epoch total loss 6.96645641\n",
      "Trained batch 247 batch loss 6.89197254 epoch total loss 6.96615505\n",
      "Trained batch 248 batch loss 6.8354907 epoch total loss 6.96562767\n",
      "Trained batch 249 batch loss 6.96374941 epoch total loss 6.96562052\n",
      "Trained batch 250 batch loss 7.22973871 epoch total loss 6.96667671\n",
      "Trained batch 251 batch loss 7.31214762 epoch total loss 6.96805286\n",
      "Trained batch 252 batch loss 7.36142302 epoch total loss 6.96961403\n",
      "Trained batch 253 batch loss 7.43224144 epoch total loss 6.9714427\n",
      "Trained batch 254 batch loss 7.34055328 epoch total loss 6.9728961\n",
      "Trained batch 255 batch loss 7.37239027 epoch total loss 6.97446299\n",
      "Trained batch 256 batch loss 7.35048628 epoch total loss 6.97593164\n",
      "Trained batch 257 batch loss 7.4791069 epoch total loss 6.97788954\n",
      "Trained batch 258 batch loss 7.49366903 epoch total loss 6.97988892\n",
      "Trained batch 259 batch loss 7.19261551 epoch total loss 6.98071\n",
      "Trained batch 260 batch loss 7.17292833 epoch total loss 6.9814496\n",
      "Trained batch 261 batch loss 6.90839911 epoch total loss 6.9811697\n",
      "Trained batch 262 batch loss 6.96228 epoch total loss 6.9810977\n",
      "Trained batch 263 batch loss 6.87084389 epoch total loss 6.98067856\n",
      "Trained batch 264 batch loss 6.79641056 epoch total loss 6.97998047\n",
      "Trained batch 265 batch loss 7.02360487 epoch total loss 6.98014498\n",
      "Trained batch 266 batch loss 7.26560116 epoch total loss 6.98121834\n",
      "Trained batch 267 batch loss 7.4140625 epoch total loss 6.98283911\n",
      "Trained batch 268 batch loss 7.17323399 epoch total loss 6.98354959\n",
      "Trained batch 269 batch loss 7.04819822 epoch total loss 6.98379\n",
      "Trained batch 270 batch loss 6.62328386 epoch total loss 6.98245478\n",
      "Trained batch 271 batch loss 7.05758572 epoch total loss 6.9827323\n",
      "Trained batch 272 batch loss 6.92095423 epoch total loss 6.98250484\n",
      "Trained batch 273 batch loss 6.85193062 epoch total loss 6.98202658\n",
      "Trained batch 274 batch loss 7.18443775 epoch total loss 6.9827652\n",
      "Trained batch 275 batch loss 7.1715045 epoch total loss 6.98345184\n",
      "Trained batch 276 batch loss 7.27264452 epoch total loss 6.9845\n",
      "Trained batch 277 batch loss 7.07949781 epoch total loss 6.98484278\n",
      "Trained batch 278 batch loss 7.35931253 epoch total loss 6.98618937\n",
      "Trained batch 279 batch loss 7.00091887 epoch total loss 6.98624229\n",
      "Trained batch 280 batch loss 6.56897116 epoch total loss 6.98475218\n",
      "Trained batch 281 batch loss 7.08359528 epoch total loss 6.98510408\n",
      "Trained batch 282 batch loss 6.75913715 epoch total loss 6.98430252\n",
      "Trained batch 283 batch loss 6.74202442 epoch total loss 6.9834466\n",
      "Trained batch 284 batch loss 6.97575521 epoch total loss 6.98341942\n",
      "Trained batch 285 batch loss 6.36014509 epoch total loss 6.98123264\n",
      "Trained batch 286 batch loss 6.26711464 epoch total loss 6.97873545\n",
      "Trained batch 287 batch loss 6.38349676 epoch total loss 6.97666168\n",
      "Trained batch 288 batch loss 7.23357105 epoch total loss 6.97755337\n",
      "Trained batch 289 batch loss 7.3325038 epoch total loss 6.9787817\n",
      "Trained batch 290 batch loss 7.30982399 epoch total loss 6.97992325\n",
      "Trained batch 291 batch loss 7.41840172 epoch total loss 6.98143\n",
      "Trained batch 292 batch loss 7.42092085 epoch total loss 6.98293543\n",
      "Trained batch 293 batch loss 7.43000221 epoch total loss 6.98446131\n",
      "Trained batch 294 batch loss 7.23014975 epoch total loss 6.98529673\n",
      "Trained batch 295 batch loss 6.89172649 epoch total loss 6.98498\n",
      "Trained batch 296 batch loss 7.13683271 epoch total loss 6.98549271\n",
      "Trained batch 297 batch loss 7.06359673 epoch total loss 6.98575497\n",
      "Trained batch 298 batch loss 7.14057684 epoch total loss 6.98627472\n",
      "Trained batch 299 batch loss 7.41126776 epoch total loss 6.98769665\n",
      "Trained batch 300 batch loss 7.29353428 epoch total loss 6.98871565\n",
      "Trained batch 301 batch loss 7.11050892 epoch total loss 6.98912096\n",
      "Trained batch 302 batch loss 7.31301689 epoch total loss 6.99019337\n",
      "Trained batch 303 batch loss 7.19548225 epoch total loss 6.99087095\n",
      "Trained batch 304 batch loss 7.23931456 epoch total loss 6.99168777\n",
      "Trained batch 305 batch loss 7.02613115 epoch total loss 6.99180079\n",
      "Trained batch 306 batch loss 6.94561148 epoch total loss 6.99164963\n",
      "Trained batch 307 batch loss 6.56936741 epoch total loss 6.99027395\n",
      "Trained batch 308 batch loss 7.11742258 epoch total loss 6.99068689\n",
      "Trained batch 309 batch loss 7.38074303 epoch total loss 6.99194956\n",
      "Trained batch 310 batch loss 7.21411419 epoch total loss 6.99266624\n",
      "Trained batch 311 batch loss 7.13690662 epoch total loss 6.99313\n",
      "Trained batch 312 batch loss 7.0819521 epoch total loss 6.99341536\n",
      "Trained batch 313 batch loss 7.22229958 epoch total loss 6.99414682\n",
      "Trained batch 314 batch loss 6.98178482 epoch total loss 6.99410725\n",
      "Trained batch 315 batch loss 6.76365089 epoch total loss 6.99337578\n",
      "Trained batch 316 batch loss 6.24833107 epoch total loss 6.99101782\n",
      "Trained batch 317 batch loss 6.30239916 epoch total loss 6.98884583\n",
      "Trained batch 318 batch loss 6.73314381 epoch total loss 6.98804188\n",
      "Trained batch 319 batch loss 6.60989714 epoch total loss 6.98685598\n",
      "Trained batch 320 batch loss 6.69641781 epoch total loss 6.98594904\n",
      "Trained batch 321 batch loss 7.08714485 epoch total loss 6.98626423\n",
      "Trained batch 322 batch loss 7.01643705 epoch total loss 6.98635769\n",
      "Trained batch 323 batch loss 6.54640102 epoch total loss 6.98499537\n",
      "Trained batch 324 batch loss 6.4318924 epoch total loss 6.98328829\n",
      "Trained batch 325 batch loss 7.08714104 epoch total loss 6.98360825\n",
      "Trained batch 326 batch loss 6.88934469 epoch total loss 6.98331928\n",
      "Trained batch 327 batch loss 7.1773057 epoch total loss 6.98391199\n",
      "Trained batch 328 batch loss 7.02908564 epoch total loss 6.98405\n",
      "Trained batch 329 batch loss 7.21097183 epoch total loss 6.9847393\n",
      "Trained batch 330 batch loss 7.09798145 epoch total loss 6.98508215\n",
      "Trained batch 331 batch loss 7.16111517 epoch total loss 6.9856143\n",
      "Trained batch 332 batch loss 7.42396 epoch total loss 6.98693466\n",
      "Trained batch 333 batch loss 7.20081663 epoch total loss 6.98757744\n",
      "Trained batch 334 batch loss 7.26639938 epoch total loss 6.9884119\n",
      "Trained batch 335 batch loss 7.37201166 epoch total loss 6.98955727\n",
      "Trained batch 336 batch loss 7.20766449 epoch total loss 6.99020672\n",
      "Trained batch 337 batch loss 6.41757536 epoch total loss 6.98850727\n",
      "Trained batch 338 batch loss 6.65620565 epoch total loss 6.98752403\n",
      "Trained batch 339 batch loss 6.14011 epoch total loss 6.98502445\n",
      "Trained batch 340 batch loss 6.21926403 epoch total loss 6.98277235\n",
      "Trained batch 341 batch loss 6.62879562 epoch total loss 6.98173475\n",
      "Trained batch 342 batch loss 6.59763956 epoch total loss 6.98061132\n",
      "Trained batch 343 batch loss 6.72272539 epoch total loss 6.97985935\n",
      "Trained batch 344 batch loss 6.2460556 epoch total loss 6.97772646\n",
      "Trained batch 345 batch loss 7.01007175 epoch total loss 6.97782\n",
      "Trained batch 346 batch loss 6.45349789 epoch total loss 6.97630501\n",
      "Trained batch 347 batch loss 6.35161066 epoch total loss 6.97450447\n",
      "Trained batch 348 batch loss 6.90622854 epoch total loss 6.97430849\n",
      "Trained batch 349 batch loss 7.0625391 epoch total loss 6.97456121\n",
      "Trained batch 350 batch loss 6.40543699 epoch total loss 6.9729352\n",
      "Trained batch 351 batch loss 6.63922596 epoch total loss 6.97198439\n",
      "Trained batch 352 batch loss 7.31449413 epoch total loss 6.97295713\n",
      "Trained batch 353 batch loss 6.64680481 epoch total loss 6.97203302\n",
      "Trained batch 354 batch loss 7.22578764 epoch total loss 6.97275\n",
      "Trained batch 355 batch loss 6.919981 epoch total loss 6.97260141\n",
      "Trained batch 356 batch loss 6.96828604 epoch total loss 6.97258902\n",
      "Trained batch 357 batch loss 7.03351164 epoch total loss 6.97275972\n",
      "Trained batch 358 batch loss 6.69985819 epoch total loss 6.97199726\n",
      "Trained batch 359 batch loss 6.53274632 epoch total loss 6.9707737\n",
      "Trained batch 360 batch loss 6.94800329 epoch total loss 6.97071075\n",
      "Trained batch 361 batch loss 6.79471 epoch total loss 6.97022295\n",
      "Trained batch 362 batch loss 7.09367371 epoch total loss 6.97056437\n",
      "Trained batch 363 batch loss 6.87500525 epoch total loss 6.97030115\n",
      "Trained batch 364 batch loss 6.74605656 epoch total loss 6.96968508\n",
      "Trained batch 365 batch loss 6.93838644 epoch total loss 6.96959925\n",
      "Trained batch 366 batch loss 7.17357302 epoch total loss 6.97015667\n",
      "Trained batch 367 batch loss 6.90386677 epoch total loss 6.96997595\n",
      "Trained batch 368 batch loss 6.41027117 epoch total loss 6.96845484\n",
      "Trained batch 369 batch loss 5.55802584 epoch total loss 6.96463251\n",
      "Trained batch 370 batch loss 5.84895611 epoch total loss 6.96161699\n",
      "Trained batch 371 batch loss 6.19941 epoch total loss 6.95956278\n",
      "Trained batch 372 batch loss 6.81927395 epoch total loss 6.95918608\n",
      "Trained batch 373 batch loss 6.97876072 epoch total loss 6.95923853\n",
      "Trained batch 374 batch loss 6.87128401 epoch total loss 6.95900345\n",
      "Trained batch 375 batch loss 7.25765848 epoch total loss 6.95979929\n",
      "Trained batch 376 batch loss 6.9291749 epoch total loss 6.95971823\n",
      "Trained batch 377 batch loss 7.15558577 epoch total loss 6.9602375\n",
      "Trained batch 378 batch loss 7.09415627 epoch total loss 6.96059179\n",
      "Trained batch 379 batch loss 6.74172592 epoch total loss 6.96001434\n",
      "Trained batch 380 batch loss 6.6615386 epoch total loss 6.95922899\n",
      "Trained batch 381 batch loss 6.63534737 epoch total loss 6.95837879\n",
      "Trained batch 382 batch loss 6.96967 epoch total loss 6.95840836\n",
      "Trained batch 383 batch loss 6.97149229 epoch total loss 6.95844269\n",
      "Trained batch 384 batch loss 6.88662481 epoch total loss 6.95825577\n",
      "Trained batch 385 batch loss 6.94885588 epoch total loss 6.95823145\n",
      "Trained batch 386 batch loss 7.20256615 epoch total loss 6.95886469\n",
      "Trained batch 387 batch loss 6.93223524 epoch total loss 6.95879555\n",
      "Trained batch 388 batch loss 6.43602753 epoch total loss 6.95744848\n",
      "Trained batch 389 batch loss 5.93948364 epoch total loss 6.9548316\n",
      "Trained batch 390 batch loss 6.57640171 epoch total loss 6.95386124\n",
      "Trained batch 391 batch loss 7.24079943 epoch total loss 6.95459461\n",
      "Trained batch 392 batch loss 7.22051573 epoch total loss 6.95527315\n",
      "Trained batch 393 batch loss 7.2625103 epoch total loss 6.95605469\n",
      "Trained batch 394 batch loss 7.11064291 epoch total loss 6.95644712\n",
      "Trained batch 395 batch loss 7.44601536 epoch total loss 6.95768642\n",
      "Trained batch 396 batch loss 7.34625912 epoch total loss 6.95866728\n",
      "Trained batch 397 batch loss 7.08099556 epoch total loss 6.95897579\n",
      "Trained batch 398 batch loss 6.56829929 epoch total loss 6.95799446\n",
      "Trained batch 399 batch loss 6.35907221 epoch total loss 6.95649338\n",
      "Trained batch 400 batch loss 6.12656975 epoch total loss 6.95441818\n",
      "Trained batch 401 batch loss 6.35656834 epoch total loss 6.95292759\n",
      "Trained batch 402 batch loss 6.76382589 epoch total loss 6.95245743\n",
      "Trained batch 403 batch loss 5.46012497 epoch total loss 6.94875479\n",
      "Trained batch 404 batch loss 5.81460333 epoch total loss 6.94594765\n",
      "Trained batch 405 batch loss 5.86997 epoch total loss 6.94329071\n",
      "Trained batch 406 batch loss 6.3345561 epoch total loss 6.94179106\n",
      "Trained batch 407 batch loss 6.43789387 epoch total loss 6.94055319\n",
      "Trained batch 408 batch loss 6.89356565 epoch total loss 6.94043827\n",
      "Trained batch 409 batch loss 7.32878876 epoch total loss 6.94138765\n",
      "Trained batch 410 batch loss 7.45475388 epoch total loss 6.94264\n",
      "Trained batch 411 batch loss 7.33792448 epoch total loss 6.94360161\n",
      "Trained batch 412 batch loss 7.3670063 epoch total loss 6.94462919\n",
      "Trained batch 413 batch loss 7.01932621 epoch total loss 6.94481\n",
      "Trained batch 414 batch loss 6.92478323 epoch total loss 6.94476175\n",
      "Trained batch 415 batch loss 7.24342346 epoch total loss 6.9454813\n",
      "Trained batch 416 batch loss 7.51353 epoch total loss 6.94684649\n",
      "Trained batch 417 batch loss 7.4476161 epoch total loss 6.94804716\n",
      "Trained batch 418 batch loss 7.46827 epoch total loss 6.94929171\n",
      "Trained batch 419 batch loss 7.04381466 epoch total loss 6.94951725\n",
      "Trained batch 420 batch loss 7.11693525 epoch total loss 6.94991589\n",
      "Trained batch 421 batch loss 6.80667067 epoch total loss 6.94957542\n",
      "Trained batch 422 batch loss 6.33209515 epoch total loss 6.94811201\n",
      "Trained batch 423 batch loss 6.89204359 epoch total loss 6.94797945\n",
      "Trained batch 424 batch loss 7.16756868 epoch total loss 6.9484973\n",
      "Trained batch 425 batch loss 6.86159754 epoch total loss 6.94829273\n",
      "Trained batch 426 batch loss 6.74240589 epoch total loss 6.9478097\n",
      "Trained batch 427 batch loss 6.81604 epoch total loss 6.94750118\n",
      "Trained batch 428 batch loss 7.21107244 epoch total loss 6.94811726\n",
      "Trained batch 429 batch loss 6.63116741 epoch total loss 6.94737816\n",
      "Trained batch 430 batch loss 6.52073097 epoch total loss 6.94638634\n",
      "Trained batch 431 batch loss 6.97296429 epoch total loss 6.94644785\n",
      "Trained batch 432 batch loss 6.88111544 epoch total loss 6.94629622\n",
      "Trained batch 433 batch loss 7.15491247 epoch total loss 6.9467783\n",
      "Trained batch 434 batch loss 6.89122248 epoch total loss 6.94665\n",
      "Trained batch 435 batch loss 7.10258579 epoch total loss 6.94700861\n",
      "Trained batch 436 batch loss 7.40197706 epoch total loss 6.94805193\n",
      "Trained batch 437 batch loss 7.25631 epoch total loss 6.94875717\n",
      "Trained batch 438 batch loss 6.90040874 epoch total loss 6.94864702\n",
      "Trained batch 439 batch loss 5.93545723 epoch total loss 6.94633913\n",
      "Trained batch 440 batch loss 6.36940956 epoch total loss 6.94502783\n",
      "Trained batch 441 batch loss 7.28504181 epoch total loss 6.94579935\n",
      "Trained batch 442 batch loss 7.41743326 epoch total loss 6.94686604\n",
      "Trained batch 443 batch loss 7.3885603 epoch total loss 6.94786358\n",
      "Trained batch 444 batch loss 7.38434458 epoch total loss 6.94884634\n",
      "Trained batch 445 batch loss 7.50930166 epoch total loss 6.95010567\n",
      "Trained batch 446 batch loss 7.40523148 epoch total loss 6.95112658\n",
      "Trained batch 447 batch loss 7.44310284 epoch total loss 6.95222712\n",
      "Trained batch 448 batch loss 7.45751143 epoch total loss 6.95335484\n",
      "Trained batch 449 batch loss 7.39319611 epoch total loss 6.95433474\n",
      "Trained batch 450 batch loss 7.01272631 epoch total loss 6.95446444\n",
      "Trained batch 451 batch loss 6.79902744 epoch total loss 6.95412\n",
      "Trained batch 452 batch loss 7.38714075 epoch total loss 6.95507812\n",
      "Trained batch 453 batch loss 7.55862188 epoch total loss 6.95641041\n",
      "Trained batch 454 batch loss 7.22582769 epoch total loss 6.95700359\n",
      "Trained batch 455 batch loss 7.13618 epoch total loss 6.95739794\n",
      "Trained batch 456 batch loss 6.98157835 epoch total loss 6.95745087\n",
      "Trained batch 457 batch loss 7.18201494 epoch total loss 6.95794249\n",
      "Trained batch 458 batch loss 6.54653788 epoch total loss 6.9570446\n",
      "Trained batch 459 batch loss 6.3535471 epoch total loss 6.95572948\n",
      "Trained batch 460 batch loss 6.56765223 epoch total loss 6.95488596\n",
      "Trained batch 461 batch loss 6.17964125 epoch total loss 6.95320463\n",
      "Trained batch 462 batch loss 7.02031946 epoch total loss 6.95334959\n",
      "Trained batch 463 batch loss 6.66510773 epoch total loss 6.95272684\n",
      "Trained batch 464 batch loss 6.94723225 epoch total loss 6.95271492\n",
      "Trained batch 465 batch loss 6.57776356 epoch total loss 6.95190907\n",
      "Trained batch 466 batch loss 7.22525024 epoch total loss 6.95249557\n",
      "Trained batch 467 batch loss 6.76195908 epoch total loss 6.95208788\n",
      "Trained batch 468 batch loss 7.29620457 epoch total loss 6.95282316\n",
      "Trained batch 469 batch loss 7.2645874 epoch total loss 6.95348787\n",
      "Trained batch 470 batch loss 6.51752377 epoch total loss 6.95256042\n",
      "Trained batch 471 batch loss 5.88957739 epoch total loss 6.95030355\n",
      "Trained batch 472 batch loss 5.35271645 epoch total loss 6.94691896\n",
      "Trained batch 473 batch loss 6.36579275 epoch total loss 6.94569\n",
      "Trained batch 474 batch loss 7.10492611 epoch total loss 6.94602633\n",
      "Trained batch 475 batch loss 7.33821964 epoch total loss 6.94685173\n",
      "Trained batch 476 batch loss 7.5187459 epoch total loss 6.94805336\n",
      "Trained batch 477 batch loss 7.43524027 epoch total loss 6.94907475\n",
      "Trained batch 478 batch loss 6.85803652 epoch total loss 6.94888449\n",
      "Trained batch 479 batch loss 6.6134038 epoch total loss 6.94818449\n",
      "Trained batch 480 batch loss 7.11575127 epoch total loss 6.94853354\n",
      "Trained batch 481 batch loss 6.96293879 epoch total loss 6.94856358\n",
      "Trained batch 482 batch loss 7.4055872 epoch total loss 6.94951153\n",
      "Trained batch 483 batch loss 7.30642748 epoch total loss 6.95025063\n",
      "Trained batch 484 batch loss 6.96227 epoch total loss 6.95027494\n",
      "Trained batch 485 batch loss 6.74390697 epoch total loss 6.94984961\n",
      "Trained batch 486 batch loss 6.81449413 epoch total loss 6.94957113\n",
      "Trained batch 487 batch loss 6.70135736 epoch total loss 6.94906139\n",
      "Trained batch 488 batch loss 7.12747574 epoch total loss 6.94942665\n",
      "Trained batch 489 batch loss 7.33276081 epoch total loss 6.95021057\n",
      "Trained batch 490 batch loss 7.35454798 epoch total loss 6.95103598\n",
      "Trained batch 491 batch loss 7.32002544 epoch total loss 6.95178747\n",
      "Trained batch 492 batch loss 7.24383211 epoch total loss 6.95238113\n",
      "Trained batch 493 batch loss 7.29574919 epoch total loss 6.95307732\n",
      "Trained batch 494 batch loss 7.31835604 epoch total loss 6.95381689\n",
      "Trained batch 495 batch loss 7.29224682 epoch total loss 6.95450068\n",
      "Trained batch 496 batch loss 7.13551617 epoch total loss 6.95486546\n",
      "Trained batch 497 batch loss 7.19086218 epoch total loss 6.95534039\n",
      "Trained batch 498 batch loss 6.81748199 epoch total loss 6.95506334\n",
      "Trained batch 499 batch loss 7.2574234 epoch total loss 6.95566893\n",
      "Trained batch 500 batch loss 6.99810886 epoch total loss 6.9557538\n",
      "Trained batch 501 batch loss 7.06549311 epoch total loss 6.95597267\n",
      "Trained batch 502 batch loss 6.91347742 epoch total loss 6.95588827\n",
      "Trained batch 503 batch loss 7.04487181 epoch total loss 6.95606518\n",
      "Trained batch 504 batch loss 7.24054527 epoch total loss 6.95662975\n",
      "Trained batch 505 batch loss 6.82422304 epoch total loss 6.95636749\n",
      "Trained batch 506 batch loss 6.89109 epoch total loss 6.95623875\n",
      "Trained batch 507 batch loss 6.66254139 epoch total loss 6.95565939\n",
      "Trained batch 508 batch loss 7.12534666 epoch total loss 6.95599318\n",
      "Trained batch 509 batch loss 6.82716751 epoch total loss 6.95574\n",
      "Trained batch 510 batch loss 7.0708766 epoch total loss 6.95596552\n",
      "Trained batch 511 batch loss 7.04741 epoch total loss 6.95614433\n",
      "Trained batch 512 batch loss 7.14154243 epoch total loss 6.95650673\n",
      "Trained batch 513 batch loss 7.25107336 epoch total loss 6.95708084\n",
      "Trained batch 514 batch loss 7.29889202 epoch total loss 6.95774555\n",
      "Trained batch 515 batch loss 7.12213326 epoch total loss 6.95806456\n",
      "Trained batch 516 batch loss 7.18696308 epoch total loss 6.95850849\n",
      "Trained batch 517 batch loss 7.19391918 epoch total loss 6.95896339\n",
      "Trained batch 518 batch loss 7.17335224 epoch total loss 6.95937729\n",
      "Trained batch 519 batch loss 6.52809954 epoch total loss 6.95854664\n",
      "Trained batch 520 batch loss 6.63470221 epoch total loss 6.95792389\n",
      "Trained batch 521 batch loss 7.08414125 epoch total loss 6.95816612\n",
      "Trained batch 522 batch loss 7.02692604 epoch total loss 6.95829773\n",
      "Trained batch 523 batch loss 7.20027304 epoch total loss 6.95876026\n",
      "Trained batch 524 batch loss 7.19232941 epoch total loss 6.9592061\n",
      "Trained batch 525 batch loss 7.25944328 epoch total loss 6.95977831\n",
      "Trained batch 526 batch loss 7.34011173 epoch total loss 6.96050119\n",
      "Trained batch 527 batch loss 7.01621723 epoch total loss 6.96060658\n",
      "Trained batch 528 batch loss 7.19608593 epoch total loss 6.96105242\n",
      "Trained batch 529 batch loss 7.39289474 epoch total loss 6.96186876\n",
      "Trained batch 530 batch loss 7.18067503 epoch total loss 6.9622817\n",
      "Trained batch 531 batch loss 7.27985191 epoch total loss 6.96287966\n",
      "Trained batch 532 batch loss 6.83034849 epoch total loss 6.96263027\n",
      "Trained batch 533 batch loss 6.23463821 epoch total loss 6.96126461\n",
      "Trained batch 534 batch loss 7.2884922 epoch total loss 6.96187735\n",
      "Trained batch 535 batch loss 7.21367788 epoch total loss 6.96234798\n",
      "Trained batch 536 batch loss 7.0966363 epoch total loss 6.9625988\n",
      "Trained batch 537 batch loss 6.78620911 epoch total loss 6.96227026\n",
      "Trained batch 538 batch loss 7.14450598 epoch total loss 6.96260881\n",
      "Trained batch 539 batch loss 7.22486877 epoch total loss 6.96309519\n",
      "Trained batch 540 batch loss 7.27650118 epoch total loss 6.96367598\n",
      "Trained batch 541 batch loss 7.21098042 epoch total loss 6.96413279\n",
      "Trained batch 542 batch loss 6.74405527 epoch total loss 6.963727\n",
      "Trained batch 543 batch loss 6.42148829 epoch total loss 6.9627285\n",
      "Trained batch 544 batch loss 6.13262367 epoch total loss 6.96120214\n",
      "Trained batch 545 batch loss 6.78455067 epoch total loss 6.96087837\n",
      "Trained batch 546 batch loss 7.37626314 epoch total loss 6.96163893\n",
      "Trained batch 547 batch loss 7.34703 epoch total loss 6.96234322\n",
      "Trained batch 548 batch loss 6.82788801 epoch total loss 6.96209812\n",
      "Trained batch 549 batch loss 6.85662746 epoch total loss 6.96190596\n",
      "Trained batch 550 batch loss 6.30005407 epoch total loss 6.9607029\n",
      "Trained batch 551 batch loss 7.06165648 epoch total loss 6.960886\n",
      "Trained batch 552 batch loss 7.21781301 epoch total loss 6.96135139\n",
      "Trained batch 553 batch loss 7.27338743 epoch total loss 6.96191597\n",
      "Trained batch 554 batch loss 7.07591152 epoch total loss 6.96212149\n",
      "Trained batch 555 batch loss 6.82114124 epoch total loss 6.96186733\n",
      "Trained batch 556 batch loss 6.98490763 epoch total loss 6.96190882\n",
      "Trained batch 557 batch loss 6.73196316 epoch total loss 6.96149588\n",
      "Trained batch 558 batch loss 6.95781469 epoch total loss 6.9614892\n",
      "Trained batch 559 batch loss 7.00497103 epoch total loss 6.96156693\n",
      "Trained batch 560 batch loss 7.25815821 epoch total loss 6.96209621\n",
      "Trained batch 561 batch loss 7.10206509 epoch total loss 6.9623456\n",
      "Trained batch 562 batch loss 7.0471077 epoch total loss 6.96249676\n",
      "Trained batch 563 batch loss 7.10250759 epoch total loss 6.96274519\n",
      "Trained batch 564 batch loss 7.32422256 epoch total loss 6.96338606\n",
      "Trained batch 565 batch loss 7.19526529 epoch total loss 6.96379662\n",
      "Trained batch 566 batch loss 6.62736702 epoch total loss 6.96320248\n",
      "Trained batch 567 batch loss 5.99952269 epoch total loss 6.96150303\n",
      "Trained batch 568 batch loss 6.32712078 epoch total loss 6.9603858\n",
      "Trained batch 569 batch loss 7.09997559 epoch total loss 6.96063137\n",
      "Trained batch 570 batch loss 6.88980198 epoch total loss 6.96050692\n",
      "Trained batch 571 batch loss 6.64682 epoch total loss 6.9599576\n",
      "Trained batch 572 batch loss 7.22950029 epoch total loss 6.96042871\n",
      "Trained batch 573 batch loss 7.11960173 epoch total loss 6.96070671\n",
      "Trained batch 574 batch loss 7.28022099 epoch total loss 6.96126318\n",
      "Trained batch 575 batch loss 6.79135418 epoch total loss 6.96096754\n",
      "Trained batch 576 batch loss 6.80096912 epoch total loss 6.96069\n",
      "Trained batch 577 batch loss 6.86356592 epoch total loss 6.9605217\n",
      "Trained batch 578 batch loss 6.68529701 epoch total loss 6.96004534\n",
      "Trained batch 579 batch loss 7.15396881 epoch total loss 6.96038055\n",
      "Trained batch 580 batch loss 7.07070255 epoch total loss 6.96057081\n",
      "Trained batch 581 batch loss 7.02641249 epoch total loss 6.9606843\n",
      "Trained batch 582 batch loss 7.04683399 epoch total loss 6.96083212\n",
      "Trained batch 583 batch loss 7.38678455 epoch total loss 6.96156263\n",
      "Trained batch 584 batch loss 7.28080368 epoch total loss 6.96210909\n",
      "Trained batch 585 batch loss 7.27377081 epoch total loss 6.96264172\n",
      "Trained batch 586 batch loss 7.11275578 epoch total loss 6.96289825\n",
      "Trained batch 587 batch loss 7.05845785 epoch total loss 6.96306086\n",
      "Trained batch 588 batch loss 6.8252759 epoch total loss 6.96282625\n",
      "Trained batch 589 batch loss 7.17560577 epoch total loss 6.96318722\n",
      "Trained batch 590 batch loss 6.85817194 epoch total loss 6.96301\n",
      "Trained batch 591 batch loss 7.26535368 epoch total loss 6.963521\n",
      "Trained batch 592 batch loss 7.11757708 epoch total loss 6.96378136\n",
      "Trained batch 593 batch loss 7.23737049 epoch total loss 6.96424246\n",
      "Trained batch 594 batch loss 7.04696131 epoch total loss 6.96438169\n",
      "Trained batch 595 batch loss 6.41590405 epoch total loss 6.96346\n",
      "Trained batch 596 batch loss 6.74762678 epoch total loss 6.96309805\n",
      "Trained batch 597 batch loss 7.14883566 epoch total loss 6.96340895\n",
      "Trained batch 598 batch loss 6.80144 epoch total loss 6.9631381\n",
      "Trained batch 599 batch loss 7.05622053 epoch total loss 6.96329355\n",
      "Trained batch 600 batch loss 6.95928621 epoch total loss 6.96328688\n",
      "Trained batch 601 batch loss 7.1204958 epoch total loss 6.96354866\n",
      "Trained batch 602 batch loss 6.80811167 epoch total loss 6.96329069\n",
      "Trained batch 603 batch loss 6.17309 epoch total loss 6.96198\n",
      "Trained batch 604 batch loss 6.67486286 epoch total loss 6.96150398\n",
      "Trained batch 605 batch loss 7.26623154 epoch total loss 6.96200752\n",
      "Trained batch 606 batch loss 7.40774918 epoch total loss 6.96274328\n",
      "Trained batch 607 batch loss 7.41536045 epoch total loss 6.96348906\n",
      "Trained batch 608 batch loss 7.0307374 epoch total loss 6.96359968\n",
      "Trained batch 609 batch loss 6.79335833 epoch total loss 6.96332026\n",
      "Trained batch 610 batch loss 6.8988204 epoch total loss 6.96321487\n",
      "Trained batch 611 batch loss 7.18141317 epoch total loss 6.9635725\n",
      "Trained batch 612 batch loss 7.31754827 epoch total loss 6.96415043\n",
      "Trained batch 613 batch loss 7.17774439 epoch total loss 6.964499\n",
      "Trained batch 614 batch loss 7.12092495 epoch total loss 6.9647541\n",
      "Trained batch 615 batch loss 7.01517391 epoch total loss 6.96483564\n",
      "Trained batch 616 batch loss 6.72269154 epoch total loss 6.96444273\n",
      "Trained batch 617 batch loss 6.78418732 epoch total loss 6.96415043\n",
      "Trained batch 618 batch loss 7.25204659 epoch total loss 6.9646163\n",
      "Trained batch 619 batch loss 6.82926464 epoch total loss 6.96439743\n",
      "Trained batch 620 batch loss 6.79282331 epoch total loss 6.96412086\n",
      "Trained batch 621 batch loss 6.31993771 epoch total loss 6.96308327\n",
      "Trained batch 622 batch loss 6.81445551 epoch total loss 6.96284437\n",
      "Trained batch 623 batch loss 6.79582119 epoch total loss 6.96257639\n",
      "Trained batch 624 batch loss 7.2898016 epoch total loss 6.96310091\n",
      "Trained batch 625 batch loss 7.36931467 epoch total loss 6.96375084\n",
      "Trained batch 626 batch loss 7.25943804 epoch total loss 6.96422291\n",
      "Trained batch 627 batch loss 7.19549227 epoch total loss 6.9645915\n",
      "Trained batch 628 batch loss 6.55657196 epoch total loss 6.96394205\n",
      "Trained batch 629 batch loss 6.47613716 epoch total loss 6.96316624\n",
      "Trained batch 630 batch loss 7.02561712 epoch total loss 6.96326494\n",
      "Trained batch 631 batch loss 7.30200958 epoch total loss 6.96380234\n",
      "Trained batch 632 batch loss 7.2763381 epoch total loss 6.96429682\n",
      "Trained batch 633 batch loss 7.09684658 epoch total loss 6.96450567\n",
      "Trained batch 634 batch loss 7.38255596 epoch total loss 6.96516514\n",
      "Trained batch 635 batch loss 7.29231691 epoch total loss 6.9656806\n",
      "Trained batch 636 batch loss 7.09887886 epoch total loss 6.96589041\n",
      "Trained batch 637 batch loss 6.95093441 epoch total loss 6.96586704\n",
      "Trained batch 638 batch loss 7.32330656 epoch total loss 6.96642733\n",
      "Trained batch 639 batch loss 7.41347885 epoch total loss 6.96712685\n",
      "Trained batch 640 batch loss 7.24719191 epoch total loss 6.96756458\n",
      "Trained batch 641 batch loss 7.12669468 epoch total loss 6.96781206\n",
      "Trained batch 642 batch loss 7.18374348 epoch total loss 6.96814823\n",
      "Trained batch 643 batch loss 7.3867774 epoch total loss 6.96879959\n",
      "Trained batch 644 batch loss 7.34765434 epoch total loss 6.96938753\n",
      "Trained batch 645 batch loss 7.27386856 epoch total loss 6.9698596\n",
      "Trained batch 646 batch loss 6.88558578 epoch total loss 6.96972942\n",
      "Trained batch 647 batch loss 6.91369057 epoch total loss 6.96964264\n",
      "Trained batch 648 batch loss 7.16227961 epoch total loss 6.96993971\n",
      "Trained batch 649 batch loss 6.4944191 epoch total loss 6.96920729\n",
      "Trained batch 650 batch loss 6.11278105 epoch total loss 6.96789\n",
      "Trained batch 651 batch loss 6.9280262 epoch total loss 6.96782875\n",
      "Trained batch 652 batch loss 7.06571102 epoch total loss 6.96797943\n",
      "Trained batch 653 batch loss 6.67059135 epoch total loss 6.96752357\n",
      "Trained batch 654 batch loss 6.73084116 epoch total loss 6.96716213\n",
      "Trained batch 655 batch loss 7.18201113 epoch total loss 6.96749\n",
      "Trained batch 656 batch loss 6.79417515 epoch total loss 6.96722555\n",
      "Trained batch 657 batch loss 7.06314516 epoch total loss 6.96737146\n",
      "Trained batch 658 batch loss 7.02914095 epoch total loss 6.9674654\n",
      "Trained batch 659 batch loss 6.64340591 epoch total loss 6.96697378\n",
      "Trained batch 660 batch loss 6.92963 epoch total loss 6.96691751\n",
      "Trained batch 661 batch loss 6.81213665 epoch total loss 6.96668291\n",
      "Trained batch 662 batch loss 7.25537634 epoch total loss 6.96711922\n",
      "Trained batch 663 batch loss 7.07418871 epoch total loss 6.96728086\n",
      "Trained batch 664 batch loss 7.42684174 epoch total loss 6.96797276\n",
      "Trained batch 665 batch loss 7.44929314 epoch total loss 6.96869659\n",
      "Trained batch 666 batch loss 7.39621353 epoch total loss 6.96933794\n",
      "Trained batch 667 batch loss 7.05887365 epoch total loss 6.96947241\n",
      "Trained batch 668 batch loss 6.61943245 epoch total loss 6.96894884\n",
      "Trained batch 669 batch loss 6.30994606 epoch total loss 6.9679637\n",
      "Trained batch 670 batch loss 5.7875495 epoch total loss 6.96620226\n",
      "Trained batch 671 batch loss 5.96934 epoch total loss 6.96471643\n",
      "Trained batch 672 batch loss 6.13383 epoch total loss 6.96348\n",
      "Trained batch 673 batch loss 6.68438864 epoch total loss 6.96306562\n",
      "Trained batch 674 batch loss 7.08254528 epoch total loss 6.96324253\n",
      "Trained batch 675 batch loss 6.70362091 epoch total loss 6.9628582\n",
      "Trained batch 676 batch loss 7.00715685 epoch total loss 6.962924\n",
      "Trained batch 677 batch loss 6.69137239 epoch total loss 6.96252298\n",
      "Trained batch 678 batch loss 6.82738733 epoch total loss 6.96232319\n",
      "Trained batch 679 batch loss 6.80071735 epoch total loss 6.96208525\n",
      "Trained batch 680 batch loss 6.53100967 epoch total loss 6.96145153\n",
      "Trained batch 681 batch loss 6.47097635 epoch total loss 6.96073151\n",
      "Trained batch 682 batch loss 6.82458735 epoch total loss 6.96053219\n",
      "Trained batch 683 batch loss 6.61405563 epoch total loss 6.96002531\n",
      "Trained batch 684 batch loss 6.95572042 epoch total loss 6.96001863\n",
      "Trained batch 685 batch loss 6.87351942 epoch total loss 6.95989227\n",
      "Trained batch 686 batch loss 7.25039387 epoch total loss 6.96031618\n",
      "Trained batch 687 batch loss 6.98480701 epoch total loss 6.96035194\n",
      "Trained batch 688 batch loss 7.07772112 epoch total loss 6.96052217\n",
      "Trained batch 689 batch loss 6.95473528 epoch total loss 6.96051359\n",
      "Trained batch 690 batch loss 7.12872267 epoch total loss 6.96075773\n",
      "Trained batch 691 batch loss 7.07558584 epoch total loss 6.96092415\n",
      "Trained batch 692 batch loss 6.72732449 epoch total loss 6.96058702\n",
      "Trained batch 693 batch loss 6.5825963 epoch total loss 6.96004105\n",
      "Trained batch 694 batch loss 7.09872246 epoch total loss 6.96024084\n",
      "Trained batch 695 batch loss 7.5355587 epoch total loss 6.96106911\n",
      "Trained batch 696 batch loss 7.31029701 epoch total loss 6.96157026\n",
      "Trained batch 697 batch loss 6.90864182 epoch total loss 6.96149445\n",
      "Trained batch 698 batch loss 6.80013084 epoch total loss 6.96126366\n",
      "Trained batch 699 batch loss 7.06595564 epoch total loss 6.96141338\n",
      "Trained batch 700 batch loss 6.54760551 epoch total loss 6.96082163\n",
      "Trained batch 701 batch loss 6.9728303 epoch total loss 6.96083879\n",
      "Trained batch 702 batch loss 6.65801048 epoch total loss 6.96040726\n",
      "Trained batch 703 batch loss 6.96653605 epoch total loss 6.96041584\n",
      "Trained batch 704 batch loss 7.00680971 epoch total loss 6.96048164\n",
      "Trained batch 705 batch loss 7.38301659 epoch total loss 6.96108103\n",
      "Trained batch 706 batch loss 7.3448391 epoch total loss 6.96162415\n",
      "Trained batch 707 batch loss 7.18816376 epoch total loss 6.96194458\n",
      "Trained batch 708 batch loss 7.25776625 epoch total loss 6.96236229\n",
      "Trained batch 709 batch loss 7.1816 epoch total loss 6.96267176\n",
      "Trained batch 710 batch loss 7.05730867 epoch total loss 6.96280479\n",
      "Trained batch 711 batch loss 7.1794405 epoch total loss 6.96310902\n",
      "Trained batch 712 batch loss 7.10373545 epoch total loss 6.96330643\n",
      "Trained batch 713 batch loss 7.0401926 epoch total loss 6.96341372\n",
      "Trained batch 714 batch loss 6.89903259 epoch total loss 6.96332359\n",
      "Trained batch 715 batch loss 6.72571278 epoch total loss 6.96299124\n",
      "Trained batch 716 batch loss 6.95610523 epoch total loss 6.96298122\n",
      "Trained batch 717 batch loss 6.47683907 epoch total loss 6.96230364\n",
      "Trained batch 718 batch loss 7.01724291 epoch total loss 6.96238\n",
      "Trained batch 719 batch loss 7.07473898 epoch total loss 6.96253633\n",
      "Trained batch 720 batch loss 6.47905445 epoch total loss 6.96186447\n",
      "Trained batch 721 batch loss 6.60505 epoch total loss 6.96136951\n",
      "Trained batch 722 batch loss 7.00103903 epoch total loss 6.96142435\n",
      "Trained batch 723 batch loss 7.27498436 epoch total loss 6.9618578\n",
      "Trained batch 724 batch loss 7.11440086 epoch total loss 6.96206856\n",
      "Trained batch 725 batch loss 7.24446583 epoch total loss 6.96245813\n",
      "Trained batch 726 batch loss 7.20494175 epoch total loss 6.9627924\n",
      "Trained batch 727 batch loss 6.73460627 epoch total loss 6.96247816\n",
      "Trained batch 728 batch loss 7.01410389 epoch total loss 6.96254921\n",
      "Trained batch 729 batch loss 6.70267868 epoch total loss 6.96219254\n",
      "Trained batch 730 batch loss 6.78610563 epoch total loss 6.96195173\n",
      "Trained batch 731 batch loss 6.91857433 epoch total loss 6.96189213\n",
      "Trained batch 732 batch loss 7.36552525 epoch total loss 6.96244383\n",
      "Trained batch 733 batch loss 7.36935759 epoch total loss 6.96299839\n",
      "Trained batch 734 batch loss 7.48824406 epoch total loss 6.96371412\n",
      "Trained batch 735 batch loss 7.3576827 epoch total loss 6.96425056\n",
      "Trained batch 736 batch loss 7.29078484 epoch total loss 6.9646945\n",
      "Trained batch 737 batch loss 6.63309145 epoch total loss 6.96424484\n",
      "Trained batch 738 batch loss 7.25625658 epoch total loss 6.96464062\n",
      "Trained batch 739 batch loss 7.2571907 epoch total loss 6.96503687\n",
      "Trained batch 740 batch loss 7.36395884 epoch total loss 6.9655757\n",
      "Trained batch 741 batch loss 7.32566309 epoch total loss 6.96606159\n",
      "Trained batch 742 batch loss 7.26808643 epoch total loss 6.96646833\n",
      "Trained batch 743 batch loss 7.20149231 epoch total loss 6.96678495\n",
      "Trained batch 744 batch loss 7.35821962 epoch total loss 6.96731138\n",
      "Trained batch 745 batch loss 6.84579611 epoch total loss 6.9671483\n",
      "Trained batch 746 batch loss 6.87482548 epoch total loss 6.9670248\n",
      "Trained batch 747 batch loss 6.4335351 epoch total loss 6.9663105\n",
      "Trained batch 748 batch loss 6.54045153 epoch total loss 6.96574116\n",
      "Trained batch 749 batch loss 7.22376919 epoch total loss 6.96608543\n",
      "Trained batch 750 batch loss 6.77824 epoch total loss 6.96583509\n",
      "Trained batch 751 batch loss 6.93877792 epoch total loss 6.96579933\n",
      "Trained batch 752 batch loss 6.96747255 epoch total loss 6.96580172\n",
      "Trained batch 753 batch loss 6.78593254 epoch total loss 6.96556282\n",
      "Trained batch 754 batch loss 6.64573431 epoch total loss 6.96513844\n",
      "Trained batch 755 batch loss 6.65855265 epoch total loss 6.96473265\n",
      "Trained batch 756 batch loss 7.14110374 epoch total loss 6.96496582\n",
      "Trained batch 757 batch loss 7.04851675 epoch total loss 6.96507597\n",
      "Trained batch 758 batch loss 6.67077446 epoch total loss 6.96468782\n",
      "Trained batch 759 batch loss 7.40389156 epoch total loss 6.96526623\n",
      "Trained batch 760 batch loss 7.12924814 epoch total loss 6.96548223\n",
      "Trained batch 761 batch loss 7.32747316 epoch total loss 6.96595812\n",
      "Trained batch 762 batch loss 6.92664289 epoch total loss 6.96590662\n",
      "Trained batch 763 batch loss 7.02991772 epoch total loss 6.96599054\n",
      "Trained batch 764 batch loss 7.32740879 epoch total loss 6.96646404\n",
      "Trained batch 765 batch loss 7.17108 epoch total loss 6.96673107\n",
      "Trained batch 766 batch loss 7.23944378 epoch total loss 6.96708679\n",
      "Trained batch 767 batch loss 7.00469446 epoch total loss 6.96713638\n",
      "Trained batch 768 batch loss 6.96670341 epoch total loss 6.96713591\n",
      "Trained batch 769 batch loss 6.69203234 epoch total loss 6.9667778\n",
      "Trained batch 770 batch loss 7.26787138 epoch total loss 6.96716928\n",
      "Trained batch 771 batch loss 7.15787745 epoch total loss 6.96741629\n",
      "Trained batch 772 batch loss 7.35476732 epoch total loss 6.9679184\n",
      "Trained batch 773 batch loss 7.38302565 epoch total loss 6.96845484\n",
      "Trained batch 774 batch loss 7.1449275 epoch total loss 6.96868324\n",
      "Trained batch 775 batch loss 7.13177061 epoch total loss 6.96889353\n",
      "Trained batch 776 batch loss 6.24491453 epoch total loss 6.96796083\n",
      "Trained batch 777 batch loss 6.27917624 epoch total loss 6.96707487\n",
      "Trained batch 778 batch loss 6.94632292 epoch total loss 6.96704769\n",
      "Trained batch 779 batch loss 7.04873848 epoch total loss 6.96715307\n",
      "Trained batch 780 batch loss 7.00739479 epoch total loss 6.96720457\n",
      "Trained batch 781 batch loss 7.23596334 epoch total loss 6.96754837\n",
      "Trained batch 782 batch loss 6.94979668 epoch total loss 6.96752548\n",
      "Trained batch 783 batch loss 6.61342907 epoch total loss 6.96707296\n",
      "Trained batch 784 batch loss 7.02837324 epoch total loss 6.96715117\n",
      "Trained batch 785 batch loss 6.55301 epoch total loss 6.96662378\n",
      "Trained batch 786 batch loss 6.91200876 epoch total loss 6.96655464\n",
      "Trained batch 787 batch loss 6.23702669 epoch total loss 6.96562719\n",
      "Trained batch 788 batch loss 6.99809074 epoch total loss 6.96566868\n",
      "Trained batch 789 batch loss 7.11253357 epoch total loss 6.96585417\n",
      "Trained batch 790 batch loss 6.89094162 epoch total loss 6.96575975\n",
      "Trained batch 791 batch loss 7.16304779 epoch total loss 6.96600914\n",
      "Trained batch 792 batch loss 7.14380217 epoch total loss 6.96623421\n",
      "Trained batch 793 batch loss 7.06577635 epoch total loss 6.96635962\n",
      "Trained batch 794 batch loss 6.92307615 epoch total loss 6.96630478\n",
      "Trained batch 795 batch loss 6.98203087 epoch total loss 6.96632433\n",
      "Trained batch 796 batch loss 7.18738508 epoch total loss 6.96660233\n",
      "Trained batch 797 batch loss 7.18139076 epoch total loss 6.96687174\n",
      "Trained batch 798 batch loss 7.21390724 epoch total loss 6.96718121\n",
      "Trained batch 799 batch loss 7.35754871 epoch total loss 6.96766949\n",
      "Trained batch 800 batch loss 6.49675179 epoch total loss 6.96708059\n",
      "Trained batch 801 batch loss 6.58378649 epoch total loss 6.96660233\n",
      "Trained batch 802 batch loss 6.79509926 epoch total loss 6.96638823\n",
      "Trained batch 803 batch loss 6.41485 epoch total loss 6.96570158\n",
      "Trained batch 804 batch loss 6.45535803 epoch total loss 6.96506739\n",
      "Trained batch 805 batch loss 6.80657053 epoch total loss 6.96487045\n",
      "Trained batch 806 batch loss 7.17960548 epoch total loss 6.965137\n",
      "Trained batch 807 batch loss 6.84689474 epoch total loss 6.96499\n",
      "Trained batch 808 batch loss 7.26234293 epoch total loss 6.96535778\n",
      "Trained batch 809 batch loss 7.0851593 epoch total loss 6.96550608\n",
      "Trained batch 810 batch loss 7.20955467 epoch total loss 6.96580696\n",
      "Trained batch 811 batch loss 7.35922956 epoch total loss 6.96629238\n",
      "Trained batch 812 batch loss 7.3585453 epoch total loss 6.96677542\n",
      "Trained batch 813 batch loss 7.15749121 epoch total loss 6.96701\n",
      "Trained batch 814 batch loss 7.15814781 epoch total loss 6.9672451\n",
      "Trained batch 815 batch loss 7.19868326 epoch total loss 6.96752882\n",
      "Trained batch 816 batch loss 6.66421 epoch total loss 6.96715689\n",
      "Trained batch 817 batch loss 6.82083035 epoch total loss 6.96697807\n",
      "Trained batch 818 batch loss 7.16494274 epoch total loss 6.96722031\n",
      "Trained batch 819 batch loss 7.33054304 epoch total loss 6.96766376\n",
      "Trained batch 820 batch loss 7.04753 epoch total loss 6.96776104\n",
      "Trained batch 821 batch loss 6.43932533 epoch total loss 6.96711731\n",
      "Trained batch 822 batch loss 6.69322252 epoch total loss 6.96678448\n",
      "Trained batch 823 batch loss 6.50278044 epoch total loss 6.96622086\n",
      "Trained batch 824 batch loss 6.764956 epoch total loss 6.96597672\n",
      "Trained batch 825 batch loss 6.86689711 epoch total loss 6.96585655\n",
      "Trained batch 826 batch loss 6.2834425 epoch total loss 6.96503\n",
      "Trained batch 827 batch loss 6.70997286 epoch total loss 6.96472168\n",
      "Trained batch 828 batch loss 6.4128437 epoch total loss 6.96405554\n",
      "Trained batch 829 batch loss 6.07895088 epoch total loss 6.9629879\n",
      "Trained batch 830 batch loss 6.55131245 epoch total loss 6.96249199\n",
      "Trained batch 831 batch loss 6.74146318 epoch total loss 6.96222591\n",
      "Trained batch 832 batch loss 6.62082291 epoch total loss 6.96181536\n",
      "Trained batch 833 batch loss 7.30375385 epoch total loss 6.96222591\n",
      "Trained batch 834 batch loss 7.32744026 epoch total loss 6.96266413\n",
      "Trained batch 835 batch loss 7.27198362 epoch total loss 6.96303463\n",
      "Trained batch 836 batch loss 7.29277563 epoch total loss 6.96342897\n",
      "Trained batch 837 batch loss 7.25705624 epoch total loss 6.96377945\n",
      "Trained batch 838 batch loss 6.79456425 epoch total loss 6.96357775\n",
      "Trained batch 839 batch loss 6.44907522 epoch total loss 6.96296453\n",
      "Trained batch 840 batch loss 6.54631233 epoch total loss 6.96246862\n",
      "Trained batch 841 batch loss 6.93481731 epoch total loss 6.9624362\n",
      "Trained batch 842 batch loss 7.05673742 epoch total loss 6.96254778\n",
      "Trained batch 843 batch loss 7.25492525 epoch total loss 6.96289492\n",
      "Trained batch 844 batch loss 7.21023178 epoch total loss 6.96318817\n",
      "Trained batch 845 batch loss 7.07850742 epoch total loss 6.96332455\n",
      "Trained batch 846 batch loss 7.32147408 epoch total loss 6.9637475\n",
      "Trained batch 847 batch loss 7.40623045 epoch total loss 6.96427\n",
      "Trained batch 848 batch loss 7.27348423 epoch total loss 6.9646349\n",
      "Trained batch 849 batch loss 7.26414251 epoch total loss 6.96498775\n",
      "Trained batch 850 batch loss 7.25517511 epoch total loss 6.96532917\n",
      "Trained batch 851 batch loss 7.10234785 epoch total loss 6.96549034\n",
      "Trained batch 852 batch loss 7.35809135 epoch total loss 6.96595097\n",
      "Trained batch 853 batch loss 7.51443338 epoch total loss 6.96659422\n",
      "Trained batch 854 batch loss 6.91159821 epoch total loss 6.96653\n",
      "Trained batch 855 batch loss 7.20155716 epoch total loss 6.96680498\n",
      "Trained batch 856 batch loss 7.33571482 epoch total loss 6.96723604\n",
      "Trained batch 857 batch loss 7.37408781 epoch total loss 6.96771097\n",
      "Trained batch 858 batch loss 7.07768536 epoch total loss 6.96783876\n",
      "Trained batch 859 batch loss 7.1244092 epoch total loss 6.96802139\n",
      "Trained batch 860 batch loss 7.14051628 epoch total loss 6.96822214\n",
      "Trained batch 861 batch loss 7.04631376 epoch total loss 6.96831274\n",
      "Trained batch 862 batch loss 7.40373 epoch total loss 6.96881819\n",
      "Trained batch 863 batch loss 6.90149069 epoch total loss 6.96874\n",
      "Trained batch 864 batch loss 6.83185339 epoch total loss 6.96858168\n",
      "Trained batch 865 batch loss 7.20274115 epoch total loss 6.96885204\n",
      "Trained batch 866 batch loss 7.09022045 epoch total loss 6.96899223\n",
      "Trained batch 867 batch loss 7.0054841 epoch total loss 6.96903419\n",
      "Trained batch 868 batch loss 6.95989895 epoch total loss 6.96902418\n",
      "Trained batch 869 batch loss 6.66909313 epoch total loss 6.96867847\n",
      "Trained batch 870 batch loss 6.3178792 epoch total loss 6.96793079\n",
      "Trained batch 871 batch loss 6.21444893 epoch total loss 6.96706533\n",
      "Trained batch 872 batch loss 6.1316576 epoch total loss 6.96610737\n",
      "Trained batch 873 batch loss 6.40841627 epoch total loss 6.96546841\n",
      "Trained batch 874 batch loss 6.90145 epoch total loss 6.96539497\n",
      "Trained batch 875 batch loss 6.81207943 epoch total loss 6.96522\n",
      "Trained batch 876 batch loss 7.1547718 epoch total loss 6.96543646\n",
      "Trained batch 877 batch loss 7.440516 epoch total loss 6.96597767\n",
      "Trained batch 878 batch loss 7.29866648 epoch total loss 6.96635675\n",
      "Trained batch 879 batch loss 7.18809509 epoch total loss 6.966609\n",
      "Trained batch 880 batch loss 7.35431194 epoch total loss 6.96705\n",
      "Trained batch 881 batch loss 7.19620943 epoch total loss 6.96731\n",
      "Trained batch 882 batch loss 7.3240509 epoch total loss 6.96771479\n",
      "Trained batch 883 batch loss 7.32796 epoch total loss 6.96812296\n",
      "Trained batch 884 batch loss 7.34477329 epoch total loss 6.96854877\n",
      "Trained batch 885 batch loss 7.03837204 epoch total loss 6.96862793\n",
      "Trained batch 886 batch loss 7.01286554 epoch total loss 6.968678\n",
      "Trained batch 887 batch loss 6.8995595 epoch total loss 6.9686\n",
      "Trained batch 888 batch loss 6.52032518 epoch total loss 6.9680953\n",
      "Trained batch 889 batch loss 6.68678904 epoch total loss 6.96777916\n",
      "Trained batch 890 batch loss 7.09385967 epoch total loss 6.9679203\n",
      "Trained batch 891 batch loss 7.08559275 epoch total loss 6.96805239\n",
      "Trained batch 892 batch loss 7.01640272 epoch total loss 6.96810675\n",
      "Trained batch 893 batch loss 6.74899864 epoch total loss 6.96786165\n",
      "Trained batch 894 batch loss 7.04079676 epoch total loss 6.96794319\n",
      "Trained batch 895 batch loss 6.77482557 epoch total loss 6.96772766\n",
      "Trained batch 896 batch loss 7.01958561 epoch total loss 6.96778536\n",
      "Trained batch 897 batch loss 6.44419193 epoch total loss 6.96720171\n",
      "Trained batch 898 batch loss 7.0682807 epoch total loss 6.96731472\n",
      "Trained batch 899 batch loss 7.14442 epoch total loss 6.96751165\n",
      "Trained batch 900 batch loss 7.16811419 epoch total loss 6.96773434\n",
      "Trained batch 901 batch loss 7.4069581 epoch total loss 6.96822166\n",
      "Trained batch 902 batch loss 7.41654539 epoch total loss 6.96871853\n",
      "Trained batch 903 batch loss 7.49459887 epoch total loss 6.96930122\n",
      "Trained batch 904 batch loss 7.522861 epoch total loss 6.96991348\n",
      "Trained batch 905 batch loss 7.20007753 epoch total loss 6.97016811\n",
      "Trained batch 906 batch loss 7.07071495 epoch total loss 6.97027874\n",
      "Trained batch 907 batch loss 7.40686226 epoch total loss 6.97076035\n",
      "Trained batch 908 batch loss 7.36987591 epoch total loss 6.9712\n",
      "Trained batch 909 batch loss 7.17002 epoch total loss 6.97141886\n",
      "Trained batch 910 batch loss 7.22666502 epoch total loss 6.97169924\n",
      "Trained batch 911 batch loss 6.95616579 epoch total loss 6.97168159\n",
      "Trained batch 912 batch loss 7.01771307 epoch total loss 6.97173214\n",
      "Trained batch 913 batch loss 7.06002283 epoch total loss 6.97182894\n",
      "Trained batch 914 batch loss 6.70304108 epoch total loss 6.97153473\n",
      "Trained batch 915 batch loss 7.12005 epoch total loss 6.97169733\n",
      "Trained batch 916 batch loss 6.26922131 epoch total loss 6.97093\n",
      "Trained batch 917 batch loss 6.32799959 epoch total loss 6.97022915\n",
      "Trained batch 918 batch loss 6.9717207 epoch total loss 6.97023058\n",
      "Trained batch 919 batch loss 7.31203651 epoch total loss 6.97060251\n",
      "Trained batch 920 batch loss 7.23286057 epoch total loss 6.97088766\n",
      "Trained batch 921 batch loss 7.25941563 epoch total loss 6.97120094\n",
      "Trained batch 922 batch loss 7.26227379 epoch total loss 6.97151661\n",
      "Trained batch 923 batch loss 7.3044095 epoch total loss 6.9718771\n",
      "Trained batch 924 batch loss 7.28233242 epoch total loss 6.97221279\n",
      "Trained batch 925 batch loss 7.16566181 epoch total loss 6.97242165\n",
      "Trained batch 926 batch loss 6.62267494 epoch total loss 6.97204399\n",
      "Trained batch 927 batch loss 7.22071934 epoch total loss 6.97231245\n",
      "Trained batch 928 batch loss 6.97081327 epoch total loss 6.97231054\n",
      "Trained batch 929 batch loss 7.01801348 epoch total loss 6.97235966\n",
      "Trained batch 930 batch loss 7.01716089 epoch total loss 6.97240782\n",
      "Trained batch 931 batch loss 6.68415499 epoch total loss 6.97209835\n",
      "Trained batch 932 batch loss 6.82472944 epoch total loss 6.97194\n",
      "Trained batch 933 batch loss 7.19256306 epoch total loss 6.97217655\n",
      "Trained batch 934 batch loss 7.01353884 epoch total loss 6.9722209\n",
      "Trained batch 935 batch loss 6.88388252 epoch total loss 6.97212601\n",
      "Trained batch 936 batch loss 6.66498804 epoch total loss 6.97179794\n",
      "Trained batch 937 batch loss 7.09275675 epoch total loss 6.97192717\n",
      "Trained batch 938 batch loss 6.55824518 epoch total loss 6.97148609\n",
      "Trained batch 939 batch loss 6.77562332 epoch total loss 6.97127724\n",
      "Trained batch 940 batch loss 7.05413866 epoch total loss 6.97136545\n",
      "Trained batch 941 batch loss 6.66260624 epoch total loss 6.97103739\n",
      "Trained batch 942 batch loss 7.09271288 epoch total loss 6.97116661\n",
      "Trained batch 943 batch loss 7.01490355 epoch total loss 6.97121334\n",
      "Trained batch 944 batch loss 7.31278038 epoch total loss 6.97157526\n",
      "Trained batch 945 batch loss 7.14644861 epoch total loss 6.97176027\n",
      "Trained batch 946 batch loss 6.78429 epoch total loss 6.97156191\n",
      "Trained batch 947 batch loss 7.09756947 epoch total loss 6.97169495\n",
      "Trained batch 948 batch loss 6.69887877 epoch total loss 6.97140741\n",
      "Trained batch 949 batch loss 6.56954145 epoch total loss 6.97098351\n",
      "Trained batch 950 batch loss 6.82160568 epoch total loss 6.97082663\n",
      "Trained batch 951 batch loss 7.03976774 epoch total loss 6.97089863\n",
      "Trained batch 952 batch loss 7.24235773 epoch total loss 6.97118378\n",
      "Trained batch 953 batch loss 7.31084442 epoch total loss 6.97154045\n",
      "Trained batch 954 batch loss 7.219841 epoch total loss 6.97180033\n",
      "Trained batch 955 batch loss 7.23059225 epoch total loss 6.97207117\n",
      "Trained batch 956 batch loss 7.15102339 epoch total loss 6.97225857\n",
      "Trained batch 957 batch loss 7.61277437 epoch total loss 6.97292757\n",
      "Trained batch 958 batch loss 7.08799076 epoch total loss 6.97304773\n",
      "Trained batch 959 batch loss 7.27474737 epoch total loss 6.97336245\n",
      "Trained batch 960 batch loss 7.17497158 epoch total loss 6.97357225\n",
      "Trained batch 961 batch loss 6.97060728 epoch total loss 6.97356939\n",
      "Trained batch 962 batch loss 7.0833683 epoch total loss 6.97368336\n",
      "Trained batch 963 batch loss 7.11867237 epoch total loss 6.97383404\n",
      "Trained batch 964 batch loss 7.24122477 epoch total loss 6.97411156\n",
      "Trained batch 965 batch loss 7.41557693 epoch total loss 6.97456884\n",
      "Trained batch 966 batch loss 7.13277435 epoch total loss 6.97473288\n",
      "Trained batch 967 batch loss 6.99380493 epoch total loss 6.97475243\n",
      "Trained batch 968 batch loss 7.35168791 epoch total loss 6.97514153\n",
      "Trained batch 969 batch loss 7.20666504 epoch total loss 6.97538042\n",
      "Trained batch 970 batch loss 7.32087421 epoch total loss 6.97573662\n",
      "Trained batch 971 batch loss 7.26971674 epoch total loss 6.97603893\n",
      "Trained batch 972 batch loss 7.0679307 epoch total loss 6.97613335\n",
      "Trained batch 973 batch loss 6.9691658 epoch total loss 6.97612619\n",
      "Trained batch 974 batch loss 7.20789766 epoch total loss 6.97636461\n",
      "Trained batch 975 batch loss 7.30225849 epoch total loss 6.97669888\n",
      "Trained batch 976 batch loss 7.38848972 epoch total loss 6.97712088\n",
      "Trained batch 977 batch loss 7.02624035 epoch total loss 6.97717142\n",
      "Trained batch 978 batch loss 6.80403709 epoch total loss 6.97699451\n",
      "Trained batch 979 batch loss 7.2971549 epoch total loss 6.97732162\n",
      "Trained batch 980 batch loss 7.15988588 epoch total loss 6.97750759\n",
      "Trained batch 981 batch loss 6.98269367 epoch total loss 6.97751331\n",
      "Trained batch 982 batch loss 7.37687206 epoch total loss 6.97792\n",
      "Trained batch 983 batch loss 7.14438057 epoch total loss 6.97808933\n",
      "Trained batch 984 batch loss 7.11497 epoch total loss 6.97822809\n",
      "Trained batch 985 batch loss 6.96119 epoch total loss 6.9782114\n",
      "Trained batch 986 batch loss 7.30058193 epoch total loss 6.97853851\n",
      "Trained batch 987 batch loss 7.10993814 epoch total loss 6.97867155\n",
      "Trained batch 988 batch loss 6.95701 epoch total loss 6.97864962\n",
      "Trained batch 989 batch loss 6.74679708 epoch total loss 6.97841501\n",
      "Trained batch 990 batch loss 7.34961414 epoch total loss 6.97879\n",
      "Trained batch 991 batch loss 7.37089777 epoch total loss 6.97918558\n",
      "Trained batch 992 batch loss 7.41743708 epoch total loss 6.97962761\n",
      "Trained batch 993 batch loss 7.25590515 epoch total loss 6.97990561\n",
      "Trained batch 994 batch loss 7.04610252 epoch total loss 6.97997189\n",
      "Trained batch 995 batch loss 7.12200928 epoch total loss 6.98011494\n",
      "Trained batch 996 batch loss 7.21231222 epoch total loss 6.98034811\n",
      "Trained batch 997 batch loss 7.28284 epoch total loss 6.98065138\n",
      "Trained batch 998 batch loss 7.08527899 epoch total loss 6.98075628\n",
      "Trained batch 999 batch loss 6.88947725 epoch total loss 6.98066521\n",
      "Trained batch 1000 batch loss 7.08593845 epoch total loss 6.98077059\n",
      "Trained batch 1001 batch loss 6.83640862 epoch total loss 6.98062611\n",
      "Trained batch 1002 batch loss 7.23606634 epoch total loss 6.98088121\n",
      "Trained batch 1003 batch loss 7.22858095 epoch total loss 6.98112774\n",
      "Trained batch 1004 batch loss 7.20871 epoch total loss 6.98135424\n",
      "Trained batch 1005 batch loss 7.27554083 epoch total loss 6.98164701\n",
      "Trained batch 1006 batch loss 7.19317055 epoch total loss 6.9818573\n",
      "Trained batch 1007 batch loss 7.08603096 epoch total loss 6.98196077\n",
      "Trained batch 1008 batch loss 7.20439625 epoch total loss 6.98218155\n",
      "Trained batch 1009 batch loss 7.32897377 epoch total loss 6.98252535\n",
      "Trained batch 1010 batch loss 6.93789 epoch total loss 6.98248148\n",
      "Trained batch 1011 batch loss 6.93466663 epoch total loss 6.9824338\n",
      "Trained batch 1012 batch loss 7.2449584 epoch total loss 6.98269367\n",
      "Trained batch 1013 batch loss 7.28218079 epoch total loss 6.98298931\n",
      "Trained batch 1014 batch loss 7.29238844 epoch total loss 6.98329449\n",
      "Trained batch 1015 batch loss 7.46147823 epoch total loss 6.9837656\n",
      "Trained batch 1016 batch loss 7.5451 epoch total loss 6.98431778\n",
      "Trained batch 1017 batch loss 7.43684721 epoch total loss 6.98476315\n",
      "Trained batch 1018 batch loss 7.18192625 epoch total loss 6.98495674\n",
      "Trained batch 1019 batch loss 7.07559109 epoch total loss 6.98504591\n",
      "Trained batch 1020 batch loss 7.41124 epoch total loss 6.98546362\n",
      "Trained batch 1021 batch loss 6.74843502 epoch total loss 6.9852314\n",
      "Trained batch 1022 batch loss 6.94101524 epoch total loss 6.98518801\n",
      "Trained batch 1023 batch loss 7.1849823 epoch total loss 6.98538351\n",
      "Trained batch 1024 batch loss 7.33566809 epoch total loss 6.9857254\n",
      "Trained batch 1025 batch loss 7.27458572 epoch total loss 6.98600721\n",
      "Trained batch 1026 batch loss 7.28476334 epoch total loss 6.98629808\n",
      "Trained batch 1027 batch loss 7.26358938 epoch total loss 6.98656845\n",
      "Trained batch 1028 batch loss 6.36596441 epoch total loss 6.9859643\n",
      "Trained batch 1029 batch loss 6.5008769 epoch total loss 6.98549318\n",
      "Trained batch 1030 batch loss 6.29193211 epoch total loss 6.98482\n",
      "Trained batch 1031 batch loss 6.38024187 epoch total loss 6.98423338\n",
      "Trained batch 1032 batch loss 6.04805613 epoch total loss 6.98332596\n",
      "Trained batch 1033 batch loss 5.82430077 epoch total loss 6.98220396\n",
      "Trained batch 1034 batch loss 5.97502851 epoch total loss 6.98123\n",
      "Trained batch 1035 batch loss 5.57579613 epoch total loss 6.97987175\n",
      "Trained batch 1036 batch loss 6.43177843 epoch total loss 6.97934294\n",
      "Trained batch 1037 batch loss 6.98619652 epoch total loss 6.97934961\n",
      "Trained batch 1038 batch loss 7.36159325 epoch total loss 6.97971821\n",
      "Trained batch 1039 batch loss 7.10054493 epoch total loss 6.97983408\n",
      "Trained batch 1040 batch loss 7.05166769 epoch total loss 6.9799037\n",
      "Trained batch 1041 batch loss 7.23406029 epoch total loss 6.98014736\n",
      "Trained batch 1042 batch loss 7.30800152 epoch total loss 6.98046207\n",
      "Trained batch 1043 batch loss 6.79320717 epoch total loss 6.98028231\n",
      "Trained batch 1044 batch loss 6.8321991 epoch total loss 6.98014\n",
      "Trained batch 1045 batch loss 6.40987587 epoch total loss 6.97959471\n",
      "Trained batch 1046 batch loss 6.38627863 epoch total loss 6.97902727\n",
      "Trained batch 1047 batch loss 6.82080269 epoch total loss 6.97887611\n",
      "Trained batch 1048 batch loss 6.45804548 epoch total loss 6.97837925\n",
      "Trained batch 1049 batch loss 5.93367815 epoch total loss 6.97738314\n",
      "Trained batch 1050 batch loss 6.05558681 epoch total loss 6.97650528\n",
      "Trained batch 1051 batch loss 6.08303165 epoch total loss 6.97565508\n",
      "Trained batch 1052 batch loss 6.75309753 epoch total loss 6.97544336\n",
      "Trained batch 1053 batch loss 6.08734083 epoch total loss 6.97460032\n",
      "Trained batch 1054 batch loss 6.60580444 epoch total loss 6.97425032\n",
      "Trained batch 1055 batch loss 7.06336927 epoch total loss 6.97433472\n",
      "Trained batch 1056 batch loss 6.87236547 epoch total loss 6.9742384\n",
      "Trained batch 1057 batch loss 6.53022814 epoch total loss 6.9738183\n",
      "Trained batch 1058 batch loss 6.57747316 epoch total loss 6.97344398\n",
      "Trained batch 1059 batch loss 7.02144957 epoch total loss 6.97348928\n",
      "Trained batch 1060 batch loss 7.17770863 epoch total loss 6.97368193\n",
      "Trained batch 1061 batch loss 6.84747171 epoch total loss 6.97356319\n",
      "Trained batch 1062 batch loss 7.19655514 epoch total loss 6.97377348\n",
      "Trained batch 1063 batch loss 6.94069386 epoch total loss 6.97374249\n",
      "Trained batch 1064 batch loss 6.49303293 epoch total loss 6.97329092\n",
      "Trained batch 1065 batch loss 6.62140703 epoch total loss 6.97296047\n",
      "Trained batch 1066 batch loss 6.98586035 epoch total loss 6.97297287\n",
      "Trained batch 1067 batch loss 7.22494078 epoch total loss 6.9732089\n",
      "Trained batch 1068 batch loss 7.18732071 epoch total loss 6.97340965\n",
      "Trained batch 1069 batch loss 7.42784834 epoch total loss 6.97383451\n",
      "Trained batch 1070 batch loss 7.04906654 epoch total loss 6.97390461\n",
      "Trained batch 1071 batch loss 7.34594488 epoch total loss 6.97425175\n",
      "Trained batch 1072 batch loss 7.22231627 epoch total loss 6.97448301\n",
      "Trained batch 1073 batch loss 7.14928055 epoch total loss 6.97464609\n",
      "Trained batch 1074 batch loss 7.35805464 epoch total loss 6.97500324\n",
      "Trained batch 1075 batch loss 7.3013134 epoch total loss 6.97530651\n",
      "Trained batch 1076 batch loss 7.23168755 epoch total loss 6.97554445\n",
      "Trained batch 1077 batch loss 7.34523535 epoch total loss 6.97588778\n",
      "Trained batch 1078 batch loss 7.17865753 epoch total loss 6.97607613\n",
      "Trained batch 1079 batch loss 7.16415 epoch total loss 6.97625\n",
      "Trained batch 1080 batch loss 7.13032913 epoch total loss 6.97639275\n",
      "Trained batch 1081 batch loss 7.2467761 epoch total loss 6.97664309\n",
      "Trained batch 1082 batch loss 7.29551363 epoch total loss 6.97693729\n",
      "Trained batch 1083 batch loss 7.37192822 epoch total loss 6.97730255\n",
      "Trained batch 1084 batch loss 7.22834778 epoch total loss 6.97753429\n",
      "Trained batch 1085 batch loss 7.01155233 epoch total loss 6.97756577\n",
      "Trained batch 1086 batch loss 7.22041607 epoch total loss 6.97778893\n",
      "Trained batch 1087 batch loss 7.05245399 epoch total loss 6.97785759\n",
      "Trained batch 1088 batch loss 6.98439884 epoch total loss 6.97786331\n",
      "Trained batch 1089 batch loss 7.23115 epoch total loss 6.97809601\n",
      "Trained batch 1090 batch loss 7.24509192 epoch total loss 6.9783411\n",
      "Trained batch 1091 batch loss 7.03911209 epoch total loss 6.97839642\n",
      "Trained batch 1092 batch loss 6.64774466 epoch total loss 6.9780941\n",
      "Trained batch 1093 batch loss 6.32071161 epoch total loss 6.97749281\n",
      "Trained batch 1094 batch loss 6.38566 epoch total loss 6.9769516\n",
      "Trained batch 1095 batch loss 7.17116165 epoch total loss 6.97712946\n",
      "Trained batch 1096 batch loss 6.73494244 epoch total loss 6.97690821\n",
      "Trained batch 1097 batch loss 7.02095222 epoch total loss 6.97694826\n",
      "Trained batch 1098 batch loss 7.30399418 epoch total loss 6.97724628\n",
      "Trained batch 1099 batch loss 7.31215763 epoch total loss 6.97755098\n",
      "Trained batch 1100 batch loss 7.14240074 epoch total loss 6.97770119\n",
      "Trained batch 1101 batch loss 7.0086503 epoch total loss 6.97772932\n",
      "Trained batch 1102 batch loss 6.85247231 epoch total loss 6.97761583\n",
      "Trained batch 1103 batch loss 7.28936338 epoch total loss 6.9778986\n",
      "Trained batch 1104 batch loss 6.73058224 epoch total loss 6.97767448\n",
      "Trained batch 1105 batch loss 7.29179955 epoch total loss 6.97795868\n",
      "Trained batch 1106 batch loss 7.05675411 epoch total loss 6.97803\n",
      "Trained batch 1107 batch loss 7.01472855 epoch total loss 6.97806311\n",
      "Trained batch 1108 batch loss 6.94182253 epoch total loss 6.97803\n",
      "Trained batch 1109 batch loss 7.11974764 epoch total loss 6.978158\n",
      "Trained batch 1110 batch loss 7.22388 epoch total loss 6.97837973\n",
      "Trained batch 1111 batch loss 7.14599848 epoch total loss 6.97853041\n",
      "Trained batch 1112 batch loss 7.18018 epoch total loss 6.97871208\n",
      "Trained batch 1113 batch loss 7.06602859 epoch total loss 6.97879028\n",
      "Trained batch 1114 batch loss 6.85789394 epoch total loss 6.97868156\n",
      "Trained batch 1115 batch loss 6.4672451 epoch total loss 6.97822332\n",
      "Trained batch 1116 batch loss 6.99081659 epoch total loss 6.97823429\n",
      "Trained batch 1117 batch loss 7.30442572 epoch total loss 6.97852612\n",
      "Trained batch 1118 batch loss 7.16546345 epoch total loss 6.97869349\n",
      "Trained batch 1119 batch loss 6.94226456 epoch total loss 6.97866106\n",
      "Trained batch 1120 batch loss 7.40404 epoch total loss 6.97904062\n",
      "Trained batch 1121 batch loss 6.94988537 epoch total loss 6.9790144\n",
      "Trained batch 1122 batch loss 7.09578609 epoch total loss 6.97911835\n",
      "Trained batch 1123 batch loss 6.92234898 epoch total loss 6.9790678\n",
      "Trained batch 1124 batch loss 7.26507902 epoch total loss 6.97932243\n",
      "Trained batch 1125 batch loss 6.71653128 epoch total loss 6.97908831\n",
      "Trained batch 1126 batch loss 7.04196024 epoch total loss 6.97914457\n",
      "Trained batch 1127 batch loss 6.77893543 epoch total loss 6.97896671\n",
      "Trained batch 1128 batch loss 6.44778681 epoch total loss 6.9784956\n",
      "Trained batch 1129 batch loss 5.92289162 epoch total loss 6.97756052\n",
      "Trained batch 1130 batch loss 6.6182375 epoch total loss 6.97724247\n",
      "Trained batch 1131 batch loss 7.00056171 epoch total loss 6.97726297\n",
      "Trained batch 1132 batch loss 7.131423 epoch total loss 6.97739935\n",
      "Trained batch 1133 batch loss 7.30339718 epoch total loss 6.97768688\n",
      "Trained batch 1134 batch loss 7.22727633 epoch total loss 6.9779067\n",
      "Trained batch 1135 batch loss 7.34041405 epoch total loss 6.97822618\n",
      "Trained batch 1136 batch loss 7.02507877 epoch total loss 6.97826719\n",
      "Trained batch 1137 batch loss 7.34406233 epoch total loss 6.97858906\n",
      "Trained batch 1138 batch loss 7.41213226 epoch total loss 6.97897\n",
      "Trained batch 1139 batch loss 7.4258008 epoch total loss 6.97936249\n",
      "Trained batch 1140 batch loss 7.14148 epoch total loss 6.97950459\n",
      "Trained batch 1141 batch loss 7.13159943 epoch total loss 6.9796381\n",
      "Trained batch 1142 batch loss 6.42070436 epoch total loss 6.97914886\n",
      "Trained batch 1143 batch loss 6.30889177 epoch total loss 6.97856283\n",
      "Trained batch 1144 batch loss 6.85721159 epoch total loss 6.9784565\n",
      "Trained batch 1145 batch loss 6.89709 epoch total loss 6.97838545\n",
      "Trained batch 1146 batch loss 7.11016655 epoch total loss 6.97850084\n",
      "Trained batch 1147 batch loss 7.01923323 epoch total loss 6.97853613\n",
      "Trained batch 1148 batch loss 6.71191311 epoch total loss 6.97830391\n",
      "Trained batch 1149 batch loss 6.95035744 epoch total loss 6.97827911\n",
      "Trained batch 1150 batch loss 7.09015656 epoch total loss 6.97837687\n",
      "Trained batch 1151 batch loss 7.2728858 epoch total loss 6.97863293\n",
      "Trained batch 1152 batch loss 7.24189281 epoch total loss 6.97886086\n",
      "Trained batch 1153 batch loss 7.36836243 epoch total loss 6.97919893\n",
      "Trained batch 1154 batch loss 7.19868565 epoch total loss 6.97938871\n",
      "Trained batch 1155 batch loss 6.92970753 epoch total loss 6.9793458\n",
      "Trained batch 1156 batch loss 6.53902531 epoch total loss 6.97896528\n",
      "Trained batch 1157 batch loss 6.92274189 epoch total loss 6.97891665\n",
      "Trained batch 1158 batch loss 7.11062193 epoch total loss 6.97903061\n",
      "Trained batch 1159 batch loss 6.70963192 epoch total loss 6.97879791\n",
      "Trained batch 1160 batch loss 6.88454294 epoch total loss 6.97871685\n",
      "Trained batch 1161 batch loss 6.98432398 epoch total loss 6.97872162\n",
      "Trained batch 1162 batch loss 6.69041 epoch total loss 6.97847366\n",
      "Trained batch 1163 batch loss 6.88652802 epoch total loss 6.97839451\n",
      "Trained batch 1164 batch loss 7.03097296 epoch total loss 6.97844\n",
      "Trained batch 1165 batch loss 6.70880079 epoch total loss 6.97820854\n",
      "Trained batch 1166 batch loss 6.86293554 epoch total loss 6.97810936\n",
      "Trained batch 1167 batch loss 6.98971701 epoch total loss 6.97811937\n",
      "Trained batch 1168 batch loss 6.69773912 epoch total loss 6.97787952\n",
      "Trained batch 1169 batch loss 6.35224867 epoch total loss 6.97734404\n",
      "Trained batch 1170 batch loss 6.60361385 epoch total loss 6.97702456\n",
      "Trained batch 1171 batch loss 7.00582361 epoch total loss 6.97704935\n",
      "Trained batch 1172 batch loss 7.26622057 epoch total loss 6.97729588\n",
      "Trained batch 1173 batch loss 7.09465408 epoch total loss 6.97739601\n",
      "Trained batch 1174 batch loss 7.47436666 epoch total loss 6.97781944\n",
      "Trained batch 1175 batch loss 7.27136278 epoch total loss 6.97806931\n",
      "Trained batch 1176 batch loss 7.09596968 epoch total loss 6.97816944\n",
      "Trained batch 1177 batch loss 7.23120356 epoch total loss 6.97838449\n",
      "Trained batch 1178 batch loss 7.14706612 epoch total loss 6.97852802\n",
      "Trained batch 1179 batch loss 7.18128872 epoch total loss 6.9787\n",
      "Trained batch 1180 batch loss 7.37623072 epoch total loss 6.97903681\n",
      "Trained batch 1181 batch loss 7.32202053 epoch total loss 6.97932768\n",
      "Trained batch 1182 batch loss 7.3527174 epoch total loss 6.97964334\n",
      "Trained batch 1183 batch loss 7.28858566 epoch total loss 6.97990513\n",
      "Trained batch 1184 batch loss 7.27353811 epoch total loss 6.98015308\n",
      "Trained batch 1185 batch loss 7.13769197 epoch total loss 6.98028564\n",
      "Trained batch 1186 batch loss 7.12960196 epoch total loss 6.98041201\n",
      "Trained batch 1187 batch loss 6.80481482 epoch total loss 6.98026371\n",
      "Trained batch 1188 batch loss 7.30464506 epoch total loss 6.98053694\n",
      "Trained batch 1189 batch loss 7.33871222 epoch total loss 6.9808383\n",
      "Trained batch 1190 batch loss 7.24911 epoch total loss 6.98106384\n",
      "Trained batch 1191 batch loss 7.25074339 epoch total loss 6.98129034\n",
      "Trained batch 1192 batch loss 7.13203382 epoch total loss 6.9814167\n",
      "Trained batch 1193 batch loss 7.1756897 epoch total loss 6.98158\n",
      "Trained batch 1194 batch loss 6.56506252 epoch total loss 6.98123121\n",
      "Trained batch 1195 batch loss 6.58559656 epoch total loss 6.98090029\n",
      "Trained batch 1196 batch loss 6.71433687 epoch total loss 6.98067713\n",
      "Trained batch 1197 batch loss 6.55940151 epoch total loss 6.98032522\n",
      "Trained batch 1198 batch loss 6.56997299 epoch total loss 6.97998285\n",
      "Trained batch 1199 batch loss 6.42826796 epoch total loss 6.97952318\n",
      "Trained batch 1200 batch loss 6.17097664 epoch total loss 6.97884941\n",
      "Trained batch 1201 batch loss 6.29572868 epoch total loss 6.97828054\n",
      "Trained batch 1202 batch loss 6.74271393 epoch total loss 6.97808504\n",
      "Trained batch 1203 batch loss 7.12941933 epoch total loss 6.9782114\n",
      "Trained batch 1204 batch loss 7.29840803 epoch total loss 6.97847748\n",
      "Trained batch 1205 batch loss 6.57622147 epoch total loss 6.97814369\n",
      "Trained batch 1206 batch loss 6.75563574 epoch total loss 6.97795916\n",
      "Trained batch 1207 batch loss 6.85730028 epoch total loss 6.9778595\n",
      "Trained batch 1208 batch loss 7.21054935 epoch total loss 6.97805262\n",
      "Trained batch 1209 batch loss 6.83658791 epoch total loss 6.97793579\n",
      "Trained batch 1210 batch loss 6.82815218 epoch total loss 6.97781181\n",
      "Trained batch 1211 batch loss 6.70048618 epoch total loss 6.97758245\n",
      "Trained batch 1212 batch loss 7.08392954 epoch total loss 6.97767\n",
      "Trained batch 1213 batch loss 7.0342803 epoch total loss 6.97771692\n",
      "Trained batch 1214 batch loss 7.14386463 epoch total loss 6.97785378\n",
      "Trained batch 1215 batch loss 6.62810421 epoch total loss 6.97756577\n",
      "Trained batch 1216 batch loss 6.84392118 epoch total loss 6.97745562\n",
      "Trained batch 1217 batch loss 7.3389492 epoch total loss 6.97775269\n",
      "Trained batch 1218 batch loss 7.02898884 epoch total loss 6.97779465\n",
      "Trained batch 1219 batch loss 6.65156555 epoch total loss 6.97752714\n",
      "Trained batch 1220 batch loss 6.99990225 epoch total loss 6.97754526\n",
      "Trained batch 1221 batch loss 7.27250051 epoch total loss 6.97778702\n",
      "Trained batch 1222 batch loss 6.84401655 epoch total loss 6.97767735\n",
      "Trained batch 1223 batch loss 7.2744112 epoch total loss 6.97792\n",
      "Trained batch 1224 batch loss 7.31197548 epoch total loss 6.97819233\n",
      "Trained batch 1225 batch loss 6.56686211 epoch total loss 6.97785616\n",
      "Trained batch 1226 batch loss 6.78771925 epoch total loss 6.97770166\n",
      "Trained batch 1227 batch loss 7.00558329 epoch total loss 6.97772455\n",
      "Trained batch 1228 batch loss 6.79550171 epoch total loss 6.97757626\n",
      "Trained batch 1229 batch loss 7.14469671 epoch total loss 6.97771215\n",
      "Trained batch 1230 batch loss 7.17698765 epoch total loss 6.97787428\n",
      "Trained batch 1231 batch loss 6.76018333 epoch total loss 6.9776969\n",
      "Trained batch 1232 batch loss 7.01981306 epoch total loss 6.97773075\n",
      "Trained batch 1233 batch loss 6.6911459 epoch total loss 6.97749853\n",
      "Trained batch 1234 batch loss 7.40151548 epoch total loss 6.97784233\n",
      "Trained batch 1235 batch loss 7.13812256 epoch total loss 6.97797155\n",
      "Trained batch 1236 batch loss 6.83122253 epoch total loss 6.97785282\n",
      "Trained batch 1237 batch loss 6.6652689 epoch total loss 6.97759962\n",
      "Trained batch 1238 batch loss 6.49281406 epoch total loss 6.97720861\n",
      "Trained batch 1239 batch loss 7.19114304 epoch total loss 6.97738123\n",
      "Trained batch 1240 batch loss 6.61250305 epoch total loss 6.97708702\n",
      "Trained batch 1241 batch loss 7.31343412 epoch total loss 6.97735786\n",
      "Trained batch 1242 batch loss 6.94159079 epoch total loss 6.97732925\n",
      "Trained batch 1243 batch loss 6.73523808 epoch total loss 6.97713423\n",
      "Trained batch 1244 batch loss 6.78441668 epoch total loss 6.97697926\n",
      "Trained batch 1245 batch loss 7.05192471 epoch total loss 6.97703934\n",
      "Trained batch 1246 batch loss 6.5260334 epoch total loss 6.97667789\n",
      "Trained batch 1247 batch loss 6.99260139 epoch total loss 6.97669029\n",
      "Trained batch 1248 batch loss 6.24950647 epoch total loss 6.97610712\n",
      "Trained batch 1249 batch loss 6.71957 epoch total loss 6.9759016\n",
      "Trained batch 1250 batch loss 6.36329 epoch total loss 6.97541189\n",
      "Trained batch 1251 batch loss 6.67053032 epoch total loss 6.97516823\n",
      "Trained batch 1252 batch loss 6.91138029 epoch total loss 6.97511721\n",
      "Trained batch 1253 batch loss 6.61398315 epoch total loss 6.9748292\n",
      "Trained batch 1254 batch loss 6.82797718 epoch total loss 6.97471237\n",
      "Trained batch 1255 batch loss 7.22648287 epoch total loss 6.97491264\n",
      "Trained batch 1256 batch loss 7.25031137 epoch total loss 6.97513199\n",
      "Trained batch 1257 batch loss 7.20897532 epoch total loss 6.97531796\n",
      "Trained batch 1258 batch loss 7.04849911 epoch total loss 6.97537613\n",
      "Trained batch 1259 batch loss 6.91321564 epoch total loss 6.97532701\n",
      "Trained batch 1260 batch loss 6.50676489 epoch total loss 6.97495508\n",
      "Trained batch 1261 batch loss 6.9499836 epoch total loss 6.97493553\n",
      "Trained batch 1262 batch loss 7.38088417 epoch total loss 6.97525692\n",
      "Trained batch 1263 batch loss 7.17269135 epoch total loss 6.97541332\n",
      "Trained batch 1264 batch loss 7.44324541 epoch total loss 6.97578382\n",
      "Trained batch 1265 batch loss 7.31480742 epoch total loss 6.97605133\n",
      "Trained batch 1266 batch loss 6.21929073 epoch total loss 6.97545385\n",
      "Trained batch 1267 batch loss 6.53207159 epoch total loss 6.97510433\n",
      "Trained batch 1268 batch loss 6.63646173 epoch total loss 6.9748373\n",
      "Trained batch 1269 batch loss 7.11181784 epoch total loss 6.97494555\n",
      "Trained batch 1270 batch loss 7.36084127 epoch total loss 6.97525\n",
      "Trained batch 1271 batch loss 7.33927774 epoch total loss 6.97553587\n",
      "Trained batch 1272 batch loss 7.29304409 epoch total loss 6.97578573\n",
      "Trained batch 1273 batch loss 7.32292175 epoch total loss 6.97605848\n",
      "Trained batch 1274 batch loss 7.31468868 epoch total loss 6.97632408\n",
      "Trained batch 1275 batch loss 7.18222046 epoch total loss 6.97648573\n",
      "Trained batch 1276 batch loss 7.35258198 epoch total loss 6.97678041\n",
      "Trained batch 1277 batch loss 7.33391953 epoch total loss 6.97706032\n",
      "Trained batch 1278 batch loss 7.29507256 epoch total loss 6.97730923\n",
      "Trained batch 1279 batch loss 7.32284403 epoch total loss 6.97757959\n",
      "Trained batch 1280 batch loss 6.79602623 epoch total loss 6.9774375\n",
      "Trained batch 1281 batch loss 6.78544283 epoch total loss 6.97728729\n",
      "Trained batch 1282 batch loss 7.37377691 epoch total loss 6.97759676\n",
      "Trained batch 1283 batch loss 7.48579311 epoch total loss 6.97799253\n",
      "Trained batch 1284 batch loss 7.32061672 epoch total loss 6.97825909\n",
      "Trained batch 1285 batch loss 7.49837446 epoch total loss 6.97866392\n",
      "Trained batch 1286 batch loss 7.37245607 epoch total loss 6.97896957\n",
      "Trained batch 1287 batch loss 7.11469936 epoch total loss 6.97907495\n",
      "Trained batch 1288 batch loss 6.72826672 epoch total loss 6.97888041\n",
      "Trained batch 1289 batch loss 7.14720154 epoch total loss 6.97901106\n",
      "Trained batch 1290 batch loss 7.26380253 epoch total loss 6.97923183\n",
      "Trained batch 1291 batch loss 7.04360962 epoch total loss 6.9792819\n",
      "Trained batch 1292 batch loss 6.9751687 epoch total loss 6.97927904\n",
      "Trained batch 1293 batch loss 7.1494751 epoch total loss 6.97941065\n",
      "Trained batch 1294 batch loss 6.7025919 epoch total loss 6.97919655\n",
      "Trained batch 1295 batch loss 6.94628334 epoch total loss 6.9791708\n",
      "Trained batch 1296 batch loss 6.88446951 epoch total loss 6.97909832\n",
      "Trained batch 1297 batch loss 7.58816099 epoch total loss 6.97956753\n",
      "Trained batch 1298 batch loss 7.2145052 epoch total loss 6.97974873\n",
      "Trained batch 1299 batch loss 7.41378784 epoch total loss 6.98008299\n",
      "Trained batch 1300 batch loss 7.294909 epoch total loss 6.98032522\n",
      "Trained batch 1301 batch loss 7.11776161 epoch total loss 6.98043108\n",
      "Trained batch 1302 batch loss 6.91382885 epoch total loss 6.98038\n",
      "Trained batch 1303 batch loss 7.17183733 epoch total loss 6.9805274\n",
      "Trained batch 1304 batch loss 7.2479291 epoch total loss 6.98073244\n",
      "Trained batch 1305 batch loss 7.32537746 epoch total loss 6.98099613\n",
      "Trained batch 1306 batch loss 7.40982533 epoch total loss 6.98132515\n",
      "Trained batch 1307 batch loss 7.40598297 epoch total loss 6.98165\n",
      "Trained batch 1308 batch loss 7.30508041 epoch total loss 6.98189688\n",
      "Trained batch 1309 batch loss 7.13311863 epoch total loss 6.98201227\n",
      "Trained batch 1310 batch loss 7.16368389 epoch total loss 6.98215151\n",
      "Trained batch 1311 batch loss 7.26475096 epoch total loss 6.98236656\n",
      "Trained batch 1312 batch loss 7.35850906 epoch total loss 6.98265314\n",
      "Trained batch 1313 batch loss 7.0093236 epoch total loss 6.98267412\n",
      "Trained batch 1314 batch loss 7.17099285 epoch total loss 6.98281717\n",
      "Trained batch 1315 batch loss 6.93580198 epoch total loss 6.98278141\n",
      "Trained batch 1316 batch loss 6.64467049 epoch total loss 6.98252439\n",
      "Trained batch 1317 batch loss 6.48551464 epoch total loss 6.98214674\n",
      "Trained batch 1318 batch loss 7.0944891 epoch total loss 6.98223209\n",
      "Trained batch 1319 batch loss 7.11794 epoch total loss 6.98233509\n",
      "Trained batch 1320 batch loss 7.28449202 epoch total loss 6.98256397\n",
      "Trained batch 1321 batch loss 6.90146065 epoch total loss 6.98250246\n",
      "Trained batch 1322 batch loss 6.48336506 epoch total loss 6.98212481\n",
      "Trained batch 1323 batch loss 6.91506338 epoch total loss 6.98207426\n",
      "Trained batch 1324 batch loss 7.13130856 epoch total loss 6.98218679\n",
      "Trained batch 1325 batch loss 7.40784645 epoch total loss 6.98250818\n",
      "Trained batch 1326 batch loss 7.35653687 epoch total loss 6.98279\n",
      "Trained batch 1327 batch loss 6.75130367 epoch total loss 6.98261547\n",
      "Trained batch 1328 batch loss 6.53219175 epoch total loss 6.98227644\n",
      "Trained batch 1329 batch loss 7.18184042 epoch total loss 6.98242617\n",
      "Trained batch 1330 batch loss 7.08048964 epoch total loss 6.9824996\n",
      "Trained batch 1331 batch loss 7.18250799 epoch total loss 6.98265028\n",
      "Trained batch 1332 batch loss 7.07015705 epoch total loss 6.98271608\n",
      "Trained batch 1333 batch loss 6.52558184 epoch total loss 6.98237276\n",
      "Trained batch 1334 batch loss 6.77273 epoch total loss 6.9822154\n",
      "Trained batch 1335 batch loss 7.1245 epoch total loss 6.98232174\n",
      "Trained batch 1336 batch loss 7.26202345 epoch total loss 6.98253059\n",
      "Trained batch 1337 batch loss 7.31688213 epoch total loss 6.98278046\n",
      "Trained batch 1338 batch loss 7.40305185 epoch total loss 6.98309469\n",
      "Trained batch 1339 batch loss 7.33415222 epoch total loss 6.98335695\n",
      "Trained batch 1340 batch loss 7.30463696 epoch total loss 6.9835968\n",
      "Trained batch 1341 batch loss 7.14350891 epoch total loss 6.98371601\n",
      "Trained batch 1342 batch loss 7.15295362 epoch total loss 6.98384237\n",
      "Trained batch 1343 batch loss 7.13316059 epoch total loss 6.98395348\n",
      "Trained batch 1344 batch loss 7.31535578 epoch total loss 6.9842\n",
      "Trained batch 1345 batch loss 6.58216572 epoch total loss 6.98390102\n",
      "Trained batch 1346 batch loss 5.81937456 epoch total loss 6.98303556\n",
      "Trained batch 1347 batch loss 5.86909437 epoch total loss 6.98220873\n",
      "Trained batch 1348 batch loss 6.74064445 epoch total loss 6.98202944\n",
      "Trained batch 1349 batch loss 7.03356314 epoch total loss 6.98206711\n",
      "Trained batch 1350 batch loss 7.23090696 epoch total loss 6.98225117\n",
      "Trained batch 1351 batch loss 6.82727766 epoch total loss 6.98213625\n",
      "Trained batch 1352 batch loss 6.27009106 epoch total loss 6.98161\n",
      "Trained batch 1353 batch loss 6.76965 epoch total loss 6.98145342\n",
      "Trained batch 1354 batch loss 7.28091288 epoch total loss 6.98167467\n",
      "Trained batch 1355 batch loss 7.50211954 epoch total loss 6.98205853\n",
      "Trained batch 1356 batch loss 7.36050272 epoch total loss 6.98233747\n",
      "Trained batch 1357 batch loss 7.27550697 epoch total loss 6.98255348\n",
      "Trained batch 1358 batch loss 7.36510086 epoch total loss 6.98283529\n",
      "Trained batch 1359 batch loss 7.24399853 epoch total loss 6.98302746\n",
      "Trained batch 1360 batch loss 7.26843929 epoch total loss 6.98323774\n",
      "Trained batch 1361 batch loss 7.15016747 epoch total loss 6.98336029\n",
      "Trained batch 1362 batch loss 6.85673857 epoch total loss 6.98326731\n",
      "Trained batch 1363 batch loss 7.13585043 epoch total loss 6.98337889\n",
      "Trained batch 1364 batch loss 7.35056686 epoch total loss 6.9836483\n",
      "Trained batch 1365 batch loss 7.05596638 epoch total loss 6.98370123\n",
      "Trained batch 1366 batch loss 6.19956684 epoch total loss 6.98312664\n",
      "Trained batch 1367 batch loss 6.41228676 epoch total loss 6.98270893\n",
      "Trained batch 1368 batch loss 7.00826645 epoch total loss 6.98272753\n",
      "Trained batch 1369 batch loss 7.19947672 epoch total loss 6.98288536\n",
      "Trained batch 1370 batch loss 7.21200943 epoch total loss 6.98305273\n",
      "Trained batch 1371 batch loss 7.25235796 epoch total loss 6.98324871\n",
      "Trained batch 1372 batch loss 7.302598 epoch total loss 6.98348188\n",
      "Trained batch 1373 batch loss 6.84200287 epoch total loss 6.98337841\n",
      "Trained batch 1374 batch loss 6.65028715 epoch total loss 6.98313618\n",
      "Trained batch 1375 batch loss 7.3595295 epoch total loss 6.98341\n",
      "Trained batch 1376 batch loss 6.84183502 epoch total loss 6.98330688\n",
      "Trained batch 1377 batch loss 6.99801445 epoch total loss 6.98331738\n",
      "Trained batch 1378 batch loss 6.81438065 epoch total loss 6.98319483\n",
      "Trained batch 1379 batch loss 6.76018858 epoch total loss 6.98303318\n",
      "Trained batch 1380 batch loss 6.97715425 epoch total loss 6.98302889\n",
      "Trained batch 1381 batch loss 7.04789686 epoch total loss 6.9830761\n",
      "Trained batch 1382 batch loss 7.25387907 epoch total loss 6.98327208\n",
      "Trained batch 1383 batch loss 6.98049212 epoch total loss 6.98326969\n",
      "Trained batch 1384 batch loss 6.76578665 epoch total loss 6.98311281\n",
      "Trained batch 1385 batch loss 7.01339245 epoch total loss 6.98313475\n",
      "Trained batch 1386 batch loss 7.14643812 epoch total loss 6.98325253\n",
      "Trained batch 1387 batch loss 6.44080925 epoch total loss 6.98286104\n",
      "Trained batch 1388 batch loss 6.92794561 epoch total loss 6.98282146\n",
      "Epoch 6 train loss 6.982821464538574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 03:07:37.915044: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "2025-03-19 03:07:37.915099: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "\t [[RemoteCall]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 1 batch loss 7.19564486\n",
      "Validated batch 2 batch loss 6.98520517\n",
      "Validated batch 3 batch loss 6.91641665\n",
      "Validated batch 4 batch loss 6.5713644\n",
      "Validated batch 5 batch loss 6.80451393\n",
      "Validated batch 6 batch loss 6.98231125\n",
      "Validated batch 7 batch loss 6.88346\n",
      "Validated batch 8 batch loss 7.19304037\n",
      "Validated batch 9 batch loss 7.21389246\n",
      "Validated batch 10 batch loss 6.79742575\n",
      "Validated batch 11 batch loss 6.99807501\n",
      "Validated batch 12 batch loss 6.46326256\n",
      "Validated batch 13 batch loss 7.07143879\n",
      "Validated batch 14 batch loss 6.80502224\n",
      "Validated batch 15 batch loss 7.08075476\n",
      "Validated batch 16 batch loss 7.18957233\n",
      "Validated batch 17 batch loss 6.95847654\n",
      "Validated batch 18 batch loss 6.28852177\n",
      "Validated batch 19 batch loss 6.66972065\n",
      "Validated batch 20 batch loss 7.15359259\n",
      "Validated batch 21 batch loss 6.71017218\n",
      "Validated batch 22 batch loss 6.7308445\n",
      "Validated batch 23 batch loss 6.83270073\n",
      "Validated batch 24 batch loss 6.881001\n",
      "Validated batch 25 batch loss 6.51177025\n",
      "Validated batch 26 batch loss 7.03528309\n",
      "Validated batch 27 batch loss 7.04112959\n",
      "Validated batch 28 batch loss 6.82502651\n",
      "Validated batch 29 batch loss 7.09865713\n",
      "Validated batch 30 batch loss 7.04573298\n",
      "Validated batch 31 batch loss 7.26013803\n",
      "Validated batch 32 batch loss 6.83732843\n",
      "Validated batch 33 batch loss 7.07947826\n",
      "Validated batch 34 batch loss 6.72883701\n",
      "Validated batch 35 batch loss 6.92935801\n",
      "Validated batch 36 batch loss 7.04950333\n",
      "Validated batch 37 batch loss 7.24330521\n",
      "Validated batch 38 batch loss 7.1800518\n",
      "Validated batch 39 batch loss 6.90503168\n",
      "Validated batch 40 batch loss 7.30692148\n",
      "Validated batch 41 batch loss 6.4568944\n",
      "Validated batch 42 batch loss 7.26016712\n",
      "Validated batch 43 batch loss 7.37245607\n",
      "Validated batch 44 batch loss 7.16754484\n",
      "Validated batch 45 batch loss 7.21507215\n",
      "Validated batch 46 batch loss 6.79347372\n",
      "Validated batch 47 batch loss 6.46576786\n",
      "Validated batch 48 batch loss 7.30537128\n",
      "Validated batch 49 batch loss 7.06405544\n",
      "Validated batch 50 batch loss 7.13547802\n",
      "Validated batch 51 batch loss 6.94763708\n",
      "Validated batch 52 batch loss 7.35789251\n",
      "Validated batch 53 batch loss 6.86583233\n",
      "Validated batch 54 batch loss 7.33817053\n",
      "Validated batch 55 batch loss 6.80846405\n",
      "Validated batch 56 batch loss 6.92905283\n",
      "Validated batch 57 batch loss 6.89559412\n",
      "Validated batch 58 batch loss 6.58288908\n",
      "Validated batch 59 batch loss 6.33789444\n",
      "Validated batch 60 batch loss 7.38561583\n",
      "Validated batch 61 batch loss 7.19918251\n",
      "Validated batch 62 batch loss 6.70518827\n",
      "Validated batch 63 batch loss 7.22816706\n",
      "Validated batch 64 batch loss 7.1755414\n",
      "Validated batch 65 batch loss 7.15276623\n",
      "Validated batch 66 batch loss 7.35304356\n",
      "Validated batch 67 batch loss 7.21524191\n",
      "Validated batch 68 batch loss 6.95997381\n",
      "Validated batch 69 batch loss 6.58948374\n",
      "Validated batch 70 batch loss 7.0176177\n",
      "Validated batch 71 batch loss 6.92715454\n",
      "Validated batch 72 batch loss 6.76493263\n",
      "Validated batch 73 batch loss 6.80845\n",
      "Validated batch 74 batch loss 7.00046921\n",
      "Validated batch 75 batch loss 7.14930296\n",
      "Validated batch 76 batch loss 6.90639305\n",
      "Validated batch 77 batch loss 7.13466644\n",
      "Validated batch 78 batch loss 6.86804\n",
      "Validated batch 79 batch loss 7.25302219\n",
      "Validated batch 80 batch loss 7.38281631\n",
      "Validated batch 81 batch loss 7.07767534\n",
      "Validated batch 82 batch loss 7.00940084\n",
      "Validated batch 83 batch loss 6.87385178\n",
      "Validated batch 84 batch loss 7.17670202\n",
      "Validated batch 85 batch loss 6.94912529\n",
      "Validated batch 86 batch loss 7.18571711\n",
      "Validated batch 87 batch loss 6.82771206\n",
      "Validated batch 88 batch loss 6.97172308\n",
      "Validated batch 89 batch loss 7.04770184\n",
      "Validated batch 90 batch loss 6.82137346\n",
      "Validated batch 91 batch loss 7.0072422\n",
      "Validated batch 92 batch loss 7.06594944\n",
      "Validated batch 93 batch loss 6.90082932\n",
      "Validated batch 94 batch loss 7.04730272\n",
      "Validated batch 95 batch loss 6.79833221\n",
      "Validated batch 96 batch loss 7.27790117\n",
      "Validated batch 97 batch loss 7.37607861\n",
      "Validated batch 98 batch loss 7.2038393\n",
      "Validated batch 99 batch loss 7.2077651\n",
      "Validated batch 100 batch loss 7.03954268\n",
      "Validated batch 101 batch loss 7.2946558\n",
      "Validated batch 102 batch loss 7.25341606\n",
      "Validated batch 103 batch loss 7.42640066\n",
      "Validated batch 104 batch loss 7.35781574\n",
      "Validated batch 105 batch loss 6.57494354\n",
      "Validated batch 106 batch loss 7.23742294\n",
      "Validated batch 107 batch loss 6.98690796\n",
      "Validated batch 108 batch loss 7.02164793\n",
      "Validated batch 109 batch loss 7.06569576\n",
      "Validated batch 110 batch loss 6.30984354\n",
      "Validated batch 111 batch loss 6.66614103\n",
      "Validated batch 112 batch loss 7.29202366\n",
      "Validated batch 113 batch loss 6.94928265\n",
      "Validated batch 114 batch loss 7.38821459\n",
      "Validated batch 115 batch loss 6.96677065\n",
      "Validated batch 116 batch loss 7.49644661\n",
      "Validated batch 117 batch loss 7.03156757\n",
      "Validated batch 118 batch loss 7.16083765\n",
      "Validated batch 119 batch loss 7.10388803\n",
      "Validated batch 120 batch loss 7.17190218\n",
      "Validated batch 121 batch loss 7.23854637\n",
      "Validated batch 122 batch loss 7.02437162\n",
      "Validated batch 123 batch loss 6.78862667\n",
      "Validated batch 124 batch loss 6.7748785\n",
      "Validated batch 125 batch loss 6.83807659\n",
      "Validated batch 126 batch loss 7.14413881\n",
      "Validated batch 127 batch loss 7.04324341\n",
      "Validated batch 128 batch loss 6.75720549\n",
      "Validated batch 129 batch loss 6.69842243\n",
      "Validated batch 130 batch loss 6.87548637\n",
      "Validated batch 131 batch loss 7.2348547\n",
      "Validated batch 132 batch loss 7.24997473\n",
      "Validated batch 133 batch loss 7.13139057\n",
      "Validated batch 134 batch loss 7.45374393\n",
      "Validated batch 135 batch loss 7.54487\n",
      "Validated batch 136 batch loss 7.30968\n",
      "Validated batch 137 batch loss 6.90101767\n",
      "Validated batch 138 batch loss 6.66061735\n",
      "Validated batch 139 batch loss 7.05821609\n",
      "Validated batch 140 batch loss 7.39676714\n",
      "Validated batch 141 batch loss 7.01197529\n",
      "Validated batch 142 batch loss 6.36139297\n",
      "Validated batch 143 batch loss 6.87248945\n",
      "Validated batch 144 batch loss 6.73362446\n",
      "Validated batch 145 batch loss 6.50079775\n",
      "Validated batch 146 batch loss 6.8126173\n",
      "Validated batch 147 batch loss 6.41033792\n",
      "Validated batch 148 batch loss 7.16932964\n",
      "Validated batch 149 batch loss 6.79083252\n",
      "Validated batch 150 batch loss 6.66277218\n",
      "Validated batch 151 batch loss 6.43435669\n",
      "Validated batch 152 batch loss 7.33488655\n",
      "Validated batch 153 batch loss 6.68758774\n",
      "Validated batch 154 batch loss 7.17763424\n",
      "Validated batch 155 batch loss 6.86121464\n",
      "Validated batch 156 batch loss 7.19704723\n",
      "Validated batch 157 batch loss 6.96485615\n",
      "Validated batch 158 batch loss 6.98595524\n",
      "Validated batch 159 batch loss 7.02059555\n",
      "Validated batch 160 batch loss 6.16485262\n",
      "Validated batch 161 batch loss 7.06050444\n",
      "Validated batch 162 batch loss 7.09103394\n",
      "Validated batch 163 batch loss 6.95900249\n",
      "Validated batch 164 batch loss 6.75772762\n",
      "Validated batch 165 batch loss 6.17257786\n",
      "Validated batch 166 batch loss 6.96677208\n",
      "Validated batch 167 batch loss 7.10567713\n",
      "Validated batch 168 batch loss 6.50731325\n",
      "Validated batch 169 batch loss 7.09636545\n",
      "Validated batch 170 batch loss 7.15928555\n",
      "Validated batch 171 batch loss 7.16696739\n",
      "Validated batch 172 batch loss 7.27979231\n",
      "Validated batch 173 batch loss 6.9812088\n",
      "Validated batch 174 batch loss 6.7417469\n",
      "Validated batch 175 batch loss 7.07232761\n",
      "Validated batch 176 batch loss 7.10360479\n",
      "Validated batch 177 batch loss 7.32921\n",
      "Validated batch 178 batch loss 7.13163757\n",
      "Validated batch 179 batch loss 7.09369135\n",
      "Validated batch 180 batch loss 7.31550169\n",
      "Validated batch 181 batch loss 6.92055082\n",
      "Validated batch 182 batch loss 7.31638622\n",
      "Validated batch 183 batch loss 7.01799822\n",
      "Validated batch 184 batch loss 7.22956181\n",
      "Validated batch 185 batch loss 3.68072939\n",
      "Epoch 6 val loss 6.973454475402832\n",
      "Model /home/minho/Desktop/aiffel/pose_estimation/models/model-epoch-6-loss-6.9735.weights.h5 saved.\n",
      "Start epoch 7 with learning rate 0.001\n",
      "Start distributed training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 03:07:46.371527: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "2025-03-19 03:07:46.371576: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "\t [[RemoteCall]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1 batch loss 6.58166885 epoch total loss 6.58166885\n",
      "Trained batch 2 batch loss 6.95725584 epoch total loss 6.76946259\n",
      "Trained batch 3 batch loss 6.77617931 epoch total loss 6.77170181\n",
      "Trained batch 4 batch loss 6.99327946 epoch total loss 6.82709599\n",
      "Trained batch 5 batch loss 6.83348799 epoch total loss 6.82837439\n",
      "Trained batch 6 batch loss 6.51538 epoch total loss 6.77620888\n",
      "Trained batch 7 batch loss 6.97134733 epoch total loss 6.80408573\n",
      "Trained batch 8 batch loss 6.95554161 epoch total loss 6.82301807\n",
      "Trained batch 9 batch loss 6.65407562 epoch total loss 6.8042469\n",
      "Trained batch 10 batch loss 7.22933388 epoch total loss 6.84675503\n",
      "Trained batch 11 batch loss 6.91718721 epoch total loss 6.85315847\n",
      "Trained batch 12 batch loss 6.35027742 epoch total loss 6.81125212\n",
      "Trained batch 13 batch loss 7.19462156 epoch total loss 6.84074163\n",
      "Trained batch 14 batch loss 7.1123209 epoch total loss 6.86014032\n",
      "Trained batch 15 batch loss 7.25440121 epoch total loss 6.88642406\n",
      "Trained batch 16 batch loss 7.24985123 epoch total loss 6.90913868\n",
      "Trained batch 17 batch loss 7.29471207 epoch total loss 6.93181944\n",
      "Trained batch 18 batch loss 6.59418488 epoch total loss 6.91306162\n",
      "Trained batch 19 batch loss 7.03809118 epoch total loss 6.91964245\n",
      "Trained batch 20 batch loss 6.750453 epoch total loss 6.91118336\n",
      "Trained batch 21 batch loss 6.56889772 epoch total loss 6.89488363\n",
      "Trained batch 22 batch loss 6.91153955 epoch total loss 6.89564085\n",
      "Trained batch 23 batch loss 7.33197451 epoch total loss 6.91461182\n",
      "Trained batch 24 batch loss 7.18054152 epoch total loss 6.92569208\n",
      "Trained batch 25 batch loss 7.04285717 epoch total loss 6.93037891\n",
      "Trained batch 26 batch loss 7.12133217 epoch total loss 6.93772364\n",
      "Trained batch 27 batch loss 7.33521318 epoch total loss 6.95244551\n",
      "Trained batch 28 batch loss 6.71968365 epoch total loss 6.9441328\n",
      "Trained batch 29 batch loss 7.0777173 epoch total loss 6.94873905\n",
      "Trained batch 30 batch loss 6.78443241 epoch total loss 6.9432621\n",
      "Trained batch 31 batch loss 7.02958393 epoch total loss 6.94604683\n",
      "Trained batch 32 batch loss 7.07140064 epoch total loss 6.94996405\n",
      "Trained batch 33 batch loss 7.28431749 epoch total loss 6.96009588\n",
      "Trained batch 34 batch loss 7.38964415 epoch total loss 6.97272968\n",
      "Trained batch 35 batch loss 7.28427124 epoch total loss 6.9816308\n",
      "Trained batch 36 batch loss 7.03342915 epoch total loss 6.98307\n",
      "Trained batch 37 batch loss 7.35935402 epoch total loss 6.99324\n",
      "Trained batch 38 batch loss 7.24358654 epoch total loss 6.99982834\n",
      "Trained batch 39 batch loss 7.23095 epoch total loss 7.00575447\n",
      "Trained batch 40 batch loss 6.97899628 epoch total loss 7.00508595\n",
      "Trained batch 41 batch loss 6.4769125 epoch total loss 6.99220324\n",
      "Trained batch 42 batch loss 6.50831938 epoch total loss 6.98068237\n",
      "Trained batch 43 batch loss 7.12285423 epoch total loss 6.98398876\n",
      "Trained batch 44 batch loss 7.10224152 epoch total loss 6.98667622\n",
      "Trained batch 45 batch loss 7.22411966 epoch total loss 6.9919529\n",
      "Trained batch 46 batch loss 7.44185352 epoch total loss 7.0017333\n",
      "Trained batch 47 batch loss 7.26429 epoch total loss 7.00731945\n",
      "Trained batch 48 batch loss 7.4617734 epoch total loss 7.01678705\n",
      "Trained batch 49 batch loss 7.22344875 epoch total loss 7.02100468\n",
      "Trained batch 50 batch loss 6.9277091 epoch total loss 7.01913881\n",
      "Trained batch 51 batch loss 6.97886515 epoch total loss 7.01834869\n",
      "Trained batch 52 batch loss 7.16753244 epoch total loss 7.02121782\n",
      "Trained batch 53 batch loss 7.16146278 epoch total loss 7.02386427\n",
      "Trained batch 54 batch loss 7.29107618 epoch total loss 7.02881241\n",
      "Trained batch 55 batch loss 7.04142857 epoch total loss 7.02904224\n",
      "Trained batch 56 batch loss 7.06638908 epoch total loss 7.02970886\n",
      "Trained batch 57 batch loss 7.28850555 epoch total loss 7.03424931\n",
      "Trained batch 58 batch loss 7.11870289 epoch total loss 7.03570557\n",
      "Trained batch 59 batch loss 7.3947258 epoch total loss 7.04179049\n",
      "Trained batch 60 batch loss 7.20930815 epoch total loss 7.04458284\n",
      "Trained batch 61 batch loss 7.12336493 epoch total loss 7.04587412\n",
      "Trained batch 62 batch loss 7.11502504 epoch total loss 7.04698896\n",
      "Trained batch 63 batch loss 6.88976288 epoch total loss 7.04449368\n",
      "Trained batch 64 batch loss 7.34437943 epoch total loss 7.04917955\n",
      "Trained batch 65 batch loss 7.09507 epoch total loss 7.04988527\n",
      "Trained batch 66 batch loss 6.93910885 epoch total loss 7.04820728\n",
      "Trained batch 67 batch loss 6.38518906 epoch total loss 7.03831148\n",
      "Trained batch 68 batch loss 6.74217653 epoch total loss 7.03395653\n",
      "Trained batch 69 batch loss 6.72272158 epoch total loss 7.02944613\n",
      "Trained batch 70 batch loss 6.61468744 epoch total loss 7.02352095\n",
      "Trained batch 71 batch loss 7.09003067 epoch total loss 7.02445745\n",
      "Trained batch 72 batch loss 7.00126553 epoch total loss 7.02413511\n",
      "Trained batch 73 batch loss 7.04860687 epoch total loss 7.02447\n",
      "Trained batch 74 batch loss 6.9657321 epoch total loss 7.02367687\n",
      "Trained batch 75 batch loss 6.15491915 epoch total loss 7.01209307\n",
      "Trained batch 76 batch loss 5.78010941 epoch total loss 6.99588251\n",
      "Trained batch 77 batch loss 6.22925949 epoch total loss 6.98592615\n",
      "Trained batch 78 batch loss 6.62787151 epoch total loss 6.98133564\n",
      "Trained batch 79 batch loss 6.92688704 epoch total loss 6.98064661\n",
      "Trained batch 80 batch loss 7.07859182 epoch total loss 6.98187113\n",
      "Trained batch 81 batch loss 6.92097855 epoch total loss 6.98111916\n",
      "Trained batch 82 batch loss 6.9523716 epoch total loss 6.98076868\n",
      "Trained batch 83 batch loss 6.91867113 epoch total loss 6.980021\n",
      "Trained batch 84 batch loss 6.8364625 epoch total loss 6.97831202\n",
      "Trained batch 85 batch loss 7.13357306 epoch total loss 6.9801383\n",
      "Trained batch 86 batch loss 7.29560232 epoch total loss 6.98380661\n",
      "Trained batch 87 batch loss 7.0872674 epoch total loss 6.98499584\n",
      "Trained batch 88 batch loss 6.78326082 epoch total loss 6.98270369\n",
      "Trained batch 89 batch loss 6.19912052 epoch total loss 6.97389889\n",
      "Trained batch 90 batch loss 5.9564786 epoch total loss 6.96259403\n",
      "Trained batch 91 batch loss 6.12646294 epoch total loss 6.95340586\n",
      "Trained batch 92 batch loss 6.33051634 epoch total loss 6.94663525\n",
      "Trained batch 93 batch loss 6.7484107 epoch total loss 6.94450378\n",
      "Trained batch 94 batch loss 6.78262901 epoch total loss 6.94278193\n",
      "Trained batch 95 batch loss 7.08683777 epoch total loss 6.94429874\n",
      "Trained batch 96 batch loss 7.42325306 epoch total loss 6.94928789\n",
      "Trained batch 97 batch loss 7.2232666 epoch total loss 6.95211267\n",
      "Trained batch 98 batch loss 7.30036592 epoch total loss 6.95566607\n",
      "Trained batch 99 batch loss 7.2297864 epoch total loss 6.95843506\n",
      "Trained batch 100 batch loss 7.289361 epoch total loss 6.96174431\n",
      "Trained batch 101 batch loss 7.33376074 epoch total loss 6.9654274\n",
      "Trained batch 102 batch loss 7.20799065 epoch total loss 6.96780586\n",
      "Trained batch 103 batch loss 7.43511915 epoch total loss 6.97234297\n",
      "Trained batch 104 batch loss 6.96772 epoch total loss 6.97229815\n",
      "Trained batch 105 batch loss 7.02065182 epoch total loss 6.97275877\n",
      "Trained batch 106 batch loss 7.18971 epoch total loss 6.97480536\n",
      "Trained batch 107 batch loss 6.20979786 epoch total loss 6.96765518\n",
      "Trained batch 108 batch loss 6.81545353 epoch total loss 6.96624565\n",
      "Trained batch 109 batch loss 6.94420433 epoch total loss 6.96604395\n",
      "Trained batch 110 batch loss 6.8559761 epoch total loss 6.96504307\n",
      "Trained batch 111 batch loss 6.70020771 epoch total loss 6.96265697\n",
      "Trained batch 112 batch loss 7.04527712 epoch total loss 6.96339464\n",
      "Trained batch 113 batch loss 6.64076138 epoch total loss 6.96053934\n",
      "Trained batch 114 batch loss 7.29846668 epoch total loss 6.96350384\n",
      "Trained batch 115 batch loss 7.00761843 epoch total loss 6.96388721\n",
      "Trained batch 116 batch loss 6.82865524 epoch total loss 6.96272182\n",
      "Trained batch 117 batch loss 6.88990164 epoch total loss 6.96209908\n",
      "Trained batch 118 batch loss 7.03046417 epoch total loss 6.96267843\n",
      "Trained batch 119 batch loss 6.66307974 epoch total loss 6.96016073\n",
      "Trained batch 120 batch loss 6.87886715 epoch total loss 6.95948315\n",
      "Trained batch 121 batch loss 7.02942467 epoch total loss 6.96006155\n",
      "Trained batch 122 batch loss 7.40159512 epoch total loss 6.96368074\n",
      "Trained batch 123 batch loss 7.19518852 epoch total loss 6.96556282\n",
      "Trained batch 124 batch loss 7.18440962 epoch total loss 6.96732759\n",
      "Trained batch 125 batch loss 7.15228319 epoch total loss 6.96880722\n",
      "Trained batch 126 batch loss 7.25399494 epoch total loss 6.97107029\n",
      "Trained batch 127 batch loss 7.02177048 epoch total loss 6.97147\n",
      "Trained batch 128 batch loss 7.00078106 epoch total loss 6.97169876\n",
      "Trained batch 129 batch loss 6.74997425 epoch total loss 6.96998024\n",
      "Trained batch 130 batch loss 6.88988209 epoch total loss 6.96936417\n",
      "Trained batch 131 batch loss 7.04911 epoch total loss 6.96997309\n",
      "Trained batch 132 batch loss 7.11965466 epoch total loss 6.97110701\n",
      "Trained batch 133 batch loss 6.98285103 epoch total loss 6.97119522\n",
      "Trained batch 134 batch loss 6.22111702 epoch total loss 6.96559763\n",
      "Trained batch 135 batch loss 6.2462821 epoch total loss 6.96026945\n",
      "Trained batch 136 batch loss 6.46524096 epoch total loss 6.95662975\n",
      "Trained batch 137 batch loss 6.26901913 epoch total loss 6.95161057\n",
      "Trained batch 138 batch loss 6.7690587 epoch total loss 6.95028782\n",
      "Trained batch 139 batch loss 7.27086067 epoch total loss 6.95259428\n",
      "Trained batch 140 batch loss 7.11944103 epoch total loss 6.9537859\n",
      "Trained batch 141 batch loss 7.18536329 epoch total loss 6.95542812\n",
      "Trained batch 142 batch loss 7.0584693 epoch total loss 6.95615387\n",
      "Trained batch 143 batch loss 7.21519709 epoch total loss 6.95796537\n",
      "Trained batch 144 batch loss 6.97390509 epoch total loss 6.958076\n",
      "Trained batch 145 batch loss 6.96973515 epoch total loss 6.95815659\n",
      "Trained batch 146 batch loss 6.95861435 epoch total loss 6.95815945\n",
      "Trained batch 147 batch loss 6.92042446 epoch total loss 6.95790291\n",
      "Trained batch 148 batch loss 6.62810707 epoch total loss 6.95567465\n",
      "Trained batch 149 batch loss 6.96850681 epoch total loss 6.95576048\n",
      "Trained batch 150 batch loss 6.85869837 epoch total loss 6.95511293\n",
      "Trained batch 151 batch loss 6.39430952 epoch total loss 6.95139885\n",
      "Trained batch 152 batch loss 6.81984806 epoch total loss 6.95053339\n",
      "Trained batch 153 batch loss 6.73780584 epoch total loss 6.94914293\n",
      "Trained batch 154 batch loss 6.79403591 epoch total loss 6.94813585\n",
      "Trained batch 155 batch loss 7.06284523 epoch total loss 6.94887638\n",
      "Trained batch 156 batch loss 6.41936064 epoch total loss 6.9454813\n",
      "Trained batch 157 batch loss 7.01106882 epoch total loss 6.94589949\n",
      "Trained batch 158 batch loss 6.98280239 epoch total loss 6.94613314\n",
      "Trained batch 159 batch loss 6.85146141 epoch total loss 6.94553757\n",
      "Trained batch 160 batch loss 7.10235548 epoch total loss 6.94651699\n",
      "Trained batch 161 batch loss 7.00466108 epoch total loss 6.94687796\n",
      "Trained batch 162 batch loss 6.6233387 epoch total loss 6.94488096\n",
      "Trained batch 163 batch loss 6.81613493 epoch total loss 6.94409084\n",
      "Trained batch 164 batch loss 6.98137808 epoch total loss 6.94431829\n",
      "Trained batch 165 batch loss 6.60445404 epoch total loss 6.94225836\n",
      "Trained batch 166 batch loss 6.70097828 epoch total loss 6.94080496\n",
      "Trained batch 167 batch loss 7.21417665 epoch total loss 6.94244194\n",
      "Trained batch 168 batch loss 6.717484 epoch total loss 6.94110346\n",
      "Trained batch 169 batch loss 6.79897738 epoch total loss 6.94026232\n",
      "Trained batch 170 batch loss 7.02053738 epoch total loss 6.94073391\n",
      "Trained batch 171 batch loss 7.11658239 epoch total loss 6.94176245\n",
      "Trained batch 172 batch loss 6.74769258 epoch total loss 6.94063425\n",
      "Trained batch 173 batch loss 6.35889721 epoch total loss 6.9372716\n",
      "Trained batch 174 batch loss 6.71090698 epoch total loss 6.93597078\n",
      "Trained batch 175 batch loss 6.54111624 epoch total loss 6.93371439\n",
      "Trained batch 176 batch loss 6.89769459 epoch total loss 6.93351\n",
      "Trained batch 177 batch loss 7.21289825 epoch total loss 6.93508816\n",
      "Trained batch 178 batch loss 7.0736084 epoch total loss 6.93586636\n",
      "Trained batch 179 batch loss 7.01232386 epoch total loss 6.9362936\n",
      "Trained batch 180 batch loss 7.35771704 epoch total loss 6.9386344\n",
      "Trained batch 181 batch loss 6.73715591 epoch total loss 6.93752146\n",
      "Trained batch 182 batch loss 6.78477144 epoch total loss 6.93668222\n",
      "Trained batch 183 batch loss 6.61656284 epoch total loss 6.93493319\n",
      "Trained batch 184 batch loss 7.18498039 epoch total loss 6.93629169\n",
      "Trained batch 185 batch loss 6.97485781 epoch total loss 6.93650055\n",
      "Trained batch 186 batch loss 7.02446461 epoch total loss 6.93697309\n",
      "Trained batch 187 batch loss 7.32332039 epoch total loss 6.93903923\n",
      "Trained batch 188 batch loss 7.4414196 epoch total loss 6.94171143\n",
      "Trained batch 189 batch loss 7.30935144 epoch total loss 6.94365644\n",
      "Trained batch 190 batch loss 7.29916477 epoch total loss 6.94552755\n",
      "Trained batch 191 batch loss 7.35154247 epoch total loss 6.94765377\n",
      "Trained batch 192 batch loss 7.17842865 epoch total loss 6.94885588\n",
      "Trained batch 193 batch loss 6.92075682 epoch total loss 6.94871044\n",
      "Trained batch 194 batch loss 6.94957209 epoch total loss 6.94871473\n",
      "Trained batch 195 batch loss 7.22628307 epoch total loss 6.95013857\n",
      "Trained batch 196 batch loss 7.21979904 epoch total loss 6.95151424\n",
      "Trained batch 197 batch loss 6.7057023 epoch total loss 6.95026636\n",
      "Trained batch 198 batch loss 6.68648911 epoch total loss 6.94893456\n",
      "Trained batch 199 batch loss 7.1200881 epoch total loss 6.94979477\n",
      "Trained batch 200 batch loss 7.1663847 epoch total loss 6.95087767\n",
      "Trained batch 201 batch loss 6.98858929 epoch total loss 6.95106554\n",
      "Trained batch 202 batch loss 6.71905565 epoch total loss 6.94991732\n",
      "Trained batch 203 batch loss 6.95424223 epoch total loss 6.94993877\n",
      "Trained batch 204 batch loss 7.38946915 epoch total loss 6.9520936\n",
      "Trained batch 205 batch loss 7.41963291 epoch total loss 6.95437431\n",
      "Trained batch 206 batch loss 7.12380075 epoch total loss 6.95519686\n",
      "Trained batch 207 batch loss 6.77119541 epoch total loss 6.95430803\n",
      "Trained batch 208 batch loss 6.72578812 epoch total loss 6.9532094\n",
      "Trained batch 209 batch loss 6.62945604 epoch total loss 6.95166063\n",
      "Trained batch 210 batch loss 6.77044582 epoch total loss 6.95079756\n",
      "Trained batch 211 batch loss 6.80688667 epoch total loss 6.95011568\n",
      "Trained batch 212 batch loss 6.72579956 epoch total loss 6.94905758\n",
      "Trained batch 213 batch loss 6.81879187 epoch total loss 6.94844627\n",
      "Trained batch 214 batch loss 6.92935801 epoch total loss 6.94835663\n",
      "Trained batch 215 batch loss 6.92548561 epoch total loss 6.94825077\n",
      "Trained batch 216 batch loss 7.21133804 epoch total loss 6.94946861\n",
      "Trained batch 217 batch loss 6.77080536 epoch total loss 6.94864511\n",
      "Trained batch 218 batch loss 7.1865263 epoch total loss 6.94973612\n",
      "Trained batch 219 batch loss 7.25654078 epoch total loss 6.95113754\n",
      "Trained batch 220 batch loss 7.06438 epoch total loss 6.95165205\n",
      "Trained batch 221 batch loss 6.85854721 epoch total loss 6.95123053\n",
      "Trained batch 222 batch loss 5.90318394 epoch total loss 6.94650936\n",
      "Trained batch 223 batch loss 6.41771698 epoch total loss 6.94413853\n",
      "Trained batch 224 batch loss 6.97408724 epoch total loss 6.94427204\n",
      "Trained batch 225 batch loss 7.41399956 epoch total loss 6.94635963\n",
      "Trained batch 226 batch loss 7.23880959 epoch total loss 6.94765329\n",
      "Trained batch 227 batch loss 7.42452431 epoch total loss 6.94975424\n",
      "Trained batch 228 batch loss 7.41867971 epoch total loss 6.95181131\n",
      "Trained batch 229 batch loss 7.33878088 epoch total loss 6.95350075\n",
      "Trained batch 230 batch loss 7.44478178 epoch total loss 6.95563698\n",
      "Trained batch 231 batch loss 7.34273767 epoch total loss 6.95731306\n",
      "Trained batch 232 batch loss 7.31785536 epoch total loss 6.95886707\n",
      "Trained batch 233 batch loss 6.99389172 epoch total loss 6.95901728\n",
      "Trained batch 234 batch loss 7.18307 epoch total loss 6.95997524\n",
      "Trained batch 235 batch loss 7.24662209 epoch total loss 6.96119452\n",
      "Trained batch 236 batch loss 7.41088343 epoch total loss 6.9631\n",
      "Trained batch 237 batch loss 7.16167545 epoch total loss 6.96393776\n",
      "Trained batch 238 batch loss 7.08669567 epoch total loss 6.96445322\n",
      "Trained batch 239 batch loss 7.02243233 epoch total loss 6.96469593\n",
      "Trained batch 240 batch loss 7.45392704 epoch total loss 6.96673489\n",
      "Trained batch 241 batch loss 7.29530525 epoch total loss 6.96809816\n",
      "Trained batch 242 batch loss 7.14042377 epoch total loss 6.96881\n",
      "Trained batch 243 batch loss 6.92800713 epoch total loss 6.96864176\n",
      "Trained batch 244 batch loss 6.52449703 epoch total loss 6.96682167\n",
      "Trained batch 245 batch loss 7.0866785 epoch total loss 6.96731091\n",
      "Trained batch 246 batch loss 7.13443565 epoch total loss 6.9679904\n",
      "Trained batch 247 batch loss 7.12328434 epoch total loss 6.96861887\n",
      "Trained batch 248 batch loss 7.15120602 epoch total loss 6.96935558\n",
      "Trained batch 249 batch loss 6.88592196 epoch total loss 6.96902\n",
      "Trained batch 250 batch loss 6.64274645 epoch total loss 6.96771479\n",
      "Trained batch 251 batch loss 7.18407726 epoch total loss 6.96857691\n",
      "Trained batch 252 batch loss 7.15949488 epoch total loss 6.9693346\n",
      "Trained batch 253 batch loss 7.22945929 epoch total loss 6.97036314\n",
      "Trained batch 254 batch loss 7.40428972 epoch total loss 6.97207117\n",
      "Trained batch 255 batch loss 7.2715 epoch total loss 6.97324562\n",
      "Trained batch 256 batch loss 7.24501133 epoch total loss 6.97430706\n",
      "Trained batch 257 batch loss 7.16310692 epoch total loss 6.97504139\n",
      "Trained batch 258 batch loss 7.15717268 epoch total loss 6.97574759\n",
      "Trained batch 259 batch loss 7.1371007 epoch total loss 6.97637081\n",
      "Trained batch 260 batch loss 7.3447032 epoch total loss 6.97778749\n",
      "Trained batch 261 batch loss 7.42127275 epoch total loss 6.97948647\n",
      "Trained batch 262 batch loss 7.40016031 epoch total loss 6.98109198\n",
      "Trained batch 263 batch loss 7.25370502 epoch total loss 6.98212862\n",
      "Trained batch 264 batch loss 7.28889084 epoch total loss 6.98329067\n",
      "Trained batch 265 batch loss 7.36612177 epoch total loss 6.98473501\n",
      "Trained batch 266 batch loss 7.32169485 epoch total loss 6.98600197\n",
      "Trained batch 267 batch loss 7.36596203 epoch total loss 6.98742485\n",
      "Trained batch 268 batch loss 7.36061478 epoch total loss 6.98881721\n",
      "Trained batch 269 batch loss 7.29004574 epoch total loss 6.98993731\n",
      "Trained batch 270 batch loss 7.33099365 epoch total loss 6.9912\n",
      "Trained batch 271 batch loss 7.23772717 epoch total loss 6.99211\n",
      "Trained batch 272 batch loss 7.39428663 epoch total loss 6.99358797\n",
      "Trained batch 273 batch loss 7.29160595 epoch total loss 6.99468\n",
      "Trained batch 274 batch loss 7.19524717 epoch total loss 6.99541187\n",
      "Trained batch 275 batch loss 7.12411213 epoch total loss 6.99587965\n",
      "Trained batch 276 batch loss 7.02869034 epoch total loss 6.99599886\n",
      "Trained batch 277 batch loss 6.9658823 epoch total loss 6.99589\n",
      "Trained batch 278 batch loss 7.20348167 epoch total loss 6.99663687\n",
      "Trained batch 279 batch loss 7.02734661 epoch total loss 6.99674702\n",
      "Trained batch 280 batch loss 7.15059423 epoch total loss 6.99729681\n",
      "Trained batch 281 batch loss 6.85823774 epoch total loss 6.99680185\n",
      "Trained batch 282 batch loss 6.75504494 epoch total loss 6.9959445\n",
      "Trained batch 283 batch loss 6.82829523 epoch total loss 6.99535179\n",
      "Trained batch 284 batch loss 7.2228055 epoch total loss 6.9961524\n",
      "Trained batch 285 batch loss 7.24339819 epoch total loss 6.99702024\n",
      "Trained batch 286 batch loss 7.40932 epoch total loss 6.99846172\n",
      "Trained batch 287 batch loss 7.54148436 epoch total loss 7.00035381\n",
      "Trained batch 288 batch loss 7.26942348 epoch total loss 7.00128794\n",
      "Trained batch 289 batch loss 7.01657486 epoch total loss 7.00134087\n",
      "Trained batch 290 batch loss 6.85213184 epoch total loss 7.00082684\n",
      "Trained batch 291 batch loss 6.89411831 epoch total loss 7.00046\n",
      "Trained batch 292 batch loss 6.95160103 epoch total loss 7.00029325\n",
      "Trained batch 293 batch loss 7.2592454 epoch total loss 7.00117636\n",
      "Trained batch 294 batch loss 7.33270502 epoch total loss 7.00230455\n",
      "Trained batch 295 batch loss 7.2622385 epoch total loss 7.00318527\n",
      "Trained batch 296 batch loss 7.38631868 epoch total loss 7.00447941\n",
      "Trained batch 297 batch loss 7.29503393 epoch total loss 7.0054574\n",
      "Trained batch 298 batch loss 7.41661739 epoch total loss 7.00683689\n",
      "Trained batch 299 batch loss 7.32017565 epoch total loss 7.0078845\n",
      "Trained batch 300 batch loss 7.54919434 epoch total loss 7.00968933\n",
      "Trained batch 301 batch loss 7.45151901 epoch total loss 7.01115656\n",
      "Trained batch 302 batch loss 7.37679 epoch total loss 7.01236725\n",
      "Trained batch 303 batch loss 7.03426075 epoch total loss 7.01243925\n",
      "Trained batch 304 batch loss 7.06466866 epoch total loss 7.01261091\n",
      "Trained batch 305 batch loss 7.3678484 epoch total loss 7.01377583\n",
      "Trained batch 306 batch loss 7.31087589 epoch total loss 7.01474667\n",
      "Trained batch 307 batch loss 7.26678419 epoch total loss 7.01556778\n",
      "Trained batch 308 batch loss 7.35883379 epoch total loss 7.01668262\n",
      "Trained batch 309 batch loss 7.32008266 epoch total loss 7.01766443\n",
      "Trained batch 310 batch loss 7.32969666 epoch total loss 7.01867056\n",
      "Trained batch 311 batch loss 7.35908175 epoch total loss 7.01976538\n",
      "Trained batch 312 batch loss 7.21204519 epoch total loss 7.02038193\n",
      "Trained batch 313 batch loss 7.13523912 epoch total loss 7.02074862\n",
      "Trained batch 314 batch loss 7.09960175 epoch total loss 7.021\n",
      "Trained batch 315 batch loss 7.24012327 epoch total loss 7.02169609\n",
      "Trained batch 316 batch loss 7.01934242 epoch total loss 7.02168846\n",
      "Trained batch 317 batch loss 7.21980047 epoch total loss 7.02231312\n",
      "Trained batch 318 batch loss 6.83786821 epoch total loss 7.02173328\n",
      "Trained batch 319 batch loss 7.22680378 epoch total loss 7.02237606\n",
      "Trained batch 320 batch loss 7.16697025 epoch total loss 7.0228281\n",
      "Trained batch 321 batch loss 7.06695175 epoch total loss 7.02296495\n",
      "Trained batch 322 batch loss 6.78046656 epoch total loss 7.02221203\n",
      "Trained batch 323 batch loss 6.73405409 epoch total loss 7.02132034\n",
      "Trained batch 324 batch loss 7.2144866 epoch total loss 7.02191687\n",
      "Trained batch 325 batch loss 7.19175863 epoch total loss 7.022439\n",
      "Trained batch 326 batch loss 6.66123 epoch total loss 7.02133083\n",
      "Trained batch 327 batch loss 6.96444035 epoch total loss 7.02115679\n",
      "Trained batch 328 batch loss 7.09032488 epoch total loss 7.02136755\n",
      "Trained batch 329 batch loss 6.91072178 epoch total loss 7.0210309\n",
      "Trained batch 330 batch loss 6.73159742 epoch total loss 7.020154\n",
      "Trained batch 331 batch loss 7.28475189 epoch total loss 7.02095318\n",
      "Trained batch 332 batch loss 7.11067 epoch total loss 7.02122355\n",
      "Trained batch 333 batch loss 7.19298744 epoch total loss 7.02173901\n",
      "Trained batch 334 batch loss 7.12891436 epoch total loss 7.02205944\n",
      "Trained batch 335 batch loss 7.259583 epoch total loss 7.0227685\n",
      "Trained batch 336 batch loss 6.83881903 epoch total loss 7.02222109\n",
      "Trained batch 337 batch loss 6.24502563 epoch total loss 7.0199151\n",
      "Trained batch 338 batch loss 6.97016048 epoch total loss 7.01976824\n",
      "Trained batch 339 batch loss 7.30856085 epoch total loss 7.02062035\n",
      "Trained batch 340 batch loss 7.13930702 epoch total loss 7.02096939\n",
      "Trained batch 341 batch loss 6.95699835 epoch total loss 7.02078199\n",
      "Trained batch 342 batch loss 6.96055365 epoch total loss 7.02060556\n",
      "Trained batch 343 batch loss 6.97367668 epoch total loss 7.02046871\n",
      "Trained batch 344 batch loss 6.82144928 epoch total loss 7.01989031\n",
      "Trained batch 345 batch loss 7.38420296 epoch total loss 7.0209465\n",
      "Trained batch 346 batch loss 7.10773373 epoch total loss 7.02119732\n",
      "Trained batch 347 batch loss 6.8556242 epoch total loss 7.02072048\n",
      "Trained batch 348 batch loss 7.03004169 epoch total loss 7.02074718\n",
      "Trained batch 349 batch loss 7.0171423 epoch total loss 7.02073669\n",
      "Trained batch 350 batch loss 6.61642313 epoch total loss 7.01958132\n",
      "Trained batch 351 batch loss 6.97675228 epoch total loss 7.01945972\n",
      "Trained batch 352 batch loss 7.23390198 epoch total loss 7.02006865\n",
      "Trained batch 353 batch loss 6.87415314 epoch total loss 7.0196557\n",
      "Trained batch 354 batch loss 6.95538568 epoch total loss 7.01947403\n",
      "Trained batch 355 batch loss 6.5439105 epoch total loss 7.01813459\n",
      "Trained batch 356 batch loss 6.61800909 epoch total loss 7.01701\n",
      "Trained batch 357 batch loss 6.62817478 epoch total loss 7.01592112\n",
      "Trained batch 358 batch loss 7.24847412 epoch total loss 7.01657104\n",
      "Trained batch 359 batch loss 7.23002386 epoch total loss 7.01716518\n",
      "Trained batch 360 batch loss 7.40219688 epoch total loss 7.01823473\n",
      "Trained batch 361 batch loss 7.34913778 epoch total loss 7.01915121\n",
      "Trained batch 362 batch loss 7.44484091 epoch total loss 7.02032709\n",
      "Trained batch 363 batch loss 7.23262262 epoch total loss 7.02091217\n",
      "Trained batch 364 batch loss 6.94262886 epoch total loss 7.02069712\n",
      "Trained batch 365 batch loss 6.47362137 epoch total loss 7.01919794\n",
      "Trained batch 366 batch loss 5.97545815 epoch total loss 7.01634598\n",
      "Trained batch 367 batch loss 6.02309608 epoch total loss 7.01364\n",
      "Trained batch 368 batch loss 5.90584183 epoch total loss 7.01062918\n",
      "Trained batch 369 batch loss 6.63868189 epoch total loss 7.00962162\n",
      "Trained batch 370 batch loss 7.04979658 epoch total loss 7.00973\n",
      "Trained batch 371 batch loss 7.41956758 epoch total loss 7.01083517\n",
      "Trained batch 372 batch loss 7.29135418 epoch total loss 7.01158857\n",
      "Trained batch 373 batch loss 7.1700058 epoch total loss 7.01201344\n",
      "Trained batch 374 batch loss 7.28768253 epoch total loss 7.01275\n",
      "Trained batch 375 batch loss 6.99806595 epoch total loss 7.01271105\n",
      "Trained batch 376 batch loss 7.13838625 epoch total loss 7.01304531\n",
      "Trained batch 377 batch loss 6.75522327 epoch total loss 7.01236105\n",
      "Trained batch 378 batch loss 6.63153839 epoch total loss 7.01135397\n",
      "Trained batch 379 batch loss 6.22855282 epoch total loss 7.00928831\n",
      "Trained batch 380 batch loss 7.16675949 epoch total loss 7.00970268\n",
      "Trained batch 381 batch loss 6.90418673 epoch total loss 7.00942612\n",
      "Trained batch 382 batch loss 7.03975964 epoch total loss 7.00950527\n",
      "Trained batch 383 batch loss 6.81490278 epoch total loss 7.00899744\n",
      "Trained batch 384 batch loss 6.85336685 epoch total loss 7.00859213\n",
      "Trained batch 385 batch loss 6.95105171 epoch total loss 7.00844288\n",
      "Trained batch 386 batch loss 6.35220718 epoch total loss 7.00674295\n",
      "Trained batch 387 batch loss 6.83271027 epoch total loss 7.0062933\n",
      "Trained batch 388 batch loss 6.97188663 epoch total loss 7.00620461\n",
      "Trained batch 389 batch loss 7.0101409 epoch total loss 7.0062151\n",
      "Trained batch 390 batch loss 7.03368282 epoch total loss 7.00628567\n",
      "Trained batch 391 batch loss 7.05295134 epoch total loss 7.00640488\n",
      "Trained batch 392 batch loss 7.15065861 epoch total loss 7.00677299\n",
      "Trained batch 393 batch loss 7.13300419 epoch total loss 7.00709438\n",
      "Trained batch 394 batch loss 7.27806282 epoch total loss 7.00778198\n",
      "Trained batch 395 batch loss 7.39753485 epoch total loss 7.00876856\n",
      "Trained batch 396 batch loss 7.31241179 epoch total loss 7.00953579\n",
      "Trained batch 397 batch loss 7.06112051 epoch total loss 7.00966549\n",
      "Trained batch 398 batch loss 7.14293718 epoch total loss 7.00999975\n",
      "Trained batch 399 batch loss 7.31728172 epoch total loss 7.01077032\n",
      "Trained batch 400 batch loss 7.20741367 epoch total loss 7.01126242\n",
      "Trained batch 401 batch loss 7.22453547 epoch total loss 7.01179409\n",
      "Trained batch 402 batch loss 6.38506413 epoch total loss 7.01023531\n",
      "Trained batch 403 batch loss 6.60198784 epoch total loss 7.00922203\n",
      "Trained batch 404 batch loss 7.07536602 epoch total loss 7.00938606\n",
      "Trained batch 405 batch loss 7.37412786 epoch total loss 7.01028633\n",
      "Trained batch 406 batch loss 6.97873068 epoch total loss 7.01020861\n",
      "Trained batch 407 batch loss 7.05954123 epoch total loss 7.01033\n",
      "Trained batch 408 batch loss 7.1487155 epoch total loss 7.01066923\n",
      "Trained batch 409 batch loss 7.28846312 epoch total loss 7.01134872\n",
      "Trained batch 410 batch loss 7.13944721 epoch total loss 7.01166105\n",
      "Trained batch 411 batch loss 7.10960531 epoch total loss 7.01189947\n",
      "Trained batch 412 batch loss 6.440063 epoch total loss 7.01051092\n",
      "Trained batch 413 batch loss 6.13345623 epoch total loss 7.00838757\n",
      "Trained batch 414 batch loss 5.81670809 epoch total loss 7.0055089\n",
      "Trained batch 415 batch loss 6.60662317 epoch total loss 7.00454807\n",
      "Trained batch 416 batch loss 6.66561794 epoch total loss 7.00373316\n",
      "Trained batch 417 batch loss 7.06054163 epoch total loss 7.00386953\n",
      "Trained batch 418 batch loss 7.12287235 epoch total loss 7.00415373\n",
      "Trained batch 419 batch loss 7.12018 epoch total loss 7.00443077\n",
      "Trained batch 420 batch loss 6.93722582 epoch total loss 7.00427055\n",
      "Trained batch 421 batch loss 6.90723 epoch total loss 7.00404024\n",
      "Trained batch 422 batch loss 7.08895969 epoch total loss 7.00424099\n",
      "Trained batch 423 batch loss 6.60657883 epoch total loss 7.00330114\n",
      "Trained batch 424 batch loss 6.58365393 epoch total loss 7.00231171\n",
      "Trained batch 425 batch loss 7.054214 epoch total loss 7.00243378\n",
      "Trained batch 426 batch loss 6.90838718 epoch total loss 7.00221348\n",
      "Trained batch 427 batch loss 7.05712223 epoch total loss 7.00234175\n",
      "Trained batch 428 batch loss 6.86293268 epoch total loss 7.00201654\n",
      "Trained batch 429 batch loss 7.25823689 epoch total loss 7.00261402\n",
      "Trained batch 430 batch loss 6.83816814 epoch total loss 7.00223112\n",
      "Trained batch 431 batch loss 6.52093077 epoch total loss 7.00111485\n",
      "Trained batch 432 batch loss 6.68487263 epoch total loss 7.00038242\n",
      "Trained batch 433 batch loss 5.91111851 epoch total loss 6.99786711\n",
      "Trained batch 434 batch loss 7.07858419 epoch total loss 6.99805307\n",
      "Trained batch 435 batch loss 7.20625687 epoch total loss 6.99853182\n",
      "Trained batch 436 batch loss 6.48807573 epoch total loss 6.99736071\n",
      "Trained batch 437 batch loss 6.25160551 epoch total loss 6.99565458\n",
      "Trained batch 438 batch loss 5.76976252 epoch total loss 6.99285603\n",
      "Trained batch 439 batch loss 7.1672287 epoch total loss 6.99325323\n",
      "Trained batch 440 batch loss 6.91778278 epoch total loss 6.99308157\n",
      "Trained batch 441 batch loss 7.22753382 epoch total loss 6.99361324\n",
      "Trained batch 442 batch loss 6.78522491 epoch total loss 6.99314165\n",
      "Trained batch 443 batch loss 7.18953276 epoch total loss 6.99358463\n",
      "Trained batch 444 batch loss 7.21822119 epoch total loss 6.99409056\n",
      "Trained batch 445 batch loss 7.14164305 epoch total loss 6.99442196\n",
      "Trained batch 446 batch loss 6.22586155 epoch total loss 6.99269867\n",
      "Trained batch 447 batch loss 6.99746227 epoch total loss 6.99270964\n",
      "Trained batch 448 batch loss 7.26702356 epoch total loss 6.9933219\n",
      "Trained batch 449 batch loss 6.87063694 epoch total loss 6.99304867\n",
      "Trained batch 450 batch loss 6.90806532 epoch total loss 6.99286\n",
      "Trained batch 451 batch loss 6.99978495 epoch total loss 6.9928751\n",
      "Trained batch 452 batch loss 7.37069178 epoch total loss 6.99371052\n",
      "Trained batch 453 batch loss 7.34993362 epoch total loss 6.99449682\n",
      "Trained batch 454 batch loss 7.38239384 epoch total loss 6.99535131\n",
      "Trained batch 455 batch loss 7.19640493 epoch total loss 6.99579287\n",
      "Trained batch 456 batch loss 7.41999292 epoch total loss 6.9967227\n",
      "Trained batch 457 batch loss 7.033566 epoch total loss 6.99680328\n",
      "Trained batch 458 batch loss 7.00374365 epoch total loss 6.99681807\n",
      "Trained batch 459 batch loss 7.24096 epoch total loss 6.99735\n",
      "Trained batch 460 batch loss 7.01741695 epoch total loss 6.99739361\n",
      "Trained batch 461 batch loss 6.93576241 epoch total loss 6.99726\n",
      "Trained batch 462 batch loss 7.306005 epoch total loss 6.99792814\n",
      "Trained batch 463 batch loss 7.26021528 epoch total loss 6.99849463\n",
      "Trained batch 464 batch loss 7.24127626 epoch total loss 6.99901772\n",
      "Trained batch 465 batch loss 7.05403233 epoch total loss 6.99913597\n",
      "Trained batch 466 batch loss 7.34758949 epoch total loss 6.99988365\n",
      "Trained batch 467 batch loss 7.41366577 epoch total loss 7.00076962\n",
      "Trained batch 468 batch loss 7.51390743 epoch total loss 7.00186586\n",
      "Trained batch 469 batch loss 7.34041739 epoch total loss 7.0025878\n",
      "Trained batch 470 batch loss 7.32281256 epoch total loss 7.00326872\n",
      "Trained batch 471 batch loss 7.48002863 epoch total loss 7.00428104\n",
      "Trained batch 472 batch loss 7.26472807 epoch total loss 7.00483274\n",
      "Trained batch 473 batch loss 7.30850744 epoch total loss 7.00547504\n",
      "Trained batch 474 batch loss 6.85999 epoch total loss 7.00516796\n",
      "Trained batch 475 batch loss 7.05558681 epoch total loss 7.0052743\n",
      "Trained batch 476 batch loss 7.03011322 epoch total loss 7.00532627\n",
      "Trained batch 477 batch loss 7.02080345 epoch total loss 7.0053587\n",
      "Trained batch 478 batch loss 7.06864929 epoch total loss 7.00549126\n",
      "Trained batch 479 batch loss 7.02608109 epoch total loss 7.00553417\n",
      "Trained batch 480 batch loss 7.2397933 epoch total loss 7.00602198\n",
      "Trained batch 481 batch loss 7.04293537 epoch total loss 7.00609875\n",
      "Trained batch 482 batch loss 7.25104284 epoch total loss 7.00660706\n",
      "Trained batch 483 batch loss 7.16588068 epoch total loss 7.00693655\n",
      "Trained batch 484 batch loss 7.18894196 epoch total loss 7.00731277\n",
      "Trained batch 485 batch loss 7.31280661 epoch total loss 7.0079422\n",
      "Trained batch 486 batch loss 7.49108076 epoch total loss 7.00893641\n",
      "Trained batch 487 batch loss 7.30462503 epoch total loss 7.00954342\n",
      "Trained batch 488 batch loss 7.07655811 epoch total loss 7.00968122\n",
      "Trained batch 489 batch loss 6.39614773 epoch total loss 7.00842667\n",
      "Trained batch 490 batch loss 6.45833206 epoch total loss 7.00730371\n",
      "Trained batch 491 batch loss 6.25296211 epoch total loss 7.00576735\n",
      "Trained batch 492 batch loss 6.30043 epoch total loss 7.00433397\n",
      "Trained batch 493 batch loss 6.88235044 epoch total loss 7.00408649\n",
      "Trained batch 494 batch loss 6.65838194 epoch total loss 7.00338697\n",
      "Trained batch 495 batch loss 6.07693958 epoch total loss 7.00151539\n",
      "Trained batch 496 batch loss 6.68234396 epoch total loss 7.00087166\n",
      "Trained batch 497 batch loss 6.6945076 epoch total loss 7.00025558\n",
      "Trained batch 498 batch loss 6.28641605 epoch total loss 6.99882174\n",
      "Trained batch 499 batch loss 6.79314613 epoch total loss 6.99840975\n",
      "Trained batch 500 batch loss 7.19802427 epoch total loss 6.99880886\n",
      "Trained batch 501 batch loss 7.05245399 epoch total loss 6.99891615\n",
      "Trained batch 502 batch loss 6.90392113 epoch total loss 6.99872684\n",
      "Trained batch 503 batch loss 7.19541168 epoch total loss 6.99911737\n",
      "Trained batch 504 batch loss 7.08249187 epoch total loss 6.99928331\n",
      "Trained batch 505 batch loss 6.72787046 epoch total loss 6.99874544\n",
      "Trained batch 506 batch loss 6.73181534 epoch total loss 6.99821806\n",
      "Trained batch 507 batch loss 6.34028721 epoch total loss 6.99692059\n",
      "Trained batch 508 batch loss 7.04478216 epoch total loss 6.99701452\n",
      "Trained batch 509 batch loss 7.19091797 epoch total loss 6.99739552\n",
      "Trained batch 510 batch loss 7.28350925 epoch total loss 6.99795628\n",
      "Trained batch 511 batch loss 7.40839338 epoch total loss 6.99875975\n",
      "Trained batch 512 batch loss 7.3195591 epoch total loss 6.99938631\n",
      "Trained batch 513 batch loss 7.36804295 epoch total loss 7.00010538\n",
      "Trained batch 514 batch loss 7.46555 epoch total loss 7.00101089\n",
      "Trained batch 515 batch loss 7.01412725 epoch total loss 7.00103617\n",
      "Trained batch 516 batch loss 7.14196396 epoch total loss 7.00130939\n",
      "Trained batch 517 batch loss 7.48787403 epoch total loss 7.00225\n",
      "Trained batch 518 batch loss 7.29309225 epoch total loss 7.00281191\n",
      "Trained batch 519 batch loss 7.22261381 epoch total loss 7.00323534\n",
      "Trained batch 520 batch loss 7.26617956 epoch total loss 7.00374079\n",
      "Trained batch 521 batch loss 6.88018084 epoch total loss 7.0035038\n",
      "Trained batch 522 batch loss 6.80435705 epoch total loss 7.00312233\n",
      "Trained batch 523 batch loss 6.60906792 epoch total loss 7.00236893\n",
      "Trained batch 524 batch loss 6.55782461 epoch total loss 7.00152063\n",
      "Trained batch 525 batch loss 6.13441277 epoch total loss 6.99986935\n",
      "Trained batch 526 batch loss 6.51042366 epoch total loss 6.99893904\n",
      "Trained batch 527 batch loss 6.31451607 epoch total loss 6.99764\n",
      "Trained batch 528 batch loss 6.85741043 epoch total loss 6.99737453\n",
      "Trained batch 529 batch loss 7.18167686 epoch total loss 6.9977231\n",
      "Trained batch 530 batch loss 7.09666061 epoch total loss 6.99790955\n",
      "Trained batch 531 batch loss 7.28196478 epoch total loss 6.99844456\n",
      "Trained batch 532 batch loss 7.24370289 epoch total loss 6.99890566\n",
      "Trained batch 533 batch loss 7.09952497 epoch total loss 6.99909449\n",
      "Trained batch 534 batch loss 7.21582794 epoch total loss 6.99950027\n",
      "Trained batch 535 batch loss 6.30868483 epoch total loss 6.998209\n",
      "Trained batch 536 batch loss 6.63993216 epoch total loss 6.99754047\n",
      "Trained batch 537 batch loss 6.61916399 epoch total loss 6.99683571\n",
      "Trained batch 538 batch loss 7.20356798 epoch total loss 6.99722\n",
      "Trained batch 539 batch loss 6.95922327 epoch total loss 6.99714947\n",
      "Trained batch 540 batch loss 7.22736883 epoch total loss 6.99757576\n",
      "Trained batch 541 batch loss 7.12449932 epoch total loss 6.99781036\n",
      "Trained batch 542 batch loss 7.21290398 epoch total loss 6.99820709\n",
      "Trained batch 543 batch loss 7.12731361 epoch total loss 6.99844456\n",
      "Trained batch 544 batch loss 6.67203856 epoch total loss 6.9978447\n",
      "Trained batch 545 batch loss 6.81685781 epoch total loss 6.99751282\n",
      "Trained batch 546 batch loss 7.14954472 epoch total loss 6.99779177\n",
      "Trained batch 547 batch loss 7.14444923 epoch total loss 6.99805975\n",
      "Trained batch 548 batch loss 7.21201134 epoch total loss 6.99845028\n",
      "Trained batch 549 batch loss 7.23175287 epoch total loss 6.99887514\n",
      "Trained batch 550 batch loss 7.16893816 epoch total loss 6.99918413\n",
      "Trained batch 551 batch loss 7.0347271 epoch total loss 6.9992485\n",
      "Trained batch 552 batch loss 7.21449709 epoch total loss 6.99963856\n",
      "Trained batch 553 batch loss 7.1971488 epoch total loss 6.99999619\n",
      "Trained batch 554 batch loss 6.24492502 epoch total loss 6.99863291\n",
      "Trained batch 555 batch loss 6.54627514 epoch total loss 6.99781799\n",
      "Trained batch 556 batch loss 5.79859209 epoch total loss 6.99566126\n",
      "Trained batch 557 batch loss 7.14086056 epoch total loss 6.99592209\n",
      "Trained batch 558 batch loss 6.44267797 epoch total loss 6.99493027\n",
      "Trained batch 559 batch loss 6.61979914 epoch total loss 6.99425936\n",
      "Trained batch 560 batch loss 7.25465631 epoch total loss 6.99472427\n",
      "Trained batch 561 batch loss 7.00415373 epoch total loss 6.99474096\n",
      "Trained batch 562 batch loss 6.22742414 epoch total loss 6.99337626\n",
      "Trained batch 563 batch loss 6.94142485 epoch total loss 6.99328375\n",
      "Trained batch 564 batch loss 7.12359238 epoch total loss 6.99351454\n",
      "Trained batch 565 batch loss 7.46048975 epoch total loss 6.9943409\n",
      "Trained batch 566 batch loss 7.22451544 epoch total loss 6.99474812\n",
      "Trained batch 567 batch loss 7.26185894 epoch total loss 6.99521923\n",
      "Trained batch 568 batch loss 7.26228714 epoch total loss 6.99568939\n",
      "Trained batch 569 batch loss 7.39083672 epoch total loss 6.99638367\n",
      "Trained batch 570 batch loss 7.01924753 epoch total loss 6.9964242\n",
      "Trained batch 571 batch loss 6.74594498 epoch total loss 6.99598503\n",
      "Trained batch 572 batch loss 6.88408136 epoch total loss 6.99578953\n",
      "Trained batch 573 batch loss 5.96026516 epoch total loss 6.99398232\n",
      "Trained batch 574 batch loss 6.42752123 epoch total loss 6.99299526\n",
      "Trained batch 575 batch loss 6.59255695 epoch total loss 6.9922986\n",
      "Trained batch 576 batch loss 5.85907269 epoch total loss 6.99033165\n",
      "Trained batch 577 batch loss 5.74613953 epoch total loss 6.98817492\n",
      "Trained batch 578 batch loss 5.83681679 epoch total loss 6.98618317\n",
      "Trained batch 579 batch loss 5.9893384 epoch total loss 6.98446131\n",
      "Trained batch 580 batch loss 6.20221043 epoch total loss 6.98311281\n",
      "Trained batch 581 batch loss 7.00653601 epoch total loss 6.98315287\n",
      "Trained batch 582 batch loss 7.09134 epoch total loss 6.98333883\n",
      "Trained batch 583 batch loss 7.19407749 epoch total loss 6.98370028\n",
      "Trained batch 584 batch loss 7.49090528 epoch total loss 6.98456907\n",
      "Trained batch 585 batch loss 7.30301952 epoch total loss 6.98511314\n",
      "Trained batch 586 batch loss 6.5829 epoch total loss 6.98442698\n",
      "Trained batch 587 batch loss 5.98333073 epoch total loss 6.98272133\n",
      "Trained batch 588 batch loss 6.77749443 epoch total loss 6.98237228\n",
      "Trained batch 589 batch loss 7.08527613 epoch total loss 6.98254728\n",
      "Trained batch 590 batch loss 7.31587124 epoch total loss 6.98311186\n",
      "Trained batch 591 batch loss 7.36542559 epoch total loss 6.98375845\n",
      "Trained batch 592 batch loss 7.17349148 epoch total loss 6.98407888\n",
      "Trained batch 593 batch loss 7.13598 epoch total loss 6.98433447\n",
      "Trained batch 594 batch loss 7.03682518 epoch total loss 6.98442268\n",
      "Trained batch 595 batch loss 7.18431807 epoch total loss 6.98475838\n",
      "Trained batch 596 batch loss 7.43427515 epoch total loss 6.98551226\n",
      "Trained batch 597 batch loss 7.22791815 epoch total loss 6.98591852\n",
      "Trained batch 598 batch loss 7.27428389 epoch total loss 6.9864006\n",
      "Trained batch 599 batch loss 7.29172754 epoch total loss 6.98691034\n",
      "Trained batch 600 batch loss 7.26677799 epoch total loss 6.98737621\n",
      "Trained batch 601 batch loss 7.17893839 epoch total loss 6.98769474\n",
      "Trained batch 602 batch loss 7.29073238 epoch total loss 6.9881978\n",
      "Trained batch 603 batch loss 7.05717373 epoch total loss 6.98831224\n",
      "Trained batch 604 batch loss 7.23697948 epoch total loss 6.98872328\n",
      "Trained batch 605 batch loss 7.12264585 epoch total loss 6.98894453\n",
      "Trained batch 606 batch loss 7.38131809 epoch total loss 6.98959208\n",
      "Trained batch 607 batch loss 7.28139973 epoch total loss 6.99007273\n",
      "Trained batch 608 batch loss 7.08032799 epoch total loss 6.9902215\n",
      "Trained batch 609 batch loss 7.07513094 epoch total loss 6.99036121\n",
      "Trained batch 610 batch loss 7.11222029 epoch total loss 6.99056101\n",
      "Trained batch 611 batch loss 7.04199791 epoch total loss 6.99064493\n",
      "Trained batch 612 batch loss 7.14952087 epoch total loss 6.99090433\n",
      "Trained batch 613 batch loss 7.17798185 epoch total loss 6.99121\n",
      "Trained batch 614 batch loss 7.03141975 epoch total loss 6.99127531\n",
      "Trained batch 615 batch loss 6.8606205 epoch total loss 6.99106312\n",
      "Trained batch 616 batch loss 5.88749123 epoch total loss 6.98927212\n",
      "Trained batch 617 batch loss 6.76542425 epoch total loss 6.98890972\n",
      "Trained batch 618 batch loss 6.90809584 epoch total loss 6.98877907\n",
      "Trained batch 619 batch loss 7.08143425 epoch total loss 6.98892879\n",
      "Trained batch 620 batch loss 6.95903206 epoch total loss 6.98888063\n",
      "Trained batch 621 batch loss 7.23299026 epoch total loss 6.98927355\n",
      "Trained batch 622 batch loss 7.27029276 epoch total loss 6.98972559\n",
      "Trained batch 623 batch loss 7.27638054 epoch total loss 6.99018574\n",
      "Trained batch 624 batch loss 6.8986578 epoch total loss 6.99003887\n",
      "Trained batch 625 batch loss 6.8235364 epoch total loss 6.9897728\n",
      "Trained batch 626 batch loss 6.9318924 epoch total loss 6.98968077\n",
      "Trained batch 627 batch loss 6.9831543 epoch total loss 6.98967075\n",
      "Trained batch 628 batch loss 7.15085888 epoch total loss 6.98992729\n",
      "Trained batch 629 batch loss 6.78760529 epoch total loss 6.98960543\n",
      "Trained batch 630 batch loss 6.60199 epoch total loss 6.98899031\n",
      "Trained batch 631 batch loss 6.73077488 epoch total loss 6.98858166\n",
      "Trained batch 632 batch loss 7.23074198 epoch total loss 6.98896503\n",
      "Trained batch 633 batch loss 7.17748451 epoch total loss 6.98926258\n",
      "Trained batch 634 batch loss 7.08031178 epoch total loss 6.98940563\n",
      "Trained batch 635 batch loss 6.98702 epoch total loss 6.98940182\n",
      "Trained batch 636 batch loss 7.03313351 epoch total loss 6.98947048\n",
      "Trained batch 637 batch loss 6.69229841 epoch total loss 6.98900414\n",
      "Trained batch 638 batch loss 7.41187 epoch total loss 6.98966742\n",
      "Trained batch 639 batch loss 7.13411474 epoch total loss 6.98989344\n",
      "Trained batch 640 batch loss 6.94228745 epoch total loss 6.98981953\n",
      "Trained batch 641 batch loss 7.32970047 epoch total loss 6.99034929\n",
      "Trained batch 642 batch loss 6.65362263 epoch total loss 6.98982525\n",
      "Trained batch 643 batch loss 6.54165745 epoch total loss 6.98912811\n",
      "Trained batch 644 batch loss 6.35476494 epoch total loss 6.98814344\n",
      "Trained batch 645 batch loss 6.04141808 epoch total loss 6.98667574\n",
      "Trained batch 646 batch loss 6.30728912 epoch total loss 6.98562384\n",
      "Trained batch 647 batch loss 7.03496742 epoch total loss 6.9857\n",
      "Trained batch 648 batch loss 7.04055309 epoch total loss 6.98578501\n",
      "Trained batch 649 batch loss 7.51650238 epoch total loss 6.98660278\n",
      "Trained batch 650 batch loss 6.89893866 epoch total loss 6.98646784\n",
      "Trained batch 651 batch loss 7.37513542 epoch total loss 6.98706484\n",
      "Trained batch 652 batch loss 7.54447031 epoch total loss 6.98791933\n",
      "Trained batch 653 batch loss 7.20148 epoch total loss 6.98824692\n",
      "Trained batch 654 batch loss 6.72919178 epoch total loss 6.98785067\n",
      "Trained batch 655 batch loss 7.02071238 epoch total loss 6.98790026\n",
      "Trained batch 656 batch loss 6.73903 epoch total loss 6.98752117\n",
      "Trained batch 657 batch loss 6.60482645 epoch total loss 6.98693895\n",
      "Trained batch 658 batch loss 6.67006 epoch total loss 6.98645735\n",
      "Trained batch 659 batch loss 6.87123 epoch total loss 6.98628235\n",
      "Trained batch 660 batch loss 6.87297106 epoch total loss 6.98611069\n",
      "Trained batch 661 batch loss 7.20489502 epoch total loss 6.98644209\n",
      "Trained batch 662 batch loss 7.3452282 epoch total loss 6.98698378\n",
      "Trained batch 663 batch loss 7.26782131 epoch total loss 6.98740721\n",
      "Trained batch 664 batch loss 7.27605057 epoch total loss 6.98784161\n",
      "Trained batch 665 batch loss 7.20345688 epoch total loss 6.98816586\n",
      "Trained batch 666 batch loss 6.95199919 epoch total loss 6.98811197\n",
      "Trained batch 667 batch loss 7.18389416 epoch total loss 6.9884057\n",
      "Trained batch 668 batch loss 7.18497038 epoch total loss 6.9887\n",
      "Trained batch 669 batch loss 6.77437162 epoch total loss 6.98838\n",
      "Trained batch 670 batch loss 7.13081026 epoch total loss 6.98859262\n",
      "Trained batch 671 batch loss 6.53422546 epoch total loss 6.98791504\n",
      "Trained batch 672 batch loss 6.85319614 epoch total loss 6.98771429\n",
      "Trained batch 673 batch loss 7.10632944 epoch total loss 6.98789072\n",
      "Trained batch 674 batch loss 7.40155745 epoch total loss 6.98850441\n",
      "Trained batch 675 batch loss 7.40179 epoch total loss 6.98911667\n",
      "Trained batch 676 batch loss 7.15888691 epoch total loss 6.98936749\n",
      "Trained batch 677 batch loss 7.10157394 epoch total loss 6.98953342\n",
      "Trained batch 678 batch loss 7.04657221 epoch total loss 6.98961735\n",
      "Trained batch 679 batch loss 7.1888485 epoch total loss 6.9899106\n",
      "Trained batch 680 batch loss 7.26809931 epoch total loss 6.99031973\n",
      "Trained batch 681 batch loss 7.37732172 epoch total loss 6.99088812\n",
      "Trained batch 682 batch loss 7.29569101 epoch total loss 6.99133539\n",
      "Trained batch 683 batch loss 7.39232969 epoch total loss 6.99192238\n",
      "Trained batch 684 batch loss 7.31327248 epoch total loss 6.99239254\n",
      "Trained batch 685 batch loss 7.2899971 epoch total loss 6.99282694\n",
      "Trained batch 686 batch loss 7.23827171 epoch total loss 6.99318457\n",
      "Trained batch 687 batch loss 7.06343174 epoch total loss 6.99328709\n",
      "Trained batch 688 batch loss 6.70034504 epoch total loss 6.99286079\n",
      "Trained batch 689 batch loss 7.36732197 epoch total loss 6.99340439\n",
      "Trained batch 690 batch loss 7.47517 epoch total loss 6.99410248\n",
      "Trained batch 691 batch loss 7.28278112 epoch total loss 6.99452\n",
      "Trained batch 692 batch loss 7.21221828 epoch total loss 6.9948349\n",
      "Trained batch 693 batch loss 7.10331964 epoch total loss 6.99499178\n",
      "Trained batch 694 batch loss 6.6690979 epoch total loss 6.99452209\n",
      "Trained batch 695 batch loss 7.08414507 epoch total loss 6.99465084\n",
      "Trained batch 696 batch loss 6.97949362 epoch total loss 6.99462891\n",
      "Trained batch 697 batch loss 7.18986511 epoch total loss 6.99490929\n",
      "Trained batch 698 batch loss 7.12184334 epoch total loss 6.99509144\n",
      "Trained batch 699 batch loss 7.41432476 epoch total loss 6.9956913\n",
      "Trained batch 700 batch loss 7.34678459 epoch total loss 6.99619293\n",
      "Trained batch 701 batch loss 7.23195744 epoch total loss 6.9965291\n",
      "Trained batch 702 batch loss 7.14685345 epoch total loss 6.9967432\n",
      "Trained batch 703 batch loss 7.06817293 epoch total loss 6.99684525\n",
      "Trained batch 704 batch loss 7.56075048 epoch total loss 6.99764585\n",
      "Trained batch 705 batch loss 7.36019039 epoch total loss 6.99816036\n",
      "Trained batch 706 batch loss 7.34259 epoch total loss 6.99864864\n",
      "Trained batch 707 batch loss 6.97525597 epoch total loss 6.99861526\n",
      "Trained batch 708 batch loss 6.97526455 epoch total loss 6.99858189\n",
      "Trained batch 709 batch loss 6.48237896 epoch total loss 6.99785423\n",
      "Trained batch 710 batch loss 6.53765106 epoch total loss 6.99720573\n",
      "Trained batch 711 batch loss 7.164433 epoch total loss 6.99744129\n",
      "Trained batch 712 batch loss 6.98448515 epoch total loss 6.9974227\n",
      "Trained batch 713 batch loss 7.25812578 epoch total loss 6.99778891\n",
      "Trained batch 714 batch loss 6.8997674 epoch total loss 6.99765158\n",
      "Trained batch 715 batch loss 6.69387913 epoch total loss 6.99722672\n",
      "Trained batch 716 batch loss 6.65562487 epoch total loss 6.99675\n",
      "Trained batch 717 batch loss 6.93675 epoch total loss 6.99666595\n",
      "Trained batch 718 batch loss 6.70469379 epoch total loss 6.99625921\n",
      "Trained batch 719 batch loss 7.15982151 epoch total loss 6.99648619\n",
      "Trained batch 720 batch loss 6.97894335 epoch total loss 6.99646187\n",
      "Trained batch 721 batch loss 7.36111 epoch total loss 6.99696827\n",
      "Trained batch 722 batch loss 7.15288544 epoch total loss 6.9971838\n",
      "Trained batch 723 batch loss 7.27924919 epoch total loss 6.99757433\n",
      "Trained batch 724 batch loss 6.88528347 epoch total loss 6.99741888\n",
      "Trained batch 725 batch loss 6.71277952 epoch total loss 6.99702644\n",
      "Trained batch 726 batch loss 6.7424221 epoch total loss 6.99667549\n",
      "Trained batch 727 batch loss 6.98500395 epoch total loss 6.99665928\n",
      "Trained batch 728 batch loss 7.0270524 epoch total loss 6.99670076\n",
      "Trained batch 729 batch loss 6.8033514 epoch total loss 6.99643517\n",
      "Trained batch 730 batch loss 6.85868502 epoch total loss 6.99624681\n",
      "Trained batch 731 batch loss 7.21588802 epoch total loss 6.99654722\n",
      "Trained batch 732 batch loss 7.2344532 epoch total loss 6.99687243\n",
      "Trained batch 733 batch loss 6.1900692 epoch total loss 6.99577141\n",
      "Trained batch 734 batch loss 6.61983728 epoch total loss 6.99525881\n",
      "Trained batch 735 batch loss 7.19979906 epoch total loss 6.99553728\n",
      "Trained batch 736 batch loss 7.21655369 epoch total loss 6.99583769\n",
      "Trained batch 737 batch loss 7.45593071 epoch total loss 6.99646235\n",
      "Trained batch 738 batch loss 7.18964481 epoch total loss 6.99672365\n",
      "Trained batch 739 batch loss 7.16726351 epoch total loss 6.99695492\n",
      "Trained batch 740 batch loss 7.26792908 epoch total loss 6.99732113\n",
      "Trained batch 741 batch loss 7.00317574 epoch total loss 6.99732924\n",
      "Trained batch 742 batch loss 6.24136066 epoch total loss 6.99631\n",
      "Trained batch 743 batch loss 6.34587097 epoch total loss 6.99543476\n",
      "Trained batch 744 batch loss 6.93138933 epoch total loss 6.99534798\n",
      "Trained batch 745 batch loss 6.61894941 epoch total loss 6.99484301\n",
      "Trained batch 746 batch loss 6.67275286 epoch total loss 6.99441147\n",
      "Trained batch 747 batch loss 6.70525932 epoch total loss 6.99402428\n",
      "Trained batch 748 batch loss 6.26667309 epoch total loss 6.99305201\n",
      "Trained batch 749 batch loss 6.55670595 epoch total loss 6.99246931\n",
      "Trained batch 750 batch loss 6.20569801 epoch total loss 6.99142\n",
      "Trained batch 751 batch loss 6.84827709 epoch total loss 6.99122906\n",
      "Trained batch 752 batch loss 6.93733 epoch total loss 6.99115753\n",
      "Trained batch 753 batch loss 7.0575 epoch total loss 6.99124575\n",
      "Trained batch 754 batch loss 7.09223127 epoch total loss 6.99137974\n",
      "Trained batch 755 batch loss 7.27264452 epoch total loss 6.99175215\n",
      "Trained batch 756 batch loss 7.25832 epoch total loss 6.99210501\n",
      "Trained batch 757 batch loss 7.43444109 epoch total loss 6.99268913\n",
      "Trained batch 758 batch loss 7.436234 epoch total loss 6.99327421\n",
      "Trained batch 759 batch loss 7.11542368 epoch total loss 6.99343491\n",
      "Trained batch 760 batch loss 7.18150139 epoch total loss 6.99368238\n",
      "Trained batch 761 batch loss 7.06070328 epoch total loss 6.9937706\n",
      "Trained batch 762 batch loss 6.62033224 epoch total loss 6.99328\n",
      "Trained batch 763 batch loss 6.08443689 epoch total loss 6.99208879\n",
      "Trained batch 764 batch loss 6.60796785 epoch total loss 6.99158621\n",
      "Trained batch 765 batch loss 6.86930561 epoch total loss 6.99142599\n",
      "Trained batch 766 batch loss 5.98620033 epoch total loss 6.99011374\n",
      "Trained batch 767 batch loss 6.8166461 epoch total loss 6.98988724\n",
      "Trained batch 768 batch loss 6.67129755 epoch total loss 6.98947287\n",
      "Trained batch 769 batch loss 6.87616348 epoch total loss 6.98932505\n",
      "Trained batch 770 batch loss 7.31711197 epoch total loss 6.98975039\n",
      "Trained batch 771 batch loss 6.96970797 epoch total loss 6.98972464\n",
      "Trained batch 772 batch loss 7.28832436 epoch total loss 6.99011087\n",
      "Trained batch 773 batch loss 7.18043089 epoch total loss 6.9903574\n",
      "Trained batch 774 batch loss 6.23236 epoch total loss 6.98937845\n",
      "Trained batch 775 batch loss 5.51323032 epoch total loss 6.98747349\n",
      "Trained batch 776 batch loss 5.60741758 epoch total loss 6.98569489\n",
      "Trained batch 777 batch loss 6.8363533 epoch total loss 6.9855032\n",
      "Trained batch 778 batch loss 7.42970371 epoch total loss 6.98607397\n",
      "Trained batch 779 batch loss 7.19103718 epoch total loss 6.98633671\n",
      "Trained batch 780 batch loss 7.4125843 epoch total loss 6.98688316\n",
      "Trained batch 781 batch loss 7.24148369 epoch total loss 6.98721\n",
      "Trained batch 782 batch loss 6.86969709 epoch total loss 6.98705912\n",
      "Trained batch 783 batch loss 7.20056581 epoch total loss 6.98733234\n",
      "Trained batch 784 batch loss 7.13453722 epoch total loss 6.98752\n",
      "Trained batch 785 batch loss 6.93428326 epoch total loss 6.98745203\n",
      "Trained batch 786 batch loss 6.33895 epoch total loss 6.9866271\n",
      "Trained batch 787 batch loss 7.21002626 epoch total loss 6.98691082\n",
      "Trained batch 788 batch loss 6.79079771 epoch total loss 6.98666191\n",
      "Trained batch 789 batch loss 7.29470348 epoch total loss 6.98705292\n",
      "Trained batch 790 batch loss 7.14222 epoch total loss 6.9872489\n",
      "Trained batch 791 batch loss 7.26659441 epoch total loss 6.98760223\n",
      "Trained batch 792 batch loss 6.98351097 epoch total loss 6.98759699\n",
      "Trained batch 793 batch loss 6.89538383 epoch total loss 6.98748064\n",
      "Trained batch 794 batch loss 6.5681529 epoch total loss 6.98695278\n",
      "Trained batch 795 batch loss 6.8257432 epoch total loss 6.98675\n",
      "Trained batch 796 batch loss 7.01042652 epoch total loss 6.98677969\n",
      "Trained batch 797 batch loss 7.3299942 epoch total loss 6.98721027\n",
      "Trained batch 798 batch loss 7.22845316 epoch total loss 6.98751259\n",
      "Trained batch 799 batch loss 7.26102734 epoch total loss 6.98785543\n",
      "Trained batch 800 batch loss 7.51537561 epoch total loss 6.98851442\n",
      "Trained batch 801 batch loss 7.33418131 epoch total loss 6.98894548\n",
      "Trained batch 802 batch loss 6.87274122 epoch total loss 6.98880053\n",
      "Trained batch 803 batch loss 6.82822895 epoch total loss 6.98860025\n",
      "Trained batch 804 batch loss 6.95179272 epoch total loss 6.98855448\n",
      "Trained batch 805 batch loss 7.00795937 epoch total loss 6.98857832\n",
      "Trained batch 806 batch loss 6.99673367 epoch total loss 6.98858833\n",
      "Trained batch 807 batch loss 6.87032413 epoch total loss 6.98844147\n",
      "Trained batch 808 batch loss 6.46636 epoch total loss 6.98779535\n",
      "Trained batch 809 batch loss 6.82911825 epoch total loss 6.98759937\n",
      "Trained batch 810 batch loss 7.0441761 epoch total loss 6.98766899\n",
      "Trained batch 811 batch loss 6.56689787 epoch total loss 6.98715\n",
      "Trained batch 812 batch loss 6.44808483 epoch total loss 6.98648643\n",
      "Trained batch 813 batch loss 6.57644844 epoch total loss 6.98598242\n",
      "Trained batch 814 batch loss 6.91345787 epoch total loss 6.98589325\n",
      "Trained batch 815 batch loss 7.05271482 epoch total loss 6.98597527\n",
      "Trained batch 816 batch loss 7.10551071 epoch total loss 6.98612165\n",
      "Trained batch 817 batch loss 6.98632479 epoch total loss 6.98612213\n",
      "Trained batch 818 batch loss 7.09295559 epoch total loss 6.98625231\n",
      "Trained batch 819 batch loss 6.82495546 epoch total loss 6.98605585\n",
      "Trained batch 820 batch loss 7.11924601 epoch total loss 6.98621798\n",
      "Trained batch 821 batch loss 6.88667202 epoch total loss 6.98609686\n",
      "Trained batch 822 batch loss 7.07910204 epoch total loss 6.98621\n",
      "Trained batch 823 batch loss 6.63254929 epoch total loss 6.98578\n",
      "Trained batch 824 batch loss 6.18860865 epoch total loss 6.98481226\n",
      "Trained batch 825 batch loss 6.12079668 epoch total loss 6.98376465\n",
      "Trained batch 826 batch loss 6.87795782 epoch total loss 6.98363686\n",
      "Trained batch 827 batch loss 6.94884682 epoch total loss 6.98359442\n",
      "Trained batch 828 batch loss 6.94389248 epoch total loss 6.98354626\n",
      "Trained batch 829 batch loss 7.2844 epoch total loss 6.98390913\n",
      "Trained batch 830 batch loss 7.00668526 epoch total loss 6.98393679\n",
      "Trained batch 831 batch loss 7.1292491 epoch total loss 6.98411179\n",
      "Trained batch 832 batch loss 7.29904175 epoch total loss 6.98449\n",
      "Trained batch 833 batch loss 7.41821909 epoch total loss 6.9850111\n",
      "Trained batch 834 batch loss 7.18287039 epoch total loss 6.98524857\n",
      "Trained batch 835 batch loss 7.38784552 epoch total loss 6.98573065\n",
      "Trained batch 836 batch loss 7.15846682 epoch total loss 6.9859376\n",
      "Trained batch 837 batch loss 7.16950226 epoch total loss 6.98615646\n",
      "Trained batch 838 batch loss 6.89947271 epoch total loss 6.98605299\n",
      "Trained batch 839 batch loss 6.73491478 epoch total loss 6.98575354\n",
      "Trained batch 840 batch loss 7.11215782 epoch total loss 6.98590422\n",
      "Trained batch 841 batch loss 7.34414339 epoch total loss 6.98633051\n",
      "Trained batch 842 batch loss 6.77615404 epoch total loss 6.98608112\n",
      "Trained batch 843 batch loss 6.55153942 epoch total loss 6.98556566\n",
      "Trained batch 844 batch loss 6.35923862 epoch total loss 6.9848237\n",
      "Trained batch 845 batch loss 6.54652834 epoch total loss 6.9843049\n",
      "Trained batch 846 batch loss 7.09364 epoch total loss 6.9844346\n",
      "Trained batch 847 batch loss 6.2297163 epoch total loss 6.98354292\n",
      "Trained batch 848 batch loss 6.47616863 epoch total loss 6.98294449\n",
      "Trained batch 849 batch loss 7.4945364 epoch total loss 6.98354721\n",
      "Trained batch 850 batch loss 7.15696 epoch total loss 6.9837513\n",
      "Trained batch 851 batch loss 6.85959721 epoch total loss 6.98360491\n",
      "Trained batch 852 batch loss 7.06411839 epoch total loss 6.98369932\n",
      "Trained batch 853 batch loss 6.64123297 epoch total loss 6.98329782\n",
      "Trained batch 854 batch loss 6.8380518 epoch total loss 6.98312759\n",
      "Trained batch 855 batch loss 6.51775026 epoch total loss 6.98258305\n",
      "Trained batch 856 batch loss 6.72044945 epoch total loss 6.98227644\n",
      "Trained batch 857 batch loss 7.1061492 epoch total loss 6.98242092\n",
      "Trained batch 858 batch loss 6.36439323 epoch total loss 6.98170042\n",
      "Trained batch 859 batch loss 6.53133678 epoch total loss 6.9811759\n",
      "Trained batch 860 batch loss 6.58315563 epoch total loss 6.98071289\n",
      "Trained batch 861 batch loss 6.47799969 epoch total loss 6.98012924\n",
      "Trained batch 862 batch loss 6.94827461 epoch total loss 6.98009205\n",
      "Trained batch 863 batch loss 6.70696783 epoch total loss 6.97977543\n",
      "Trained batch 864 batch loss 7.18748379 epoch total loss 6.98001623\n",
      "Trained batch 865 batch loss 7.05959654 epoch total loss 6.98010826\n",
      "Trained batch 866 batch loss 7.29067802 epoch total loss 6.98046637\n",
      "Trained batch 867 batch loss 7.14383221 epoch total loss 6.98065519\n",
      "Trained batch 868 batch loss 7.17512131 epoch total loss 6.98087931\n",
      "Trained batch 869 batch loss 7.17987967 epoch total loss 6.98110819\n",
      "Trained batch 870 batch loss 6.86292028 epoch total loss 6.98097229\n",
      "Trained batch 871 batch loss 7.32038593 epoch total loss 6.98136187\n",
      "Trained batch 872 batch loss 6.96919346 epoch total loss 6.98134804\n",
      "Trained batch 873 batch loss 7.1841321 epoch total loss 6.98158026\n",
      "Trained batch 874 batch loss 7.26094103 epoch total loss 6.98189974\n",
      "Trained batch 875 batch loss 7.04482603 epoch total loss 6.98197174\n",
      "Trained batch 876 batch loss 6.98954105 epoch total loss 6.98198032\n",
      "Trained batch 877 batch loss 7.13006639 epoch total loss 6.98214912\n",
      "Trained batch 878 batch loss 7.2858429 epoch total loss 6.98249483\n",
      "Trained batch 879 batch loss 6.85831165 epoch total loss 6.98235369\n",
      "Trained batch 880 batch loss 6.86890936 epoch total loss 6.98222494\n",
      "Trained batch 881 batch loss 7.14577246 epoch total loss 6.98241091\n",
      "Trained batch 882 batch loss 7.26203394 epoch total loss 6.982728\n",
      "Trained batch 883 batch loss 7.45426941 epoch total loss 6.98326206\n",
      "Trained batch 884 batch loss 7.27446604 epoch total loss 6.98359108\n",
      "Trained batch 885 batch loss 7.17840958 epoch total loss 6.9838109\n",
      "Trained batch 886 batch loss 7.12131882 epoch total loss 6.98396587\n",
      "Trained batch 887 batch loss 6.97703266 epoch total loss 6.98395824\n",
      "Trained batch 888 batch loss 7.41824 epoch total loss 6.98444748\n",
      "Trained batch 889 batch loss 6.93672895 epoch total loss 6.9843936\n",
      "Trained batch 890 batch loss 7.08060169 epoch total loss 6.98450184\n",
      "Trained batch 891 batch loss 7.14612198 epoch total loss 6.98468304\n",
      "Trained batch 892 batch loss 6.16297483 epoch total loss 6.98376179\n",
      "Trained batch 893 batch loss 6.49155664 epoch total loss 6.98321104\n",
      "Trained batch 894 batch loss 6.16598368 epoch total loss 6.98229694\n",
      "Trained batch 895 batch loss 7.00300121 epoch total loss 6.98232\n",
      "Trained batch 896 batch loss 7.30248928 epoch total loss 6.98267698\n",
      "Trained batch 897 batch loss 7.4120822 epoch total loss 6.98315573\n",
      "Trained batch 898 batch loss 7.39330816 epoch total loss 6.98361206\n",
      "Trained batch 899 batch loss 7.29418421 epoch total loss 6.98395729\n",
      "Trained batch 900 batch loss 7.43184757 epoch total loss 6.98445463\n",
      "Trained batch 901 batch loss 7.06904793 epoch total loss 6.98454857\n",
      "Trained batch 902 batch loss 7.05855799 epoch total loss 6.98463058\n",
      "Trained batch 903 batch loss 6.99538517 epoch total loss 6.98464251\n",
      "Trained batch 904 batch loss 7.14541149 epoch total loss 6.98482084\n",
      "Trained batch 905 batch loss 7.1878314 epoch total loss 6.98504496\n",
      "Trained batch 906 batch loss 7.27539 epoch total loss 6.98536539\n",
      "Trained batch 907 batch loss 7.27080202 epoch total loss 6.98568058\n",
      "Trained batch 908 batch loss 7.31098604 epoch total loss 6.98603868\n",
      "Trained batch 909 batch loss 6.97116137 epoch total loss 6.98602247\n",
      "Trained batch 910 batch loss 7.31042194 epoch total loss 6.98637915\n",
      "Trained batch 911 batch loss 7.16335917 epoch total loss 6.9865737\n",
      "Trained batch 912 batch loss 7.14716911 epoch total loss 6.98674965\n",
      "Trained batch 913 batch loss 6.77212143 epoch total loss 6.98651409\n",
      "Trained batch 914 batch loss 7.10527468 epoch total loss 6.98664427\n",
      "Trained batch 915 batch loss 7.24828386 epoch total loss 6.98693\n",
      "Trained batch 916 batch loss 7.3396759 epoch total loss 6.98731518\n",
      "Trained batch 917 batch loss 7.24514246 epoch total loss 6.98759651\n",
      "Trained batch 918 batch loss 7.15266418 epoch total loss 6.98777628\n",
      "Trained batch 919 batch loss 6.86221838 epoch total loss 6.98764\n",
      "Trained batch 920 batch loss 6.46908903 epoch total loss 6.98707628\n",
      "Trained batch 921 batch loss 6.27047443 epoch total loss 6.98629856\n",
      "Trained batch 922 batch loss 6.2238493 epoch total loss 6.98547125\n",
      "Trained batch 923 batch loss 6.03068161 epoch total loss 6.98443699\n",
      "Trained batch 924 batch loss 6.05034447 epoch total loss 6.98342609\n",
      "Trained batch 925 batch loss 5.8276186 epoch total loss 6.9821763\n",
      "Trained batch 926 batch loss 6.01064682 epoch total loss 6.98112726\n",
      "Trained batch 927 batch loss 5.76319933 epoch total loss 6.97981358\n",
      "Trained batch 928 batch loss 6.81425714 epoch total loss 6.97963524\n",
      "Trained batch 929 batch loss 7.30968332 epoch total loss 6.97999048\n",
      "Trained batch 930 batch loss 6.96735382 epoch total loss 6.97997665\n",
      "Trained batch 931 batch loss 7.22030926 epoch total loss 6.98023462\n",
      "Trained batch 932 batch loss 7.0878129 epoch total loss 6.98035049\n",
      "Trained batch 933 batch loss 7.08350229 epoch total loss 6.98046112\n",
      "Trained batch 934 batch loss 7.1814518 epoch total loss 6.98067617\n",
      "Trained batch 935 batch loss 7.09636211 epoch total loss 6.98079967\n",
      "Trained batch 936 batch loss 6.88395357 epoch total loss 6.9806962\n",
      "Trained batch 937 batch loss 6.97109842 epoch total loss 6.98068619\n",
      "Trained batch 938 batch loss 6.77280235 epoch total loss 6.98046446\n",
      "Trained batch 939 batch loss 6.63254261 epoch total loss 6.98009396\n",
      "Trained batch 940 batch loss 6.86584663 epoch total loss 6.97997236\n",
      "Trained batch 941 batch loss 6.69273 epoch total loss 6.97966719\n",
      "Trained batch 942 batch loss 7.07795334 epoch total loss 6.97977161\n",
      "Trained batch 943 batch loss 7.17534208 epoch total loss 6.97997904\n",
      "Trained batch 944 batch loss 6.98473406 epoch total loss 6.97998428\n",
      "Trained batch 945 batch loss 7.29449749 epoch total loss 6.98031664\n",
      "Trained batch 946 batch loss 6.92382431 epoch total loss 6.98025703\n",
      "Trained batch 947 batch loss 6.83814907 epoch total loss 6.98010731\n",
      "Trained batch 948 batch loss 6.82870245 epoch total loss 6.97994757\n",
      "Trained batch 949 batch loss 6.4495306 epoch total loss 6.97938871\n",
      "Trained batch 950 batch loss 6.85963297 epoch total loss 6.97926283\n",
      "Trained batch 951 batch loss 7.16051435 epoch total loss 6.97945356\n",
      "Trained batch 952 batch loss 7.06416 epoch total loss 6.97954226\n",
      "Trained batch 953 batch loss 7.35690784 epoch total loss 6.97993851\n",
      "Trained batch 954 batch loss 7.13381815 epoch total loss 6.98009968\n",
      "Trained batch 955 batch loss 6.99777174 epoch total loss 6.9801178\n",
      "Trained batch 956 batch loss 7.28659964 epoch total loss 6.98043871\n",
      "Trained batch 957 batch loss 6.99503899 epoch total loss 6.98045397\n",
      "Trained batch 958 batch loss 6.84838867 epoch total loss 6.98031569\n",
      "Trained batch 959 batch loss 7.17522812 epoch total loss 6.98051929\n",
      "Trained batch 960 batch loss 7.17347956 epoch total loss 6.98072\n",
      "Trained batch 961 batch loss 6.85170746 epoch total loss 6.98058558\n",
      "Trained batch 962 batch loss 6.14060974 epoch total loss 6.97971249\n",
      "Trained batch 963 batch loss 5.69858789 epoch total loss 6.97838211\n",
      "Trained batch 964 batch loss 5.67747879 epoch total loss 6.97703266\n",
      "Trained batch 965 batch loss 6.4395709 epoch total loss 6.97647524\n",
      "Trained batch 966 batch loss 6.70758343 epoch total loss 6.97619724\n",
      "Trained batch 967 batch loss 7.25518417 epoch total loss 6.97648573\n",
      "Trained batch 968 batch loss 6.69534254 epoch total loss 6.97619534\n",
      "Trained batch 969 batch loss 7.04978 epoch total loss 6.97627115\n",
      "Trained batch 970 batch loss 7.06575871 epoch total loss 6.97636366\n",
      "Trained batch 971 batch loss 6.66439867 epoch total loss 6.97604275\n",
      "Trained batch 972 batch loss 6.28499079 epoch total loss 6.97533178\n",
      "Trained batch 973 batch loss 6.52329302 epoch total loss 6.97486734\n",
      "Trained batch 974 batch loss 6.7619276 epoch total loss 6.97464848\n",
      "Trained batch 975 batch loss 6.661901 epoch total loss 6.97432804\n",
      "Trained batch 976 batch loss 6.70172262 epoch total loss 6.97404861\n",
      "Trained batch 977 batch loss 6.95751047 epoch total loss 6.97403145\n",
      "Trained batch 978 batch loss 7.34720421 epoch total loss 6.97441339\n",
      "Trained batch 979 batch loss 6.97324276 epoch total loss 6.97441196\n",
      "Trained batch 980 batch loss 7.5412941 epoch total loss 6.97499037\n",
      "Trained batch 981 batch loss 7.18565273 epoch total loss 6.97520494\n",
      "Trained batch 982 batch loss 7.38302279 epoch total loss 6.97562027\n",
      "Trained batch 983 batch loss 7.16972828 epoch total loss 6.97581768\n",
      "Trained batch 984 batch loss 7.23023701 epoch total loss 6.9760766\n",
      "Trained batch 985 batch loss 6.40472746 epoch total loss 6.97549677\n",
      "Trained batch 986 batch loss 6.53272963 epoch total loss 6.97504759\n",
      "Trained batch 987 batch loss 6.97676086 epoch total loss 6.97504902\n",
      "Trained batch 988 batch loss 7.12162924 epoch total loss 6.97519732\n",
      "Trained batch 989 batch loss 7.17480516 epoch total loss 6.97539949\n",
      "Trained batch 990 batch loss 6.55975294 epoch total loss 6.9749794\n",
      "Trained batch 991 batch loss 6.23114252 epoch total loss 6.97422838\n",
      "Trained batch 992 batch loss 7.24550247 epoch total loss 6.97450209\n",
      "Trained batch 993 batch loss 6.89014435 epoch total loss 6.97441721\n",
      "Trained batch 994 batch loss 6.49555731 epoch total loss 6.9739356\n",
      "Trained batch 995 batch loss 6.89635658 epoch total loss 6.9738574\n",
      "Trained batch 996 batch loss 7.10413408 epoch total loss 6.97398806\n",
      "Trained batch 997 batch loss 7.21745682 epoch total loss 6.9742322\n",
      "Trained batch 998 batch loss 6.78775215 epoch total loss 6.97404528\n",
      "Trained batch 999 batch loss 6.52427387 epoch total loss 6.97359514\n",
      "Trained batch 1000 batch loss 7.15069437 epoch total loss 6.97377253\n",
      "Trained batch 1001 batch loss 7.01050091 epoch total loss 6.97380924\n",
      "Trained batch 1002 batch loss 6.63065386 epoch total loss 6.97346735\n",
      "Trained batch 1003 batch loss 6.65918 epoch total loss 6.97315359\n",
      "Trained batch 1004 batch loss 7.03596306 epoch total loss 6.97321653\n",
      "Trained batch 1005 batch loss 6.7425456 epoch total loss 6.97298717\n",
      "Trained batch 1006 batch loss 7.06042194 epoch total loss 6.97307396\n",
      "Trained batch 1007 batch loss 7.16583729 epoch total loss 6.97326565\n",
      "Trained batch 1008 batch loss 7.069242 epoch total loss 6.97336102\n",
      "Trained batch 1009 batch loss 7.10419083 epoch total loss 6.97349072\n",
      "Trained batch 1010 batch loss 6.83291912 epoch total loss 6.97335148\n",
      "Trained batch 1011 batch loss 6.80814266 epoch total loss 6.97318792\n",
      "Trained batch 1012 batch loss 7.25841665 epoch total loss 6.97346973\n",
      "Trained batch 1013 batch loss 7.03455496 epoch total loss 6.97353029\n",
      "Trained batch 1014 batch loss 7.3324132 epoch total loss 6.97388411\n",
      "Trained batch 1015 batch loss 7.28091526 epoch total loss 6.97418642\n",
      "Trained batch 1016 batch loss 7.19165802 epoch total loss 6.974401\n",
      "Trained batch 1017 batch loss 6.82622433 epoch total loss 6.97425508\n",
      "Trained batch 1018 batch loss 6.75779152 epoch total loss 6.97404242\n",
      "Trained batch 1019 batch loss 6.87934828 epoch total loss 6.97394943\n",
      "Trained batch 1020 batch loss 7.09223223 epoch total loss 6.97406578\n",
      "Trained batch 1021 batch loss 6.95475578 epoch total loss 6.97404671\n",
      "Trained batch 1022 batch loss 6.89578915 epoch total loss 6.97397\n",
      "Trained batch 1023 batch loss 7.01579523 epoch total loss 6.97401094\n",
      "Trained batch 1024 batch loss 7.19093466 epoch total loss 6.97422266\n",
      "Trained batch 1025 batch loss 7.27767944 epoch total loss 6.97451878\n",
      "Trained batch 1026 batch loss 7.3026619 epoch total loss 6.97483873\n",
      "Trained batch 1027 batch loss 7.24430847 epoch total loss 6.97510099\n",
      "Trained batch 1028 batch loss 6.99005079 epoch total loss 6.97511578\n",
      "Trained batch 1029 batch loss 7.33095 epoch total loss 6.97546148\n",
      "Trained batch 1030 batch loss 7.3627553 epoch total loss 6.97583771\n",
      "Trained batch 1031 batch loss 7.31084299 epoch total loss 6.97616291\n",
      "Trained batch 1032 batch loss 6.6172514 epoch total loss 6.97581482\n",
      "Trained batch 1033 batch loss 7.11347485 epoch total loss 6.97594786\n",
      "Trained batch 1034 batch loss 7.11467743 epoch total loss 6.97608232\n",
      "Trained batch 1035 batch loss 7.28030729 epoch total loss 6.97637606\n",
      "Trained batch 1036 batch loss 7.33722591 epoch total loss 6.97672462\n",
      "Trained batch 1037 batch loss 7.47609 epoch total loss 6.97720623\n",
      "Trained batch 1038 batch loss 7.48836422 epoch total loss 6.97769833\n",
      "Trained batch 1039 batch loss 7.5339241 epoch total loss 6.97823381\n",
      "Trained batch 1040 batch loss 6.9595089 epoch total loss 6.97821569\n",
      "Trained batch 1041 batch loss 7.04224777 epoch total loss 6.97827721\n",
      "Trained batch 1042 batch loss 6.90441561 epoch total loss 6.97820616\n",
      "Trained batch 1043 batch loss 6.52303 epoch total loss 6.97777\n",
      "Trained batch 1044 batch loss 7.17245102 epoch total loss 6.9779563\n",
      "Trained batch 1045 batch loss 7.13414764 epoch total loss 6.97810602\n",
      "Trained batch 1046 batch loss 7.33350611 epoch total loss 6.97844553\n",
      "Trained batch 1047 batch loss 7.3231535 epoch total loss 6.97877502\n",
      "Trained batch 1048 batch loss 7.18418741 epoch total loss 6.978971\n",
      "Trained batch 1049 batch loss 6.50176334 epoch total loss 6.9785161\n",
      "Trained batch 1050 batch loss 7.15560341 epoch total loss 6.9786849\n",
      "Trained batch 1051 batch loss 6.93574 epoch total loss 6.97864389\n",
      "Trained batch 1052 batch loss 6.81205797 epoch total loss 6.97848558\n",
      "Trained batch 1053 batch loss 6.86107874 epoch total loss 6.97837353\n",
      "Trained batch 1054 batch loss 6.3054738 epoch total loss 6.97773552\n",
      "Trained batch 1055 batch loss 6.41468096 epoch total loss 6.97720146\n",
      "Trained batch 1056 batch loss 6.78701735 epoch total loss 6.97702169\n",
      "Trained batch 1057 batch loss 7.22585392 epoch total loss 6.97725725\n",
      "Trained batch 1058 batch loss 7.27967787 epoch total loss 6.97754335\n",
      "Trained batch 1059 batch loss 7.36795139 epoch total loss 6.97791195\n",
      "Trained batch 1060 batch loss 7.39850426 epoch total loss 6.97830868\n",
      "Trained batch 1061 batch loss 6.88392925 epoch total loss 6.97821951\n",
      "Trained batch 1062 batch loss 6.37951088 epoch total loss 6.97765589\n",
      "Trained batch 1063 batch loss 7.04166698 epoch total loss 6.97771597\n",
      "Trained batch 1064 batch loss 6.44845533 epoch total loss 6.97721815\n",
      "Trained batch 1065 batch loss 6.68063164 epoch total loss 6.97693968\n",
      "Trained batch 1066 batch loss 6.80932236 epoch total loss 6.97678232\n",
      "Trained batch 1067 batch loss 6.67667675 epoch total loss 6.97650099\n",
      "Trained batch 1068 batch loss 6.14206791 epoch total loss 6.97572\n",
      "Trained batch 1069 batch loss 6.5314908 epoch total loss 6.97530413\n",
      "Trained batch 1070 batch loss 6.09161139 epoch total loss 6.97447824\n",
      "Trained batch 1071 batch loss 6.33414555 epoch total loss 6.97388029\n",
      "Trained batch 1072 batch loss 6.81595 epoch total loss 6.97373295\n",
      "Trained batch 1073 batch loss 7.15326357 epoch total loss 6.97390032\n",
      "Trained batch 1074 batch loss 7.28517866 epoch total loss 6.97419\n",
      "Trained batch 1075 batch loss 6.68588543 epoch total loss 6.97392225\n",
      "Trained batch 1076 batch loss 6.37963772 epoch total loss 6.9733696\n",
      "Trained batch 1077 batch loss 6.72866917 epoch total loss 6.97314215\n",
      "Trained batch 1078 batch loss 7.11329889 epoch total loss 6.97327232\n",
      "Trained batch 1079 batch loss 6.88521147 epoch total loss 6.97319078\n",
      "Trained batch 1080 batch loss 7.15693331 epoch total loss 6.97336054\n",
      "Trained batch 1081 batch loss 6.75276 epoch total loss 6.97315693\n",
      "Trained batch 1082 batch loss 7.05522251 epoch total loss 6.97323275\n",
      "Trained batch 1083 batch loss 6.97988367 epoch total loss 6.97323895\n",
      "Trained batch 1084 batch loss 7.04775047 epoch total loss 6.97330761\n",
      "Trained batch 1085 batch loss 6.11632872 epoch total loss 6.97251749\n",
      "Trained batch 1086 batch loss 5.88879776 epoch total loss 6.97151947\n",
      "Trained batch 1087 batch loss 6.38475466 epoch total loss 6.97097969\n",
      "Trained batch 1088 batch loss 6.30041647 epoch total loss 6.97036362\n",
      "Trained batch 1089 batch loss 7.20931339 epoch total loss 6.97058296\n",
      "Trained batch 1090 batch loss 7.20892715 epoch total loss 6.97080183\n",
      "Trained batch 1091 batch loss 6.43483782 epoch total loss 6.97031069\n",
      "Trained batch 1092 batch loss 6.40875244 epoch total loss 6.96979618\n",
      "Trained batch 1093 batch loss 7.03412771 epoch total loss 6.96985531\n",
      "Trained batch 1094 batch loss 7.38706303 epoch total loss 6.97023678\n",
      "Trained batch 1095 batch loss 7.39055586 epoch total loss 6.97062063\n",
      "Trained batch 1096 batch loss 7.38391066 epoch total loss 6.97099781\n",
      "Trained batch 1097 batch loss 7.32653618 epoch total loss 6.97132206\n",
      "Trained batch 1098 batch loss 7.33176136 epoch total loss 6.97165\n",
      "Trained batch 1099 batch loss 7.04297161 epoch total loss 6.97171497\n",
      "Trained batch 1100 batch loss 7.3904171 epoch total loss 6.97209549\n",
      "Trained batch 1101 batch loss 7.21823072 epoch total loss 6.97231913\n",
      "Trained batch 1102 batch loss 7.18794346 epoch total loss 6.97251511\n",
      "Trained batch 1103 batch loss 7.04383945 epoch total loss 6.97257948\n",
      "Trained batch 1104 batch loss 7.35369587 epoch total loss 6.97292471\n",
      "Trained batch 1105 batch loss 6.485 epoch total loss 6.97248316\n",
      "Trained batch 1106 batch loss 6.54994535 epoch total loss 6.97210073\n",
      "Trained batch 1107 batch loss 6.40574121 epoch total loss 6.97158909\n",
      "Trained batch 1108 batch loss 6.74914885 epoch total loss 6.97138834\n",
      "Trained batch 1109 batch loss 6.9590373 epoch total loss 6.97137737\n",
      "Trained batch 1110 batch loss 7.38541031 epoch total loss 6.97175026\n",
      "Trained batch 1111 batch loss 6.94688129 epoch total loss 6.97172737\n",
      "Trained batch 1112 batch loss 7.05213261 epoch total loss 6.9718\n",
      "Trained batch 1113 batch loss 7.08659649 epoch total loss 6.97190285\n",
      "Trained batch 1114 batch loss 6.77104759 epoch total loss 6.9717226\n",
      "Trained batch 1115 batch loss 6.57278585 epoch total loss 6.97136497\n",
      "Trained batch 1116 batch loss 6.85158491 epoch total loss 6.97125769\n",
      "Trained batch 1117 batch loss 7.1359272 epoch total loss 6.97140455\n",
      "Trained batch 1118 batch loss 7.32795334 epoch total loss 6.97172403\n",
      "Trained batch 1119 batch loss 7.25484896 epoch total loss 6.97197676\n",
      "Trained batch 1120 batch loss 7.00928926 epoch total loss 6.97201\n",
      "Trained batch 1121 batch loss 6.93864155 epoch total loss 6.97198\n",
      "Trained batch 1122 batch loss 6.96904278 epoch total loss 6.97197771\n",
      "Trained batch 1123 batch loss 7.04619455 epoch total loss 6.97204399\n",
      "Trained batch 1124 batch loss 7.39210558 epoch total loss 6.97241783\n",
      "Trained batch 1125 batch loss 6.97729635 epoch total loss 6.97242212\n",
      "Trained batch 1126 batch loss 6.64041233 epoch total loss 6.97212744\n",
      "Trained batch 1127 batch loss 6.75504208 epoch total loss 6.9719348\n",
      "Trained batch 1128 batch loss 6.93848419 epoch total loss 6.97190523\n",
      "Trained batch 1129 batch loss 6.70335484 epoch total loss 6.97166729\n",
      "Trained batch 1130 batch loss 6.64998531 epoch total loss 6.97138262\n",
      "Trained batch 1131 batch loss 7.17447138 epoch total loss 6.97156191\n",
      "Trained batch 1132 batch loss 6.98712206 epoch total loss 6.97157574\n",
      "Trained batch 1133 batch loss 6.29445171 epoch total loss 6.97097826\n",
      "Trained batch 1134 batch loss 6.8738265 epoch total loss 6.97089243\n",
      "Trained batch 1135 batch loss 6.98970938 epoch total loss 6.97090912\n",
      "Trained batch 1136 batch loss 7.05200338 epoch total loss 6.97098064\n",
      "Trained batch 1137 batch loss 7.04084206 epoch total loss 6.97104216\n",
      "Trained batch 1138 batch loss 7.11588383 epoch total loss 6.97116947\n",
      "Trained batch 1139 batch loss 7.26942396 epoch total loss 6.97143126\n",
      "Trained batch 1140 batch loss 7.28372288 epoch total loss 6.97170544\n",
      "Trained batch 1141 batch loss 7.2433033 epoch total loss 6.97194338\n",
      "Trained batch 1142 batch loss 6.82978487 epoch total loss 6.97181845\n",
      "Trained batch 1143 batch loss 6.72045708 epoch total loss 6.97159863\n",
      "Trained batch 1144 batch loss 7.10336781 epoch total loss 6.97171402\n",
      "Trained batch 1145 batch loss 7.10436296 epoch total loss 6.97183\n",
      "Trained batch 1146 batch loss 7.03154135 epoch total loss 6.97188187\n",
      "Trained batch 1147 batch loss 7.08590889 epoch total loss 6.97198153\n",
      "Trained batch 1148 batch loss 6.76234913 epoch total loss 6.9717989\n",
      "Trained batch 1149 batch loss 6.77577162 epoch total loss 6.97162819\n",
      "Trained batch 1150 batch loss 7.09901237 epoch total loss 6.97173929\n",
      "Trained batch 1151 batch loss 6.80001402 epoch total loss 6.97158957\n",
      "Trained batch 1152 batch loss 6.8409524 epoch total loss 6.97147608\n",
      "Trained batch 1153 batch loss 6.98139048 epoch total loss 6.97148466\n",
      "Trained batch 1154 batch loss 6.57220316 epoch total loss 6.97113895\n",
      "Trained batch 1155 batch loss 7.37821245 epoch total loss 6.97149134\n",
      "Trained batch 1156 batch loss 7.26685524 epoch total loss 6.9717474\n",
      "Trained batch 1157 batch loss 7.05396891 epoch total loss 6.97181845\n",
      "Trained batch 1158 batch loss 7.09316206 epoch total loss 6.97192335\n",
      "Trained batch 1159 batch loss 7.02418947 epoch total loss 6.97196865\n",
      "Trained batch 1160 batch loss 6.92280817 epoch total loss 6.97192621\n",
      "Trained batch 1161 batch loss 7.15355444 epoch total loss 6.97208261\n",
      "Trained batch 1162 batch loss 6.87378407 epoch total loss 6.97199821\n",
      "Trained batch 1163 batch loss 7.0679 epoch total loss 6.97208071\n",
      "Trained batch 1164 batch loss 7.02627563 epoch total loss 6.97212744\n",
      "Trained batch 1165 batch loss 7.14880848 epoch total loss 6.97227907\n",
      "Trained batch 1166 batch loss 7.16646957 epoch total loss 6.97244549\n",
      "Trained batch 1167 batch loss 7.35297394 epoch total loss 6.97277164\n",
      "Trained batch 1168 batch loss 6.80786276 epoch total loss 6.97263098\n",
      "Trained batch 1169 batch loss 6.86133957 epoch total loss 6.97253561\n",
      "Trained batch 1170 batch loss 7.07341576 epoch total loss 6.97262144\n",
      "Trained batch 1171 batch loss 6.90763807 epoch total loss 6.97256613\n",
      "Trained batch 1172 batch loss 7.16436672 epoch total loss 6.97273\n",
      "Trained batch 1173 batch loss 6.45620155 epoch total loss 6.97228956\n",
      "Trained batch 1174 batch loss 6.83753872 epoch total loss 6.97217464\n",
      "Trained batch 1175 batch loss 6.5792408 epoch total loss 6.97184\n",
      "Trained batch 1176 batch loss 6.57020617 epoch total loss 6.97149849\n",
      "Trained batch 1177 batch loss 6.99065542 epoch total loss 6.9715147\n",
      "Trained batch 1178 batch loss 6.4851141 epoch total loss 6.97110176\n",
      "Trained batch 1179 batch loss 6.11467028 epoch total loss 6.97037506\n",
      "Trained batch 1180 batch loss 5.92528 epoch total loss 6.9694891\n",
      "Trained batch 1181 batch loss 6.00899887 epoch total loss 6.96867561\n",
      "Trained batch 1182 batch loss 6.63785076 epoch total loss 6.96839571\n",
      "Trained batch 1183 batch loss 6.18328571 epoch total loss 6.96773195\n",
      "Trained batch 1184 batch loss 6.46532822 epoch total loss 6.96730757\n",
      "Trained batch 1185 batch loss 6.93177843 epoch total loss 6.96727753\n",
      "Trained batch 1186 batch loss 6.55559921 epoch total loss 6.96693039\n",
      "Trained batch 1187 batch loss 6.76685953 epoch total loss 6.96676159\n",
      "Trained batch 1188 batch loss 6.48575497 epoch total loss 6.96635628\n",
      "Trained batch 1189 batch loss 6.92694616 epoch total loss 6.9663229\n",
      "Trained batch 1190 batch loss 7.00507212 epoch total loss 6.96635532\n",
      "Trained batch 1191 batch loss 7.26235867 epoch total loss 6.96660423\n",
      "Trained batch 1192 batch loss 7.25525475 epoch total loss 6.96684599\n",
      "Trained batch 1193 batch loss 7.37568808 epoch total loss 6.96718884\n",
      "Trained batch 1194 batch loss 7.14892673 epoch total loss 6.96734142\n",
      "Trained batch 1195 batch loss 7.29011154 epoch total loss 6.96761179\n",
      "Trained batch 1196 batch loss 7.3922019 epoch total loss 6.96796703\n",
      "Trained batch 1197 batch loss 7.19414282 epoch total loss 6.96815586\n",
      "Trained batch 1198 batch loss 7.43191385 epoch total loss 6.96854305\n",
      "Trained batch 1199 batch loss 7.20878744 epoch total loss 6.96874332\n",
      "Trained batch 1200 batch loss 6.6283989 epoch total loss 6.96845961\n",
      "Trained batch 1201 batch loss 7.10014248 epoch total loss 6.96856928\n",
      "Trained batch 1202 batch loss 7.01479864 epoch total loss 6.9686079\n",
      "Trained batch 1203 batch loss 7.27938461 epoch total loss 6.96886587\n",
      "Trained batch 1204 batch loss 6.61610889 epoch total loss 6.96857309\n",
      "Trained batch 1205 batch loss 6.62457561 epoch total loss 6.96828794\n",
      "Trained batch 1206 batch loss 6.99826717 epoch total loss 6.96831274\n",
      "Trained batch 1207 batch loss 7.05982637 epoch total loss 6.96838856\n",
      "Trained batch 1208 batch loss 6.95495462 epoch total loss 6.96837711\n",
      "Trained batch 1209 batch loss 7.04113436 epoch total loss 6.96843719\n",
      "Trained batch 1210 batch loss 6.94385195 epoch total loss 6.96841764\n",
      "Trained batch 1211 batch loss 6.84937954 epoch total loss 6.96831942\n",
      "Trained batch 1212 batch loss 7.21138906 epoch total loss 6.96851969\n",
      "Trained batch 1213 batch loss 7.20698 epoch total loss 6.96871614\n",
      "Trained batch 1214 batch loss 6.38954 epoch total loss 6.96823931\n",
      "Trained batch 1215 batch loss 6.22791529 epoch total loss 6.96762943\n",
      "Trained batch 1216 batch loss 6.21934 epoch total loss 6.96701431\n",
      "Trained batch 1217 batch loss 6.55447245 epoch total loss 6.96667576\n",
      "Trained batch 1218 batch loss 7.08685064 epoch total loss 6.96677446\n",
      "Trained batch 1219 batch loss 7.33577871 epoch total loss 6.96707726\n",
      "Trained batch 1220 batch loss 7.00800371 epoch total loss 6.96711063\n",
      "Trained batch 1221 batch loss 7.08670616 epoch total loss 6.96720886\n",
      "Trained batch 1222 batch loss 6.22234154 epoch total loss 6.96659946\n",
      "Trained batch 1223 batch loss 6.78721571 epoch total loss 6.9664526\n",
      "Trained batch 1224 batch loss 7.20368719 epoch total loss 6.96664667\n",
      "Trained batch 1225 batch loss 7.19432068 epoch total loss 6.96683264\n",
      "Trained batch 1226 batch loss 7.04416227 epoch total loss 6.96689558\n",
      "Trained batch 1227 batch loss 6.89665508 epoch total loss 6.96683836\n",
      "Trained batch 1228 batch loss 6.95225811 epoch total loss 6.96682644\n",
      "Trained batch 1229 batch loss 7.07909584 epoch total loss 6.96691751\n",
      "Trained batch 1230 batch loss 6.81415272 epoch total loss 6.96679354\n",
      "Trained batch 1231 batch loss 6.99970961 epoch total loss 6.96682072\n",
      "Trained batch 1232 batch loss 7.23799181 epoch total loss 6.96704102\n",
      "Trained batch 1233 batch loss 6.96501207 epoch total loss 6.96703911\n",
      "Trained batch 1234 batch loss 7.06264305 epoch total loss 6.96711636\n",
      "Trained batch 1235 batch loss 7.07925797 epoch total loss 6.96720743\n",
      "Trained batch 1236 batch loss 6.90533113 epoch total loss 6.96715736\n",
      "Trained batch 1237 batch loss 6.56385946 epoch total loss 6.96683073\n",
      "Trained batch 1238 batch loss 7.2109127 epoch total loss 6.96702814\n",
      "Trained batch 1239 batch loss 7.50577641 epoch total loss 6.96746302\n",
      "Trained batch 1240 batch loss 7.3591876 epoch total loss 6.96777916\n",
      "Trained batch 1241 batch loss 7.3941412 epoch total loss 6.96812296\n",
      "Trained batch 1242 batch loss 7.3572464 epoch total loss 6.96843624\n",
      "Trained batch 1243 batch loss 7.32141066 epoch total loss 6.96872\n",
      "Trained batch 1244 batch loss 6.95160198 epoch total loss 6.96870613\n",
      "Trained batch 1245 batch loss 6.79488611 epoch total loss 6.96856642\n",
      "Trained batch 1246 batch loss 7.03925467 epoch total loss 6.96862316\n",
      "Trained batch 1247 batch loss 7.05324793 epoch total loss 6.96869135\n",
      "Trained batch 1248 batch loss 6.72951078 epoch total loss 6.96849966\n",
      "Trained batch 1249 batch loss 6.98917389 epoch total loss 6.96851635\n",
      "Trained batch 1250 batch loss 7.04227161 epoch total loss 6.968575\n",
      "Trained batch 1251 batch loss 6.79073715 epoch total loss 6.9684329\n",
      "Trained batch 1252 batch loss 7.0965848 epoch total loss 6.96853542\n",
      "Trained batch 1253 batch loss 7.1047883 epoch total loss 6.96864414\n",
      "Trained batch 1254 batch loss 7.34419823 epoch total loss 6.96894312\n",
      "Trained batch 1255 batch loss 7.08328056 epoch total loss 6.96903419\n",
      "Trained batch 1256 batch loss 7.42565298 epoch total loss 6.96939754\n",
      "Trained batch 1257 batch loss 7.47182465 epoch total loss 6.96979713\n",
      "Trained batch 1258 batch loss 7.37272 epoch total loss 6.97011805\n",
      "Trained batch 1259 batch loss 7.02396584 epoch total loss 6.97016096\n",
      "Trained batch 1260 batch loss 6.73662424 epoch total loss 6.96997547\n",
      "Trained batch 1261 batch loss 6.75497484 epoch total loss 6.96980476\n",
      "Trained batch 1262 batch loss 7.11752033 epoch total loss 6.96992159\n",
      "Trained batch 1263 batch loss 7.43238592 epoch total loss 6.9702878\n",
      "Trained batch 1264 batch loss 7.5264411 epoch total loss 6.97072792\n",
      "Trained batch 1265 batch loss 7.21355963 epoch total loss 6.97092\n",
      "Trained batch 1266 batch loss 6.30438042 epoch total loss 6.97039366\n",
      "Trained batch 1267 batch loss 6.41328764 epoch total loss 6.96995401\n",
      "Trained batch 1268 batch loss 6.79598618 epoch total loss 6.96981668\n",
      "Trained batch 1269 batch loss 7.33465624 epoch total loss 6.97010422\n",
      "Trained batch 1270 batch loss 6.98314238 epoch total loss 6.97011471\n",
      "Trained batch 1271 batch loss 7.34960175 epoch total loss 6.97041368\n",
      "Trained batch 1272 batch loss 7.34836674 epoch total loss 6.97071075\n",
      "Trained batch 1273 batch loss 7.20736694 epoch total loss 6.97089624\n",
      "Trained batch 1274 batch loss 7.24490356 epoch total loss 6.97111177\n",
      "Trained batch 1275 batch loss 7.30282164 epoch total loss 6.97137165\n",
      "Trained batch 1276 batch loss 7.31831169 epoch total loss 6.97164392\n",
      "Trained batch 1277 batch loss 7.29705143 epoch total loss 6.97189856\n",
      "Trained batch 1278 batch loss 7.27608681 epoch total loss 6.9721365\n",
      "Trained batch 1279 batch loss 7.10307741 epoch total loss 6.97223949\n",
      "Trained batch 1280 batch loss 7.10951853 epoch total loss 6.97234631\n",
      "Trained batch 1281 batch loss 7.08672667 epoch total loss 6.97243595\n",
      "Trained batch 1282 batch loss 7.28789234 epoch total loss 6.972682\n",
      "Trained batch 1283 batch loss 7.45508671 epoch total loss 6.97305822\n",
      "Trained batch 1284 batch loss 7.1408267 epoch total loss 6.97318888\n",
      "Trained batch 1285 batch loss 7.23988914 epoch total loss 6.9733963\n",
      "Trained batch 1286 batch loss 7.22065926 epoch total loss 6.97358894\n",
      "Trained batch 1287 batch loss 7.03029203 epoch total loss 6.97363281\n",
      "Trained batch 1288 batch loss 7.2585144 epoch total loss 6.97385406\n",
      "Trained batch 1289 batch loss 7.30363464 epoch total loss 6.97411\n",
      "Trained batch 1290 batch loss 7.14643192 epoch total loss 6.97424364\n",
      "Trained batch 1291 batch loss 7.40005 epoch total loss 6.97457361\n",
      "Trained batch 1292 batch loss 7.12405396 epoch total loss 6.97468948\n",
      "Trained batch 1293 batch loss 7.20243216 epoch total loss 6.97486544\n",
      "Trained batch 1294 batch loss 7.32089472 epoch total loss 6.97513294\n",
      "Trained batch 1295 batch loss 7.15960312 epoch total loss 6.97527504\n",
      "Trained batch 1296 batch loss 7.19354439 epoch total loss 6.97544336\n",
      "Trained batch 1297 batch loss 7.06575394 epoch total loss 6.97551298\n",
      "Trained batch 1298 batch loss 7.2242732 epoch total loss 6.97570467\n",
      "Trained batch 1299 batch loss 6.86888409 epoch total loss 6.97562265\n",
      "Trained batch 1300 batch loss 6.45764732 epoch total loss 6.97522449\n",
      "Trained batch 1301 batch loss 6.35568285 epoch total loss 6.97474813\n",
      "Trained batch 1302 batch loss 6.81304121 epoch total loss 6.97462416\n",
      "Trained batch 1303 batch loss 7.22840834 epoch total loss 6.97481918\n",
      "Trained batch 1304 batch loss 7.16104412 epoch total loss 6.97496223\n",
      "Trained batch 1305 batch loss 7.12909126 epoch total loss 6.97508\n",
      "Trained batch 1306 batch loss 7.08081913 epoch total loss 6.97516108\n",
      "Trained batch 1307 batch loss 7.07045174 epoch total loss 6.97523403\n",
      "Trained batch 1308 batch loss 7.28741693 epoch total loss 6.97547245\n",
      "Trained batch 1309 batch loss 6.9661603 epoch total loss 6.9754653\n",
      "Trained batch 1310 batch loss 6.79275322 epoch total loss 6.97532558\n",
      "Trained batch 1311 batch loss 6.89868116 epoch total loss 6.97526693\n",
      "Trained batch 1312 batch loss 7.26098728 epoch total loss 6.97548485\n",
      "Trained batch 1313 batch loss 7.15759468 epoch total loss 6.97562313\n",
      "Trained batch 1314 batch loss 7.34873533 epoch total loss 6.97590685\n",
      "Trained batch 1315 batch loss 7.34460258 epoch total loss 6.97618771\n",
      "Trained batch 1316 batch loss 7.48263884 epoch total loss 6.97657204\n",
      "Trained batch 1317 batch loss 6.87601185 epoch total loss 6.97649574\n",
      "Trained batch 1318 batch loss 6.23374939 epoch total loss 6.97593212\n",
      "Trained batch 1319 batch loss 6.10609818 epoch total loss 6.97527266\n",
      "Trained batch 1320 batch loss 6.55199575 epoch total loss 6.97495174\n",
      "Trained batch 1321 batch loss 7.47426224 epoch total loss 6.97533035\n",
      "Trained batch 1322 batch loss 7.13407516 epoch total loss 6.97545\n",
      "Trained batch 1323 batch loss 7.2225771 epoch total loss 6.97563696\n",
      "Trained batch 1324 batch loss 7.40978718 epoch total loss 6.97596502\n",
      "Trained batch 1325 batch loss 7.11543274 epoch total loss 6.97607\n",
      "Trained batch 1326 batch loss 7.42523527 epoch total loss 6.97640848\n",
      "Trained batch 1327 batch loss 7.07155704 epoch total loss 6.97648\n",
      "Trained batch 1328 batch loss 6.93662453 epoch total loss 6.97645\n",
      "Trained batch 1329 batch loss 6.88786936 epoch total loss 6.97638321\n",
      "Trained batch 1330 batch loss 7.12433147 epoch total loss 6.97649431\n",
      "Trained batch 1331 batch loss 7.20806456 epoch total loss 6.97666836\n",
      "Trained batch 1332 batch loss 7.05666637 epoch total loss 6.97672796\n",
      "Trained batch 1333 batch loss 7.34470081 epoch total loss 6.97700405\n",
      "Trained batch 1334 batch loss 7.21561241 epoch total loss 6.97718334\n",
      "Trained batch 1335 batch loss 7.27432585 epoch total loss 6.97740602\n",
      "Trained batch 1336 batch loss 7.40172625 epoch total loss 6.97772312\n",
      "Trained batch 1337 batch loss 7.25531054 epoch total loss 6.97793055\n",
      "Trained batch 1338 batch loss 6.84996367 epoch total loss 6.9778347\n",
      "Trained batch 1339 batch loss 6.51514101 epoch total loss 6.97748947\n",
      "Trained batch 1340 batch loss 7.46598625 epoch total loss 6.97785378\n",
      "Trained batch 1341 batch loss 7.01052284 epoch total loss 6.97787857\n",
      "Trained batch 1342 batch loss 6.51552343 epoch total loss 6.97753382\n",
      "Trained batch 1343 batch loss 6.48005915 epoch total loss 6.97716379\n",
      "Trained batch 1344 batch loss 6.18775177 epoch total loss 6.97657633\n",
      "Trained batch 1345 batch loss 6.83393717 epoch total loss 6.97647047\n",
      "Trained batch 1346 batch loss 6.9968853 epoch total loss 6.97648573\n",
      "Trained batch 1347 batch loss 7.32656336 epoch total loss 6.97674513\n",
      "Trained batch 1348 batch loss 7.33951569 epoch total loss 6.97701454\n",
      "Trained batch 1349 batch loss 7.13278437 epoch total loss 6.97713\n",
      "Trained batch 1350 batch loss 6.90823746 epoch total loss 6.97707891\n",
      "Trained batch 1351 batch loss 6.98716164 epoch total loss 6.97708654\n",
      "Trained batch 1352 batch loss 6.58849096 epoch total loss 6.97679949\n",
      "Trained batch 1353 batch loss 7.04694939 epoch total loss 6.97685099\n",
      "Trained batch 1354 batch loss 6.83442831 epoch total loss 6.97674561\n",
      "Trained batch 1355 batch loss 6.91659164 epoch total loss 6.97670174\n",
      "Trained batch 1356 batch loss 6.76951 epoch total loss 6.97654867\n",
      "Trained batch 1357 batch loss 7.16804266 epoch total loss 6.97669\n",
      "Trained batch 1358 batch loss 6.92191362 epoch total loss 6.97664928\n",
      "Trained batch 1359 batch loss 6.64806414 epoch total loss 6.976408\n",
      "Trained batch 1360 batch loss 6.99049902 epoch total loss 6.97641802\n",
      "Trained batch 1361 batch loss 6.73587418 epoch total loss 6.97624159\n",
      "Trained batch 1362 batch loss 6.7060051 epoch total loss 6.97604322\n",
      "Trained batch 1363 batch loss 6.86431646 epoch total loss 6.97596121\n",
      "Trained batch 1364 batch loss 6.61331463 epoch total loss 6.97569561\n",
      "Trained batch 1365 batch loss 6.58466148 epoch total loss 6.97540903\n",
      "Trained batch 1366 batch loss 6.46996212 epoch total loss 6.97503901\n",
      "Trained batch 1367 batch loss 5.85493135 epoch total loss 6.97421932\n",
      "Trained batch 1368 batch loss 6.0779047 epoch total loss 6.97356415\n",
      "Trained batch 1369 batch loss 7.18809175 epoch total loss 6.9737215\n",
      "Trained batch 1370 batch loss 6.98966 epoch total loss 6.97373247\n",
      "Trained batch 1371 batch loss 7.00601387 epoch total loss 6.97375584\n",
      "Trained batch 1372 batch loss 7.30469561 epoch total loss 6.97399712\n",
      "Trained batch 1373 batch loss 7.31493282 epoch total loss 6.97424507\n",
      "Trained batch 1374 batch loss 7.29028559 epoch total loss 6.97447491\n",
      "Trained batch 1375 batch loss 7.25543976 epoch total loss 6.97467947\n",
      "Trained batch 1376 batch loss 7.28373146 epoch total loss 6.97490454\n",
      "Trained batch 1377 batch loss 7.41287374 epoch total loss 6.97522306\n",
      "Trained batch 1378 batch loss 7.44371557 epoch total loss 6.97556257\n",
      "Trained batch 1379 batch loss 7.0290966 epoch total loss 6.97560167\n",
      "Trained batch 1380 batch loss 7.01791 epoch total loss 6.97563171\n",
      "Trained batch 1381 batch loss 6.30903435 epoch total loss 6.97514915\n",
      "Trained batch 1382 batch loss 6.59071064 epoch total loss 6.97487068\n",
      "Trained batch 1383 batch loss 7.04855442 epoch total loss 6.97492409\n",
      "Trained batch 1384 batch loss 7.17826462 epoch total loss 6.97507143\n",
      "Trained batch 1385 batch loss 6.29368877 epoch total loss 6.97458\n",
      "Trained batch 1386 batch loss 7.01323128 epoch total loss 6.97460794\n",
      "Trained batch 1387 batch loss 6.76416254 epoch total loss 6.97445679\n",
      "Trained batch 1388 batch loss 6.68598366 epoch total loss 6.97424841\n",
      "Epoch 7 train loss 6.97424840927124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 03:08:45.531610: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "2025-03-19 03:08:45.531670: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "\t [[RemoteCall]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 1 batch loss 6.38864946\n",
      "Validated batch 2 batch loss 7.41107607\n",
      "Validated batch 3 batch loss 7.05545092\n",
      "Validated batch 4 batch loss 7.15478325\n",
      "Validated batch 5 batch loss 6.984725\n",
      "Validated batch 6 batch loss 7.34468842\n",
      "Validated batch 7 batch loss 6.92830086\n",
      "Validated batch 8 batch loss 7.3658042\n",
      "Validated batch 9 batch loss 6.73022175\n",
      "Validated batch 10 batch loss 6.95652342\n",
      "Validated batch 11 batch loss 6.79769611\n",
      "Validated batch 12 batch loss 6.36224794\n",
      "Validated batch 13 batch loss 6.55932188\n",
      "Validated batch 14 batch loss 7.38902426\n",
      "Validated batch 15 batch loss 7.19078827\n",
      "Validated batch 16 batch loss 6.70170212\n",
      "Validated batch 17 batch loss 7.23240757\n",
      "Validated batch 18 batch loss 7.13973236\n",
      "Validated batch 19 batch loss 7.13829899\n",
      "Validated batch 20 batch loss 7.42535782\n",
      "Validated batch 21 batch loss 7.23134756\n",
      "Validated batch 22 batch loss 6.88053894\n",
      "Validated batch 23 batch loss 6.60178\n",
      "Validated batch 24 batch loss 6.68788767\n",
      "Validated batch 25 batch loss 6.89961576\n",
      "Validated batch 26 batch loss 6.55217457\n",
      "Validated batch 27 batch loss 6.60500717\n",
      "Validated batch 28 batch loss 6.65532684\n",
      "Validated batch 29 batch loss 6.84203577\n",
      "Validated batch 30 batch loss 6.99193764\n",
      "Validated batch 31 batch loss 7.13069916\n",
      "Validated batch 32 batch loss 7.2798872\n",
      "Validated batch 33 batch loss 7.25755501\n",
      "Validated batch 34 batch loss 7.15912104\n",
      "Validated batch 35 batch loss 6.62848473\n",
      "Validated batch 36 batch loss 7.16309404\n",
      "Validated batch 37 batch loss 6.94648027\n",
      "Validated batch 38 batch loss 7.18071\n",
      "Validated batch 39 batch loss 7.34664392\n",
      "Validated batch 40 batch loss 6.87073469\n",
      "Validated batch 41 batch loss 7.28644705\n",
      "Validated batch 42 batch loss 7.0990653\n",
      "Validated batch 43 batch loss 7.19386435\n",
      "Validated batch 44 batch loss 7.03013611\n",
      "Validated batch 45 batch loss 7.20025\n",
      "Validated batch 46 batch loss 7.26346493\n",
      "Validated batch 47 batch loss 7.29716682\n",
      "Validated batch 48 batch loss 7.05833912\n",
      "Validated batch 49 batch loss 6.83547\n",
      "Validated batch 50 batch loss 6.73522472\n",
      "Validated batch 51 batch loss 6.65485287\n",
      "Validated batch 52 batch loss 7.05851221\n",
      "Validated batch 53 batch loss 6.77978516\n",
      "Validated batch 54 batch loss 7.18745041\n",
      "Validated batch 55 batch loss 7.26131296\n",
      "Validated batch 56 batch loss 6.90360308\n",
      "Validated batch 57 batch loss 6.87655354\n",
      "Validated batch 58 batch loss 6.52301407\n",
      "Validated batch 59 batch loss 6.96671915\n",
      "Validated batch 60 batch loss 6.82774067\n",
      "Validated batch 61 batch loss 7.26593\n",
      "Validated batch 62 batch loss 6.87931061\n",
      "Validated batch 63 batch loss 6.93287945\n",
      "Validated batch 64 batch loss 6.30417728\n",
      "Validated batch 65 batch loss 6.7299571\n",
      "Validated batch 66 batch loss 7.07093859\n",
      "Validated batch 67 batch loss 6.68367672\n",
      "Validated batch 68 batch loss 6.78235245\n",
      "Validated batch 69 batch loss 6.81990099\n",
      "Validated batch 70 batch loss 7.50386\n",
      "Validated batch 71 batch loss 7.03210258\n",
      "Validated batch 72 batch loss 7.09273434\n",
      "Validated batch 73 batch loss 7.02737331\n",
      "Validated batch 74 batch loss 7.10857773\n",
      "Validated batch 75 batch loss 7.29550695\n",
      "Validated batch 76 batch loss 6.9876771\n",
      "Validated batch 77 batch loss 6.66587973\n",
      "Validated batch 78 batch loss 6.84302711\n",
      "Validated batch 79 batch loss 6.80979681\n",
      "Validated batch 80 batch loss 7.19217777\n",
      "Validated batch 81 batch loss 6.87525749\n",
      "Validated batch 82 batch loss 6.71376133\n",
      "Validated batch 83 batch loss 6.59801579\n",
      "Validated batch 84 batch loss 6.96962547\n",
      "Validated batch 85 batch loss 7.34692097\n",
      "Validated batch 86 batch loss 7.29710197\n",
      "Validated batch 87 batch loss 7.10057\n",
      "Validated batch 88 batch loss 7.46903515\n",
      "Validated batch 89 batch loss 7.41581821\n",
      "Validated batch 90 batch loss 7.36018085\n",
      "Validated batch 91 batch loss 6.86066866\n",
      "Validated batch 92 batch loss 6.64131784\n",
      "Validated batch 93 batch loss 6.62338591\n",
      "Validated batch 94 batch loss 6.59730196\n",
      "Validated batch 95 batch loss 6.97372341\n",
      "Validated batch 96 batch loss 7.15170145\n",
      "Validated batch 97 batch loss 6.61925507\n",
      "Validated batch 98 batch loss 7.21388817\n",
      "Validated batch 99 batch loss 7.09471\n",
      "Validated batch 100 batch loss 7.07260084\n",
      "Validated batch 101 batch loss 6.94013357\n",
      "Validated batch 102 batch loss 7.04584312\n",
      "Validated batch 103 batch loss 6.88847256\n",
      "Validated batch 104 batch loss 7.02758408\n",
      "Validated batch 105 batch loss 6.90129\n",
      "Validated batch 106 batch loss 7.23604345\n",
      "Validated batch 107 batch loss 7.23466873\n",
      "Validated batch 108 batch loss 6.95183611\n",
      "Validated batch 109 batch loss 7.00757408\n",
      "Validated batch 110 batch loss 6.45331144\n",
      "Validated batch 111 batch loss 7.18511629\n",
      "Validated batch 112 batch loss 7.30334949\n",
      "Validated batch 113 batch loss 7.15583611\n",
      "Validated batch 114 batch loss 7.26153946\n",
      "Validated batch 115 batch loss 6.79770947\n",
      "Validated batch 116 batch loss 6.94505692\n",
      "Validated batch 117 batch loss 7.12402582\n",
      "Validated batch 118 batch loss 6.78081656\n",
      "Validated batch 119 batch loss 7.19805574\n",
      "Validated batch 120 batch loss 7.35537815\n",
      "Validated batch 121 batch loss 7.19249058\n",
      "Validated batch 122 batch loss 7.21406507\n",
      "Validated batch 123 batch loss 7.06661558\n",
      "Validated batch 124 batch loss 7.23961353\n",
      "Validated batch 125 batch loss 7.2447319\n",
      "Validated batch 126 batch loss 7.39914036\n",
      "Validated batch 127 batch loss 7.34334803\n",
      "Validated batch 128 batch loss 6.5400486\n",
      "Validated batch 129 batch loss 7.23146582\n",
      "Validated batch 130 batch loss 6.94926262\n",
      "Validated batch 131 batch loss 7.02479267\n",
      "Validated batch 132 batch loss 7.07409811\n",
      "Validated batch 133 batch loss 6.54105282\n",
      "Validated batch 134 batch loss 6.47637177\n",
      "Validated batch 135 batch loss 7.32045555\n",
      "Validated batch 136 batch loss 6.90293074\n",
      "Validated batch 137 batch loss 7.36564732\n",
      "Validated batch 138 batch loss 7.01290131\n",
      "Validated batch 139 batch loss 7.34315586\n",
      "Validated batch 140 batch loss 7.4170413\n",
      "Validated batch 141 batch loss 7.01808834\n",
      "Validated batch 142 batch loss 6.39569521\n",
      "Validated batch 143 batch loss 6.88948154\n",
      "Validated batch 144 batch loss 6.71592283\n",
      "Validated batch 145 batch loss 6.47796774\n",
      "Validated batch 146 batch loss 6.80126\n",
      "Validated batch 147 batch loss 6.40367031\n",
      "Validated batch 148 batch loss 7.19815302\n",
      "Validated batch 149 batch loss 6.76337528\n",
      "Validated batch 150 batch loss 6.66809\n",
      "Validated batch 151 batch loss 6.4198885\n",
      "Validated batch 152 batch loss 7.26373672\n",
      "Validated batch 153 batch loss 6.69584751\n",
      "Validated batch 154 batch loss 7.17414474\n",
      "Validated batch 155 batch loss 6.8354125\n",
      "Validated batch 156 batch loss 7.21692038\n",
      "Validated batch 157 batch loss 6.95214701\n",
      "Validated batch 158 batch loss 6.91796398\n",
      "Validated batch 159 batch loss 7.04390812\n",
      "Validated batch 160 batch loss 6.15747881\n",
      "Validated batch 161 batch loss 7.06942797\n",
      "Validated batch 162 batch loss 7.33863735\n",
      "Validated batch 163 batch loss 6.93968582\n",
      "Validated batch 164 batch loss 6.80753136\n",
      "Validated batch 165 batch loss 6.79736471\n",
      "Validated batch 166 batch loss 6.83075619\n",
      "Validated batch 167 batch loss 7.10890532\n",
      "Validated batch 168 batch loss 6.7895627\n",
      "Validated batch 169 batch loss 7.2609086\n",
      "Validated batch 170 batch loss 7.09122849\n",
      "Validated batch 171 batch loss 6.95671797\n",
      "Validated batch 172 batch loss 7.30192\n",
      "Validated batch 173 batch loss 7.34121704\n",
      "Validated batch 174 batch loss 6.77057266\n",
      "Validated batch 175 batch loss 6.88271\n",
      "Validated batch 176 batch loss 7.34416819\n",
      "Validated batch 177 batch loss 6.82540894\n",
      "Validated batch 178 batch loss 7.14622974\n",
      "Validated batch 179 batch loss 6.71652222\n",
      "Validated batch 180 batch loss 7.28717327\n",
      "Validated batch 181 batch loss 6.66518211\n",
      "Validated batch 182 batch loss 7.05850077\n",
      "Validated batch 183 batch loss 6.91487694\n",
      "Validated batch 184 batch loss 6.94453764\n",
      "Validated batch 185 batch loss 3.6510756\n",
      "Epoch 7 val loss 6.966338157653809\n",
      "Model /home/minho/Desktop/aiffel/pose_estimation/models/model-epoch-7-loss-6.9663.weights.h5 saved.\n",
      "Start epoch 8 with learning rate 0.001\n",
      "Start distributed training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 03:08:54.171661: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "2025-03-19 03:08:54.171707: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "\t [[RemoteCall]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1 batch loss 5.71107674 epoch total loss 5.71107674\n",
      "Trained batch 2 batch loss 6.23986864 epoch total loss 5.97547245\n",
      "Trained batch 3 batch loss 6.89716053 epoch total loss 6.28270197\n",
      "Trained batch 4 batch loss 7.12645 epoch total loss 6.49363899\n",
      "Trained batch 5 batch loss 7.23637581 epoch total loss 6.64218616\n",
      "Trained batch 6 batch loss 7.14017105 epoch total loss 6.72518349\n",
      "Trained batch 7 batch loss 6.42003632 epoch total loss 6.68159103\n",
      "Trained batch 8 batch loss 7.00652599 epoch total loss 6.72220802\n",
      "Trained batch 9 batch loss 7.00366068 epoch total loss 6.75348091\n",
      "Trained batch 10 batch loss 7.01685619 epoch total loss 6.77981806\n",
      "Trained batch 11 batch loss 7.06178045 epoch total loss 6.80545092\n",
      "Trained batch 12 batch loss 6.97030735 epoch total loss 6.81918907\n",
      "Trained batch 13 batch loss 7.02818775 epoch total loss 6.83526611\n",
      "Trained batch 14 batch loss 6.63695192 epoch total loss 6.82110119\n",
      "Trained batch 15 batch loss 7.02039671 epoch total loss 6.8343873\n",
      "Trained batch 16 batch loss 6.95936441 epoch total loss 6.84219837\n",
      "Trained batch 17 batch loss 7.08301115 epoch total loss 6.85636377\n",
      "Trained batch 18 batch loss 7.05840397 epoch total loss 6.86758804\n",
      "Trained batch 19 batch loss 6.89971828 epoch total loss 6.86927891\n",
      "Trained batch 20 batch loss 7.07566786 epoch total loss 6.87959814\n",
      "Trained batch 21 batch loss 7.05460072 epoch total loss 6.88793135\n",
      "Trained batch 22 batch loss 6.74540567 epoch total loss 6.88145304\n",
      "Trained batch 23 batch loss 7.07652426 epoch total loss 6.88993454\n",
      "Trained batch 24 batch loss 7.15274811 epoch total loss 6.90088511\n",
      "Trained batch 25 batch loss 7.2556963 epoch total loss 6.91507769\n",
      "Trained batch 26 batch loss 7.25936508 epoch total loss 6.92831945\n",
      "Trained batch 27 batch loss 7.4664669 epoch total loss 6.94825077\n",
      "Trained batch 28 batch loss 7.26939917 epoch total loss 6.95972\n",
      "Trained batch 29 batch loss 7.29966736 epoch total loss 6.97144222\n",
      "Trained batch 30 batch loss 7.16922 epoch total loss 6.97803497\n",
      "Trained batch 31 batch loss 7.13323355 epoch total loss 6.98304176\n",
      "Trained batch 32 batch loss 7.43726921 epoch total loss 6.99723625\n",
      "Trained batch 33 batch loss 7.22134542 epoch total loss 7.00402737\n",
      "Trained batch 34 batch loss 6.94545126 epoch total loss 7.00230455\n",
      "Trained batch 35 batch loss 6.9161253 epoch total loss 6.99984217\n",
      "Trained batch 36 batch loss 7.14826632 epoch total loss 7.00396538\n",
      "Trained batch 37 batch loss 6.2666688 epoch total loss 6.98403835\n",
      "Trained batch 38 batch loss 6.86248398 epoch total loss 6.98083973\n",
      "Trained batch 39 batch loss 7.04681873 epoch total loss 6.98253155\n",
      "Trained batch 40 batch loss 7.18728065 epoch total loss 6.98765039\n",
      "Trained batch 41 batch loss 7.11314726 epoch total loss 6.99071169\n",
      "Trained batch 42 batch loss 6.55425 epoch total loss 6.98032\n",
      "Trained batch 43 batch loss 6.81348133 epoch total loss 6.97644\n",
      "Trained batch 44 batch loss 7.13788939 epoch total loss 6.98010874\n",
      "Trained batch 45 batch loss 7.02675915 epoch total loss 6.98114538\n",
      "Trained batch 46 batch loss 6.77241039 epoch total loss 6.9766078\n",
      "Trained batch 47 batch loss 7.2457881 epoch total loss 6.98233509\n",
      "Trained batch 48 batch loss 7.3795805 epoch total loss 6.9906106\n",
      "Trained batch 49 batch loss 6.94072437 epoch total loss 6.98959303\n",
      "Trained batch 50 batch loss 7.13898849 epoch total loss 6.99258041\n",
      "Trained batch 51 batch loss 7.25077438 epoch total loss 6.99764299\n",
      "Trained batch 52 batch loss 6.93700933 epoch total loss 6.99647713\n",
      "Trained batch 53 batch loss 7.17973757 epoch total loss 6.99993515\n",
      "Trained batch 54 batch loss 7.36525154 epoch total loss 7.00670052\n",
      "Trained batch 55 batch loss 7.44049311 epoch total loss 7.0145874\n",
      "Trained batch 56 batch loss 7.49066973 epoch total loss 7.02308893\n",
      "Trained batch 57 batch loss 7.40885496 epoch total loss 7.0298562\n",
      "Trained batch 58 batch loss 7.30521297 epoch total loss 7.0346036\n",
      "Trained batch 59 batch loss 7.10887146 epoch total loss 7.03586292\n",
      "Trained batch 60 batch loss 7.45799494 epoch total loss 7.04289865\n",
      "Trained batch 61 batch loss 7.17530203 epoch total loss 7.04506874\n",
      "Trained batch 62 batch loss 6.96624184 epoch total loss 7.04379749\n",
      "Trained batch 63 batch loss 7.27479839 epoch total loss 7.04746437\n",
      "Trained batch 64 batch loss 7.15007782 epoch total loss 7.04906797\n",
      "Trained batch 65 batch loss 7.33800936 epoch total loss 7.05351305\n",
      "Trained batch 66 batch loss 7.12985134 epoch total loss 7.05467\n",
      "Trained batch 67 batch loss 7.13929415 epoch total loss 7.055933\n",
      "Trained batch 68 batch loss 7.29577494 epoch total loss 7.05945969\n",
      "Trained batch 69 batch loss 7.07518435 epoch total loss 7.05968809\n",
      "Trained batch 70 batch loss 6.84881306 epoch total loss 7.05667543\n",
      "Trained batch 71 batch loss 6.70651197 epoch total loss 7.05174351\n",
      "Trained batch 72 batch loss 7.16500235 epoch total loss 7.05331659\n",
      "Trained batch 73 batch loss 7.11487532 epoch total loss 7.05416\n",
      "Trained batch 74 batch loss 7.15027094 epoch total loss 7.05545855\n",
      "Trained batch 75 batch loss 7.3418 epoch total loss 7.05927658\n",
      "Trained batch 76 batch loss 7.26525736 epoch total loss 7.06198692\n",
      "Trained batch 77 batch loss 7.29607153 epoch total loss 7.06502724\n",
      "Trained batch 78 batch loss 7.21046448 epoch total loss 7.06689119\n",
      "Trained batch 79 batch loss 7.08221436 epoch total loss 7.06708527\n",
      "Trained batch 80 batch loss 6.9318018 epoch total loss 7.0653944\n",
      "Trained batch 81 batch loss 6.77643347 epoch total loss 7.06182718\n",
      "Trained batch 82 batch loss 7.28714657 epoch total loss 7.0645752\n",
      "Trained batch 83 batch loss 6.91273069 epoch total loss 7.06274557\n",
      "Trained batch 84 batch loss 6.81979704 epoch total loss 7.05985355\n",
      "Trained batch 85 batch loss 6.56585884 epoch total loss 7.05404186\n",
      "Trained batch 86 batch loss 5.99474907 epoch total loss 7.04172468\n",
      "Trained batch 87 batch loss 6.46871662 epoch total loss 7.03513813\n",
      "Trained batch 88 batch loss 6.54644 epoch total loss 7.02958488\n",
      "Trained batch 89 batch loss 6.79644299 epoch total loss 7.02696514\n",
      "Trained batch 90 batch loss 6.89440489 epoch total loss 7.02549219\n",
      "Trained batch 91 batch loss 6.50509644 epoch total loss 7.01977396\n",
      "Trained batch 92 batch loss 6.79959059 epoch total loss 7.01738\n",
      "Trained batch 93 batch loss 6.81000185 epoch total loss 7.01515055\n",
      "Trained batch 94 batch loss 6.99908209 epoch total loss 7.01497936\n",
      "Trained batch 95 batch loss 6.95394564 epoch total loss 7.01433706\n",
      "Trained batch 96 batch loss 6.84146357 epoch total loss 7.01253653\n",
      "Trained batch 97 batch loss 7.12298 epoch total loss 7.01367521\n",
      "Trained batch 98 batch loss 7.06193638 epoch total loss 7.01416779\n",
      "Trained batch 99 batch loss 6.9383378 epoch total loss 7.01340199\n",
      "Trained batch 100 batch loss 6.95401287 epoch total loss 7.01280832\n",
      "Trained batch 101 batch loss 7.07701159 epoch total loss 7.01344395\n",
      "Trained batch 102 batch loss 7.13004255 epoch total loss 7.0145874\n",
      "Trained batch 103 batch loss 7.0118413 epoch total loss 7.0145607\n",
      "Trained batch 104 batch loss 7.24987602 epoch total loss 7.01682329\n",
      "Trained batch 105 batch loss 7.21439171 epoch total loss 7.01870537\n",
      "Trained batch 106 batch loss 6.53936672 epoch total loss 7.01418304\n",
      "Trained batch 107 batch loss 6.62242746 epoch total loss 7.01052189\n",
      "Trained batch 108 batch loss 7.11804819 epoch total loss 7.01151752\n",
      "Trained batch 109 batch loss 7.30055141 epoch total loss 7.01416922\n",
      "Trained batch 110 batch loss 6.79295397 epoch total loss 7.01215839\n",
      "Trained batch 111 batch loss 6.91764593 epoch total loss 7.01130676\n",
      "Trained batch 112 batch loss 6.42586565 epoch total loss 7.00607967\n",
      "Trained batch 113 batch loss 6.64413071 epoch total loss 7.00287628\n",
      "Trained batch 114 batch loss 6.66199589 epoch total loss 6.99988604\n",
      "Trained batch 115 batch loss 6.82850409 epoch total loss 6.99839544\n",
      "Trained batch 116 batch loss 6.26844454 epoch total loss 6.99210262\n",
      "Trained batch 117 batch loss 5.79466248 epoch total loss 6.98186827\n",
      "Trained batch 118 batch loss 6.04441833 epoch total loss 6.97392416\n",
      "Trained batch 119 batch loss 6.18668556 epoch total loss 6.96730852\n",
      "Trained batch 120 batch loss 6.43226 epoch total loss 6.96285\n",
      "Trained batch 121 batch loss 6.38756609 epoch total loss 6.95809555\n",
      "Trained batch 122 batch loss 6.64181185 epoch total loss 6.95550299\n",
      "Trained batch 123 batch loss 7.09468651 epoch total loss 6.95663404\n",
      "Trained batch 124 batch loss 6.79301643 epoch total loss 6.95531511\n",
      "Trained batch 125 batch loss 6.5455513 epoch total loss 6.95203686\n",
      "Trained batch 126 batch loss 6.66079044 epoch total loss 6.94972515\n",
      "Trained batch 127 batch loss 6.71537113 epoch total loss 6.94788\n",
      "Trained batch 128 batch loss 7.25015402 epoch total loss 6.95024157\n",
      "Trained batch 129 batch loss 7.25104523 epoch total loss 6.9525733\n",
      "Trained batch 130 batch loss 7.28421688 epoch total loss 6.95512438\n",
      "Trained batch 131 batch loss 7.03056192 epoch total loss 6.9557004\n",
      "Trained batch 132 batch loss 6.95832109 epoch total loss 6.95572042\n",
      "Trained batch 133 batch loss 6.74537563 epoch total loss 6.95413876\n",
      "Trained batch 134 batch loss 7.17916298 epoch total loss 6.9558177\n",
      "Trained batch 135 batch loss 7.09982681 epoch total loss 6.95688486\n",
      "Trained batch 136 batch loss 7.33266115 epoch total loss 6.95964766\n",
      "Trained batch 137 batch loss 7.18905544 epoch total loss 6.96132183\n",
      "Trained batch 138 batch loss 7.23736477 epoch total loss 6.96332216\n",
      "Trained batch 139 batch loss 7.30572462 epoch total loss 6.9657855\n",
      "Trained batch 140 batch loss 6.93059492 epoch total loss 6.96553421\n",
      "Trained batch 141 batch loss 6.87293386 epoch total loss 6.96487761\n",
      "Trained batch 142 batch loss 6.4565444 epoch total loss 6.96129751\n",
      "Trained batch 143 batch loss 6.06495476 epoch total loss 6.95502949\n",
      "Trained batch 144 batch loss 6.44894123 epoch total loss 6.95151472\n",
      "Trained batch 145 batch loss 6.63304615 epoch total loss 6.94931841\n",
      "Trained batch 146 batch loss 6.37623 epoch total loss 6.94539309\n",
      "Trained batch 147 batch loss 7.11373234 epoch total loss 6.94653797\n",
      "Trained batch 148 batch loss 7.08418703 epoch total loss 6.94746876\n",
      "Trained batch 149 batch loss 6.87843418 epoch total loss 6.94700527\n",
      "Trained batch 150 batch loss 6.67878246 epoch total loss 6.94521713\n",
      "Trained batch 151 batch loss 6.75029516 epoch total loss 6.94392586\n",
      "Trained batch 152 batch loss 6.12994337 epoch total loss 6.9385705\n",
      "Trained batch 153 batch loss 6.63631582 epoch total loss 6.93659544\n",
      "Trained batch 154 batch loss 7.16659212 epoch total loss 6.93808889\n",
      "Trained batch 155 batch loss 7.219841 epoch total loss 6.9399066\n",
      "Trained batch 156 batch loss 6.62941074 epoch total loss 6.93791628\n",
      "Trained batch 157 batch loss 7.15928 epoch total loss 6.93932629\n",
      "Trained batch 158 batch loss 7.1395669 epoch total loss 6.94059324\n",
      "Trained batch 159 batch loss 7.03193951 epoch total loss 6.94116831\n",
      "Trained batch 160 batch loss 7.08488703 epoch total loss 6.94206619\n",
      "Trained batch 161 batch loss 7.0323987 epoch total loss 6.94262695\n",
      "Trained batch 162 batch loss 7.00602341 epoch total loss 6.94301796\n",
      "Trained batch 163 batch loss 7.32564116 epoch total loss 6.94536591\n",
      "Trained batch 164 batch loss 7.1531496 epoch total loss 6.94663286\n",
      "Trained batch 165 batch loss 6.74017763 epoch total loss 6.94538212\n",
      "Trained batch 166 batch loss 6.38595152 epoch total loss 6.94201231\n",
      "Trained batch 167 batch loss 6.29362631 epoch total loss 6.93812943\n",
      "Trained batch 168 batch loss 6.11906528 epoch total loss 6.93325377\n",
      "Trained batch 169 batch loss 6.31762791 epoch total loss 6.92961073\n",
      "Trained batch 170 batch loss 7.20494938 epoch total loss 6.93123055\n",
      "Trained batch 171 batch loss 6.89153051 epoch total loss 6.93099833\n",
      "Trained batch 172 batch loss 7.22331 epoch total loss 6.9326973\n",
      "Trained batch 173 batch loss 6.97404861 epoch total loss 6.93293619\n",
      "Trained batch 174 batch loss 7.32998705 epoch total loss 6.93521786\n",
      "Trained batch 175 batch loss 7.24172974 epoch total loss 6.93696928\n",
      "Trained batch 176 batch loss 6.40443802 epoch total loss 6.93394327\n",
      "Trained batch 177 batch loss 6.23233318 epoch total loss 6.92997932\n",
      "Trained batch 178 batch loss 6.90862417 epoch total loss 6.92985916\n",
      "Trained batch 179 batch loss 6.1959734 epoch total loss 6.92575884\n",
      "Trained batch 180 batch loss 6.25332069 epoch total loss 6.92202282\n",
      "Trained batch 181 batch loss 6.73779678 epoch total loss 6.92100477\n",
      "Trained batch 182 batch loss 6.81883669 epoch total loss 6.92044353\n",
      "Trained batch 183 batch loss 6.75673771 epoch total loss 6.91954899\n",
      "Trained batch 184 batch loss 7.2156539 epoch total loss 6.92115831\n",
      "Trained batch 185 batch loss 6.78324127 epoch total loss 6.92041302\n",
      "Trained batch 186 batch loss 7.2612977 epoch total loss 6.92224598\n",
      "Trained batch 187 batch loss 7.34630156 epoch total loss 6.92451334\n",
      "Trained batch 188 batch loss 6.66041374 epoch total loss 6.92310858\n",
      "Trained batch 189 batch loss 5.78832245 epoch total loss 6.91710472\n",
      "Trained batch 190 batch loss 5.22219753 epoch total loss 6.90818405\n",
      "Trained batch 191 batch loss 6.39280462 epoch total loss 6.90548563\n",
      "Trained batch 192 batch loss 7.16864634 epoch total loss 6.90685654\n",
      "Trained batch 193 batch loss 7.17546082 epoch total loss 6.90824795\n",
      "Trained batch 194 batch loss 7.30144358 epoch total loss 6.91027451\n",
      "Trained batch 195 batch loss 7.39597034 epoch total loss 6.9127655\n",
      "Trained batch 196 batch loss 7.32112312 epoch total loss 6.91484928\n",
      "Trained batch 197 batch loss 6.97218466 epoch total loss 6.91514\n",
      "Trained batch 198 batch loss 6.8762188 epoch total loss 6.9149437\n",
      "Trained batch 199 batch loss 6.73905563 epoch total loss 6.91405964\n",
      "Trained batch 200 batch loss 6.15386391 epoch total loss 6.91025829\n",
      "Trained batch 201 batch loss 6.90540695 epoch total loss 6.91023397\n",
      "Trained batch 202 batch loss 7.4095788 epoch total loss 6.9127059\n",
      "Trained batch 203 batch loss 6.97901201 epoch total loss 6.91303253\n",
      "Trained batch 204 batch loss 7.35726738 epoch total loss 6.91521025\n",
      "Trained batch 205 batch loss 7.0110817 epoch total loss 6.91567802\n",
      "Trained batch 206 batch loss 6.91016197 epoch total loss 6.91565132\n",
      "Trained batch 207 batch loss 7.15694666 epoch total loss 6.91681719\n",
      "Trained batch 208 batch loss 6.83525801 epoch total loss 6.91642475\n",
      "Trained batch 209 batch loss 6.7819252 epoch total loss 6.9157815\n",
      "Trained batch 210 batch loss 6.84826517 epoch total loss 6.91546\n",
      "Trained batch 211 batch loss 7.20296574 epoch total loss 6.91682291\n",
      "Trained batch 212 batch loss 7.12506723 epoch total loss 6.91780519\n",
      "Trained batch 213 batch loss 7.41183138 epoch total loss 6.92012453\n",
      "Trained batch 214 batch loss 7.47355366 epoch total loss 6.9227109\n",
      "Trained batch 215 batch loss 7.24550724 epoch total loss 6.92421198\n",
      "Trained batch 216 batch loss 7.27124119 epoch total loss 6.92581844\n",
      "Trained batch 217 batch loss 6.81707907 epoch total loss 6.92531776\n",
      "Trained batch 218 batch loss 7.08794832 epoch total loss 6.92606354\n",
      "Trained batch 219 batch loss 6.89468384 epoch total loss 6.92592\n",
      "Trained batch 220 batch loss 7.18263674 epoch total loss 6.92708683\n",
      "Trained batch 221 batch loss 7.13050461 epoch total loss 6.92800713\n",
      "Trained batch 222 batch loss 7.08447552 epoch total loss 6.92871189\n",
      "Trained batch 223 batch loss 6.96448421 epoch total loss 6.92887259\n",
      "Trained batch 224 batch loss 7.42946434 epoch total loss 6.93110704\n",
      "Trained batch 225 batch loss 7.20950031 epoch total loss 6.93234444\n",
      "Trained batch 226 batch loss 7.31653738 epoch total loss 6.93404436\n",
      "Trained batch 227 batch loss 7.26105499 epoch total loss 6.93548489\n",
      "Trained batch 228 batch loss 7.17267323 epoch total loss 6.93652534\n",
      "Trained batch 229 batch loss 6.2460494 epoch total loss 6.93351078\n",
      "Trained batch 230 batch loss 6.13174582 epoch total loss 6.93002462\n",
      "Trained batch 231 batch loss 6.16152716 epoch total loss 6.92669773\n",
      "Trained batch 232 batch loss 6.99399519 epoch total loss 6.92698765\n",
      "Trained batch 233 batch loss 6.22567558 epoch total loss 6.92397785\n",
      "Trained batch 234 batch loss 6.41831923 epoch total loss 6.9218173\n",
      "Trained batch 235 batch loss 6.41020727 epoch total loss 6.91963959\n",
      "Trained batch 236 batch loss 6.68258524 epoch total loss 6.91863537\n",
      "Trained batch 237 batch loss 6.84941244 epoch total loss 6.91834307\n",
      "Trained batch 238 batch loss 6.64349365 epoch total loss 6.91718817\n",
      "Trained batch 239 batch loss 6.28491402 epoch total loss 6.91454268\n",
      "Trained batch 240 batch loss 6.01894283 epoch total loss 6.91081095\n",
      "Trained batch 241 batch loss 6.87982845 epoch total loss 6.91068268\n",
      "Trained batch 242 batch loss 7.139328 epoch total loss 6.91162729\n",
      "Trained batch 243 batch loss 7.186234 epoch total loss 6.9127574\n",
      "Trained batch 244 batch loss 6.83497715 epoch total loss 6.91243839\n",
      "Trained batch 245 batch loss 6.84350538 epoch total loss 6.91215706\n",
      "Trained batch 246 batch loss 6.79099417 epoch total loss 6.91166496\n",
      "Trained batch 247 batch loss 7.05698729 epoch total loss 6.91225338\n",
      "Trained batch 248 batch loss 6.54999733 epoch total loss 6.91079283\n",
      "Trained batch 249 batch loss 6.75059319 epoch total loss 6.91014957\n",
      "Trained batch 250 batch loss 6.6616807 epoch total loss 6.90915537\n",
      "Trained batch 251 batch loss 6.64930296 epoch total loss 6.90812\n",
      "Trained batch 252 batch loss 6.28743076 epoch total loss 6.90565729\n",
      "Trained batch 253 batch loss 6.318326 epoch total loss 6.90333557\n",
      "Trained batch 254 batch loss 6.85201025 epoch total loss 6.90313387\n",
      "Trained batch 255 batch loss 6.58838463 epoch total loss 6.90189934\n",
      "Trained batch 256 batch loss 7.07427835 epoch total loss 6.90257263\n",
      "Trained batch 257 batch loss 6.98031139 epoch total loss 6.90287542\n",
      "Trained batch 258 batch loss 7.15332699 epoch total loss 6.90384579\n",
      "Trained batch 259 batch loss 7.29117107 epoch total loss 6.90534115\n",
      "Trained batch 260 batch loss 7.16162252 epoch total loss 6.90632677\n",
      "Trained batch 261 batch loss 7.26393509 epoch total loss 6.9076972\n",
      "Trained batch 262 batch loss 6.81959 epoch total loss 6.90736055\n",
      "Trained batch 263 batch loss 7.34109974 epoch total loss 6.90901\n",
      "Trained batch 264 batch loss 7.3716464 epoch total loss 6.91076231\n",
      "Trained batch 265 batch loss 7.17487097 epoch total loss 6.91175938\n",
      "Trained batch 266 batch loss 7.28063 epoch total loss 6.91314602\n",
      "Trained batch 267 batch loss 7.20895147 epoch total loss 6.91425419\n",
      "Trained batch 268 batch loss 6.84702873 epoch total loss 6.91400337\n",
      "Trained batch 269 batch loss 7.26237535 epoch total loss 6.91529799\n",
      "Trained batch 270 batch loss 7.38402843 epoch total loss 6.91703415\n",
      "Trained batch 271 batch loss 7.4126 epoch total loss 6.91886282\n",
      "Trained batch 272 batch loss 7.23877525 epoch total loss 6.92003918\n",
      "Trained batch 273 batch loss 7.16991949 epoch total loss 6.92095423\n",
      "Trained batch 274 batch loss 7.41303968 epoch total loss 6.92275047\n",
      "Trained batch 275 batch loss 7.17463779 epoch total loss 6.92366648\n",
      "Trained batch 276 batch loss 7.07309151 epoch total loss 6.92420816\n",
      "Trained batch 277 batch loss 7.00043154 epoch total loss 6.9244833\n",
      "Trained batch 278 batch loss 7.09158754 epoch total loss 6.92508459\n",
      "Trained batch 279 batch loss 7.13618422 epoch total loss 6.92584133\n",
      "Trained batch 280 batch loss 6.6416049 epoch total loss 6.92482615\n",
      "Trained batch 281 batch loss 6.31721449 epoch total loss 6.92266369\n",
      "Trained batch 282 batch loss 6.1688323 epoch total loss 6.91999054\n",
      "Trained batch 283 batch loss 6.64691687 epoch total loss 6.9190259\n",
      "Trained batch 284 batch loss 6.32820463 epoch total loss 6.91694593\n",
      "Trained batch 285 batch loss 6.12832 epoch total loss 6.91417837\n",
      "Trained batch 286 batch loss 6.10887051 epoch total loss 6.91136265\n",
      "Trained batch 287 batch loss 6.21080351 epoch total loss 6.90892172\n",
      "Trained batch 288 batch loss 6.81008 epoch total loss 6.90857887\n",
      "Trained batch 289 batch loss 6.56468391 epoch total loss 6.90738869\n",
      "Trained batch 290 batch loss 7.02164412 epoch total loss 6.90778255\n",
      "Trained batch 291 batch loss 7.15871572 epoch total loss 6.90864468\n",
      "Trained batch 292 batch loss 7.33905602 epoch total loss 6.91011906\n",
      "Trained batch 293 batch loss 7.37266922 epoch total loss 6.91169786\n",
      "Trained batch 294 batch loss 7.36152029 epoch total loss 6.91322803\n",
      "Trained batch 295 batch loss 7.24121666 epoch total loss 6.91433954\n",
      "Trained batch 296 batch loss 7.32888317 epoch total loss 6.91574\n",
      "Trained batch 297 batch loss 7.22789049 epoch total loss 6.91679096\n",
      "Trained batch 298 batch loss 7.423944 epoch total loss 6.91849232\n",
      "Trained batch 299 batch loss 7.16228962 epoch total loss 6.91930771\n",
      "Trained batch 300 batch loss 7.04510784 epoch total loss 6.91972733\n",
      "Trained batch 301 batch loss 6.99526453 epoch total loss 6.91997862\n",
      "Trained batch 302 batch loss 6.46775818 epoch total loss 6.91848135\n",
      "Trained batch 303 batch loss 6.51886892 epoch total loss 6.91716242\n",
      "Trained batch 304 batch loss 6.65368462 epoch total loss 6.91629505\n",
      "Trained batch 305 batch loss 6.9903841 epoch total loss 6.91653824\n",
      "Trained batch 306 batch loss 7.28424501 epoch total loss 6.91774\n",
      "Trained batch 307 batch loss 6.96227121 epoch total loss 6.91788435\n",
      "Trained batch 308 batch loss 6.52825165 epoch total loss 6.91662\n",
      "Trained batch 309 batch loss 6.07013607 epoch total loss 6.91388\n",
      "Trained batch 310 batch loss 6.53157139 epoch total loss 6.91264629\n",
      "Trained batch 311 batch loss 7.31212807 epoch total loss 6.91393042\n",
      "Trained batch 312 batch loss 7.18625689 epoch total loss 6.9148035\n",
      "Trained batch 313 batch loss 7.28065872 epoch total loss 6.91597271\n",
      "Trained batch 314 batch loss 7.32449198 epoch total loss 6.91727352\n",
      "Trained batch 315 batch loss 7.42791653 epoch total loss 6.91889477\n",
      "Trained batch 316 batch loss 7.35987282 epoch total loss 6.92029047\n",
      "Trained batch 317 batch loss 7.48742151 epoch total loss 6.92207909\n",
      "Trained batch 318 batch loss 7.18130445 epoch total loss 6.92289448\n",
      "Trained batch 319 batch loss 7.16842794 epoch total loss 6.92366457\n",
      "Trained batch 320 batch loss 7.26952171 epoch total loss 6.92474508\n",
      "Trained batch 321 batch loss 6.95467901 epoch total loss 6.92483807\n",
      "Trained batch 322 batch loss 7.28695107 epoch total loss 6.92596245\n",
      "Trained batch 323 batch loss 7.38676691 epoch total loss 6.92738914\n",
      "Trained batch 324 batch loss 7.2795229 epoch total loss 6.92847586\n",
      "Trained batch 325 batch loss 6.90640688 epoch total loss 6.92840815\n",
      "Trained batch 326 batch loss 7.29111099 epoch total loss 6.92952061\n",
      "Trained batch 327 batch loss 6.47302628 epoch total loss 6.9281249\n",
      "Trained batch 328 batch loss 6.97674513 epoch total loss 6.9282732\n",
      "Trained batch 329 batch loss 6.97208 epoch total loss 6.92840672\n",
      "Trained batch 330 batch loss 6.72277164 epoch total loss 6.92778301\n",
      "Trained batch 331 batch loss 7.18564749 epoch total loss 6.92856216\n",
      "Trained batch 332 batch loss 7.06336641 epoch total loss 6.92896843\n",
      "Trained batch 333 batch loss 6.92602301 epoch total loss 6.92895937\n",
      "Trained batch 334 batch loss 7.21317196 epoch total loss 6.92981052\n",
      "Trained batch 335 batch loss 7.35531712 epoch total loss 6.93108034\n",
      "Trained batch 336 batch loss 7.21803093 epoch total loss 6.93193436\n",
      "Trained batch 337 batch loss 6.72962427 epoch total loss 6.93133402\n",
      "Trained batch 338 batch loss 6.87402248 epoch total loss 6.93116474\n",
      "Trained batch 339 batch loss 6.60704279 epoch total loss 6.93020821\n",
      "Trained batch 340 batch loss 6.72774267 epoch total loss 6.92961264\n",
      "Trained batch 341 batch loss 6.70722246 epoch total loss 6.9289608\n",
      "Trained batch 342 batch loss 6.76996803 epoch total loss 6.92849588\n",
      "Trained batch 343 batch loss 6.87543392 epoch total loss 6.92834139\n",
      "Trained batch 344 batch loss 6.98269129 epoch total loss 6.92849922\n",
      "Trained batch 345 batch loss 7.10822344 epoch total loss 6.9290204\n",
      "Trained batch 346 batch loss 6.96053886 epoch total loss 6.929111\n",
      "Trained batch 347 batch loss 6.93933868 epoch total loss 6.92914104\n",
      "Trained batch 348 batch loss 7.45485115 epoch total loss 6.93065166\n",
      "Trained batch 349 batch loss 7.40234947 epoch total loss 6.93200302\n",
      "Trained batch 350 batch loss 7.12661743 epoch total loss 6.93255949\n",
      "Trained batch 351 batch loss 7.10315609 epoch total loss 6.93304586\n",
      "Trained batch 352 batch loss 7.49402952 epoch total loss 6.93463945\n",
      "Trained batch 353 batch loss 7.25912237 epoch total loss 6.9355588\n",
      "Trained batch 354 batch loss 6.55838442 epoch total loss 6.93449306\n",
      "Trained batch 355 batch loss 6.7422533 epoch total loss 6.93395138\n",
      "Trained batch 356 batch loss 6.45875406 epoch total loss 6.93261671\n",
      "Trained batch 357 batch loss 6.16851616 epoch total loss 6.93047619\n",
      "Trained batch 358 batch loss 6.18764925 epoch total loss 6.92840147\n",
      "Trained batch 359 batch loss 6.21683931 epoch total loss 6.92641926\n",
      "Trained batch 360 batch loss 5.9034667 epoch total loss 6.92357779\n",
      "Trained batch 361 batch loss 5.67547035 epoch total loss 6.92012072\n",
      "Trained batch 362 batch loss 5.61927652 epoch total loss 6.91652775\n",
      "Trained batch 363 batch loss 6.29037762 epoch total loss 6.91480255\n",
      "Trained batch 364 batch loss 6.95085192 epoch total loss 6.91490173\n",
      "Trained batch 365 batch loss 7.18813562 epoch total loss 6.91565037\n",
      "Trained batch 366 batch loss 7.10476875 epoch total loss 6.91616726\n",
      "Trained batch 367 batch loss 7.46353149 epoch total loss 6.91765881\n",
      "Trained batch 368 batch loss 7.21058083 epoch total loss 6.91845512\n",
      "Trained batch 369 batch loss 7.17868042 epoch total loss 6.91916037\n",
      "Trained batch 370 batch loss 6.82148409 epoch total loss 6.91889668\n",
      "Trained batch 371 batch loss 7.26569462 epoch total loss 6.91983128\n",
      "Trained batch 372 batch loss 7.18229532 epoch total loss 6.92053699\n",
      "Trained batch 373 batch loss 7.38974762 epoch total loss 6.92179441\n",
      "Trained batch 374 batch loss 7.39958906 epoch total loss 6.92307234\n",
      "Trained batch 375 batch loss 7.44288 epoch total loss 6.9244585\n",
      "Trained batch 376 batch loss 7.0130558 epoch total loss 6.92469358\n",
      "Trained batch 377 batch loss 6.58452654 epoch total loss 6.92379141\n",
      "Trained batch 378 batch loss 6.90121412 epoch total loss 6.92373133\n",
      "Trained batch 379 batch loss 6.91573668 epoch total loss 6.92371035\n",
      "Trained batch 380 batch loss 7.31735611 epoch total loss 6.92474604\n",
      "Trained batch 381 batch loss 7.32615376 epoch total loss 6.9258\n",
      "Trained batch 382 batch loss 7.46506453 epoch total loss 6.92721176\n",
      "Trained batch 383 batch loss 7.21152639 epoch total loss 6.92795372\n",
      "Trained batch 384 batch loss 7.38798475 epoch total loss 6.92915154\n",
      "Trained batch 385 batch loss 7.4438777 epoch total loss 6.93048859\n",
      "Trained batch 386 batch loss 7.34046316 epoch total loss 6.93155098\n",
      "Trained batch 387 batch loss 7.53335047 epoch total loss 6.93310595\n",
      "Trained batch 388 batch loss 7.49318409 epoch total loss 6.93454933\n",
      "Trained batch 389 batch loss 7.26707745 epoch total loss 6.9354043\n",
      "Trained batch 390 batch loss 7.00736237 epoch total loss 6.93558884\n",
      "Trained batch 391 batch loss 5.79955626 epoch total loss 6.93268347\n",
      "Trained batch 392 batch loss 5.75689411 epoch total loss 6.92968369\n",
      "Trained batch 393 batch loss 6.26548767 epoch total loss 6.9279933\n",
      "Trained batch 394 batch loss 6.86554909 epoch total loss 6.92783499\n",
      "Trained batch 395 batch loss 7.13206387 epoch total loss 6.92835188\n",
      "Trained batch 396 batch loss 6.93556309 epoch total loss 6.92837\n",
      "Trained batch 397 batch loss 6.5624094 epoch total loss 6.92744827\n",
      "Trained batch 398 batch loss 6.87662363 epoch total loss 6.92732096\n",
      "Trained batch 399 batch loss 6.98242807 epoch total loss 6.92745924\n",
      "Trained batch 400 batch loss 7.12801313 epoch total loss 6.9279604\n",
      "Trained batch 401 batch loss 7.17138338 epoch total loss 6.92856741\n",
      "Trained batch 402 batch loss 7.36269236 epoch total loss 6.92964745\n",
      "Trained batch 403 batch loss 7.30617 epoch total loss 6.93058157\n",
      "Trained batch 404 batch loss 7.25566149 epoch total loss 6.93138599\n",
      "Trained batch 405 batch loss 7.21726227 epoch total loss 6.93209219\n",
      "Trained batch 406 batch loss 7.33968973 epoch total loss 6.93309593\n",
      "Trained batch 407 batch loss 6.99996185 epoch total loss 6.93326044\n",
      "Trained batch 408 batch loss 7.21197701 epoch total loss 6.93394327\n",
      "Trained batch 409 batch loss 6.97305584 epoch total loss 6.93403912\n",
      "Trained batch 410 batch loss 7.23336697 epoch total loss 6.93476915\n",
      "Trained batch 411 batch loss 6.60186148 epoch total loss 6.93395901\n",
      "Trained batch 412 batch loss 6.25301838 epoch total loss 6.93230629\n",
      "Trained batch 413 batch loss 7.02848387 epoch total loss 6.93253899\n",
      "Trained batch 414 batch loss 7.41873455 epoch total loss 6.93371344\n",
      "Trained batch 415 batch loss 7.25968361 epoch total loss 6.93449926\n",
      "Trained batch 416 batch loss 6.63662434 epoch total loss 6.93378353\n",
      "Trained batch 417 batch loss 6.94074774 epoch total loss 6.93379974\n",
      "Trained batch 418 batch loss 6.85781717 epoch total loss 6.93361807\n",
      "Trained batch 419 batch loss 6.98369 epoch total loss 6.93373775\n",
      "Trained batch 420 batch loss 7.05138063 epoch total loss 6.93401766\n",
      "Trained batch 421 batch loss 7.14705944 epoch total loss 6.93452311\n",
      "Trained batch 422 batch loss 7.21856403 epoch total loss 6.9351964\n",
      "Trained batch 423 batch loss 6.74213028 epoch total loss 6.93474\n",
      "Trained batch 424 batch loss 6.87300444 epoch total loss 6.93459463\n",
      "Trained batch 425 batch loss 7.07528925 epoch total loss 6.93492556\n",
      "Trained batch 426 batch loss 7.30298662 epoch total loss 6.93578911\n",
      "Trained batch 427 batch loss 7.32543039 epoch total loss 6.93670177\n",
      "Trained batch 428 batch loss 7.28539 epoch total loss 6.93751669\n",
      "Trained batch 429 batch loss 7.33341789 epoch total loss 6.93843937\n",
      "Trained batch 430 batch loss 7.15171719 epoch total loss 6.93893528\n",
      "Trained batch 431 batch loss 7.08416367 epoch total loss 6.9392724\n",
      "Trained batch 432 batch loss 7.08229685 epoch total loss 6.93960333\n",
      "Trained batch 433 batch loss 7.38218641 epoch total loss 6.94062519\n",
      "Trained batch 434 batch loss 7.4501071 epoch total loss 6.94179964\n",
      "Trained batch 435 batch loss 7.17626858 epoch total loss 6.94233847\n",
      "Trained batch 436 batch loss 7.09364319 epoch total loss 6.9426856\n",
      "Trained batch 437 batch loss 6.804317 epoch total loss 6.94236898\n",
      "Trained batch 438 batch loss 6.96333027 epoch total loss 6.94241667\n",
      "Trained batch 439 batch loss 6.98090363 epoch total loss 6.94250441\n",
      "Trained batch 440 batch loss 6.94072628 epoch total loss 6.94250059\n",
      "Trained batch 441 batch loss 6.62667799 epoch total loss 6.94178438\n",
      "Trained batch 442 batch loss 6.86339045 epoch total loss 6.94160652\n",
      "Trained batch 443 batch loss 6.97855568 epoch total loss 6.94169\n",
      "Trained batch 444 batch loss 7.05900145 epoch total loss 6.94195461\n",
      "Trained batch 445 batch loss 7.35579967 epoch total loss 6.94288445\n",
      "Trained batch 446 batch loss 7.39939165 epoch total loss 6.94390774\n",
      "Trained batch 447 batch loss 7.38779545 epoch total loss 6.94490051\n",
      "Trained batch 448 batch loss 7.43625546 epoch total loss 6.94599771\n",
      "Trained batch 449 batch loss 7.3760848 epoch total loss 6.9469552\n",
      "Trained batch 450 batch loss 7.0105629 epoch total loss 6.94709635\n",
      "Trained batch 451 batch loss 7.06282234 epoch total loss 6.94735289\n",
      "Trained batch 452 batch loss 7.38190794 epoch total loss 6.94831419\n",
      "Trained batch 453 batch loss 7.39059591 epoch total loss 6.94929028\n",
      "Trained batch 454 batch loss 7.36428547 epoch total loss 6.95020437\n",
      "Trained batch 455 batch loss 7.12345028 epoch total loss 6.95058537\n",
      "Trained batch 456 batch loss 7.1791153 epoch total loss 6.95108652\n",
      "Trained batch 457 batch loss 7.19614697 epoch total loss 6.95162296\n",
      "Trained batch 458 batch loss 6.74995184 epoch total loss 6.95118237\n",
      "Trained batch 459 batch loss 6.87132931 epoch total loss 6.9510088\n",
      "Trained batch 460 batch loss 6.92561054 epoch total loss 6.95095301\n",
      "Trained batch 461 batch loss 7.2420969 epoch total loss 6.95158482\n",
      "Trained batch 462 batch loss 6.86382294 epoch total loss 6.95139503\n",
      "Trained batch 463 batch loss 6.86082315 epoch total loss 6.95119953\n",
      "Trained batch 464 batch loss 7.05341196 epoch total loss 6.95142\n",
      "Trained batch 465 batch loss 6.86110163 epoch total loss 6.95122528\n",
      "Trained batch 466 batch loss 6.87760925 epoch total loss 6.95106745\n",
      "Trained batch 467 batch loss 6.8940506 epoch total loss 6.95094538\n",
      "Trained batch 468 batch loss 7.31060123 epoch total loss 6.95171404\n",
      "Trained batch 469 batch loss 7.16074181 epoch total loss 6.9521594\n",
      "Trained batch 470 batch loss 7.10547495 epoch total loss 6.95248556\n",
      "Trained batch 471 batch loss 7.30608082 epoch total loss 6.95323658\n",
      "Trained batch 472 batch loss 6.94822216 epoch total loss 6.95322609\n",
      "Trained batch 473 batch loss 7.1064105 epoch total loss 6.95355\n",
      "Trained batch 474 batch loss 7.05331564 epoch total loss 6.95376\n",
      "Trained batch 475 batch loss 7.12928343 epoch total loss 6.9541297\n",
      "Trained batch 476 batch loss 6.73115253 epoch total loss 6.95366144\n",
      "Trained batch 477 batch loss 7.0011344 epoch total loss 6.9537611\n",
      "Trained batch 478 batch loss 6.84459972 epoch total loss 6.9535327\n",
      "Trained batch 479 batch loss 7.1534853 epoch total loss 6.95395041\n",
      "Trained batch 480 batch loss 6.69425631 epoch total loss 6.95340919\n",
      "Trained batch 481 batch loss 6.98621941 epoch total loss 6.95347786\n",
      "Trained batch 482 batch loss 7.02967358 epoch total loss 6.95363617\n",
      "Trained batch 483 batch loss 6.88766432 epoch total loss 6.95349932\n",
      "Trained batch 484 batch loss 7.01407719 epoch total loss 6.95362473\n",
      "Trained batch 485 batch loss 6.62511158 epoch total loss 6.95294714\n",
      "Trained batch 486 batch loss 6.68299103 epoch total loss 6.9523921\n",
      "Trained batch 487 batch loss 6.78294039 epoch total loss 6.95204401\n",
      "Trained batch 488 batch loss 6.90514 epoch total loss 6.95194769\n",
      "Trained batch 489 batch loss 7.13839102 epoch total loss 6.95232916\n",
      "Trained batch 490 batch loss 7.40148401 epoch total loss 6.95324564\n",
      "Trained batch 491 batch loss 7.52395678 epoch total loss 6.95440769\n",
      "Trained batch 492 batch loss 7.38186121 epoch total loss 6.95527649\n",
      "Trained batch 493 batch loss 7.06746483 epoch total loss 6.95550394\n",
      "Trained batch 494 batch loss 7.03825951 epoch total loss 6.95567179\n",
      "Trained batch 495 batch loss 6.471313 epoch total loss 6.95469284\n",
      "Trained batch 496 batch loss 6.12170267 epoch total loss 6.95301342\n",
      "Trained batch 497 batch loss 5.87527275 epoch total loss 6.95084476\n",
      "Trained batch 498 batch loss 6.04270935 epoch total loss 6.94902134\n",
      "Trained batch 499 batch loss 6.67297268 epoch total loss 6.94846773\n",
      "Trained batch 500 batch loss 7.20817 epoch total loss 6.94898748\n",
      "Trained batch 501 batch loss 7.01891756 epoch total loss 6.94912672\n",
      "Trained batch 502 batch loss 7.14904213 epoch total loss 6.94952488\n",
      "Trained batch 503 batch loss 6.67771101 epoch total loss 6.94898415\n",
      "Trained batch 504 batch loss 6.95094728 epoch total loss 6.94898796\n",
      "Trained batch 505 batch loss 6.39542675 epoch total loss 6.94789219\n",
      "Trained batch 506 batch loss 6.80369139 epoch total loss 6.94760704\n",
      "Trained batch 507 batch loss 6.93072176 epoch total loss 6.94757366\n",
      "Trained batch 508 batch loss 6.80202103 epoch total loss 6.94728708\n",
      "Trained batch 509 batch loss 7.44827414 epoch total loss 6.94827127\n",
      "Trained batch 510 batch loss 7.1191926 epoch total loss 6.94860649\n",
      "Trained batch 511 batch loss 7.22924423 epoch total loss 6.94915581\n",
      "Trained batch 512 batch loss 7.28955364 epoch total loss 6.94982052\n",
      "Trained batch 513 batch loss 7.08008194 epoch total loss 6.9500742\n",
      "Trained batch 514 batch loss 6.9374733 epoch total loss 6.95005\n",
      "Trained batch 515 batch loss 7.31502676 epoch total loss 6.95075846\n",
      "Trained batch 516 batch loss 6.99766 epoch total loss 6.95084906\n",
      "Trained batch 517 batch loss 6.9801259 epoch total loss 6.9509058\n",
      "Trained batch 518 batch loss 6.96158171 epoch total loss 6.95092678\n",
      "Trained batch 519 batch loss 6.88291407 epoch total loss 6.95079565\n",
      "Trained batch 520 batch loss 6.65478802 epoch total loss 6.95022631\n",
      "Trained batch 521 batch loss 5.83673191 epoch total loss 6.94808912\n",
      "Trained batch 522 batch loss 5.2668004 epoch total loss 6.94486809\n",
      "Trained batch 523 batch loss 6.49281836 epoch total loss 6.94400406\n",
      "Trained batch 524 batch loss 7.13012 epoch total loss 6.9443593\n",
      "Trained batch 525 batch loss 6.8646965 epoch total loss 6.94420767\n",
      "Trained batch 526 batch loss 7.14113331 epoch total loss 6.94458199\n",
      "Trained batch 527 batch loss 6.81239557 epoch total loss 6.94433117\n",
      "Trained batch 528 batch loss 7.0486269 epoch total loss 6.94452858\n",
      "Trained batch 529 batch loss 7.13943529 epoch total loss 6.94489717\n",
      "Trained batch 530 batch loss 6.83569765 epoch total loss 6.94469118\n",
      "Trained batch 531 batch loss 7.02392101 epoch total loss 6.94484043\n",
      "Trained batch 532 batch loss 6.57918882 epoch total loss 6.94415283\n",
      "Trained batch 533 batch loss 6.57838583 epoch total loss 6.94346666\n",
      "Trained batch 534 batch loss 7.05452824 epoch total loss 6.94367456\n",
      "Trained batch 535 batch loss 6.86440945 epoch total loss 6.94352627\n",
      "Trained batch 536 batch loss 7.03240347 epoch total loss 6.94369221\n",
      "Trained batch 537 batch loss 6.89050579 epoch total loss 6.9435935\n",
      "Trained batch 538 batch loss 7.04773855 epoch total loss 6.9437871\n",
      "Trained batch 539 batch loss 6.78252268 epoch total loss 6.94348812\n",
      "Trained batch 540 batch loss 6.61414623 epoch total loss 6.94287825\n",
      "Trained batch 541 batch loss 6.2213974 epoch total loss 6.94154501\n",
      "Trained batch 542 batch loss 6.25791 epoch total loss 6.9402833\n",
      "Trained batch 543 batch loss 6.80742359 epoch total loss 6.94003868\n",
      "Trained batch 544 batch loss 6.16893625 epoch total loss 6.93862104\n",
      "Trained batch 545 batch loss 6.84783697 epoch total loss 6.93845463\n",
      "Trained batch 546 batch loss 6.5633955 epoch total loss 6.93776798\n",
      "Trained batch 547 batch loss 6.91119432 epoch total loss 6.93771935\n",
      "Trained batch 548 batch loss 6.85252285 epoch total loss 6.9375639\n",
      "Trained batch 549 batch loss 6.54850912 epoch total loss 6.93685532\n",
      "Trained batch 550 batch loss 7.42928648 epoch total loss 6.93775034\n",
      "Trained batch 551 batch loss 7.03142786 epoch total loss 6.93792057\n",
      "Trained batch 552 batch loss 6.10177946 epoch total loss 6.93640566\n",
      "Trained batch 553 batch loss 5.87420559 epoch total loss 6.93448496\n",
      "Trained batch 554 batch loss 6.46985769 epoch total loss 6.93364668\n",
      "Trained batch 555 batch loss 6.59707355 epoch total loss 6.93304\n",
      "Trained batch 556 batch loss 6.75710487 epoch total loss 6.932724\n",
      "Trained batch 557 batch loss 7.06500196 epoch total loss 6.93296146\n",
      "Trained batch 558 batch loss 6.69380808 epoch total loss 6.93253279\n",
      "Trained batch 559 batch loss 6.65177774 epoch total loss 6.93203068\n",
      "Trained batch 560 batch loss 7.09322309 epoch total loss 6.93231869\n",
      "Trained batch 561 batch loss 6.80337334 epoch total loss 6.93208885\n",
      "Trained batch 562 batch loss 7.00210238 epoch total loss 6.93221378\n",
      "Trained batch 563 batch loss 7.32967234 epoch total loss 6.9329195\n",
      "Trained batch 564 batch loss 7.4668231 epoch total loss 6.93386602\n",
      "Trained batch 565 batch loss 7.51856184 epoch total loss 6.93490076\n",
      "Trained batch 566 batch loss 7.32543564 epoch total loss 6.93559074\n",
      "Trained batch 567 batch loss 7.2905097 epoch total loss 6.93621683\n",
      "Trained batch 568 batch loss 7.14904 epoch total loss 6.93659115\n",
      "Trained batch 569 batch loss 7.14871025 epoch total loss 6.93696404\n",
      "Trained batch 570 batch loss 6.91812611 epoch total loss 6.93693113\n",
      "Trained batch 571 batch loss 6.54570198 epoch total loss 6.93624592\n",
      "Trained batch 572 batch loss 7.03263044 epoch total loss 6.93641472\n",
      "Trained batch 573 batch loss 6.9712677 epoch total loss 6.93647528\n",
      "Trained batch 574 batch loss 6.73114586 epoch total loss 6.93611765\n",
      "Trained batch 575 batch loss 6.7837429 epoch total loss 6.93585253\n",
      "Trained batch 576 batch loss 7.17916 epoch total loss 6.93627501\n",
      "Trained batch 577 batch loss 6.67637062 epoch total loss 6.93582439\n",
      "Trained batch 578 batch loss 6.78887367 epoch total loss 6.93557024\n",
      "Trained batch 579 batch loss 6.79347324 epoch total loss 6.93532467\n",
      "Trained batch 580 batch loss 6.97005177 epoch total loss 6.93538427\n",
      "Trained batch 581 batch loss 6.89871597 epoch total loss 6.93532133\n",
      "Trained batch 582 batch loss 6.77783298 epoch total loss 6.93505049\n",
      "Trained batch 583 batch loss 6.58457327 epoch total loss 6.9344492\n",
      "Trained batch 584 batch loss 6.64071226 epoch total loss 6.93394613\n",
      "Trained batch 585 batch loss 6.49930668 epoch total loss 6.93320322\n",
      "Trained batch 586 batch loss 6.93310452 epoch total loss 6.93320274\n",
      "Trained batch 587 batch loss 7.11723518 epoch total loss 6.9335165\n",
      "Trained batch 588 batch loss 7.00873375 epoch total loss 6.93364429\n",
      "Trained batch 589 batch loss 6.97728062 epoch total loss 6.93371868\n",
      "Trained batch 590 batch loss 7.37304115 epoch total loss 6.93446302\n",
      "Trained batch 591 batch loss 6.94836187 epoch total loss 6.93448687\n",
      "Trained batch 592 batch loss 6.97072 epoch total loss 6.9345479\n",
      "Trained batch 593 batch loss 7.0962553 epoch total loss 6.93482065\n",
      "Trained batch 594 batch loss 7.18780661 epoch total loss 6.93524694\n",
      "Trained batch 595 batch loss 7.09192705 epoch total loss 6.93551\n",
      "Trained batch 596 batch loss 7.09398794 epoch total loss 6.93577528\n",
      "Trained batch 597 batch loss 6.89549255 epoch total loss 6.93570805\n",
      "Trained batch 598 batch loss 6.87549591 epoch total loss 6.93560743\n",
      "Trained batch 599 batch loss 7.19507694 epoch total loss 6.93604088\n",
      "Trained batch 600 batch loss 7.35718298 epoch total loss 6.93674326\n",
      "Trained batch 601 batch loss 7.46874094 epoch total loss 6.93762827\n",
      "Trained batch 602 batch loss 7.48942137 epoch total loss 6.93854475\n",
      "Trained batch 603 batch loss 7.04405212 epoch total loss 6.93871927\n",
      "Trained batch 604 batch loss 7.1027627 epoch total loss 6.93899059\n",
      "Trained batch 605 batch loss 6.94044256 epoch total loss 6.93899298\n",
      "Trained batch 606 batch loss 7.24274778 epoch total loss 6.93949413\n",
      "Trained batch 607 batch loss 7.15213823 epoch total loss 6.93984509\n",
      "Trained batch 608 batch loss 7.18255091 epoch total loss 6.9402442\n",
      "Trained batch 609 batch loss 7.39905167 epoch total loss 6.94099712\n",
      "Trained batch 610 batch loss 7.37998343 epoch total loss 6.94171667\n",
      "Trained batch 611 batch loss 7.28605843 epoch total loss 6.94228029\n",
      "Trained batch 612 batch loss 7.1281004 epoch total loss 6.94258404\n",
      "Trained batch 613 batch loss 7.25516701 epoch total loss 6.94309425\n",
      "Trained batch 614 batch loss 7.00238132 epoch total loss 6.94319057\n",
      "Trained batch 615 batch loss 6.79398537 epoch total loss 6.94294834\n",
      "Trained batch 616 batch loss 6.84143066 epoch total loss 6.94278288\n",
      "Trained batch 617 batch loss 6.62846327 epoch total loss 6.94227362\n",
      "Trained batch 618 batch loss 6.75646067 epoch total loss 6.94197273\n",
      "Trained batch 619 batch loss 6.88930893 epoch total loss 6.94188738\n",
      "Trained batch 620 batch loss 6.71693087 epoch total loss 6.94152451\n",
      "Trained batch 621 batch loss 6.87844324 epoch total loss 6.94142294\n",
      "Trained batch 622 batch loss 6.92451668 epoch total loss 6.94139528\n",
      "Trained batch 623 batch loss 6.72048616 epoch total loss 6.94104099\n",
      "Trained batch 624 batch loss 6.62278318 epoch total loss 6.94053078\n",
      "Trained batch 625 batch loss 6.77913761 epoch total loss 6.94027281\n",
      "Trained batch 626 batch loss 6.96379232 epoch total loss 6.94031048\n",
      "Trained batch 627 batch loss 7.00248289 epoch total loss 6.94040966\n",
      "Trained batch 628 batch loss 7.04408455 epoch total loss 6.94057417\n",
      "Trained batch 629 batch loss 6.99519348 epoch total loss 6.94066095\n",
      "Trained batch 630 batch loss 6.95243835 epoch total loss 6.94068\n",
      "Trained batch 631 batch loss 6.92324734 epoch total loss 6.94065237\n",
      "Trained batch 632 batch loss 6.87356567 epoch total loss 6.94054651\n",
      "Trained batch 633 batch loss 6.85493183 epoch total loss 6.94041109\n",
      "Trained batch 634 batch loss 6.79259777 epoch total loss 6.94017792\n",
      "Trained batch 635 batch loss 7.17266846 epoch total loss 6.94054413\n",
      "Trained batch 636 batch loss 6.90712404 epoch total loss 6.94049168\n",
      "Trained batch 637 batch loss 7.07421398 epoch total loss 6.94070196\n",
      "Trained batch 638 batch loss 6.83202457 epoch total loss 6.94053125\n",
      "Trained batch 639 batch loss 6.48019791 epoch total loss 6.93981075\n",
      "Trained batch 640 batch loss 7.10912323 epoch total loss 6.94007492\n",
      "Trained batch 641 batch loss 6.71047735 epoch total loss 6.93971682\n",
      "Trained batch 642 batch loss 6.77946424 epoch total loss 6.93946695\n",
      "Trained batch 643 batch loss 6.63983536 epoch total loss 6.93900061\n",
      "Trained batch 644 batch loss 6.99010849 epoch total loss 6.93908\n",
      "Trained batch 645 batch loss 7.01511049 epoch total loss 6.93919802\n",
      "Trained batch 646 batch loss 6.76179934 epoch total loss 6.93892336\n",
      "Trained batch 647 batch loss 6.92311 epoch total loss 6.93889904\n",
      "Trained batch 648 batch loss 7.17809677 epoch total loss 6.93926859\n",
      "Trained batch 649 batch loss 6.497334 epoch total loss 6.93858814\n",
      "Trained batch 650 batch loss 6.62736273 epoch total loss 6.9381094\n",
      "Trained batch 651 batch loss 6.7360096 epoch total loss 6.9377985\n",
      "Trained batch 652 batch loss 6.27681494 epoch total loss 6.93678474\n",
      "Trained batch 653 batch loss 7.09367657 epoch total loss 6.93702507\n",
      "Trained batch 654 batch loss 6.96129322 epoch total loss 6.93706226\n",
      "Trained batch 655 batch loss 7.23722076 epoch total loss 6.93752098\n",
      "Trained batch 656 batch loss 6.96288776 epoch total loss 6.9375596\n",
      "Trained batch 657 batch loss 7.39908075 epoch total loss 6.93826199\n",
      "Trained batch 658 batch loss 6.86462355 epoch total loss 6.93815\n",
      "Trained batch 659 batch loss 6.38722467 epoch total loss 6.93731403\n",
      "Trained batch 660 batch loss 6.66271353 epoch total loss 6.93689775\n",
      "Trained batch 661 batch loss 6.93593311 epoch total loss 6.93689632\n",
      "Trained batch 662 batch loss 7.33153 epoch total loss 6.93749285\n",
      "Trained batch 663 batch loss 7.21601725 epoch total loss 6.93791246\n",
      "Trained batch 664 batch loss 7.36870813 epoch total loss 6.93856096\n",
      "Trained batch 665 batch loss 7.28939819 epoch total loss 6.93908882\n",
      "Trained batch 666 batch loss 7.36779165 epoch total loss 6.93973255\n",
      "Trained batch 667 batch loss 7.36714125 epoch total loss 6.94037342\n",
      "Trained batch 668 batch loss 7.14704514 epoch total loss 6.94068241\n",
      "Trained batch 669 batch loss 7.11374426 epoch total loss 6.94094133\n",
      "Trained batch 670 batch loss 6.94945431 epoch total loss 6.94095373\n",
      "Trained batch 671 batch loss 6.99699116 epoch total loss 6.94103718\n",
      "Trained batch 672 batch loss 7.01696253 epoch total loss 6.94115067\n",
      "Trained batch 673 batch loss 7.26505709 epoch total loss 6.94163179\n",
      "Trained batch 674 batch loss 7.33272696 epoch total loss 6.94221163\n",
      "Trained batch 675 batch loss 7.2831831 epoch total loss 6.94271708\n",
      "Trained batch 676 batch loss 7.27535152 epoch total loss 6.94320917\n",
      "Trained batch 677 batch loss 7.2154789 epoch total loss 6.94361115\n",
      "Trained batch 678 batch loss 7.23540354 epoch total loss 6.94404125\n",
      "Trained batch 679 batch loss 7.32226562 epoch total loss 6.9445982\n",
      "Trained batch 680 batch loss 7.24949026 epoch total loss 6.9450469\n",
      "Trained batch 681 batch loss 6.91774559 epoch total loss 6.94500685\n",
      "Trained batch 682 batch loss 7.18233442 epoch total loss 6.94535494\n",
      "Trained batch 683 batch loss 7.40357208 epoch total loss 6.94602585\n",
      "Trained batch 684 batch loss 7.02057171 epoch total loss 6.94613504\n",
      "Trained batch 685 batch loss 7.1059885 epoch total loss 6.94636822\n",
      "Trained batch 686 batch loss 7.39266253 epoch total loss 6.94701862\n",
      "Trained batch 687 batch loss 6.98797 epoch total loss 6.94707775\n",
      "Trained batch 688 batch loss 7.17902327 epoch total loss 6.94741535\n",
      "Trained batch 689 batch loss 7.3531208 epoch total loss 6.94800425\n",
      "Trained batch 690 batch loss 6.77458668 epoch total loss 6.94775248\n",
      "Trained batch 691 batch loss 6.90815449 epoch total loss 6.94769526\n",
      "Trained batch 692 batch loss 6.72076416 epoch total loss 6.94736719\n",
      "Trained batch 693 batch loss 7.32202911 epoch total loss 6.9479084\n",
      "Trained batch 694 batch loss 7.36179304 epoch total loss 6.94850445\n",
      "Trained batch 695 batch loss 7.03797531 epoch total loss 6.94863367\n",
      "Trained batch 696 batch loss 7.11077 epoch total loss 6.94886637\n",
      "Trained batch 697 batch loss 7.26412678 epoch total loss 6.94931889\n",
      "Trained batch 698 batch loss 7.37125444 epoch total loss 6.94992304\n",
      "Trained batch 699 batch loss 7.1893425 epoch total loss 6.95026588\n",
      "Trained batch 700 batch loss 7.3045516 epoch total loss 6.95077229\n",
      "Trained batch 701 batch loss 6.82614088 epoch total loss 6.95059443\n",
      "Trained batch 702 batch loss 7.04989862 epoch total loss 6.95073557\n",
      "Trained batch 703 batch loss 7.03626442 epoch total loss 6.95085716\n",
      "Trained batch 704 batch loss 6.82791233 epoch total loss 6.95068312\n",
      "Trained batch 705 batch loss 7.09186459 epoch total loss 6.95088291\n",
      "Trained batch 706 batch loss 6.71324635 epoch total loss 6.95054674\n",
      "Trained batch 707 batch loss 6.52767086 epoch total loss 6.94994879\n",
      "Trained batch 708 batch loss 7.09861755 epoch total loss 6.9501586\n",
      "Trained batch 709 batch loss 6.98565817 epoch total loss 6.95020914\n",
      "Trained batch 710 batch loss 7.28176832 epoch total loss 6.95067596\n",
      "Trained batch 711 batch loss 6.965487 epoch total loss 6.95069647\n",
      "Trained batch 712 batch loss 6.68302536 epoch total loss 6.95032072\n",
      "Trained batch 713 batch loss 7.07943153 epoch total loss 6.95050192\n",
      "Trained batch 714 batch loss 7.06258297 epoch total loss 6.9506588\n",
      "Trained batch 715 batch loss 6.64488316 epoch total loss 6.95023155\n",
      "Trained batch 716 batch loss 7.02917 epoch total loss 6.9503417\n",
      "Trained batch 717 batch loss 6.77198029 epoch total loss 6.95009327\n",
      "Trained batch 718 batch loss 6.7879 epoch total loss 6.94986773\n",
      "Trained batch 719 batch loss 7.03391314 epoch total loss 6.94998407\n",
      "Trained batch 720 batch loss 7.1503582 epoch total loss 6.95026255\n",
      "Trained batch 721 batch loss 7.37485218 epoch total loss 6.95085144\n",
      "Trained batch 722 batch loss 7.41198158 epoch total loss 6.9514904\n",
      "Trained batch 723 batch loss 6.95078278 epoch total loss 6.95148945\n",
      "Trained batch 724 batch loss 6.37770176 epoch total loss 6.95069695\n",
      "Trained batch 725 batch loss 6.65737343 epoch total loss 6.95029211\n",
      "Trained batch 726 batch loss 6.9192543 epoch total loss 6.95024967\n",
      "Trained batch 727 batch loss 6.83656693 epoch total loss 6.95009327\n",
      "Trained batch 728 batch loss 7.33124256 epoch total loss 6.95061636\n",
      "Trained batch 729 batch loss 7.20981312 epoch total loss 6.95097208\n",
      "Trained batch 730 batch loss 7.29515696 epoch total loss 6.95144367\n",
      "Trained batch 731 batch loss 7.1775341 epoch total loss 6.95175314\n",
      "Trained batch 732 batch loss 7.15559673 epoch total loss 6.95203161\n",
      "Trained batch 733 batch loss 7.14881372 epoch total loss 6.95230055\n",
      "Trained batch 734 batch loss 7.35351706 epoch total loss 6.952847\n",
      "Trained batch 735 batch loss 7.37594 epoch total loss 6.95342255\n",
      "Trained batch 736 batch loss 7.37667274 epoch total loss 6.95399761\n",
      "Trained batch 737 batch loss 6.78659916 epoch total loss 6.95377\n",
      "Trained batch 738 batch loss 6.72517157 epoch total loss 6.95346\n",
      "Trained batch 739 batch loss 6.53334141 epoch total loss 6.95289183\n",
      "Trained batch 740 batch loss 6.92612791 epoch total loss 6.95285559\n",
      "Trained batch 741 batch loss 6.99832916 epoch total loss 6.95291758\n",
      "Trained batch 742 batch loss 6.9452548 epoch total loss 6.95290709\n",
      "Trained batch 743 batch loss 7.20937395 epoch total loss 6.95325232\n",
      "Trained batch 744 batch loss 7.45152712 epoch total loss 6.95392227\n",
      "Trained batch 745 batch loss 6.88697147 epoch total loss 6.95383263\n",
      "Trained batch 746 batch loss 7.35002565 epoch total loss 6.95436382\n",
      "Trained batch 747 batch loss 7.34820318 epoch total loss 6.9548912\n",
      "Trained batch 748 batch loss 7.14699221 epoch total loss 6.95514774\n",
      "Trained batch 749 batch loss 7.1572094 epoch total loss 6.95541763\n",
      "Trained batch 750 batch loss 7.17920113 epoch total loss 6.95571613\n",
      "Trained batch 751 batch loss 7.3293848 epoch total loss 6.95621395\n",
      "Trained batch 752 batch loss 7.24518442 epoch total loss 6.95659828\n",
      "Trained batch 753 batch loss 7.15694904 epoch total loss 6.95686388\n",
      "Trained batch 754 batch loss 7.20340395 epoch total loss 6.95719099\n",
      "Trained batch 755 batch loss 7.14964962 epoch total loss 6.95744562\n",
      "Trained batch 756 batch loss 7.28735113 epoch total loss 6.95788193\n",
      "Trained batch 757 batch loss 7.35349894 epoch total loss 6.95840454\n",
      "Trained batch 758 batch loss 7.09879732 epoch total loss 6.95858955\n",
      "Trained batch 759 batch loss 7.10732031 epoch total loss 6.95878553\n",
      "Trained batch 760 batch loss 7.37593126 epoch total loss 6.95933437\n",
      "Trained batch 761 batch loss 7.43747711 epoch total loss 6.95996284\n",
      "Trained batch 762 batch loss 7.24861956 epoch total loss 6.96034145\n",
      "Trained batch 763 batch loss 7.3587594 epoch total loss 6.96086407\n",
      "Trained batch 764 batch loss 7.31005812 epoch total loss 6.96132088\n",
      "Trained batch 765 batch loss 7.33763552 epoch total loss 6.9618125\n",
      "Trained batch 766 batch loss 7.25821781 epoch total loss 6.96219969\n",
      "Trained batch 767 batch loss 7.34085751 epoch total loss 6.96269321\n",
      "Trained batch 768 batch loss 7.33405733 epoch total loss 6.96317673\n",
      "Trained batch 769 batch loss 7.32608414 epoch total loss 6.9636488\n",
      "Trained batch 770 batch loss 7.23950386 epoch total loss 6.96400738\n",
      "Trained batch 771 batch loss 7.17386055 epoch total loss 6.96427965\n",
      "Trained batch 772 batch loss 7.18426132 epoch total loss 6.96456432\n",
      "Trained batch 773 batch loss 7.26293325 epoch total loss 6.96495\n",
      "Trained batch 774 batch loss 7.23005 epoch total loss 6.96529245\n",
      "Trained batch 775 batch loss 6.89536142 epoch total loss 6.96520233\n",
      "Trained batch 776 batch loss 7.17140675 epoch total loss 6.96546793\n",
      "Trained batch 777 batch loss 7.2586422 epoch total loss 6.96584558\n",
      "Trained batch 778 batch loss 7.12789631 epoch total loss 6.96605396\n",
      "Trained batch 779 batch loss 6.82631207 epoch total loss 6.9658742\n",
      "Trained batch 780 batch loss 7.16067648 epoch total loss 6.96612406\n",
      "Trained batch 781 batch loss 7.0752182 epoch total loss 6.96626377\n",
      "Trained batch 782 batch loss 7.31592607 epoch total loss 6.96671057\n",
      "Trained batch 783 batch loss 7.27155256 epoch total loss 6.9671\n",
      "Trained batch 784 batch loss 7.10501909 epoch total loss 6.96727562\n",
      "Trained batch 785 batch loss 6.81315613 epoch total loss 6.96707916\n",
      "Trained batch 786 batch loss 7.16260815 epoch total loss 6.96732807\n",
      "Trained batch 787 batch loss 7.25712633 epoch total loss 6.96769667\n",
      "Trained batch 788 batch loss 7.00204706 epoch total loss 6.96774\n",
      "Trained batch 789 batch loss 6.9866128 epoch total loss 6.96776438\n",
      "Trained batch 790 batch loss 7.18800497 epoch total loss 6.96804285\n",
      "Trained batch 791 batch loss 6.54161596 epoch total loss 6.96750355\n",
      "Trained batch 792 batch loss 7.20052385 epoch total loss 6.96779823\n",
      "Trained batch 793 batch loss 7.33373642 epoch total loss 6.96825933\n",
      "Trained batch 794 batch loss 7.43592691 epoch total loss 6.96884823\n",
      "Trained batch 795 batch loss 7.37387943 epoch total loss 6.96935797\n",
      "Trained batch 796 batch loss 7.13018 epoch total loss 6.96956\n",
      "Trained batch 797 batch loss 6.81932735 epoch total loss 6.9693718\n",
      "Trained batch 798 batch loss 6.06555271 epoch total loss 6.96823931\n",
      "Trained batch 799 batch loss 6.9325633 epoch total loss 6.96819448\n",
      "Trained batch 800 batch loss 6.99415731 epoch total loss 6.96822691\n",
      "Trained batch 801 batch loss 6.83490181 epoch total loss 6.96806049\n",
      "Trained batch 802 batch loss 7.03203297 epoch total loss 6.9681406\n",
      "Trained batch 803 batch loss 6.50480318 epoch total loss 6.96756363\n",
      "Trained batch 804 batch loss 6.12133884 epoch total loss 6.96651125\n",
      "Trained batch 805 batch loss 6.73025846 epoch total loss 6.96621799\n",
      "Trained batch 806 batch loss 7.17447567 epoch total loss 6.96647644\n",
      "Trained batch 807 batch loss 7.1878233 epoch total loss 6.9667511\n",
      "Trained batch 808 batch loss 7.1653924 epoch total loss 6.96699667\n",
      "Trained batch 809 batch loss 7.24708939 epoch total loss 6.96734333\n",
      "Trained batch 810 batch loss 7.49000835 epoch total loss 6.96798849\n",
      "Trained batch 811 batch loss 7.24554968 epoch total loss 6.96833086\n",
      "Trained batch 812 batch loss 7.34482098 epoch total loss 6.96879435\n",
      "Trained batch 813 batch loss 7.43000507 epoch total loss 6.96936178\n",
      "Trained batch 814 batch loss 7.41616 epoch total loss 6.96991062\n",
      "Trained batch 815 batch loss 7.18479156 epoch total loss 6.97017431\n",
      "Trained batch 816 batch loss 7.01097059 epoch total loss 6.9702239\n",
      "Trained batch 817 batch loss 6.40649939 epoch total loss 6.96953392\n",
      "Trained batch 818 batch loss 6.48173857 epoch total loss 6.96893787\n",
      "Trained batch 819 batch loss 6.75852585 epoch total loss 6.96868086\n",
      "Trained batch 820 batch loss 7.11587811 epoch total loss 6.96886\n",
      "Trained batch 821 batch loss 6.82265663 epoch total loss 6.96868229\n",
      "Trained batch 822 batch loss 6.85308599 epoch total loss 6.96854162\n",
      "Trained batch 823 batch loss 7.13206 epoch total loss 6.96874\n",
      "Trained batch 824 batch loss 6.78910494 epoch total loss 6.96852207\n",
      "Trained batch 825 batch loss 7.00207615 epoch total loss 6.9685626\n",
      "Trained batch 826 batch loss 6.64566612 epoch total loss 6.96817112\n",
      "Trained batch 827 batch loss 6.0163908 epoch total loss 6.96702051\n",
      "Trained batch 828 batch loss 6.22889566 epoch total loss 6.9661293\n",
      "Trained batch 829 batch loss 6.4774847 epoch total loss 6.96554\n",
      "Trained batch 830 batch loss 6.97818947 epoch total loss 6.96555519\n",
      "Trained batch 831 batch loss 7.04096937 epoch total loss 6.96564579\n",
      "Trained batch 832 batch loss 7.08665848 epoch total loss 6.96579075\n",
      "Trained batch 833 batch loss 7.03995 epoch total loss 6.96588\n",
      "Trained batch 834 batch loss 7.24683285 epoch total loss 6.96621704\n",
      "Trained batch 835 batch loss 6.84158897 epoch total loss 6.96606827\n",
      "Trained batch 836 batch loss 6.64235353 epoch total loss 6.96568108\n",
      "Trained batch 837 batch loss 6.75336075 epoch total loss 6.96542788\n",
      "Trained batch 838 batch loss 7.00181103 epoch total loss 6.96547127\n",
      "Trained batch 839 batch loss 6.96598482 epoch total loss 6.96547174\n",
      "Trained batch 840 batch loss 6.95049334 epoch total loss 6.9654541\n",
      "Trained batch 841 batch loss 7.24944305 epoch total loss 6.9657917\n",
      "Trained batch 842 batch loss 7.10266399 epoch total loss 6.9659543\n",
      "Trained batch 843 batch loss 7.17054319 epoch total loss 6.96619701\n",
      "Trained batch 844 batch loss 7.39581347 epoch total loss 6.96670628\n",
      "Trained batch 845 batch loss 7.18713951 epoch total loss 6.96696663\n",
      "Trained batch 846 batch loss 7.30339146 epoch total loss 6.96736431\n",
      "Trained batch 847 batch loss 6.74572659 epoch total loss 6.96710253\n",
      "Trained batch 848 batch loss 6.928442 epoch total loss 6.96705675\n",
      "Trained batch 849 batch loss 6.56047869 epoch total loss 6.96657753\n",
      "Trained batch 850 batch loss 6.53039598 epoch total loss 6.96606445\n",
      "Trained batch 851 batch loss 6.51608706 epoch total loss 6.96553564\n",
      "Trained batch 852 batch loss 6.13754749 epoch total loss 6.96456385\n",
      "Trained batch 853 batch loss 6.20781183 epoch total loss 6.96367693\n",
      "Trained batch 854 batch loss 6.16771507 epoch total loss 6.96274471\n",
      "Trained batch 855 batch loss 6.68254423 epoch total loss 6.96241713\n",
      "Trained batch 856 batch loss 7.33394241 epoch total loss 6.96285105\n",
      "Trained batch 857 batch loss 7.13359451 epoch total loss 6.96305084\n",
      "Trained batch 858 batch loss 6.68195629 epoch total loss 6.96272326\n",
      "Trained batch 859 batch loss 6.56572056 epoch total loss 6.9622612\n",
      "Trained batch 860 batch loss 6.9462738 epoch total loss 6.9622426\n",
      "Trained batch 861 batch loss 6.92245388 epoch total loss 6.96219635\n",
      "Trained batch 862 batch loss 6.86069918 epoch total loss 6.96207905\n",
      "Trained batch 863 batch loss 6.96170664 epoch total loss 6.96207857\n",
      "Trained batch 864 batch loss 6.85254478 epoch total loss 6.96195173\n",
      "Trained batch 865 batch loss 6.96186066 epoch total loss 6.96195173\n",
      "Trained batch 866 batch loss 6.98783541 epoch total loss 6.96198177\n",
      "Trained batch 867 batch loss 7.23456383 epoch total loss 6.96229601\n",
      "Trained batch 868 batch loss 7.24708 epoch total loss 6.96262407\n",
      "Trained batch 869 batch loss 7.22086811 epoch total loss 6.96292114\n",
      "Trained batch 870 batch loss 7.17696857 epoch total loss 6.96316671\n",
      "Trained batch 871 batch loss 6.26871 epoch total loss 6.96236944\n",
      "Trained batch 872 batch loss 5.8405962 epoch total loss 6.96108294\n",
      "Trained batch 873 batch loss 6.72997332 epoch total loss 6.96081829\n",
      "Trained batch 874 batch loss 6.36468029 epoch total loss 6.96013641\n",
      "Trained batch 875 batch loss 6.77607346 epoch total loss 6.95992565\n",
      "Trained batch 876 batch loss 7.12259817 epoch total loss 6.96011162\n",
      "Trained batch 877 batch loss 7.21387625 epoch total loss 6.96040058\n",
      "Trained batch 878 batch loss 6.92549658 epoch total loss 6.960361\n",
      "Trained batch 879 batch loss 7.12013 epoch total loss 6.96054268\n",
      "Trained batch 880 batch loss 7.16392565 epoch total loss 6.96077394\n",
      "Trained batch 881 batch loss 7.23554707 epoch total loss 6.96108532\n",
      "Trained batch 882 batch loss 6.87822962 epoch total loss 6.96099186\n",
      "Trained batch 883 batch loss 7.14764214 epoch total loss 6.9612031\n",
      "Trained batch 884 batch loss 6.80141115 epoch total loss 6.9610219\n",
      "Trained batch 885 batch loss 6.47384787 epoch total loss 6.96047115\n",
      "Trained batch 886 batch loss 6.96895838 epoch total loss 6.96048069\n",
      "Trained batch 887 batch loss 7.1409831 epoch total loss 6.9606843\n",
      "Trained batch 888 batch loss 6.27472591 epoch total loss 6.95991182\n",
      "Trained batch 889 batch loss 6.33082724 epoch total loss 6.95920467\n",
      "Trained batch 890 batch loss 6.7103591 epoch total loss 6.95892525\n",
      "Trained batch 891 batch loss 6.784266 epoch total loss 6.95872879\n",
      "Trained batch 892 batch loss 7.12655926 epoch total loss 6.95891714\n",
      "Trained batch 893 batch loss 6.57008648 epoch total loss 6.95848179\n",
      "Trained batch 894 batch loss 7.1738534 epoch total loss 6.95872259\n",
      "Trained batch 895 batch loss 7.20582676 epoch total loss 6.95899916\n",
      "Trained batch 896 batch loss 7.32129669 epoch total loss 6.95940351\n",
      "Trained batch 897 batch loss 6.90053463 epoch total loss 6.95933771\n",
      "Trained batch 898 batch loss 7.21866703 epoch total loss 6.95962667\n",
      "Trained batch 899 batch loss 7.31838083 epoch total loss 6.96002579\n",
      "Trained batch 900 batch loss 6.23270273 epoch total loss 6.95921755\n",
      "Trained batch 901 batch loss 6.50036478 epoch total loss 6.95870876\n",
      "Trained batch 902 batch loss 7.1462121 epoch total loss 6.95891619\n",
      "Trained batch 903 batch loss 7.08402634 epoch total loss 6.95905447\n",
      "Trained batch 904 batch loss 7.03626347 epoch total loss 6.95914\n",
      "Trained batch 905 batch loss 7.20799112 epoch total loss 6.95941496\n",
      "Trained batch 906 batch loss 6.92943096 epoch total loss 6.95938158\n",
      "Trained batch 907 batch loss 6.92051411 epoch total loss 6.95933867\n",
      "Trained batch 908 batch loss 6.95104218 epoch total loss 6.95932961\n",
      "Trained batch 909 batch loss 6.96299791 epoch total loss 6.95933342\n",
      "Trained batch 910 batch loss 7.23599482 epoch total loss 6.95963717\n",
      "Trained batch 911 batch loss 6.89530802 epoch total loss 6.95956707\n",
      "Trained batch 912 batch loss 6.81318188 epoch total loss 6.95940638\n",
      "Trained batch 913 batch loss 7.12281513 epoch total loss 6.95958567\n",
      "Trained batch 914 batch loss 6.98660278 epoch total loss 6.95961523\n",
      "Trained batch 915 batch loss 7.51947 epoch total loss 6.96022701\n",
      "Trained batch 916 batch loss 7.40010452 epoch total loss 6.96070719\n",
      "Trained batch 917 batch loss 7.36762381 epoch total loss 6.96115112\n",
      "Trained batch 918 batch loss 6.85066032 epoch total loss 6.96103048\n",
      "Trained batch 919 batch loss 7.24825716 epoch total loss 6.96134281\n",
      "Trained batch 920 batch loss 6.37473154 epoch total loss 6.9607048\n",
      "Trained batch 921 batch loss 6.66159058 epoch total loss 6.96038\n",
      "Trained batch 922 batch loss 7.13518 epoch total loss 6.96057\n",
      "Trained batch 923 batch loss 7.14661741 epoch total loss 6.96077156\n",
      "Trained batch 924 batch loss 6.75679207 epoch total loss 6.96055079\n",
      "Trained batch 925 batch loss 6.86982059 epoch total loss 6.96045256\n",
      "Trained batch 926 batch loss 6.51127148 epoch total loss 6.95996714\n",
      "Trained batch 927 batch loss 6.92708349 epoch total loss 6.95993185\n",
      "Trained batch 928 batch loss 6.89483786 epoch total loss 6.95986223\n",
      "Trained batch 929 batch loss 6.57812166 epoch total loss 6.9594512\n",
      "Trained batch 930 batch loss 6.68259859 epoch total loss 6.95915365\n",
      "Trained batch 931 batch loss 7.06244183 epoch total loss 6.95926428\n",
      "Trained batch 932 batch loss 7.0497 epoch total loss 6.95936155\n",
      "Trained batch 933 batch loss 6.89349174 epoch total loss 6.95929098\n",
      "Trained batch 934 batch loss 6.90957928 epoch total loss 6.95923805\n",
      "Trained batch 935 batch loss 7.39891863 epoch total loss 6.95970821\n",
      "Trained batch 936 batch loss 7.3750515 epoch total loss 6.96015167\n",
      "Trained batch 937 batch loss 7.26841 epoch total loss 6.96048117\n",
      "Trained batch 938 batch loss 7.466012 epoch total loss 6.96101952\n",
      "Trained batch 939 batch loss 7.42432 epoch total loss 6.96151304\n",
      "Trained batch 940 batch loss 6.9895854 epoch total loss 6.96154308\n",
      "Trained batch 941 batch loss 6.84080696 epoch total loss 6.96141481\n",
      "Trained batch 942 batch loss 6.97648525 epoch total loss 6.96143103\n",
      "Trained batch 943 batch loss 7.28436518 epoch total loss 6.9617734\n",
      "Trained batch 944 batch loss 6.7884264 epoch total loss 6.96159\n",
      "Trained batch 945 batch loss 7.3165555 epoch total loss 6.96196508\n",
      "Trained batch 946 batch loss 7.00116825 epoch total loss 6.96200657\n",
      "Trained batch 947 batch loss 6.60646963 epoch total loss 6.96163082\n",
      "Trained batch 948 batch loss 6.52674818 epoch total loss 6.96117258\n",
      "Trained batch 949 batch loss 7.17694426 epoch total loss 6.96139956\n",
      "Trained batch 950 batch loss 7.5300374 epoch total loss 6.96199846\n",
      "Trained batch 951 batch loss 7.23122883 epoch total loss 6.9622817\n",
      "Trained batch 952 batch loss 7.29457378 epoch total loss 6.96263075\n",
      "Trained batch 953 batch loss 7.46611881 epoch total loss 6.96315908\n",
      "Trained batch 954 batch loss 7.3946991 epoch total loss 6.96361113\n",
      "Trained batch 955 batch loss 7.13887596 epoch total loss 6.96379471\n",
      "Trained batch 956 batch loss 6.74932289 epoch total loss 6.96357059\n",
      "Trained batch 957 batch loss 7.03619862 epoch total loss 6.96364641\n",
      "Trained batch 958 batch loss 7.12956572 epoch total loss 6.9638195\n",
      "Trained batch 959 batch loss 7.07822084 epoch total loss 6.96393824\n",
      "Trained batch 960 batch loss 7.33147907 epoch total loss 6.96432161\n",
      "Trained batch 961 batch loss 7.07307434 epoch total loss 6.96443462\n",
      "Trained batch 962 batch loss 7.04948854 epoch total loss 6.96452284\n",
      "Trained batch 963 batch loss 7.08692217 epoch total loss 6.96465\n",
      "Trained batch 964 batch loss 6.53263903 epoch total loss 6.96420193\n",
      "Trained batch 965 batch loss 6.73420238 epoch total loss 6.96396399\n",
      "Trained batch 966 batch loss 7.16963196 epoch total loss 6.96417665\n",
      "Trained batch 967 batch loss 6.83501434 epoch total loss 6.96404314\n",
      "Trained batch 968 batch loss 6.687819 epoch total loss 6.96375751\n",
      "Trained batch 969 batch loss 7.08618212 epoch total loss 6.96388435\n",
      "Trained batch 970 batch loss 7.2224474 epoch total loss 6.96415091\n",
      "Trained batch 971 batch loss 6.47589397 epoch total loss 6.96364832\n",
      "Trained batch 972 batch loss 6.21764803 epoch total loss 6.96288109\n",
      "Trained batch 973 batch loss 7.08932877 epoch total loss 6.96301126\n",
      "Trained batch 974 batch loss 7.26112223 epoch total loss 6.96331739\n",
      "Trained batch 975 batch loss 7.41738224 epoch total loss 6.96378326\n",
      "Trained batch 976 batch loss 7.27177763 epoch total loss 6.96409893\n",
      "Trained batch 977 batch loss 7.34235954 epoch total loss 6.96448612\n",
      "Trained batch 978 batch loss 7.4736414 epoch total loss 6.96500635\n",
      "Trained batch 979 batch loss 7.26186275 epoch total loss 6.96530962\n",
      "Trained batch 980 batch loss 7.43857 epoch total loss 6.96579218\n",
      "Trained batch 981 batch loss 7.29951239 epoch total loss 6.96613216\n",
      "Trained batch 982 batch loss 6.94458199 epoch total loss 6.96611071\n",
      "Trained batch 983 batch loss 6.96638 epoch total loss 6.96611071\n",
      "Trained batch 984 batch loss 6.89303064 epoch total loss 6.9660368\n",
      "Trained batch 985 batch loss 7.22547531 epoch total loss 6.9663\n",
      "Trained batch 986 batch loss 7.14609051 epoch total loss 6.96648264\n",
      "Trained batch 987 batch loss 7.43463039 epoch total loss 6.96695662\n",
      "Trained batch 988 batch loss 6.94150496 epoch total loss 6.96693087\n",
      "Trained batch 989 batch loss 7.18651295 epoch total loss 6.96715307\n",
      "Trained batch 990 batch loss 7.31906462 epoch total loss 6.96750832\n",
      "Trained batch 991 batch loss 7.34904051 epoch total loss 6.96789312\n",
      "Trained batch 992 batch loss 7.29355288 epoch total loss 6.96822119\n",
      "Trained batch 993 batch loss 7.06492758 epoch total loss 6.96831894\n",
      "Trained batch 994 batch loss 7.16063452 epoch total loss 6.96851206\n",
      "Trained batch 995 batch loss 6.91414547 epoch total loss 6.9684577\n",
      "Trained batch 996 batch loss 7.07245302 epoch total loss 6.96856165\n",
      "Trained batch 997 batch loss 7.40191174 epoch total loss 6.96899652\n",
      "Trained batch 998 batch loss 7.16676 epoch total loss 6.96919489\n",
      "Trained batch 999 batch loss 7.11424685 epoch total loss 6.96934\n",
      "Trained batch 1000 batch loss 7.06055641 epoch total loss 6.96943092\n",
      "Trained batch 1001 batch loss 6.81888819 epoch total loss 6.96928072\n",
      "Trained batch 1002 batch loss 6.77345705 epoch total loss 6.96908522\n",
      "Trained batch 1003 batch loss 7.12841082 epoch total loss 6.969244\n",
      "Trained batch 1004 batch loss 7.26982784 epoch total loss 6.96954393\n",
      "Trained batch 1005 batch loss 7.15559483 epoch total loss 6.96972895\n",
      "Trained batch 1006 batch loss 7.03534842 epoch total loss 6.9697938\n",
      "Trained batch 1007 batch loss 6.87079239 epoch total loss 6.96969557\n",
      "Trained batch 1008 batch loss 7.08938551 epoch total loss 6.9698143\n",
      "Trained batch 1009 batch loss 6.81160927 epoch total loss 6.96965742\n",
      "Trained batch 1010 batch loss 7.00321722 epoch total loss 6.9696908\n",
      "Trained batch 1011 batch loss 6.92684269 epoch total loss 6.96964836\n",
      "Trained batch 1012 batch loss 7.10262299 epoch total loss 6.96977949\n",
      "Trained batch 1013 batch loss 7.11919451 epoch total loss 6.96992731\n",
      "Trained batch 1014 batch loss 7.26111794 epoch total loss 6.97021437\n",
      "Trained batch 1015 batch loss 7.25719118 epoch total loss 6.97049713\n",
      "Trained batch 1016 batch loss 6.95527363 epoch total loss 6.97048187\n",
      "Trained batch 1017 batch loss 6.85119152 epoch total loss 6.97036457\n",
      "Trained batch 1018 batch loss 7.03788471 epoch total loss 6.97043133\n",
      "Trained batch 1019 batch loss 6.7129097 epoch total loss 6.9701786\n",
      "Trained batch 1020 batch loss 7.01880217 epoch total loss 6.97022629\n",
      "Trained batch 1021 batch loss 6.78066778 epoch total loss 6.9700408\n",
      "Trained batch 1022 batch loss 6.76390934 epoch total loss 6.96983862\n",
      "Trained batch 1023 batch loss 6.95818138 epoch total loss 6.96982718\n",
      "Trained batch 1024 batch loss 7.29307175 epoch total loss 6.97014284\n",
      "Trained batch 1025 batch loss 7.3489809 epoch total loss 6.97051239\n",
      "Trained batch 1026 batch loss 7.22395802 epoch total loss 6.97076\n",
      "Trained batch 1027 batch loss 7.11543655 epoch total loss 6.97090054\n",
      "Trained batch 1028 batch loss 6.42778 epoch total loss 6.9703722\n",
      "Trained batch 1029 batch loss 6.81993103 epoch total loss 6.97022581\n",
      "Trained batch 1030 batch loss 6.99577379 epoch total loss 6.97025061\n",
      "Trained batch 1031 batch loss 7.05215168 epoch total loss 6.97032976\n",
      "Trained batch 1032 batch loss 6.86159945 epoch total loss 6.97022486\n",
      "Trained batch 1033 batch loss 6.08621454 epoch total loss 6.96936941\n",
      "Trained batch 1034 batch loss 6.71582079 epoch total loss 6.96912384\n",
      "Trained batch 1035 batch loss 7.25330925 epoch total loss 6.9693985\n",
      "Trained batch 1036 batch loss 7.13637209 epoch total loss 6.96955967\n",
      "Trained batch 1037 batch loss 7.28485966 epoch total loss 6.96986341\n",
      "Trained batch 1038 batch loss 7.05164814 epoch total loss 6.96994257\n",
      "Trained batch 1039 batch loss 7.20296812 epoch total loss 6.97016668\n",
      "Trained batch 1040 batch loss 6.41162348 epoch total loss 6.96963\n",
      "Trained batch 1041 batch loss 6.71052074 epoch total loss 6.96938086\n",
      "Trained batch 1042 batch loss 7.06395435 epoch total loss 6.96947145\n",
      "Trained batch 1043 batch loss 6.89847755 epoch total loss 6.96940374\n",
      "Trained batch 1044 batch loss 6.97099686 epoch total loss 6.96940517\n",
      "Trained batch 1045 batch loss 7.36796904 epoch total loss 6.96978664\n",
      "Trained batch 1046 batch loss 7.19713545 epoch total loss 6.97000408\n",
      "Trained batch 1047 batch loss 7.2745595 epoch total loss 6.97029495\n",
      "Trained batch 1048 batch loss 7.10399723 epoch total loss 6.97042274\n",
      "Trained batch 1049 batch loss 6.85791397 epoch total loss 6.97031546\n",
      "Trained batch 1050 batch loss 6.41827965 epoch total loss 6.96979\n",
      "Trained batch 1051 batch loss 6.35253429 epoch total loss 6.96920252\n",
      "Trained batch 1052 batch loss 6.10610771 epoch total loss 6.96838188\n",
      "Trained batch 1053 batch loss 6.2673769 epoch total loss 6.96771622\n",
      "Trained batch 1054 batch loss 6.20844 epoch total loss 6.96699619\n",
      "Trained batch 1055 batch loss 5.70907116 epoch total loss 6.96580362\n",
      "Trained batch 1056 batch loss 5.68542099 epoch total loss 6.9645915\n",
      "Trained batch 1057 batch loss 5.8584671 epoch total loss 6.96354485\n",
      "Trained batch 1058 batch loss 6.58112669 epoch total loss 6.9631834\n",
      "Trained batch 1059 batch loss 7.17634392 epoch total loss 6.96338463\n",
      "Trained batch 1060 batch loss 7.20166731 epoch total loss 6.96360922\n",
      "Trained batch 1061 batch loss 7.09049416 epoch total loss 6.96372843\n",
      "Trained batch 1062 batch loss 7.10263491 epoch total loss 6.96385956\n",
      "Trained batch 1063 batch loss 7.18383265 epoch total loss 6.96406603\n",
      "Trained batch 1064 batch loss 7.38417 epoch total loss 6.96446085\n",
      "Trained batch 1065 batch loss 7.44094419 epoch total loss 6.9649086\n",
      "Trained batch 1066 batch loss 7.27708149 epoch total loss 6.9652009\n",
      "Trained batch 1067 batch loss 7.20196056 epoch total loss 6.96542311\n",
      "Trained batch 1068 batch loss 6.94468546 epoch total loss 6.96540403\n",
      "Trained batch 1069 batch loss 7.2702527 epoch total loss 6.96568871\n",
      "Trained batch 1070 batch loss 7.13017511 epoch total loss 6.96584272\n",
      "Trained batch 1071 batch loss 7.46551228 epoch total loss 6.96630907\n",
      "Trained batch 1072 batch loss 7.26083422 epoch total loss 6.96658373\n",
      "Trained batch 1073 batch loss 7.41009855 epoch total loss 6.96699715\n",
      "Trained batch 1074 batch loss 7.327878 epoch total loss 6.96733284\n",
      "Trained batch 1075 batch loss 7.279603 epoch total loss 6.96762371\n",
      "Trained batch 1076 batch loss 7.19881821 epoch total loss 6.96783829\n",
      "Trained batch 1077 batch loss 7.02444077 epoch total loss 6.96789074\n",
      "Trained batch 1078 batch loss 6.9469 epoch total loss 6.96787119\n",
      "Trained batch 1079 batch loss 7.2004385 epoch total loss 6.96808672\n",
      "Trained batch 1080 batch loss 7.24599838 epoch total loss 6.96834421\n",
      "Trained batch 1081 batch loss 7.41811275 epoch total loss 6.96876\n",
      "Trained batch 1082 batch loss 7.15521383 epoch total loss 6.96893215\n",
      "Trained batch 1083 batch loss 7.18598 epoch total loss 6.9691329\n",
      "Trained batch 1084 batch loss 7.06984234 epoch total loss 6.96922588\n",
      "Trained batch 1085 batch loss 7.03564072 epoch total loss 6.96928692\n",
      "Trained batch 1086 batch loss 6.8623457 epoch total loss 6.96918821\n",
      "Trained batch 1087 batch loss 6.60209608 epoch total loss 6.96885061\n",
      "Trained batch 1088 batch loss 6.32797718 epoch total loss 6.96826172\n",
      "Trained batch 1089 batch loss 6.48366117 epoch total loss 6.96781683\n",
      "Trained batch 1090 batch loss 6.65507793 epoch total loss 6.96753025\n",
      "Trained batch 1091 batch loss 7.19441271 epoch total loss 6.96773815\n",
      "Trained batch 1092 batch loss 7.40250111 epoch total loss 6.96813583\n",
      "Trained batch 1093 batch loss 7.41750193 epoch total loss 6.96854734\n",
      "Trained batch 1094 batch loss 7.33639336 epoch total loss 6.96888351\n",
      "Trained batch 1095 batch loss 7.41580057 epoch total loss 6.96929169\n",
      "Trained batch 1096 batch loss 7.02827835 epoch total loss 6.96934557\n",
      "Trained batch 1097 batch loss 7.06964207 epoch total loss 6.96943712\n",
      "Trained batch 1098 batch loss 7.22762966 epoch total loss 6.9696722\n",
      "Trained batch 1099 batch loss 7.13433123 epoch total loss 6.96982193\n",
      "Trained batch 1100 batch loss 7.12600231 epoch total loss 6.96996403\n",
      "Trained batch 1101 batch loss 7.24012947 epoch total loss 6.9702096\n",
      "Trained batch 1102 batch loss 7.33805037 epoch total loss 6.97054338\n",
      "Trained batch 1103 batch loss 7.30832577 epoch total loss 6.97084904\n",
      "Trained batch 1104 batch loss 6.94331932 epoch total loss 6.97082424\n",
      "Trained batch 1105 batch loss 7.12971592 epoch total loss 6.97096825\n",
      "Trained batch 1106 batch loss 7.25850534 epoch total loss 6.97122812\n",
      "Trained batch 1107 batch loss 7.0004878 epoch total loss 6.97125435\n",
      "Trained batch 1108 batch loss 6.74016094 epoch total loss 6.97104597\n",
      "Trained batch 1109 batch loss 7.27563095 epoch total loss 6.97132063\n",
      "Trained batch 1110 batch loss 7.08146667 epoch total loss 6.97142\n",
      "Trained batch 1111 batch loss 7.08919096 epoch total loss 6.97152567\n",
      "Trained batch 1112 batch loss 7.17989254 epoch total loss 6.97171307\n",
      "Trained batch 1113 batch loss 6.75688744 epoch total loss 6.97152\n",
      "Trained batch 1114 batch loss 6.65715551 epoch total loss 6.97123766\n",
      "Trained batch 1115 batch loss 6.94096088 epoch total loss 6.97121048\n",
      "Trained batch 1116 batch loss 7.04027224 epoch total loss 6.97127247\n",
      "Trained batch 1117 batch loss 7.20116472 epoch total loss 6.97147799\n",
      "Trained batch 1118 batch loss 7.31470871 epoch total loss 6.97178555\n",
      "Trained batch 1119 batch loss 7.21210861 epoch total loss 6.972\n",
      "Trained batch 1120 batch loss 7.20455265 epoch total loss 6.97220755\n",
      "Trained batch 1121 batch loss 6.79104185 epoch total loss 6.9720459\n",
      "Trained batch 1122 batch loss 6.95368147 epoch total loss 6.97202969\n",
      "Trained batch 1123 batch loss 7.21129656 epoch total loss 6.97224283\n",
      "Trained batch 1124 batch loss 7.13836908 epoch total loss 6.97239\n",
      "Trained batch 1125 batch loss 7.05060577 epoch total loss 6.97246027\n",
      "Trained batch 1126 batch loss 6.41518402 epoch total loss 6.97196484\n",
      "Trained batch 1127 batch loss 6.82069 epoch total loss 6.97183084\n",
      "Trained batch 1128 batch loss 7.35556221 epoch total loss 6.97217083\n",
      "Trained batch 1129 batch loss 7.31204128 epoch total loss 6.97247219\n",
      "Trained batch 1130 batch loss 6.5806694 epoch total loss 6.97212505\n",
      "Trained batch 1131 batch loss 7.09290504 epoch total loss 6.97223186\n",
      "Trained batch 1132 batch loss 6.75098372 epoch total loss 6.97203636\n",
      "Trained batch 1133 batch loss 6.2184515 epoch total loss 6.97137117\n",
      "Trained batch 1134 batch loss 6.62620592 epoch total loss 6.97106647\n",
      "Trained batch 1135 batch loss 6.47955942 epoch total loss 6.97063351\n",
      "Trained batch 1136 batch loss 6.96450329 epoch total loss 6.97062778\n",
      "Trained batch 1137 batch loss 6.49913549 epoch total loss 6.97021294\n",
      "Trained batch 1138 batch loss 6.5398159 epoch total loss 6.96983528\n",
      "Trained batch 1139 batch loss 6.23203278 epoch total loss 6.96918726\n",
      "Trained batch 1140 batch loss 6.56400108 epoch total loss 6.96883202\n",
      "Trained batch 1141 batch loss 6.53856945 epoch total loss 6.96845484\n",
      "Trained batch 1142 batch loss 6.89080048 epoch total loss 6.96838665\n",
      "Trained batch 1143 batch loss 7.18437 epoch total loss 6.96857548\n",
      "Trained batch 1144 batch loss 6.91283512 epoch total loss 6.96852684\n",
      "Trained batch 1145 batch loss 7.01825428 epoch total loss 6.96857\n",
      "Trained batch 1146 batch loss 7.23801613 epoch total loss 6.96880484\n",
      "Trained batch 1147 batch loss 7.40402126 epoch total loss 6.9691844\n",
      "Trained batch 1148 batch loss 7.47128677 epoch total loss 6.96962166\n",
      "Trained batch 1149 batch loss 6.98868275 epoch total loss 6.96963835\n",
      "Trained batch 1150 batch loss 7.15707588 epoch total loss 6.96980143\n",
      "Trained batch 1151 batch loss 7.18123817 epoch total loss 6.96998501\n",
      "Trained batch 1152 batch loss 6.94706297 epoch total loss 6.96996498\n",
      "Trained batch 1153 batch loss 6.95149422 epoch total loss 6.96994925\n",
      "Trained batch 1154 batch loss 6.66278648 epoch total loss 6.96968317\n",
      "Trained batch 1155 batch loss 6.83484268 epoch total loss 6.96956635\n",
      "Trained batch 1156 batch loss 6.77306414 epoch total loss 6.96939611\n",
      "Trained batch 1157 batch loss 6.60525084 epoch total loss 6.96908188\n",
      "Trained batch 1158 batch loss 6.71643162 epoch total loss 6.96886349\n",
      "Trained batch 1159 batch loss 6.49641037 epoch total loss 6.96845579\n",
      "Trained batch 1160 batch loss 6.74593 epoch total loss 6.9682641\n",
      "Trained batch 1161 batch loss 6.70447063 epoch total loss 6.96803713\n",
      "Trained batch 1162 batch loss 7.29352951 epoch total loss 6.96831703\n",
      "Trained batch 1163 batch loss 7.19787598 epoch total loss 6.96851444\n",
      "Trained batch 1164 batch loss 6.93738794 epoch total loss 6.96848774\n",
      "Trained batch 1165 batch loss 7.06418324 epoch total loss 6.96856976\n",
      "Trained batch 1166 batch loss 6.79229212 epoch total loss 6.9684186\n",
      "Trained batch 1167 batch loss 6.93052101 epoch total loss 6.96838617\n",
      "Trained batch 1168 batch loss 7.00185442 epoch total loss 6.96841526\n",
      "Trained batch 1169 batch loss 6.86256218 epoch total loss 6.96832466\n",
      "Trained batch 1170 batch loss 6.8540926 epoch total loss 6.96822691\n",
      "Trained batch 1171 batch loss 6.44102955 epoch total loss 6.96777678\n",
      "Trained batch 1172 batch loss 6.89461899 epoch total loss 6.96771431\n",
      "Trained batch 1173 batch loss 6.92506 epoch total loss 6.96767807\n",
      "Trained batch 1174 batch loss 6.64525652 epoch total loss 6.96740341\n",
      "Trained batch 1175 batch loss 6.94730043 epoch total loss 6.96738625\n",
      "Trained batch 1176 batch loss 6.94301891 epoch total loss 6.96736574\n",
      "Trained batch 1177 batch loss 6.43015194 epoch total loss 6.96690893\n",
      "Trained batch 1178 batch loss 6.83030224 epoch total loss 6.96679258\n",
      "Trained batch 1179 batch loss 7.15127373 epoch total loss 6.96694946\n",
      "Trained batch 1180 batch loss 6.67394781 epoch total loss 6.96670103\n",
      "Trained batch 1181 batch loss 7.07160187 epoch total loss 6.96678925\n",
      "Trained batch 1182 batch loss 7.04810858 epoch total loss 6.96685791\n",
      "Trained batch 1183 batch loss 6.67492056 epoch total loss 6.96661091\n",
      "Trained batch 1184 batch loss 6.36241627 epoch total loss 6.96610069\n",
      "Trained batch 1185 batch loss 7.30320454 epoch total loss 6.96638489\n",
      "Trained batch 1186 batch loss 7.33453703 epoch total loss 6.96669579\n",
      "Trained batch 1187 batch loss 7.28632736 epoch total loss 6.96696472\n",
      "Trained batch 1188 batch loss 7.27920055 epoch total loss 6.96722746\n",
      "Trained batch 1189 batch loss 6.62038 epoch total loss 6.96693563\n",
      "Trained batch 1190 batch loss 6.9575429 epoch total loss 6.96692801\n",
      "Trained batch 1191 batch loss 6.84464073 epoch total loss 6.96682549\n",
      "Trained batch 1192 batch loss 6.41089582 epoch total loss 6.96635962\n",
      "Trained batch 1193 batch loss 6.91501045 epoch total loss 6.96631622\n",
      "Trained batch 1194 batch loss 6.82973671 epoch total loss 6.96620226\n",
      "Trained batch 1195 batch loss 6.68267632 epoch total loss 6.96596479\n",
      "Trained batch 1196 batch loss 6.26024055 epoch total loss 6.96537447\n",
      "Trained batch 1197 batch loss 6.3387742 epoch total loss 6.9648509\n",
      "Trained batch 1198 batch loss 6.08210945 epoch total loss 6.96411419\n",
      "Trained batch 1199 batch loss 6.61617613 epoch total loss 6.9638238\n",
      "Trained batch 1200 batch loss 6.57040548 epoch total loss 6.96349621\n",
      "Trained batch 1201 batch loss 7.01127148 epoch total loss 6.96353626\n",
      "Trained batch 1202 batch loss 7.12543297 epoch total loss 6.96367073\n",
      "Trained batch 1203 batch loss 7.31756115 epoch total loss 6.96396446\n",
      "Trained batch 1204 batch loss 7.27662086 epoch total loss 6.96422386\n",
      "Trained batch 1205 batch loss 7.17627335 epoch total loss 6.96440029\n",
      "Trained batch 1206 batch loss 7.10534239 epoch total loss 6.96451759\n",
      "Trained batch 1207 batch loss 6.65224266 epoch total loss 6.96425867\n",
      "Trained batch 1208 batch loss 6.28697395 epoch total loss 6.96369839\n",
      "Trained batch 1209 batch loss 6.60663176 epoch total loss 6.96340275\n",
      "Trained batch 1210 batch loss 7.18408108 epoch total loss 6.9635849\n",
      "Trained batch 1211 batch loss 7.10731888 epoch total loss 6.96370363\n",
      "Trained batch 1212 batch loss 7.10020638 epoch total loss 6.96381664\n",
      "Trained batch 1213 batch loss 7.15498495 epoch total loss 6.96397448\n",
      "Trained batch 1214 batch loss 7.23495102 epoch total loss 6.96419764\n",
      "Trained batch 1215 batch loss 7.30837584 epoch total loss 6.96448135\n",
      "Trained batch 1216 batch loss 7.22080374 epoch total loss 6.96469212\n",
      "Trained batch 1217 batch loss 6.69299603 epoch total loss 6.96446896\n",
      "Trained batch 1218 batch loss 6.90862036 epoch total loss 6.9644227\n",
      "Trained batch 1219 batch loss 7.06300163 epoch total loss 6.96450424\n",
      "Trained batch 1220 batch loss 6.47452 epoch total loss 6.96410275\n",
      "Trained batch 1221 batch loss 6.68208122 epoch total loss 6.963871\n",
      "Trained batch 1222 batch loss 7.08910084 epoch total loss 6.96397352\n",
      "Trained batch 1223 batch loss 6.64543867 epoch total loss 6.96371317\n",
      "Trained batch 1224 batch loss 7.08441687 epoch total loss 6.9638114\n",
      "Trained batch 1225 batch loss 7.08412027 epoch total loss 6.96390963\n",
      "Trained batch 1226 batch loss 7.01509571 epoch total loss 6.96395063\n",
      "Trained batch 1227 batch loss 7.21712351 epoch total loss 6.9641571\n",
      "Trained batch 1228 batch loss 7.18911743 epoch total loss 6.96434\n",
      "Trained batch 1229 batch loss 6.79445362 epoch total loss 6.9642024\n",
      "Trained batch 1230 batch loss 6.55284643 epoch total loss 6.96386814\n",
      "Trained batch 1231 batch loss 6.98753357 epoch total loss 6.96388721\n",
      "Trained batch 1232 batch loss 6.96096897 epoch total loss 6.96388483\n",
      "Trained batch 1233 batch loss 6.95588064 epoch total loss 6.96387815\n",
      "Trained batch 1234 batch loss 7.1332283 epoch total loss 6.96401501\n",
      "Trained batch 1235 batch loss 7.05353451 epoch total loss 6.96408796\n",
      "Trained batch 1236 batch loss 6.99586201 epoch total loss 6.96411371\n",
      "Trained batch 1237 batch loss 6.71146536 epoch total loss 6.96390963\n",
      "Trained batch 1238 batch loss 6.68147707 epoch total loss 6.9636817\n",
      "Trained batch 1239 batch loss 6.87977314 epoch total loss 6.96361399\n",
      "Trained batch 1240 batch loss 7.40731955 epoch total loss 6.96397209\n",
      "Trained batch 1241 batch loss 6.49746466 epoch total loss 6.96359587\n",
      "Trained batch 1242 batch loss 5.7479682 epoch total loss 6.96261692\n",
      "Trained batch 1243 batch loss 6.83610439 epoch total loss 6.96251488\n",
      "Trained batch 1244 batch loss 7.38430595 epoch total loss 6.96285439\n",
      "Trained batch 1245 batch loss 7.20277452 epoch total loss 6.9630475\n",
      "Trained batch 1246 batch loss 7.29094648 epoch total loss 6.96331072\n",
      "Trained batch 1247 batch loss 7.19086027 epoch total loss 6.96349287\n",
      "Trained batch 1248 batch loss 7.38063288 epoch total loss 6.96382713\n",
      "Trained batch 1249 batch loss 7.1685648 epoch total loss 6.96399164\n",
      "Trained batch 1250 batch loss 7.18334246 epoch total loss 6.96416712\n",
      "Trained batch 1251 batch loss 6.783041 epoch total loss 6.96402264\n",
      "Trained batch 1252 batch loss 7.00540686 epoch total loss 6.96405602\n",
      "Trained batch 1253 batch loss 6.97970867 epoch total loss 6.96406841\n",
      "Trained batch 1254 batch loss 6.92751265 epoch total loss 6.96403933\n",
      "Trained batch 1255 batch loss 7.14430237 epoch total loss 6.96418333\n",
      "Trained batch 1256 batch loss 6.47586298 epoch total loss 6.96379423\n",
      "Trained batch 1257 batch loss 6.94103098 epoch total loss 6.96377611\n",
      "Trained batch 1258 batch loss 6.70334864 epoch total loss 6.96356916\n",
      "Trained batch 1259 batch loss 6.97739267 epoch total loss 6.96358\n",
      "Trained batch 1260 batch loss 7.02967215 epoch total loss 6.96363258\n",
      "Trained batch 1261 batch loss 6.94056034 epoch total loss 6.96361399\n",
      "Trained batch 1262 batch loss 7.15797663 epoch total loss 6.96376801\n",
      "Trained batch 1263 batch loss 6.99551821 epoch total loss 6.9637928\n",
      "Trained batch 1264 batch loss 7.12897 epoch total loss 6.96392345\n",
      "Trained batch 1265 batch loss 7.04879236 epoch total loss 6.96399069\n",
      "Trained batch 1266 batch loss 6.35341692 epoch total loss 6.96350861\n",
      "Trained batch 1267 batch loss 6.36412287 epoch total loss 6.96303558\n",
      "Trained batch 1268 batch loss 6.7762723 epoch total loss 6.96288824\n",
      "Trained batch 1269 batch loss 6.97825146 epoch total loss 6.96290064\n",
      "Trained batch 1270 batch loss 6.96460676 epoch total loss 6.96290207\n",
      "Trained batch 1271 batch loss 7.02549362 epoch total loss 6.96295118\n",
      "Trained batch 1272 batch loss 7.42335081 epoch total loss 6.96331358\n",
      "Trained batch 1273 batch loss 7.13509417 epoch total loss 6.96344852\n",
      "Trained batch 1274 batch loss 6.89506578 epoch total loss 6.96339512\n",
      "Trained batch 1275 batch loss 6.95364189 epoch total loss 6.96338749\n",
      "Trained batch 1276 batch loss 7.19656515 epoch total loss 6.96357\n",
      "Trained batch 1277 batch loss 6.82614708 epoch total loss 6.96346283\n",
      "Trained batch 1278 batch loss 7.14850092 epoch total loss 6.96360731\n",
      "Trained batch 1279 batch loss 6.93496656 epoch total loss 6.9635849\n",
      "Trained batch 1280 batch loss 6.15157652 epoch total loss 6.96295\n",
      "Trained batch 1281 batch loss 6.4622674 epoch total loss 6.96255922\n",
      "Trained batch 1282 batch loss 6.85115528 epoch total loss 6.96247244\n",
      "Trained batch 1283 batch loss 7.11301422 epoch total loss 6.96258974\n",
      "Trained batch 1284 batch loss 7.00783825 epoch total loss 6.96262503\n",
      "Trained batch 1285 batch loss 7.14079475 epoch total loss 6.96276379\n",
      "Trained batch 1286 batch loss 7.17957258 epoch total loss 6.96293259\n",
      "Trained batch 1287 batch loss 7.19406509 epoch total loss 6.96311235\n",
      "Trained batch 1288 batch loss 7.33980322 epoch total loss 6.96340466\n",
      "Trained batch 1289 batch loss 7.44441938 epoch total loss 6.96377802\n",
      "Trained batch 1290 batch loss 7.01117802 epoch total loss 6.96381426\n",
      "Trained batch 1291 batch loss 7.30033541 epoch total loss 6.96407509\n",
      "Trained batch 1292 batch loss 7.14595 epoch total loss 6.96421576\n",
      "Trained batch 1293 batch loss 6.61283493 epoch total loss 6.96394444\n",
      "Trained batch 1294 batch loss 6.99388075 epoch total loss 6.96396732\n",
      "Trained batch 1295 batch loss 7.03358078 epoch total loss 6.96402121\n",
      "Trained batch 1296 batch loss 7.33806705 epoch total loss 6.96430969\n",
      "Trained batch 1297 batch loss 7.19098473 epoch total loss 6.96448469\n",
      "Trained batch 1298 batch loss 6.29404068 epoch total loss 6.9639678\n",
      "Trained batch 1299 batch loss 6.72842932 epoch total loss 6.9637866\n",
      "Trained batch 1300 batch loss 6.36439943 epoch total loss 6.9633255\n",
      "Trained batch 1301 batch loss 7.22886086 epoch total loss 6.96352959\n",
      "Trained batch 1302 batch loss 6.88991499 epoch total loss 6.96347284\n",
      "Trained batch 1303 batch loss 6.73426199 epoch total loss 6.96329689\n",
      "Trained batch 1304 batch loss 7.03310966 epoch total loss 6.9633503\n",
      "Trained batch 1305 batch loss 6.88785601 epoch total loss 6.9632926\n",
      "Trained batch 1306 batch loss 7.31554174 epoch total loss 6.96356201\n",
      "Trained batch 1307 batch loss 7.29526663 epoch total loss 6.96381569\n",
      "Trained batch 1308 batch loss 7.32222176 epoch total loss 6.96409\n",
      "Trained batch 1309 batch loss 7.26934195 epoch total loss 6.96432304\n",
      "Trained batch 1310 batch loss 7.04715967 epoch total loss 6.96438599\n",
      "Trained batch 1311 batch loss 7.09607267 epoch total loss 6.96448612\n",
      "Trained batch 1312 batch loss 7.2642045 epoch total loss 6.964715\n",
      "Trained batch 1313 batch loss 7.22864199 epoch total loss 6.96491575\n",
      "Trained batch 1314 batch loss 6.90513372 epoch total loss 6.96487045\n",
      "Trained batch 1315 batch loss 7.24153 epoch total loss 6.96508074\n",
      "Trained batch 1316 batch loss 6.77707148 epoch total loss 6.96493816\n",
      "Trained batch 1317 batch loss 7.39515162 epoch total loss 6.9652648\n",
      "Trained batch 1318 batch loss 7.37633371 epoch total loss 6.96557665\n",
      "Trained batch 1319 batch loss 7.51108408 epoch total loss 6.96599\n",
      "Trained batch 1320 batch loss 7.50501728 epoch total loss 6.96639824\n",
      "Trained batch 1321 batch loss 7.5347147 epoch total loss 6.96682882\n",
      "Trained batch 1322 batch loss 7.14915276 epoch total loss 6.96696663\n",
      "Trained batch 1323 batch loss 6.92227125 epoch total loss 6.96693277\n",
      "Trained batch 1324 batch loss 6.86901665 epoch total loss 6.96685886\n",
      "Trained batch 1325 batch loss 7.15684652 epoch total loss 6.96700239\n",
      "Trained batch 1326 batch loss 7.26416349 epoch total loss 6.96722698\n",
      "Trained batch 1327 batch loss 7.28706789 epoch total loss 6.96746826\n",
      "Trained batch 1328 batch loss 7.30713654 epoch total loss 6.96772432\n",
      "Trained batch 1329 batch loss 7.00827265 epoch total loss 6.96775436\n",
      "Trained batch 1330 batch loss 7.26124334 epoch total loss 6.96797514\n",
      "Trained batch 1331 batch loss 7.33923721 epoch total loss 6.96825409\n",
      "Trained batch 1332 batch loss 7.03618908 epoch total loss 6.96830511\n",
      "Trained batch 1333 batch loss 7.38156557 epoch total loss 6.96861506\n",
      "Trained batch 1334 batch loss 6.9236908 epoch total loss 6.96858168\n",
      "Trained batch 1335 batch loss 6.36776114 epoch total loss 6.96813202\n",
      "Trained batch 1336 batch loss 6.95060682 epoch total loss 6.96811867\n",
      "Trained batch 1337 batch loss 7.08102131 epoch total loss 6.96820307\n",
      "Trained batch 1338 batch loss 7.12542534 epoch total loss 6.96832\n",
      "Trained batch 1339 batch loss 6.87882614 epoch total loss 6.96825314\n",
      "Trained batch 1340 batch loss 7.2543149 epoch total loss 6.96846628\n",
      "Trained batch 1341 batch loss 7.2632308 epoch total loss 6.96868658\n",
      "Trained batch 1342 batch loss 7.2060523 epoch total loss 6.96886349\n",
      "Trained batch 1343 batch loss 7.06262589 epoch total loss 6.96893311\n",
      "Trained batch 1344 batch loss 7.22890329 epoch total loss 6.96912622\n",
      "Trained batch 1345 batch loss 7.32430935 epoch total loss 6.96939039\n",
      "Trained batch 1346 batch loss 7.29942465 epoch total loss 6.96963596\n",
      "Trained batch 1347 batch loss 7.03175783 epoch total loss 6.96968222\n",
      "Trained batch 1348 batch loss 7.15477371 epoch total loss 6.96981907\n",
      "Trained batch 1349 batch loss 7.04758739 epoch total loss 6.96987724\n",
      "Trained batch 1350 batch loss 7.13950682 epoch total loss 6.97000313\n",
      "Trained batch 1351 batch loss 7.11364 epoch total loss 6.97010899\n",
      "Trained batch 1352 batch loss 7.2675662 epoch total loss 6.97032881\n",
      "Trained batch 1353 batch loss 6.60584688 epoch total loss 6.97005939\n",
      "Trained batch 1354 batch loss 7.08599806 epoch total loss 6.97014475\n",
      "Trained batch 1355 batch loss 6.70022631 epoch total loss 6.96994543\n",
      "Trained batch 1356 batch loss 7.44374657 epoch total loss 6.97029495\n",
      "Trained batch 1357 batch loss 7.20532179 epoch total loss 6.97046757\n",
      "Trained batch 1358 batch loss 7.23388958 epoch total loss 6.97066212\n",
      "Trained batch 1359 batch loss 7.28091621 epoch total loss 6.97089052\n",
      "Trained batch 1360 batch loss 7.39409065 epoch total loss 6.97120237\n",
      "Trained batch 1361 batch loss 7.20721197 epoch total loss 6.97137547\n",
      "Trained batch 1362 batch loss 6.92393446 epoch total loss 6.97134066\n",
      "Trained batch 1363 batch loss 6.69667578 epoch total loss 6.97113895\n",
      "Trained batch 1364 batch loss 7.01956367 epoch total loss 6.97117424\n",
      "Trained batch 1365 batch loss 6.21914387 epoch total loss 6.97062302\n",
      "Trained batch 1366 batch loss 7.21937895 epoch total loss 6.97080517\n",
      "Trained batch 1367 batch loss 6.95137215 epoch total loss 6.97079086\n",
      "Trained batch 1368 batch loss 7.14748096 epoch total loss 6.97092\n",
      "Trained batch 1369 batch loss 7.1565094 epoch total loss 6.97105551\n",
      "Trained batch 1370 batch loss 7.03642035 epoch total loss 6.97110319\n",
      "Trained batch 1371 batch loss 6.99614334 epoch total loss 6.97112131\n",
      "Trained batch 1372 batch loss 6.86801338 epoch total loss 6.97104597\n",
      "Trained batch 1373 batch loss 6.9751029 epoch total loss 6.97104931\n",
      "Trained batch 1374 batch loss 6.06997204 epoch total loss 6.97039413\n",
      "Trained batch 1375 batch loss 6.11918592 epoch total loss 6.96977472\n",
      "Trained batch 1376 batch loss 5.39253902 epoch total loss 6.96862841\n",
      "Trained batch 1377 batch loss 6.13453 epoch total loss 6.9680233\n",
      "Trained batch 1378 batch loss 6.57492542 epoch total loss 6.96773815\n",
      "Trained batch 1379 batch loss 7.33926868 epoch total loss 6.96800709\n",
      "Trained batch 1380 batch loss 7.11919117 epoch total loss 6.96811676\n",
      "Trained batch 1381 batch loss 6.7410183 epoch total loss 6.96795225\n",
      "Trained batch 1382 batch loss 6.96843624 epoch total loss 6.96795273\n",
      "Trained batch 1383 batch loss 7.04270554 epoch total loss 6.96800709\n",
      "Trained batch 1384 batch loss 6.15673351 epoch total loss 6.96742058\n",
      "Trained batch 1385 batch loss 6.48836327 epoch total loss 6.96707487\n",
      "Trained batch 1386 batch loss 6.48966169 epoch total loss 6.96673\n",
      "Trained batch 1387 batch loss 6.51780844 epoch total loss 6.96640635\n",
      "Trained batch 1388 batch loss 6.86555624 epoch total loss 6.96633339\n",
      "Epoch 8 train loss 6.966333389282227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 03:09:53.584699: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "2025-03-19 03:09:53.584758: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "\t [[RemoteCall]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 1 batch loss 6.36806154\n",
      "Validated batch 2 batch loss 7.38809109\n",
      "Validated batch 3 batch loss 7.09489679\n",
      "Validated batch 4 batch loss 7.07266808\n",
      "Validated batch 5 batch loss 7.01413\n",
      "Validated batch 6 batch loss 7.37156\n",
      "Validated batch 7 batch loss 6.87123489\n",
      "Validated batch 8 batch loss 7.33314657\n",
      "Validated batch 9 batch loss 6.68127918\n",
      "Validated batch 10 batch loss 6.9871254\n",
      "Validated batch 11 batch loss 6.84380484\n",
      "Validated batch 12 batch loss 6.37764263\n",
      "Validated batch 13 batch loss 6.60864162\n",
      "Validated batch 14 batch loss 7.37632513\n",
      "Validated batch 15 batch loss 7.17257214\n",
      "Validated batch 16 batch loss 6.74221325\n",
      "Validated batch 17 batch loss 7.2381587\n",
      "Validated batch 18 batch loss 7.17109776\n",
      "Validated batch 19 batch loss 7.20078421\n",
      "Validated batch 20 batch loss 7.4448247\n",
      "Validated batch 21 batch loss 7.2129817\n",
      "Validated batch 22 batch loss 6.90967941\n",
      "Validated batch 23 batch loss 6.6204257\n",
      "Validated batch 24 batch loss 6.90527248\n",
      "Validated batch 25 batch loss 7.0721364\n",
      "Validated batch 26 batch loss 6.53531885\n",
      "Validated batch 27 batch loss 6.92380238\n",
      "Validated batch 28 batch loss 7.05026054\n",
      "Validated batch 29 batch loss 7.11833382\n",
      "Validated batch 30 batch loss 6.9263382\n",
      "Validated batch 31 batch loss 6.98119354\n",
      "Validated batch 32 batch loss 6.98056602\n",
      "Validated batch 33 batch loss 7.27371025\n",
      "Validated batch 34 batch loss 7.39289618\n",
      "Validated batch 35 batch loss 7.05453777\n",
      "Validated batch 36 batch loss 7.02096272\n",
      "Validated batch 37 batch loss 6.90314627\n",
      "Validated batch 38 batch loss 7.11160851\n",
      "Validated batch 39 batch loss 6.95280743\n",
      "Validated batch 40 batch loss 6.96088076\n",
      "Validated batch 41 batch loss 6.98823643\n",
      "Validated batch 42 batch loss 6.91342449\n",
      "Validated batch 43 batch loss 7.13091278\n",
      "Validated batch 44 batch loss 6.71035528\n",
      "Validated batch 45 batch loss 7.15633535\n",
      "Validated batch 46 batch loss 7.08040476\n",
      "Validated batch 47 batch loss 7.16326046\n",
      "Validated batch 48 batch loss 7.2222476\n",
      "Validated batch 49 batch loss 7.02361822\n",
      "Validated batch 50 batch loss 6.26882\n",
      "Validated batch 51 batch loss 7.02506113\n",
      "Validated batch 52 batch loss 6.75085783\n",
      "Validated batch 53 batch loss 6.37485456\n",
      "Validated batch 54 batch loss 6.94607258\n",
      "Validated batch 55 batch loss 6.60010052\n",
      "Validated batch 56 batch loss 7.11105061\n",
      "Validated batch 57 batch loss 6.65513325\n",
      "Validated batch 58 batch loss 6.53538465\n",
      "Validated batch 59 batch loss 6.82453632\n",
      "Validated batch 60 batch loss 6.87479639\n",
      "Validated batch 61 batch loss 6.98021126\n",
      "Validated batch 62 batch loss 7.0871439\n",
      "Validated batch 63 batch loss 6.87296152\n",
      "Validated batch 64 batch loss 7.16334391\n",
      "Validated batch 65 batch loss 7.02609968\n",
      "Validated batch 66 batch loss 6.60705566\n",
      "Validated batch 67 batch loss 7.15336847\n",
      "Validated batch 68 batch loss 5.97553205\n",
      "Validated batch 69 batch loss 7.40902185\n",
      "Validated batch 70 batch loss 6.87447643\n",
      "Validated batch 71 batch loss 6.57338858\n",
      "Validated batch 72 batch loss 6.96018839\n",
      "Validated batch 73 batch loss 7.14555836\n",
      "Validated batch 74 batch loss 6.70655441\n",
      "Validated batch 75 batch loss 7.20512819\n",
      "Validated batch 76 batch loss 7.07866764\n",
      "Validated batch 77 batch loss 7.11763763\n",
      "Validated batch 78 batch loss 6.95637226\n",
      "Validated batch 79 batch loss 7.02001095\n",
      "Validated batch 80 batch loss 6.75139809\n",
      "Validated batch 81 batch loss 7.09927702\n",
      "Validated batch 82 batch loss 6.83372116\n",
      "Validated batch 83 batch loss 7.24809551\n",
      "Validated batch 84 batch loss 7.34954929\n",
      "Validated batch 85 batch loss 6.84601688\n",
      "Validated batch 86 batch loss 7.13311243\n",
      "Validated batch 87 batch loss 6.41682\n",
      "Validated batch 88 batch loss 7.19128036\n",
      "Validated batch 89 batch loss 7.29540873\n",
      "Validated batch 90 batch loss 7.12366152\n",
      "Validated batch 91 batch loss 7.27202463\n",
      "Validated batch 92 batch loss 6.79584312\n",
      "Validated batch 93 batch loss 7.48134089\n",
      "Validated batch 94 batch loss 7.02404165\n",
      "Validated batch 95 batch loss 7.14608765\n",
      "Validated batch 96 batch loss 7.08945\n",
      "Validated batch 97 batch loss 7.16936588\n",
      "Validated batch 98 batch loss 7.29312\n",
      "Validated batch 99 batch loss 7.04298687\n",
      "Validated batch 100 batch loss 6.73121166\n",
      "Validated batch 101 batch loss 6.83795404\n",
      "Validated batch 102 batch loss 6.8650322\n",
      "Validated batch 103 batch loss 7.14213181\n",
      "Validated batch 104 batch loss 6.99594212\n",
      "Validated batch 105 batch loss 6.63689\n",
      "Validated batch 106 batch loss 6.64002037\n",
      "Validated batch 107 batch loss 6.85267496\n",
      "Validated batch 108 batch loss 7.30647469\n",
      "Validated batch 109 batch loss 7.24385405\n",
      "Validated batch 110 batch loss 7.08465862\n",
      "Validated batch 111 batch loss 7.49429369\n",
      "Validated batch 112 batch loss 7.46481848\n",
      "Validated batch 113 batch loss 7.40751648\n",
      "Validated batch 114 batch loss 6.85538483\n",
      "Validated batch 115 batch loss 6.64808607\n",
      "Validated batch 116 batch loss 6.81411791\n",
      "Validated batch 117 batch loss 6.89810419\n",
      "Validated batch 118 batch loss 6.71738148\n",
      "Validated batch 119 batch loss 6.20641136\n",
      "Validated batch 120 batch loss 6.67374802\n",
      "Validated batch 121 batch loss 7.27676344\n",
      "Validated batch 122 batch loss 6.59635544\n",
      "Validated batch 123 batch loss 7.14783525\n",
      "Validated batch 124 batch loss 7.20636797\n",
      "Validated batch 125 batch loss 7.1887455\n",
      "Validated batch 126 batch loss 7.28207445\n",
      "Validated batch 127 batch loss 6.85226631\n",
      "Validated batch 128 batch loss 6.80633831\n",
      "Validated batch 129 batch loss 6.91126728\n",
      "Validated batch 130 batch loss 7.08484077\n",
      "Validated batch 131 batch loss 7.44397\n",
      "Validated batch 132 batch loss 6.90807915\n",
      "Validated batch 133 batch loss 7.29090071\n",
      "Validated batch 134 batch loss 7.27570152\n",
      "Validated batch 135 batch loss 6.94828176\n",
      "Validated batch 136 batch loss 7.13321066\n",
      "Validated batch 137 batch loss 7.17070913\n",
      "Validated batch 138 batch loss 7.28129244\n",
      "Validated batch 139 batch loss 7.251472\n",
      "Validated batch 140 batch loss 7.06503677\n",
      "Validated batch 141 batch loss 6.94044256\n",
      "Validated batch 142 batch loss 6.85695839\n",
      "Validated batch 143 batch loss 6.68311596\n",
      "Validated batch 144 batch loss 6.96169424\n",
      "Validated batch 145 batch loss 6.72896194\n",
      "Validated batch 146 batch loss 7.12956572\n",
      "Validated batch 147 batch loss 7.24000645\n",
      "Validated batch 148 batch loss 7.11879063\n",
      "Validated batch 149 batch loss 6.83376837\n",
      "Validated batch 150 batch loss 6.79847813\n",
      "Validated batch 151 batch loss 6.5969162\n",
      "Validated batch 152 batch loss 7.25901031\n",
      "Validated batch 153 batch loss 7.06517696\n",
      "Validated batch 154 batch loss 7.06257868\n",
      "Validated batch 155 batch loss 6.86331511\n",
      "Validated batch 156 batch loss 6.62345695\n",
      "Validated batch 157 batch loss 6.43163681\n",
      "Validated batch 158 batch loss 7.05210495\n",
      "Validated batch 159 batch loss 6.8654418\n",
      "Validated batch 160 batch loss 6.55152559\n",
      "Validated batch 161 batch loss 6.84449339\n",
      "Validated batch 162 batch loss 7.01817226\n",
      "Validated batch 163 batch loss 7.07253313\n",
      "Validated batch 164 batch loss 7.0524807\n",
      "Validated batch 165 batch loss 6.90153074\n",
      "Validated batch 166 batch loss 7.42655945\n",
      "Validated batch 167 batch loss 7.2531\n",
      "Validated batch 168 batch loss 7.2990613\n",
      "Validated batch 169 batch loss 6.97102308\n",
      "Validated batch 170 batch loss 7.29963827\n",
      "Validated batch 171 batch loss 7.259305\n",
      "Validated batch 172 batch loss 7.38862181\n",
      "Validated batch 173 batch loss 7.42938232\n",
      "Validated batch 174 batch loss 6.49575424\n",
      "Validated batch 175 batch loss 7.21171951\n",
      "Validated batch 176 batch loss 7.24220467\n",
      "Validated batch 177 batch loss 6.84097862\n",
      "Validated batch 178 batch loss 7.02254438\n",
      "Validated batch 179 batch loss 6.524611\n",
      "Validated batch 180 batch loss 6.44764662\n",
      "Validated batch 181 batch loss 7.38996\n",
      "Validated batch 182 batch loss 7.04569387\n",
      "Validated batch 183 batch loss 7.25108194\n",
      "Validated batch 184 batch loss 7.04202747\n",
      "Validated batch 185 batch loss 3.63551736\n",
      "Epoch 8 val loss 6.971906661987305\n",
      "Start epoch 9 with learning rate 0.001\n",
      "Start distributed training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 03:10:02.239979: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "2025-03-19 03:10:02.240025: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "\t [[RemoteCall]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1 batch loss 7.24927 epoch total loss 7.24927\n",
      "Trained batch 2 batch loss 6.87995863 epoch total loss 7.0646143\n",
      "Trained batch 3 batch loss 6.98882866 epoch total loss 7.03935242\n",
      "Trained batch 4 batch loss 6.92474842 epoch total loss 7.01070118\n",
      "Trained batch 5 batch loss 7.23997974 epoch total loss 7.0565567\n",
      "Trained batch 6 batch loss 7.20910883 epoch total loss 7.08198214\n",
      "Trained batch 7 batch loss 7.25853586 epoch total loss 7.10720444\n",
      "Trained batch 8 batch loss 7.22842 epoch total loss 7.12235641\n",
      "Trained batch 9 batch loss 7.19053411 epoch total loss 7.12993193\n",
      "Trained batch 10 batch loss 7.10036564 epoch total loss 7.12697506\n",
      "Trained batch 11 batch loss 7.13657 epoch total loss 7.12784767\n",
      "Trained batch 12 batch loss 7.0340991 epoch total loss 7.12003517\n",
      "Trained batch 13 batch loss 7.19792747 epoch total loss 7.12602711\n",
      "Trained batch 14 batch loss 7.03610849 epoch total loss 7.11960459\n",
      "Trained batch 15 batch loss 7.00376034 epoch total loss 7.11188173\n",
      "Trained batch 16 batch loss 7.31757164 epoch total loss 7.12473726\n",
      "Trained batch 17 batch loss 7.4169116 epoch total loss 7.1419239\n",
      "Trained batch 18 batch loss 7.46262884 epoch total loss 7.15974092\n",
      "Trained batch 19 batch loss 7.45437956 epoch total loss 7.17524815\n",
      "Trained batch 20 batch loss 7.55397129 epoch total loss 7.1941843\n",
      "Trained batch 21 batch loss 6.96964073 epoch total loss 7.18349123\n",
      "Trained batch 22 batch loss 6.86375666 epoch total loss 7.16895771\n",
      "Trained batch 23 batch loss 6.63523674 epoch total loss 7.14575243\n",
      "Trained batch 24 batch loss 6.33371449 epoch total loss 7.1119175\n",
      "Trained batch 25 batch loss 6.86381435 epoch total loss 7.10199356\n",
      "Trained batch 26 batch loss 6.91109085 epoch total loss 7.09465075\n",
      "Trained batch 27 batch loss 6.89096689 epoch total loss 7.0871067\n",
      "Trained batch 28 batch loss 6.79156 epoch total loss 7.07655191\n",
      "Trained batch 29 batch loss 6.79071903 epoch total loss 7.06669569\n",
      "Trained batch 30 batch loss 7.33747339 epoch total loss 7.07572174\n",
      "Trained batch 31 batch loss 6.4284 epoch total loss 7.05484056\n",
      "Trained batch 32 batch loss 5.69619226 epoch total loss 7.01238298\n",
      "Trained batch 33 batch loss 6.53129768 epoch total loss 6.99780464\n",
      "Trained batch 34 batch loss 6.73172045 epoch total loss 6.98997879\n",
      "Trained batch 35 batch loss 6.43421316 epoch total loss 6.97409964\n",
      "Trained batch 36 batch loss 6.72178745 epoch total loss 6.96709108\n",
      "Trained batch 37 batch loss 6.97417116 epoch total loss 6.96728277\n",
      "Trained batch 38 batch loss 6.89121532 epoch total loss 6.96528053\n",
      "Trained batch 39 batch loss 6.83609152 epoch total loss 6.96196795\n",
      "Trained batch 40 batch loss 6.78792524 epoch total loss 6.95761728\n",
      "Trained batch 41 batch loss 6.99028111 epoch total loss 6.95841408\n",
      "Trained batch 42 batch loss 7.36319733 epoch total loss 6.96805191\n",
      "Trained batch 43 batch loss 7.0376606 epoch total loss 6.9696703\n",
      "Trained batch 44 batch loss 6.90120173 epoch total loss 6.96811485\n",
      "Trained batch 45 batch loss 6.57455635 epoch total loss 6.95936871\n",
      "Trained batch 46 batch loss 6.28417397 epoch total loss 6.9446907\n",
      "Trained batch 47 batch loss 6.09560394 epoch total loss 6.92662525\n",
      "Trained batch 48 batch loss 6.00850582 epoch total loss 6.90749788\n",
      "Trained batch 49 batch loss 6.60790634 epoch total loss 6.90138388\n",
      "Trained batch 50 batch loss 6.63156796 epoch total loss 6.89598751\n",
      "Trained batch 51 batch loss 6.92309141 epoch total loss 6.89651918\n",
      "Trained batch 52 batch loss 7.33255053 epoch total loss 6.90490437\n",
      "Trained batch 53 batch loss 7.01569891 epoch total loss 6.90699434\n",
      "Trained batch 54 batch loss 7.40920162 epoch total loss 6.91629457\n",
      "Trained batch 55 batch loss 7.31456 epoch total loss 6.9235363\n",
      "Trained batch 56 batch loss 7.31162453 epoch total loss 6.93046618\n",
      "Trained batch 57 batch loss 7.49154139 epoch total loss 6.94030952\n",
      "Trained batch 58 batch loss 7.01559544 epoch total loss 6.94160795\n",
      "Trained batch 59 batch loss 7.35036087 epoch total loss 6.94853592\n",
      "Trained batch 60 batch loss 7.34738159 epoch total loss 6.95518351\n",
      "Trained batch 61 batch loss 6.8267169 epoch total loss 6.95307732\n",
      "Trained batch 62 batch loss 7.0356946 epoch total loss 6.95441\n",
      "Trained batch 63 batch loss 6.98852062 epoch total loss 6.95495176\n",
      "Trained batch 64 batch loss 6.34625196 epoch total loss 6.94544077\n",
      "Trained batch 65 batch loss 6.49284267 epoch total loss 6.93847752\n",
      "Trained batch 66 batch loss 6.96563387 epoch total loss 6.93888903\n",
      "Trained batch 67 batch loss 7.32477427 epoch total loss 6.94464827\n",
      "Trained batch 68 batch loss 7.301651 epoch total loss 6.94989824\n",
      "Trained batch 69 batch loss 7.41692591 epoch total loss 6.95666695\n",
      "Trained batch 70 batch loss 7.35620165 epoch total loss 6.96237421\n",
      "Trained batch 71 batch loss 7.28825331 epoch total loss 6.96696424\n",
      "Trained batch 72 batch loss 7.39934349 epoch total loss 6.97296953\n",
      "Trained batch 73 batch loss 7.40545273 epoch total loss 6.97889376\n",
      "Trained batch 74 batch loss 7.37648773 epoch total loss 6.98426676\n",
      "Trained batch 75 batch loss 7.2105732 epoch total loss 6.98728418\n",
      "Trained batch 76 batch loss 7.31401873 epoch total loss 6.99158335\n",
      "Trained batch 77 batch loss 7.32119 epoch total loss 6.99586391\n",
      "Trained batch 78 batch loss 7.3632679 epoch total loss 7.00057459\n",
      "Trained batch 79 batch loss 7.31436491 epoch total loss 7.00454664\n",
      "Trained batch 80 batch loss 7.13101101 epoch total loss 7.00612736\n",
      "Trained batch 81 batch loss 6.79424143 epoch total loss 7.00351143\n",
      "Trained batch 82 batch loss 7.22788095 epoch total loss 7.006248\n",
      "Trained batch 83 batch loss 7.18488312 epoch total loss 7.0084\n",
      "Trained batch 84 batch loss 7.22480536 epoch total loss 7.01097631\n",
      "Trained batch 85 batch loss 7.00385714 epoch total loss 7.01089239\n",
      "Trained batch 86 batch loss 6.98336506 epoch total loss 7.01057196\n",
      "Trained batch 87 batch loss 7.31586552 epoch total loss 7.014081\n",
      "Trained batch 88 batch loss 7.08336115 epoch total loss 7.01486826\n",
      "Trained batch 89 batch loss 6.58983803 epoch total loss 7.01009274\n",
      "Trained batch 90 batch loss 5.98295498 epoch total loss 6.99868\n",
      "Trained batch 91 batch loss 6.36177588 epoch total loss 6.9916811\n",
      "Trained batch 92 batch loss 6.80213785 epoch total loss 6.98962069\n",
      "Trained batch 93 batch loss 7.19997883 epoch total loss 6.99188232\n",
      "Trained batch 94 batch loss 7.1467123 epoch total loss 6.99353\n",
      "Trained batch 95 batch loss 6.93748093 epoch total loss 6.99294\n",
      "Trained batch 96 batch loss 7.17775249 epoch total loss 6.99486494\n",
      "Trained batch 97 batch loss 7.24640131 epoch total loss 6.99745798\n",
      "Trained batch 98 batch loss 6.80124378 epoch total loss 6.99545622\n",
      "Trained batch 99 batch loss 6.71408558 epoch total loss 6.99261427\n",
      "Trained batch 100 batch loss 6.70377302 epoch total loss 6.98972607\n",
      "Trained batch 101 batch loss 6.95449 epoch total loss 6.98937702\n",
      "Trained batch 102 batch loss 7.02156067 epoch total loss 6.98969221\n",
      "Trained batch 103 batch loss 7.10305548 epoch total loss 6.99079275\n",
      "Trained batch 104 batch loss 7.24619818 epoch total loss 6.99324846\n",
      "Trained batch 105 batch loss 7.18545389 epoch total loss 6.99507904\n",
      "Trained batch 106 batch loss 7.24084663 epoch total loss 6.99739742\n",
      "Trained batch 107 batch loss 7.26660967 epoch total loss 6.99991322\n",
      "Trained batch 108 batch loss 7.24302053 epoch total loss 7.00216436\n",
      "Trained batch 109 batch loss 6.72148705 epoch total loss 6.99958944\n",
      "Trained batch 110 batch loss 6.82268953 epoch total loss 6.99798155\n",
      "Trained batch 111 batch loss 6.79251862 epoch total loss 6.99613047\n",
      "Trained batch 112 batch loss 7.38756561 epoch total loss 6.99962568\n",
      "Trained batch 113 batch loss 6.91246843 epoch total loss 6.99885416\n",
      "Trained batch 114 batch loss 6.0868063 epoch total loss 6.99085379\n",
      "Trained batch 115 batch loss 6.83003187 epoch total loss 6.98945522\n",
      "Trained batch 116 batch loss 7.08020306 epoch total loss 6.99023771\n",
      "Trained batch 117 batch loss 7.25701666 epoch total loss 6.99251795\n",
      "Trained batch 118 batch loss 7.2255044 epoch total loss 6.99449253\n",
      "Trained batch 119 batch loss 7.22431803 epoch total loss 6.99642372\n",
      "Trained batch 120 batch loss 7.24335814 epoch total loss 6.99848127\n",
      "Trained batch 121 batch loss 7.09481764 epoch total loss 6.99927711\n",
      "Trained batch 122 batch loss 7.16027689 epoch total loss 7.000597\n",
      "Trained batch 123 batch loss 6.78677797 epoch total loss 6.99885893\n",
      "Trained batch 124 batch loss 6.75217581 epoch total loss 6.99686956\n",
      "Trained batch 125 batch loss 7.14056492 epoch total loss 6.99801922\n",
      "Trained batch 126 batch loss 6.93254709 epoch total loss 6.99749947\n",
      "Trained batch 127 batch loss 6.91644382 epoch total loss 6.99686146\n",
      "Trained batch 128 batch loss 7.22225666 epoch total loss 6.99862194\n",
      "Trained batch 129 batch loss 6.65468168 epoch total loss 6.99595547\n",
      "Trained batch 130 batch loss 6.86584806 epoch total loss 6.99495459\n",
      "Trained batch 131 batch loss 6.70916462 epoch total loss 6.99277306\n",
      "Trained batch 132 batch loss 7.13630342 epoch total loss 6.99386024\n",
      "Trained batch 133 batch loss 6.3171339 epoch total loss 6.98877239\n",
      "Trained batch 134 batch loss 6.04613256 epoch total loss 6.98173761\n",
      "Trained batch 135 batch loss 6.82686 epoch total loss 6.98059034\n",
      "Trained batch 136 batch loss 6.48305893 epoch total loss 6.97693205\n",
      "Trained batch 137 batch loss 6.38855028 epoch total loss 6.97263718\n",
      "Trained batch 138 batch loss 6.34778404 epoch total loss 6.96810913\n",
      "Trained batch 139 batch loss 6.87086535 epoch total loss 6.96740961\n",
      "Trained batch 140 batch loss 6.90013885 epoch total loss 6.96692896\n",
      "Trained batch 141 batch loss 7.15908194 epoch total loss 6.96829176\n",
      "Trained batch 142 batch loss 6.93589878 epoch total loss 6.96806335\n",
      "Trained batch 143 batch loss 7.28295898 epoch total loss 6.97026587\n",
      "Trained batch 144 batch loss 7.3296423 epoch total loss 6.97276115\n",
      "Trained batch 145 batch loss 6.09199762 epoch total loss 6.9666872\n",
      "Trained batch 146 batch loss 5.67144728 epoch total loss 6.95781565\n",
      "Trained batch 147 batch loss 5.6442318 epoch total loss 6.94887972\n",
      "Trained batch 148 batch loss 6.5237627 epoch total loss 6.94600677\n",
      "Trained batch 149 batch loss 7.09192324 epoch total loss 6.9469862\n",
      "Trained batch 150 batch loss 7.47344 epoch total loss 6.95049572\n",
      "Trained batch 151 batch loss 7.48694944 epoch total loss 6.95404816\n",
      "Trained batch 152 batch loss 7.36523342 epoch total loss 6.95675325\n",
      "Trained batch 153 batch loss 6.93165 epoch total loss 6.95658922\n",
      "Trained batch 154 batch loss 6.83439064 epoch total loss 6.95579529\n",
      "Trained batch 155 batch loss 7.13711262 epoch total loss 6.95696497\n",
      "Trained batch 156 batch loss 6.8743825 epoch total loss 6.95643568\n",
      "Trained batch 157 batch loss 7.31684637 epoch total loss 6.95873165\n",
      "Trained batch 158 batch loss 6.95330381 epoch total loss 6.95869684\n",
      "Trained batch 159 batch loss 7.06685066 epoch total loss 6.95937729\n",
      "Trained batch 160 batch loss 6.62469864 epoch total loss 6.95728588\n",
      "Trained batch 161 batch loss 7.06948 epoch total loss 6.95798302\n",
      "Trained batch 162 batch loss 6.86739588 epoch total loss 6.95742369\n",
      "Trained batch 163 batch loss 6.62428236 epoch total loss 6.95538\n",
      "Trained batch 164 batch loss 7.20683527 epoch total loss 6.95691299\n",
      "Trained batch 165 batch loss 7.15511 epoch total loss 6.95811415\n",
      "Trained batch 166 batch loss 7.15792799 epoch total loss 6.95931816\n",
      "Trained batch 167 batch loss 7.17179394 epoch total loss 6.96059036\n",
      "Trained batch 168 batch loss 7.27525377 epoch total loss 6.96246338\n",
      "Trained batch 169 batch loss 7.10857725 epoch total loss 6.96332741\n",
      "Trained batch 170 batch loss 6.91060591 epoch total loss 6.96301794\n",
      "Trained batch 171 batch loss 7.28238821 epoch total loss 6.96488523\n",
      "Trained batch 172 batch loss 6.73800278 epoch total loss 6.9635663\n",
      "Trained batch 173 batch loss 6.61368656 epoch total loss 6.96154356\n",
      "Trained batch 174 batch loss 7.1817174 epoch total loss 6.96280909\n",
      "Trained batch 175 batch loss 7.3282876 epoch total loss 6.96489763\n",
      "Trained batch 176 batch loss 7.16128492 epoch total loss 6.96601295\n",
      "Trained batch 177 batch loss 6.82957268 epoch total loss 6.96524239\n",
      "Trained batch 178 batch loss 6.94286108 epoch total loss 6.9651165\n",
      "Trained batch 179 batch loss 6.95920658 epoch total loss 6.9650836\n",
      "Trained batch 180 batch loss 6.75474262 epoch total loss 6.96391535\n",
      "Trained batch 181 batch loss 6.38801718 epoch total loss 6.96073389\n",
      "Trained batch 182 batch loss 7.00989819 epoch total loss 6.96100378\n",
      "Trained batch 183 batch loss 6.97087049 epoch total loss 6.96105766\n",
      "Trained batch 184 batch loss 7.07056141 epoch total loss 6.96165276\n",
      "Trained batch 185 batch loss 7.36089659 epoch total loss 6.96381044\n",
      "Trained batch 186 batch loss 7.17541599 epoch total loss 6.96494818\n",
      "Trained batch 187 batch loss 7.16196156 epoch total loss 6.96600199\n",
      "Trained batch 188 batch loss 6.95576715 epoch total loss 6.96594763\n",
      "Trained batch 189 batch loss 7.08304787 epoch total loss 6.96656704\n",
      "Trained batch 190 batch loss 7.32555676 epoch total loss 6.96845627\n",
      "Trained batch 191 batch loss 7.12177467 epoch total loss 6.96925926\n",
      "Trained batch 192 batch loss 6.82307863 epoch total loss 6.96849823\n",
      "Trained batch 193 batch loss 7.16588783 epoch total loss 6.96952105\n",
      "Trained batch 194 batch loss 6.60148478 epoch total loss 6.96762371\n",
      "Trained batch 195 batch loss 6.89464808 epoch total loss 6.96724939\n",
      "Trained batch 196 batch loss 7.21179342 epoch total loss 6.96849728\n",
      "Trained batch 197 batch loss 7.04695511 epoch total loss 6.96889544\n",
      "Trained batch 198 batch loss 7.34451962 epoch total loss 6.97079229\n",
      "Trained batch 199 batch loss 7.14683247 epoch total loss 6.9716773\n",
      "Trained batch 200 batch loss 7.07670975 epoch total loss 6.9722023\n",
      "Trained batch 201 batch loss 7.00495 epoch total loss 6.97236538\n",
      "Trained batch 202 batch loss 7.27727795 epoch total loss 6.97387457\n",
      "Trained batch 203 batch loss 7.25346518 epoch total loss 6.97525167\n",
      "Trained batch 204 batch loss 7.44820309 epoch total loss 6.97757\n",
      "Trained batch 205 batch loss 7.25914049 epoch total loss 6.97894382\n",
      "Trained batch 206 batch loss 7.29688883 epoch total loss 6.98048735\n",
      "Trained batch 207 batch loss 7.23294115 epoch total loss 6.98170662\n",
      "Trained batch 208 batch loss 7.38744164 epoch total loss 6.98365736\n",
      "Trained batch 209 batch loss 7.07541084 epoch total loss 6.98409653\n",
      "Trained batch 210 batch loss 6.71653557 epoch total loss 6.98282242\n",
      "Trained batch 211 batch loss 6.96701717 epoch total loss 6.98274755\n",
      "Trained batch 212 batch loss 7.27963209 epoch total loss 6.98414803\n",
      "Trained batch 213 batch loss 7.42112637 epoch total loss 6.9862\n",
      "Trained batch 214 batch loss 7.2813673 epoch total loss 6.98757887\n",
      "Trained batch 215 batch loss 7.27231121 epoch total loss 6.98890352\n",
      "Trained batch 216 batch loss 6.98359728 epoch total loss 6.9888792\n",
      "Trained batch 217 batch loss 7.10284424 epoch total loss 6.9894042\n",
      "Trained batch 218 batch loss 7.26773071 epoch total loss 6.99068069\n",
      "Trained batch 219 batch loss 7.33517313 epoch total loss 6.99225378\n",
      "Trained batch 220 batch loss 7.16940737 epoch total loss 6.99305916\n",
      "Trained batch 221 batch loss 7.30493069 epoch total loss 6.9944706\n",
      "Trained batch 222 batch loss 7.30414343 epoch total loss 6.99586535\n",
      "Trained batch 223 batch loss 7.11468 epoch total loss 6.99639797\n",
      "Trained batch 224 batch loss 6.68336487 epoch total loss 6.99500036\n",
      "Trained batch 225 batch loss 7.03982544 epoch total loss 6.99519968\n",
      "Trained batch 226 batch loss 6.1885314 epoch total loss 6.99163\n",
      "Trained batch 227 batch loss 6.18010902 epoch total loss 6.98805475\n",
      "Trained batch 228 batch loss 6.54668856 epoch total loss 6.98611879\n",
      "Trained batch 229 batch loss 6.04350376 epoch total loss 6.98200226\n",
      "Trained batch 230 batch loss 5.7179594 epoch total loss 6.97650671\n",
      "Trained batch 231 batch loss 5.74486637 epoch total loss 6.97117519\n",
      "Trained batch 232 batch loss 6.09758425 epoch total loss 6.96740913\n",
      "Trained batch 233 batch loss 6.32044029 epoch total loss 6.96463251\n",
      "Trained batch 234 batch loss 6.75510406 epoch total loss 6.96373749\n",
      "Trained batch 235 batch loss 7.07260656 epoch total loss 6.9642005\n",
      "Trained batch 236 batch loss 7.3071084 epoch total loss 6.9656539\n",
      "Trained batch 237 batch loss 7.42459583 epoch total loss 6.96759\n",
      "Trained batch 238 batch loss 7.32832289 epoch total loss 6.96910572\n",
      "Trained batch 239 batch loss 7.14549351 epoch total loss 6.96984386\n",
      "Trained batch 240 batch loss 6.9470768 epoch total loss 6.96974897\n",
      "Trained batch 241 batch loss 6.67344379 epoch total loss 6.96851969\n",
      "Trained batch 242 batch loss 7.29631519 epoch total loss 6.96987391\n",
      "Trained batch 243 batch loss 6.99322796 epoch total loss 6.96997\n",
      "Trained batch 244 batch loss 7.37068558 epoch total loss 6.97161245\n",
      "Trained batch 245 batch loss 7.01928043 epoch total loss 6.971807\n",
      "Trained batch 246 batch loss 7.02047 epoch total loss 6.97200537\n",
      "Trained batch 247 batch loss 7.18747425 epoch total loss 6.9728775\n",
      "Trained batch 248 batch loss 6.5735507 epoch total loss 6.9712677\n",
      "Trained batch 249 batch loss 6.59788704 epoch total loss 6.96976805\n",
      "Trained batch 250 batch loss 6.85219717 epoch total loss 6.96929789\n",
      "Trained batch 251 batch loss 6.64306164 epoch total loss 6.96799803\n",
      "Trained batch 252 batch loss 7.10663939 epoch total loss 6.9685483\n",
      "Trained batch 253 batch loss 7.13548803 epoch total loss 6.96920824\n",
      "Trained batch 254 batch loss 7.09386873 epoch total loss 6.96969938\n",
      "Trained batch 255 batch loss 6.76799297 epoch total loss 6.96890783\n",
      "Trained batch 256 batch loss 6.5613451 epoch total loss 6.96731615\n",
      "Trained batch 257 batch loss 6.95802641 epoch total loss 6.96728\n",
      "Trained batch 258 batch loss 6.86285973 epoch total loss 6.96687555\n",
      "Trained batch 259 batch loss 7.30579901 epoch total loss 6.96818399\n",
      "Trained batch 260 batch loss 7.34598875 epoch total loss 6.96963692\n",
      "Trained batch 261 batch loss 7.03011751 epoch total loss 6.96986866\n",
      "Trained batch 262 batch loss 7.14397955 epoch total loss 6.97053289\n",
      "Trained batch 263 batch loss 7.13620853 epoch total loss 6.97116327\n",
      "Trained batch 264 batch loss 7.17796135 epoch total loss 6.97194672\n",
      "Trained batch 265 batch loss 7.28440809 epoch total loss 6.97312546\n",
      "Trained batch 266 batch loss 6.95159626 epoch total loss 6.9730444\n",
      "Trained batch 267 batch loss 6.65839386 epoch total loss 6.97186613\n",
      "Trained batch 268 batch loss 6.75753689 epoch total loss 6.97106647\n",
      "Trained batch 269 batch loss 6.12899494 epoch total loss 6.96793652\n",
      "Trained batch 270 batch loss 6.3343215 epoch total loss 6.96558952\n",
      "Trained batch 271 batch loss 7.20885563 epoch total loss 6.96648741\n",
      "Trained batch 272 batch loss 6.95353842 epoch total loss 6.96643972\n",
      "Trained batch 273 batch loss 7.10555077 epoch total loss 6.96694946\n",
      "Trained batch 274 batch loss 7.24230576 epoch total loss 6.96795416\n",
      "Trained batch 275 batch loss 7.34059477 epoch total loss 6.96930933\n",
      "Trained batch 276 batch loss 6.97427654 epoch total loss 6.96932697\n",
      "Trained batch 277 batch loss 7.13055325 epoch total loss 6.96990919\n",
      "Trained batch 278 batch loss 6.79168129 epoch total loss 6.96926785\n",
      "Trained batch 279 batch loss 6.83878756 epoch total loss 6.9688\n",
      "Trained batch 280 batch loss 7.04458714 epoch total loss 6.96907043\n",
      "Trained batch 281 batch loss 7.23857 epoch total loss 6.97002935\n",
      "Trained batch 282 batch loss 6.56844234 epoch total loss 6.96860552\n",
      "Trained batch 283 batch loss 6.53626108 epoch total loss 6.96707773\n",
      "Trained batch 284 batch loss 6.30449343 epoch total loss 6.96474457\n",
      "Trained batch 285 batch loss 6.93323946 epoch total loss 6.96463394\n",
      "Trained batch 286 batch loss 6.99373627 epoch total loss 6.96473598\n",
      "Trained batch 287 batch loss 7.10686159 epoch total loss 6.96523094\n",
      "Trained batch 288 batch loss 7.24167871 epoch total loss 6.96619081\n",
      "Trained batch 289 batch loss 7.25300789 epoch total loss 6.96718359\n",
      "Trained batch 290 batch loss 7.19299746 epoch total loss 6.96796179\n",
      "Trained batch 291 batch loss 7.31711102 epoch total loss 6.96916199\n",
      "Trained batch 292 batch loss 7.24893475 epoch total loss 6.97012\n",
      "Trained batch 293 batch loss 7.16539192 epoch total loss 6.97078657\n",
      "Trained batch 294 batch loss 7.12139702 epoch total loss 6.97129869\n",
      "Trained batch 295 batch loss 7.03794956 epoch total loss 6.97152424\n",
      "Trained batch 296 batch loss 6.57857227 epoch total loss 6.97019672\n",
      "Trained batch 297 batch loss 7.1186676 epoch total loss 6.97069645\n",
      "Trained batch 298 batch loss 7.18800497 epoch total loss 6.97142553\n",
      "Trained batch 299 batch loss 7.23238707 epoch total loss 6.97229862\n",
      "Trained batch 300 batch loss 6.36016846 epoch total loss 6.97025776\n",
      "Trained batch 301 batch loss 6.60526752 epoch total loss 6.96904516\n",
      "Trained batch 302 batch loss 6.53158379 epoch total loss 6.96759653\n",
      "Trained batch 303 batch loss 6.64276743 epoch total loss 6.9665246\n",
      "Trained batch 304 batch loss 7.08873081 epoch total loss 6.9669261\n",
      "Trained batch 305 batch loss 6.86463165 epoch total loss 6.96659136\n",
      "Trained batch 306 batch loss 6.84141111 epoch total loss 6.96618176\n",
      "Trained batch 307 batch loss 6.96153784 epoch total loss 6.96616602\n",
      "Trained batch 308 batch loss 6.93116 epoch total loss 6.96605253\n",
      "Trained batch 309 batch loss 6.47468853 epoch total loss 6.96446228\n",
      "Trained batch 310 batch loss 7.08703852 epoch total loss 6.96485806\n",
      "Trained batch 311 batch loss 6.67021275 epoch total loss 6.96391058\n",
      "Trained batch 312 batch loss 6.21199226 epoch total loss 6.9615\n",
      "Trained batch 313 batch loss 6.92448044 epoch total loss 6.96138191\n",
      "Trained batch 314 batch loss 6.38631248 epoch total loss 6.95955038\n",
      "Trained batch 315 batch loss 6.87628078 epoch total loss 6.95928574\n",
      "Trained batch 316 batch loss 6.93190289 epoch total loss 6.95919895\n",
      "Trained batch 317 batch loss 7.39805269 epoch total loss 6.96058321\n",
      "Trained batch 318 batch loss 7.19641 epoch total loss 6.96132421\n",
      "Trained batch 319 batch loss 7.0010252 epoch total loss 6.96144867\n",
      "Trained batch 320 batch loss 6.83296871 epoch total loss 6.96104717\n",
      "Trained batch 321 batch loss 6.98825836 epoch total loss 6.96113205\n",
      "Trained batch 322 batch loss 6.57729959 epoch total loss 6.95994043\n",
      "Trained batch 323 batch loss 7.00832796 epoch total loss 6.96009\n",
      "Trained batch 324 batch loss 6.92463446 epoch total loss 6.95998049\n",
      "Trained batch 325 batch loss 6.43736839 epoch total loss 6.95837212\n",
      "Trained batch 326 batch loss 7.00957155 epoch total loss 6.958529\n",
      "Trained batch 327 batch loss 7.17879 epoch total loss 6.95920229\n",
      "Trained batch 328 batch loss 7.02020597 epoch total loss 6.95938873\n",
      "Trained batch 329 batch loss 7.26635695 epoch total loss 6.96032143\n",
      "Trained batch 330 batch loss 7.35879612 epoch total loss 6.96152925\n",
      "Trained batch 331 batch loss 7.35510111 epoch total loss 6.96271801\n",
      "Trained batch 332 batch loss 7.23849 epoch total loss 6.96354866\n",
      "Trained batch 333 batch loss 7.14744759 epoch total loss 6.96410084\n",
      "Trained batch 334 batch loss 7.20889568 epoch total loss 6.96483421\n",
      "Trained batch 335 batch loss 7.33822107 epoch total loss 6.96594858\n",
      "Trained batch 336 batch loss 7.15949297 epoch total loss 6.9665246\n",
      "Trained batch 337 batch loss 6.95454168 epoch total loss 6.96648884\n",
      "Trained batch 338 batch loss 7.11308861 epoch total loss 6.96692276\n",
      "Trained batch 339 batch loss 6.99644947 epoch total loss 6.96700954\n",
      "Trained batch 340 batch loss 6.69265938 epoch total loss 6.96620226\n",
      "Trained batch 341 batch loss 6.51266956 epoch total loss 6.96487236\n",
      "Trained batch 342 batch loss 6.69517231 epoch total loss 6.96408367\n",
      "Trained batch 343 batch loss 7.3066597 epoch total loss 6.96508217\n",
      "Trained batch 344 batch loss 7.18866301 epoch total loss 6.9657321\n",
      "Trained batch 345 batch loss 7.00072575 epoch total loss 6.96583366\n",
      "Trained batch 346 batch loss 6.69377708 epoch total loss 6.96504784\n",
      "Trained batch 347 batch loss 6.76644659 epoch total loss 6.96447515\n",
      "Trained batch 348 batch loss 6.70010424 epoch total loss 6.96371555\n",
      "Trained batch 349 batch loss 7.11362076 epoch total loss 6.96414471\n",
      "Trained batch 350 batch loss 7.22815371 epoch total loss 6.96489954\n",
      "Trained batch 351 batch loss 7.21030664 epoch total loss 6.96559858\n",
      "Trained batch 352 batch loss 7.22530365 epoch total loss 6.96633625\n",
      "Trained batch 353 batch loss 7.36077881 epoch total loss 6.96745396\n",
      "Trained batch 354 batch loss 7.31226206 epoch total loss 6.96842813\n",
      "Trained batch 355 batch loss 6.87273598 epoch total loss 6.96815872\n",
      "Trained batch 356 batch loss 7.19441271 epoch total loss 6.96879387\n",
      "Trained batch 357 batch loss 7.31785917 epoch total loss 6.96977186\n",
      "Trained batch 358 batch loss 7.38954258 epoch total loss 6.9709444\n",
      "Trained batch 359 batch loss 7.19755 epoch total loss 6.97157574\n",
      "Trained batch 360 batch loss 7.11072779 epoch total loss 6.97196245\n",
      "Trained batch 361 batch loss 7.23647738 epoch total loss 6.97269535\n",
      "Trained batch 362 batch loss 7.27501106 epoch total loss 6.97353029\n",
      "Trained batch 363 batch loss 7.12514973 epoch total loss 6.97394848\n",
      "Trained batch 364 batch loss 7.07108212 epoch total loss 6.97421503\n",
      "Trained batch 365 batch loss 6.84822369 epoch total loss 6.97387\n",
      "Trained batch 366 batch loss 7.00317335 epoch total loss 6.97394943\n",
      "Trained batch 367 batch loss 6.55175734 epoch total loss 6.9727993\n",
      "Trained batch 368 batch loss 6.4013772 epoch total loss 6.97124624\n",
      "Trained batch 369 batch loss 6.83067942 epoch total loss 6.97086525\n",
      "Trained batch 370 batch loss 7.01306725 epoch total loss 6.97097969\n",
      "Trained batch 371 batch loss 6.97330952 epoch total loss 6.97098589\n",
      "Trained batch 372 batch loss 7.14439678 epoch total loss 6.97145176\n",
      "Trained batch 373 batch loss 6.68904829 epoch total loss 6.97069454\n",
      "Trained batch 374 batch loss 7.29153347 epoch total loss 6.97155237\n",
      "Trained batch 375 batch loss 7.08795929 epoch total loss 6.97186279\n",
      "Trained batch 376 batch loss 7.07245684 epoch total loss 6.9721303\n",
      "Trained batch 377 batch loss 7.11220264 epoch total loss 6.97250223\n",
      "Trained batch 378 batch loss 6.94848204 epoch total loss 6.97243881\n",
      "Trained batch 379 batch loss 7.24193764 epoch total loss 6.97315\n",
      "Trained batch 380 batch loss 7.28670311 epoch total loss 6.9739747\n",
      "Trained batch 381 batch loss 6.60082722 epoch total loss 6.97299528\n",
      "Trained batch 382 batch loss 7.01113033 epoch total loss 6.97309542\n",
      "Trained batch 383 batch loss 7.3674593 epoch total loss 6.97412491\n",
      "Trained batch 384 batch loss 7.41441298 epoch total loss 6.97527122\n",
      "Trained batch 385 batch loss 7.33846807 epoch total loss 6.97621441\n",
      "Trained batch 386 batch loss 7.20920181 epoch total loss 6.97681808\n",
      "Trained batch 387 batch loss 6.84946108 epoch total loss 6.97648859\n",
      "Trained batch 388 batch loss 7.05504417 epoch total loss 6.97669077\n",
      "Trained batch 389 batch loss 7.24634552 epoch total loss 6.97738409\n",
      "Trained batch 390 batch loss 7.25163937 epoch total loss 6.97808743\n",
      "Trained batch 391 batch loss 7.3263 epoch total loss 6.97897816\n",
      "Trained batch 392 batch loss 7.0793891 epoch total loss 6.97923422\n",
      "Trained batch 393 batch loss 6.95011091 epoch total loss 6.97916031\n",
      "Trained batch 394 batch loss 6.89149189 epoch total loss 6.9789381\n",
      "Trained batch 395 batch loss 7.20270967 epoch total loss 6.97950459\n",
      "Trained batch 396 batch loss 7.22506428 epoch total loss 6.98012495\n",
      "Trained batch 397 batch loss 7.28592777 epoch total loss 6.98089504\n",
      "Trained batch 398 batch loss 7.09316969 epoch total loss 6.98117733\n",
      "Trained batch 399 batch loss 6.63807249 epoch total loss 6.98031759\n",
      "Trained batch 400 batch loss 6.3830142 epoch total loss 6.97882462\n",
      "Trained batch 401 batch loss 5.98667765 epoch total loss 6.97635\n",
      "Trained batch 402 batch loss 6.16371155 epoch total loss 6.97432899\n",
      "Trained batch 403 batch loss 6.38451481 epoch total loss 6.9728651\n",
      "Trained batch 404 batch loss 5.83524704 epoch total loss 6.97004938\n",
      "Trained batch 405 batch loss 5.79332876 epoch total loss 6.96714354\n",
      "Trained batch 406 batch loss 5.67988873 epoch total loss 6.96397305\n",
      "Trained batch 407 batch loss 6.36596 epoch total loss 6.96250391\n",
      "Trained batch 408 batch loss 7.06591082 epoch total loss 6.96275711\n",
      "Trained batch 409 batch loss 7.06164122 epoch total loss 6.96299887\n",
      "Trained batch 410 batch loss 6.96337271 epoch total loss 6.963\n",
      "Trained batch 411 batch loss 6.98736048 epoch total loss 6.96305895\n",
      "Trained batch 412 batch loss 7.29678059 epoch total loss 6.96386909\n",
      "Trained batch 413 batch loss 7.27050591 epoch total loss 6.96461153\n",
      "Trained batch 414 batch loss 6.99638271 epoch total loss 6.9646883\n",
      "Trained batch 415 batch loss 6.57165241 epoch total loss 6.96374083\n",
      "Trained batch 416 batch loss 7.07583141 epoch total loss 6.96401024\n",
      "Trained batch 417 batch loss 7.08483267 epoch total loss 6.96429968\n",
      "Trained batch 418 batch loss 6.87330961 epoch total loss 6.96408224\n",
      "Trained batch 419 batch loss 7.04632854 epoch total loss 6.9642787\n",
      "Trained batch 420 batch loss 6.67322254 epoch total loss 6.96358585\n",
      "Trained batch 421 batch loss 6.59066153 epoch total loss 6.9627\n",
      "Trained batch 422 batch loss 6.56581926 epoch total loss 6.96175957\n",
      "Trained batch 423 batch loss 7.12070513 epoch total loss 6.96213531\n",
      "Trained batch 424 batch loss 7.29646683 epoch total loss 6.96292353\n",
      "Trained batch 425 batch loss 7.22231436 epoch total loss 6.96353388\n",
      "Trained batch 426 batch loss 7.48149109 epoch total loss 6.96475\n",
      "Trained batch 427 batch loss 7.39591 epoch total loss 6.96575975\n",
      "Trained batch 428 batch loss 7.21923113 epoch total loss 6.96635199\n",
      "Trained batch 429 batch loss 7.00857639 epoch total loss 6.96645\n",
      "Trained batch 430 batch loss 6.37108231 epoch total loss 6.96506596\n",
      "Trained batch 431 batch loss 5.70619631 epoch total loss 6.96214533\n",
      "Trained batch 432 batch loss 6.13418388 epoch total loss 6.96022892\n",
      "Trained batch 433 batch loss 5.93262434 epoch total loss 6.9578557\n",
      "Trained batch 434 batch loss 6.69313383 epoch total loss 6.95724535\n",
      "Trained batch 435 batch loss 6.94935608 epoch total loss 6.95722771\n",
      "Trained batch 436 batch loss 7.2290597 epoch total loss 6.95785093\n",
      "Trained batch 437 batch loss 7.28675556 epoch total loss 6.95860386\n",
      "Trained batch 438 batch loss 7.32791901 epoch total loss 6.95944691\n",
      "Trained batch 439 batch loss 7.24624968 epoch total loss 6.96010065\n",
      "Trained batch 440 batch loss 7.429245 epoch total loss 6.96116686\n",
      "Trained batch 441 batch loss 7.05678082 epoch total loss 6.96138382\n",
      "Trained batch 442 batch loss 6.98057318 epoch total loss 6.96142673\n",
      "Trained batch 443 batch loss 7.18091583 epoch total loss 6.96192217\n",
      "Trained batch 444 batch loss 7.1299324 epoch total loss 6.96230078\n",
      "Trained batch 445 batch loss 7.17144394 epoch total loss 6.96277046\n",
      "Trained batch 446 batch loss 7.17842817 epoch total loss 6.96325397\n",
      "Trained batch 447 batch loss 7.17754412 epoch total loss 6.9637332\n",
      "Trained batch 448 batch loss 7.23664474 epoch total loss 6.96434259\n",
      "Trained batch 449 batch loss 6.86983681 epoch total loss 6.96413183\n",
      "Trained batch 450 batch loss 7.29590559 epoch total loss 6.96486902\n",
      "Trained batch 451 batch loss 6.79861116 epoch total loss 6.96450043\n",
      "Trained batch 452 batch loss 6.67014265 epoch total loss 6.96384954\n",
      "Trained batch 453 batch loss 7.10452127 epoch total loss 6.96416\n",
      "Trained batch 454 batch loss 7.09507608 epoch total loss 6.96444798\n",
      "Trained batch 455 batch loss 7.31406689 epoch total loss 6.96521616\n",
      "Trained batch 456 batch loss 6.63172102 epoch total loss 6.96448517\n",
      "Trained batch 457 batch loss 6.92784643 epoch total loss 6.96440458\n",
      "Trained batch 458 batch loss 7.38540411 epoch total loss 6.96532393\n",
      "Trained batch 459 batch loss 6.80735159 epoch total loss 6.96498\n",
      "Trained batch 460 batch loss 6.62576 epoch total loss 6.96424246\n",
      "Trained batch 461 batch loss 7.21589 epoch total loss 6.96478796\n",
      "Trained batch 462 batch loss 7.15996647 epoch total loss 6.96521044\n",
      "Trained batch 463 batch loss 7.19698954 epoch total loss 6.96571112\n",
      "Trained batch 464 batch loss 7.14169407 epoch total loss 6.96609\n",
      "Trained batch 465 batch loss 7.18361807 epoch total loss 6.96655798\n",
      "Trained batch 466 batch loss 6.77049 epoch total loss 6.96613741\n",
      "Trained batch 467 batch loss 6.54149 epoch total loss 6.96522808\n",
      "Trained batch 468 batch loss 6.54422 epoch total loss 6.96432829\n",
      "Trained batch 469 batch loss 7.12641144 epoch total loss 6.964674\n",
      "Trained batch 470 batch loss 7.01429415 epoch total loss 6.96478\n",
      "Trained batch 471 batch loss 7.29984522 epoch total loss 6.96549129\n",
      "Trained batch 472 batch loss 6.81202364 epoch total loss 6.96516609\n",
      "Trained batch 473 batch loss 7.12926722 epoch total loss 6.96551275\n",
      "Trained batch 474 batch loss 6.91464615 epoch total loss 6.96540499\n",
      "Trained batch 475 batch loss 7.04987478 epoch total loss 6.96558285\n",
      "Trained batch 476 batch loss 7.36270475 epoch total loss 6.96641731\n",
      "Trained batch 477 batch loss 6.7253232 epoch total loss 6.96591187\n",
      "Trained batch 478 batch loss 7.31459475 epoch total loss 6.96664143\n",
      "Trained batch 479 batch loss 7.26006174 epoch total loss 6.96725416\n",
      "Trained batch 480 batch loss 7.32270479 epoch total loss 6.96799469\n",
      "Trained batch 481 batch loss 6.66066 epoch total loss 6.96735573\n",
      "Trained batch 482 batch loss 6.97260952 epoch total loss 6.9673667\n",
      "Trained batch 483 batch loss 6.92849541 epoch total loss 6.96728611\n",
      "Trained batch 484 batch loss 6.99232 epoch total loss 6.96733809\n",
      "Trained batch 485 batch loss 7.16195774 epoch total loss 6.96773911\n",
      "Trained batch 486 batch loss 6.91176605 epoch total loss 6.96762419\n",
      "Trained batch 487 batch loss 7.02533197 epoch total loss 6.96774292\n",
      "Trained batch 488 batch loss 6.78565502 epoch total loss 6.96736956\n",
      "Trained batch 489 batch loss 6.71823931 epoch total loss 6.96686029\n",
      "Trained batch 490 batch loss 7.37735271 epoch total loss 6.9676981\n",
      "Trained batch 491 batch loss 7.23818207 epoch total loss 6.96824932\n",
      "Trained batch 492 batch loss 7.16854095 epoch total loss 6.96865606\n",
      "Trained batch 493 batch loss 7.37906075 epoch total loss 6.9694891\n",
      "Trained batch 494 batch loss 7.17076 epoch total loss 6.96989584\n",
      "Trained batch 495 batch loss 7.04513597 epoch total loss 6.97004795\n",
      "Trained batch 496 batch loss 7.13357162 epoch total loss 6.97037792\n",
      "Trained batch 497 batch loss 7.25982237 epoch total loss 6.97096\n",
      "Trained batch 498 batch loss 7.37418 epoch total loss 6.97177\n",
      "Trained batch 499 batch loss 7.21852875 epoch total loss 6.97226429\n",
      "Trained batch 500 batch loss 6.69590378 epoch total loss 6.97171164\n",
      "Trained batch 501 batch loss 6.74264336 epoch total loss 6.97125435\n",
      "Trained batch 502 batch loss 6.79249477 epoch total loss 6.97089815\n",
      "Trained batch 503 batch loss 6.4919858 epoch total loss 6.96994591\n",
      "Trained batch 504 batch loss 6.53677082 epoch total loss 6.96908665\n",
      "Trained batch 505 batch loss 6.09818268 epoch total loss 6.96736193\n",
      "Trained batch 506 batch loss 6.09753275 epoch total loss 6.96564293\n",
      "Trained batch 507 batch loss 6.31946182 epoch total loss 6.96436834\n",
      "Trained batch 508 batch loss 6.83462143 epoch total loss 6.96411324\n",
      "Trained batch 509 batch loss 7.21024704 epoch total loss 6.96459675\n",
      "Trained batch 510 batch loss 7.17770195 epoch total loss 6.96501446\n",
      "Trained batch 511 batch loss 6.73115349 epoch total loss 6.96455717\n",
      "Trained batch 512 batch loss 6.45932961 epoch total loss 6.96357\n",
      "Trained batch 513 batch loss 6.61437416 epoch total loss 6.96288919\n",
      "Trained batch 514 batch loss 7.08010721 epoch total loss 6.96311712\n",
      "Trained batch 515 batch loss 6.91247654 epoch total loss 6.96301889\n",
      "Trained batch 516 batch loss 7.13323927 epoch total loss 6.96334887\n",
      "Trained batch 517 batch loss 6.78562069 epoch total loss 6.96300554\n",
      "Trained batch 518 batch loss 7.08224344 epoch total loss 6.96323586\n",
      "Trained batch 519 batch loss 7.09628677 epoch total loss 6.96349192\n",
      "Trained batch 520 batch loss 7.00493622 epoch total loss 6.96357155\n",
      "Trained batch 521 batch loss 6.82038069 epoch total loss 6.96329641\n",
      "Trained batch 522 batch loss 6.82907724 epoch total loss 6.9630394\n",
      "Trained batch 523 batch loss 7.22410536 epoch total loss 6.96353865\n",
      "Trained batch 524 batch loss 7.15956593 epoch total loss 6.96391296\n",
      "Trained batch 525 batch loss 7.45040131 epoch total loss 6.96483946\n",
      "Trained batch 526 batch loss 7.4174304 epoch total loss 6.9657\n",
      "Trained batch 527 batch loss 7.24812937 epoch total loss 6.96623564\n",
      "Trained batch 528 batch loss 6.91575766 epoch total loss 6.96614027\n",
      "Trained batch 529 batch loss 6.68382263 epoch total loss 6.96560669\n",
      "Trained batch 530 batch loss 7.08949375 epoch total loss 6.96584082\n",
      "Trained batch 531 batch loss 6.7256422 epoch total loss 6.9653883\n",
      "Trained batch 532 batch loss 7.1075139 epoch total loss 6.96565533\n",
      "Trained batch 533 batch loss 7.33499718 epoch total loss 6.96634817\n",
      "Trained batch 534 batch loss 7.40718317 epoch total loss 6.96717358\n",
      "Trained batch 535 batch loss 7.25569248 epoch total loss 6.96771288\n",
      "Trained batch 536 batch loss 7.44046402 epoch total loss 6.96859455\n",
      "Trained batch 537 batch loss 7.41044712 epoch total loss 6.96941757\n",
      "Trained batch 538 batch loss 7.37928 epoch total loss 6.97017956\n",
      "Trained batch 539 batch loss 7.52993155 epoch total loss 6.97121811\n",
      "Trained batch 540 batch loss 7.48608351 epoch total loss 6.97217178\n",
      "Trained batch 541 batch loss 7.35095072 epoch total loss 6.9728713\n",
      "Trained batch 542 batch loss 6.97634745 epoch total loss 6.97287798\n",
      "Trained batch 543 batch loss 6.59149361 epoch total loss 6.9721756\n",
      "Trained batch 544 batch loss 6.62611675 epoch total loss 6.9715395\n",
      "Trained batch 545 batch loss 6.94911957 epoch total loss 6.97149849\n",
      "Trained batch 546 batch loss 6.8753705 epoch total loss 6.97132301\n",
      "Trained batch 547 batch loss 7.08438635 epoch total loss 6.97152948\n",
      "Trained batch 548 batch loss 6.45076942 epoch total loss 6.97057915\n",
      "Trained batch 549 batch loss 6.71972942 epoch total loss 6.97012234\n",
      "Trained batch 550 batch loss 7.09074068 epoch total loss 6.97034168\n",
      "Trained batch 551 batch loss 6.81289482 epoch total loss 6.97005606\n",
      "Trained batch 552 batch loss 6.8125124 epoch total loss 6.96977091\n",
      "Trained batch 553 batch loss 7.19422 epoch total loss 6.9701767\n",
      "Trained batch 554 batch loss 6.62637568 epoch total loss 6.96955633\n",
      "Trained batch 555 batch loss 6.50763 epoch total loss 6.96872425\n",
      "Trained batch 556 batch loss 7.26244974 epoch total loss 6.96925211\n",
      "Trained batch 557 batch loss 7.37934494 epoch total loss 6.96998882\n",
      "Trained batch 558 batch loss 7.24071789 epoch total loss 6.97047377\n",
      "Trained batch 559 batch loss 7.23644066 epoch total loss 6.97094965\n",
      "Trained batch 560 batch loss 7.00685787 epoch total loss 6.97101355\n",
      "Trained batch 561 batch loss 6.8414073 epoch total loss 6.97078228\n",
      "Trained batch 562 batch loss 6.65368271 epoch total loss 6.9702177\n",
      "Trained batch 563 batch loss 6.60368824 epoch total loss 6.96956682\n",
      "Trained batch 564 batch loss 6.06481361 epoch total loss 6.96796274\n",
      "Trained batch 565 batch loss 5.96676 epoch total loss 6.96619081\n",
      "Trained batch 566 batch loss 5.6962738 epoch total loss 6.96394682\n",
      "Trained batch 567 batch loss 6.37670851 epoch total loss 6.96291113\n",
      "Trained batch 568 batch loss 6.93982267 epoch total loss 6.96287107\n",
      "Trained batch 569 batch loss 7.36517429 epoch total loss 6.96357822\n",
      "Trained batch 570 batch loss 6.59531307 epoch total loss 6.96293163\n",
      "Trained batch 571 batch loss 6.67707968 epoch total loss 6.96243095\n",
      "Trained batch 572 batch loss 6.64373064 epoch total loss 6.96187401\n",
      "Trained batch 573 batch loss 7.1457119 epoch total loss 6.96219492\n",
      "Trained batch 574 batch loss 7.39505291 epoch total loss 6.9629488\n",
      "Trained batch 575 batch loss 7.24548435 epoch total loss 6.96344042\n",
      "Trained batch 576 batch loss 7.38872957 epoch total loss 6.96417856\n",
      "Trained batch 577 batch loss 7.28881454 epoch total loss 6.96474123\n",
      "Trained batch 578 batch loss 7.20480824 epoch total loss 6.96515656\n",
      "Trained batch 579 batch loss 7.18941641 epoch total loss 6.96554422\n",
      "Trained batch 580 batch loss 7.30230904 epoch total loss 6.96612453\n",
      "Trained batch 581 batch loss 6.82557917 epoch total loss 6.96588278\n",
      "Trained batch 582 batch loss 7.31135511 epoch total loss 6.96647644\n",
      "Trained batch 583 batch loss 7.18802118 epoch total loss 6.96685648\n",
      "Trained batch 584 batch loss 7.07807636 epoch total loss 6.96704674\n",
      "Trained batch 585 batch loss 5.89572287 epoch total loss 6.96521568\n",
      "Trained batch 586 batch loss 6.0247407 epoch total loss 6.96361065\n",
      "Trained batch 587 batch loss 6.62961674 epoch total loss 6.96304178\n",
      "Trained batch 588 batch loss 7.19570494 epoch total loss 6.96343756\n",
      "Trained batch 589 batch loss 7.2388 epoch total loss 6.96390533\n",
      "Trained batch 590 batch loss 6.80648661 epoch total loss 6.96363878\n",
      "Trained batch 591 batch loss 6.87498617 epoch total loss 6.96348858\n",
      "Trained batch 592 batch loss 7.27972746 epoch total loss 6.96402311\n",
      "Trained batch 593 batch loss 6.90451479 epoch total loss 6.9639225\n",
      "Trained batch 594 batch loss 7.17659187 epoch total loss 6.96428061\n",
      "Trained batch 595 batch loss 7.01391459 epoch total loss 6.96436357\n",
      "Trained batch 596 batch loss 7.33363342 epoch total loss 6.96498299\n",
      "Trained batch 597 batch loss 7.02380753 epoch total loss 6.96508169\n",
      "Trained batch 598 batch loss 6.84721565 epoch total loss 6.96488476\n",
      "Trained batch 599 batch loss 6.73766565 epoch total loss 6.96450567\n",
      "Trained batch 600 batch loss 6.36295 epoch total loss 6.96350241\n",
      "Trained batch 601 batch loss 6.17620516 epoch total loss 6.96219254\n",
      "Trained batch 602 batch loss 6.25698376 epoch total loss 6.96102095\n",
      "Trained batch 603 batch loss 6.51543236 epoch total loss 6.96028233\n",
      "Trained batch 604 batch loss 7.23969603 epoch total loss 6.96074533\n",
      "Trained batch 605 batch loss 7.02268648 epoch total loss 6.9608469\n",
      "Trained batch 606 batch loss 7.25467873 epoch total loss 6.96133232\n",
      "Trained batch 607 batch loss 6.89704943 epoch total loss 6.96122646\n",
      "Trained batch 608 batch loss 7.22896576 epoch total loss 6.96166658\n",
      "Trained batch 609 batch loss 7.05804443 epoch total loss 6.96182489\n",
      "Trained batch 610 batch loss 7.18199301 epoch total loss 6.96218634\n",
      "Trained batch 611 batch loss 7.0696063 epoch total loss 6.96236229\n",
      "Trained batch 612 batch loss 6.21165133 epoch total loss 6.96113539\n",
      "Trained batch 613 batch loss 6.02266073 epoch total loss 6.95960426\n",
      "Trained batch 614 batch loss 6.64694357 epoch total loss 6.959095\n",
      "Trained batch 615 batch loss 7.15755844 epoch total loss 6.95941782\n",
      "Trained batch 616 batch loss 7.46778154 epoch total loss 6.96024323\n",
      "Trained batch 617 batch loss 7.38454962 epoch total loss 6.9609313\n",
      "Trained batch 618 batch loss 7.40221 epoch total loss 6.9616456\n",
      "Trained batch 619 batch loss 7.33989334 epoch total loss 6.96225643\n",
      "Trained batch 620 batch loss 7.39308453 epoch total loss 6.96295118\n",
      "Trained batch 621 batch loss 7.40659475 epoch total loss 6.96366596\n",
      "Trained batch 622 batch loss 7.20929241 epoch total loss 6.96406126\n",
      "Trained batch 623 batch loss 6.89671516 epoch total loss 6.96395254\n",
      "Trained batch 624 batch loss 7.27454853 epoch total loss 6.96445036\n",
      "Trained batch 625 batch loss 7.15812635 epoch total loss 6.9647603\n",
      "Trained batch 626 batch loss 7.35588598 epoch total loss 6.96538496\n",
      "Trained batch 627 batch loss 7.19713068 epoch total loss 6.96575499\n",
      "Trained batch 628 batch loss 7.25111532 epoch total loss 6.96620893\n",
      "Trained batch 629 batch loss 7.21787071 epoch total loss 6.966609\n",
      "Trained batch 630 batch loss 7.01841116 epoch total loss 6.96669149\n",
      "Trained batch 631 batch loss 7.09647799 epoch total loss 6.96689749\n",
      "Trained batch 632 batch loss 6.87575102 epoch total loss 6.96675348\n",
      "Trained batch 633 batch loss 6.81054354 epoch total loss 6.96650696\n",
      "Trained batch 634 batch loss 6.79479 epoch total loss 6.96623611\n",
      "Trained batch 635 batch loss 6.48613405 epoch total loss 6.96548033\n",
      "Trained batch 636 batch loss 6.84931 epoch total loss 6.9652977\n",
      "Trained batch 637 batch loss 7.03434277 epoch total loss 6.96540546\n",
      "Trained batch 638 batch loss 7.00631523 epoch total loss 6.96547\n",
      "Trained batch 639 batch loss 6.87441587 epoch total loss 6.96532726\n",
      "Trained batch 640 batch loss 7.19953156 epoch total loss 6.96569347\n",
      "Trained batch 641 batch loss 6.96273899 epoch total loss 6.96568918\n",
      "Trained batch 642 batch loss 7.15559101 epoch total loss 6.9659853\n",
      "Trained batch 643 batch loss 7.08739567 epoch total loss 6.96617413\n",
      "Trained batch 644 batch loss 6.72682571 epoch total loss 6.96580267\n",
      "Trained batch 645 batch loss 6.65324402 epoch total loss 6.9653182\n",
      "Trained batch 646 batch loss 6.51877975 epoch total loss 6.96462679\n",
      "Trained batch 647 batch loss 6.87359571 epoch total loss 6.96448612\n",
      "Trained batch 648 batch loss 7.02177334 epoch total loss 6.96457481\n",
      "Trained batch 649 batch loss 7.39918852 epoch total loss 6.96524477\n",
      "Trained batch 650 batch loss 6.98255253 epoch total loss 6.965271\n",
      "Trained batch 651 batch loss 7.27229548 epoch total loss 6.96574306\n",
      "Trained batch 652 batch loss 7.19637489 epoch total loss 6.96609688\n",
      "Trained batch 653 batch loss 7.19809675 epoch total loss 6.96645212\n",
      "Trained batch 654 batch loss 6.44691753 epoch total loss 6.96565771\n",
      "Trained batch 655 batch loss 6.07789135 epoch total loss 6.96430254\n",
      "Trained batch 656 batch loss 6.29375792 epoch total loss 6.96328068\n",
      "Trained batch 657 batch loss 6.43596172 epoch total loss 6.96247816\n",
      "Trained batch 658 batch loss 6.82883406 epoch total loss 6.96227455\n",
      "Trained batch 659 batch loss 7.0230341 epoch total loss 6.96236658\n",
      "Trained batch 660 batch loss 7.0461874 epoch total loss 6.9624939\n",
      "Trained batch 661 batch loss 6.99216604 epoch total loss 6.9625392\n",
      "Trained batch 662 batch loss 7.11957788 epoch total loss 6.96277618\n",
      "Trained batch 663 batch loss 7.19787598 epoch total loss 6.96313095\n",
      "Trained batch 664 batch loss 7.31769943 epoch total loss 6.96366501\n",
      "Trained batch 665 batch loss 6.8752017 epoch total loss 6.96353149\n",
      "Trained batch 666 batch loss 7.14850616 epoch total loss 6.96380949\n",
      "Trained batch 667 batch loss 6.88732243 epoch total loss 6.96369457\n",
      "Trained batch 668 batch loss 6.54913425 epoch total loss 6.96307421\n",
      "Trained batch 669 batch loss 6.47791243 epoch total loss 6.96234894\n",
      "Trained batch 670 batch loss 7.28904915 epoch total loss 6.96283674\n",
      "Trained batch 671 batch loss 6.38590717 epoch total loss 6.96197653\n",
      "Trained batch 672 batch loss 6.55710363 epoch total loss 6.96137428\n",
      "Trained batch 673 batch loss 6.07423258 epoch total loss 6.96005583\n",
      "Trained batch 674 batch loss 6.04338312 epoch total loss 6.95869589\n",
      "Trained batch 675 batch loss 7.03425312 epoch total loss 6.95880795\n",
      "Trained batch 676 batch loss 7.14560509 epoch total loss 6.95908403\n",
      "Trained batch 677 batch loss 6.9494915 epoch total loss 6.95907\n",
      "Trained batch 678 batch loss 7.20984221 epoch total loss 6.95944\n",
      "Trained batch 679 batch loss 7.27263832 epoch total loss 6.95990133\n",
      "Trained batch 680 batch loss 7.41005135 epoch total loss 6.96056318\n",
      "Trained batch 681 batch loss 7.30591393 epoch total loss 6.96107101\n",
      "Trained batch 682 batch loss 7.20479536 epoch total loss 6.96142817\n",
      "Trained batch 683 batch loss 7.4630475 epoch total loss 6.96216202\n",
      "Trained batch 684 batch loss 7.41273403 epoch total loss 6.96282053\n",
      "Trained batch 685 batch loss 7.02775097 epoch total loss 6.96291542\n",
      "Trained batch 686 batch loss 7.10106897 epoch total loss 6.96311712\n",
      "Trained batch 687 batch loss 6.44480133 epoch total loss 6.96236277\n",
      "Trained batch 688 batch loss 6.27240038 epoch total loss 6.96136\n",
      "Trained batch 689 batch loss 6.9606123 epoch total loss 6.96135855\n",
      "Trained batch 690 batch loss 6.57723761 epoch total loss 6.9608016\n",
      "Trained batch 691 batch loss 7.00015306 epoch total loss 6.96085835\n",
      "Trained batch 692 batch loss 6.91297674 epoch total loss 6.9607892\n",
      "Trained batch 693 batch loss 6.97634506 epoch total loss 6.96081209\n",
      "Trained batch 694 batch loss 6.78145599 epoch total loss 6.96055317\n",
      "Trained batch 695 batch loss 6.63006496 epoch total loss 6.96007776\n",
      "Trained batch 696 batch loss 6.86648464 epoch total loss 6.95994329\n",
      "Trained batch 697 batch loss 6.6480689 epoch total loss 6.95949554\n",
      "Trained batch 698 batch loss 6.99609184 epoch total loss 6.959548\n",
      "Trained batch 699 batch loss 6.59747267 epoch total loss 6.95903063\n",
      "Trained batch 700 batch loss 6.63976049 epoch total loss 6.9585743\n",
      "Trained batch 701 batch loss 6.57825708 epoch total loss 6.95803165\n",
      "Trained batch 702 batch loss 6.86992216 epoch total loss 6.95790625\n",
      "Trained batch 703 batch loss 7.43717289 epoch total loss 6.95858765\n",
      "Trained batch 704 batch loss 6.84294176 epoch total loss 6.95842314\n",
      "Trained batch 705 batch loss 7.0925107 epoch total loss 6.95861292\n",
      "Trained batch 706 batch loss 7.17302084 epoch total loss 6.95891666\n",
      "Trained batch 707 batch loss 6.46401119 epoch total loss 6.95821619\n",
      "Trained batch 708 batch loss 6.9693141 epoch total loss 6.95823193\n",
      "Trained batch 709 batch loss 7.24612522 epoch total loss 6.95863819\n",
      "Trained batch 710 batch loss 7.07071 epoch total loss 6.95879602\n",
      "Trained batch 711 batch loss 7.38020611 epoch total loss 6.95938873\n",
      "Trained batch 712 batch loss 7.12830162 epoch total loss 6.9596262\n",
      "Trained batch 713 batch loss 6.95187855 epoch total loss 6.95961523\n",
      "Trained batch 714 batch loss 6.32016087 epoch total loss 6.95871973\n",
      "Trained batch 715 batch loss 7.01002073 epoch total loss 6.95879173\n",
      "Trained batch 716 batch loss 7.14226723 epoch total loss 6.95904779\n",
      "Trained batch 717 batch loss 7.24896431 epoch total loss 6.95945215\n",
      "Trained batch 718 batch loss 7.16026735 epoch total loss 6.95973158\n",
      "Trained batch 719 batch loss 7.33705091 epoch total loss 6.96025658\n",
      "Trained batch 720 batch loss 7.37362576 epoch total loss 6.96083\n",
      "Trained batch 721 batch loss 7.00482512 epoch total loss 6.96089125\n",
      "Trained batch 722 batch loss 7.21797228 epoch total loss 6.96124744\n",
      "Trained batch 723 batch loss 6.66091204 epoch total loss 6.96083212\n",
      "Trained batch 724 batch loss 6.96824932 epoch total loss 6.96084261\n",
      "Trained batch 725 batch loss 6.82602119 epoch total loss 6.96065664\n",
      "Trained batch 726 batch loss 6.40253592 epoch total loss 6.9598875\n",
      "Trained batch 727 batch loss 6.83915949 epoch total loss 6.95972204\n",
      "Trained batch 728 batch loss 7.12733078 epoch total loss 6.95995235\n",
      "Trained batch 729 batch loss 6.97306347 epoch total loss 6.95997047\n",
      "Trained batch 730 batch loss 6.76683712 epoch total loss 6.95970535\n",
      "Trained batch 731 batch loss 6.6950345 epoch total loss 6.95934296\n",
      "Trained batch 732 batch loss 6.58628416 epoch total loss 6.95883369\n",
      "Trained batch 733 batch loss 6.64145 epoch total loss 6.95840073\n",
      "Trained batch 734 batch loss 7.2012763 epoch total loss 6.95873165\n",
      "Trained batch 735 batch loss 6.66195107 epoch total loss 6.95832825\n",
      "Trained batch 736 batch loss 6.86931276 epoch total loss 6.95820665\n",
      "Trained batch 737 batch loss 7.22742176 epoch total loss 6.95857239\n",
      "Trained batch 738 batch loss 7.37722778 epoch total loss 6.95914\n",
      "Trained batch 739 batch loss 6.94264793 epoch total loss 6.95911789\n",
      "Trained batch 740 batch loss 7.0466423 epoch total loss 6.95923662\n",
      "Trained batch 741 batch loss 7.34933424 epoch total loss 6.95976257\n",
      "Trained batch 742 batch loss 7.30563784 epoch total loss 6.96022892\n",
      "Trained batch 743 batch loss 7.13082123 epoch total loss 6.96045828\n",
      "Trained batch 744 batch loss 6.98663473 epoch total loss 6.96049404\n",
      "Trained batch 745 batch loss 7.02961731 epoch total loss 6.96058702\n",
      "Trained batch 746 batch loss 7.04033566 epoch total loss 6.96069384\n",
      "Trained batch 747 batch loss 7.22542286 epoch total loss 6.9610486\n",
      "Trained batch 748 batch loss 7.27740288 epoch total loss 6.96147156\n",
      "Trained batch 749 batch loss 7.45269537 epoch total loss 6.96212721\n",
      "Trained batch 750 batch loss 7.42478323 epoch total loss 6.96274424\n",
      "Trained batch 751 batch loss 7.46667051 epoch total loss 6.96341515\n",
      "Trained batch 752 batch loss 7.42039299 epoch total loss 6.96402311\n",
      "Trained batch 753 batch loss 7.1553793 epoch total loss 6.96427679\n",
      "Trained batch 754 batch loss 7.32475185 epoch total loss 6.96475506\n",
      "Trained batch 755 batch loss 7.15633965 epoch total loss 6.96500874\n",
      "Trained batch 756 batch loss 7.06071281 epoch total loss 6.9651351\n",
      "Trained batch 757 batch loss 7.19587898 epoch total loss 6.96544\n",
      "Trained batch 758 batch loss 6.87606382 epoch total loss 6.96532154\n",
      "Trained batch 759 batch loss 7.0717454 epoch total loss 6.96546221\n",
      "Trained batch 760 batch loss 6.89767551 epoch total loss 6.96537256\n",
      "Trained batch 761 batch loss 6.70742512 epoch total loss 6.96503353\n",
      "Trained batch 762 batch loss 6.60990238 epoch total loss 6.96456766\n",
      "Trained batch 763 batch loss 6.54442453 epoch total loss 6.96401691\n",
      "Trained batch 764 batch loss 6.29589891 epoch total loss 6.9631424\n",
      "Trained batch 765 batch loss 6.7535634 epoch total loss 6.96286821\n",
      "Trained batch 766 batch loss 7.44301605 epoch total loss 6.96349478\n",
      "Trained batch 767 batch loss 7.45441198 epoch total loss 6.96413517\n",
      "Trained batch 768 batch loss 7.19796515 epoch total loss 6.96443939\n",
      "Trained batch 769 batch loss 7.32568502 epoch total loss 6.96490908\n",
      "Trained batch 770 batch loss 7.28655815 epoch total loss 6.96532679\n",
      "Trained batch 771 batch loss 7.23094749 epoch total loss 6.96567154\n",
      "Trained batch 772 batch loss 6.98338509 epoch total loss 6.96569443\n",
      "Trained batch 773 batch loss 7.08080912 epoch total loss 6.9658432\n",
      "Trained batch 774 batch loss 7.09443 epoch total loss 6.96600914\n",
      "Trained batch 775 batch loss 7.21277237 epoch total loss 6.96632767\n",
      "Trained batch 776 batch loss 7.12211084 epoch total loss 6.96652842\n",
      "Trained batch 777 batch loss 7.26088 epoch total loss 6.96690702\n",
      "Trained batch 778 batch loss 7.19176626 epoch total loss 6.96719599\n",
      "Trained batch 779 batch loss 7.20914698 epoch total loss 6.96750641\n",
      "Trained batch 780 batch loss 7.04837275 epoch total loss 6.96761\n",
      "Trained batch 781 batch loss 7.19308519 epoch total loss 6.96789837\n",
      "Trained batch 782 batch loss 7.18005371 epoch total loss 6.96817\n",
      "Trained batch 783 batch loss 6.96766376 epoch total loss 6.96816921\n",
      "Trained batch 784 batch loss 6.65512657 epoch total loss 6.96777\n",
      "Trained batch 785 batch loss 7.05598783 epoch total loss 6.96788311\n",
      "Trained batch 786 batch loss 7.18863487 epoch total loss 6.96816349\n",
      "Trained batch 787 batch loss 6.70595837 epoch total loss 6.96783066\n",
      "Trained batch 788 batch loss 6.71446562 epoch total loss 6.96750879\n",
      "Trained batch 789 batch loss 6.74684906 epoch total loss 6.96722937\n",
      "Trained batch 790 batch loss 7.13064861 epoch total loss 6.96743679\n",
      "Trained batch 791 batch loss 7.18991184 epoch total loss 6.96771765\n",
      "Trained batch 792 batch loss 7.24561834 epoch total loss 6.9680686\n",
      "Trained batch 793 batch loss 7.31888199 epoch total loss 6.9685111\n",
      "Trained batch 794 batch loss 7.12030077 epoch total loss 6.96870184\n",
      "Trained batch 795 batch loss 7.44889736 epoch total loss 6.96930599\n",
      "Trained batch 796 batch loss 7.4815774 epoch total loss 6.96994925\n",
      "Trained batch 797 batch loss 6.87408924 epoch total loss 6.96982908\n",
      "Trained batch 798 batch loss 7.32913113 epoch total loss 6.97027922\n",
      "Trained batch 799 batch loss 7.24801207 epoch total loss 6.97062683\n",
      "Trained batch 800 batch loss 7.3152833 epoch total loss 6.97105789\n",
      "Trained batch 801 batch loss 7.28990078 epoch total loss 6.97145605\n",
      "Trained batch 802 batch loss 7.15453863 epoch total loss 6.97168398\n",
      "Trained batch 803 batch loss 7.45440531 epoch total loss 6.97228527\n",
      "Trained batch 804 batch loss 6.95717192 epoch total loss 6.9722662\n",
      "Trained batch 805 batch loss 6.24689674 epoch total loss 6.97136545\n",
      "Trained batch 806 batch loss 6.89228916 epoch total loss 6.97126722\n",
      "Trained batch 807 batch loss 7.36284351 epoch total loss 6.97175217\n",
      "Trained batch 808 batch loss 6.73481369 epoch total loss 6.97145891\n",
      "Trained batch 809 batch loss 7.03809929 epoch total loss 6.9715414\n",
      "Trained batch 810 batch loss 7.07300043 epoch total loss 6.97166681\n",
      "Trained batch 811 batch loss 7.23396158 epoch total loss 6.97199059\n",
      "Trained batch 812 batch loss 7.15384912 epoch total loss 6.97221422\n",
      "Trained batch 813 batch loss 7.1063323 epoch total loss 6.97237921\n",
      "Trained batch 814 batch loss 6.82217836 epoch total loss 6.97219515\n",
      "Trained batch 815 batch loss 6.90685177 epoch total loss 6.97211456\n",
      "Trained batch 816 batch loss 6.85321331 epoch total loss 6.97196865\n",
      "Trained batch 817 batch loss 6.28144 epoch total loss 6.97112322\n",
      "Trained batch 818 batch loss 7.07197237 epoch total loss 6.97124624\n",
      "Trained batch 819 batch loss 6.59586525 epoch total loss 6.970788\n",
      "Trained batch 820 batch loss 6.76834536 epoch total loss 6.970541\n",
      "Trained batch 821 batch loss 6.99780464 epoch total loss 6.97057486\n",
      "Trained batch 822 batch loss 7.0971 epoch total loss 6.97072887\n",
      "Trained batch 823 batch loss 7.00845909 epoch total loss 6.97077417\n",
      "Trained batch 824 batch loss 7.07966328 epoch total loss 6.97090626\n",
      "Trained batch 825 batch loss 7.11867046 epoch total loss 6.97108555\n",
      "Trained batch 826 batch loss 7.37390852 epoch total loss 6.97157335\n",
      "Trained batch 827 batch loss 7.24442482 epoch total loss 6.97190332\n",
      "Trained batch 828 batch loss 7.33565664 epoch total loss 6.97234249\n",
      "Trained batch 829 batch loss 7.34109354 epoch total loss 6.97278738\n",
      "Trained batch 830 batch loss 7.24615574 epoch total loss 6.97311687\n",
      "Trained batch 831 batch loss 6.89680243 epoch total loss 6.97302532\n",
      "Trained batch 832 batch loss 6.83951473 epoch total loss 6.97286463\n",
      "Trained batch 833 batch loss 7.22325659 epoch total loss 6.97316504\n",
      "Trained batch 834 batch loss 7.29173 epoch total loss 6.97354698\n",
      "Trained batch 835 batch loss 7.34067965 epoch total loss 6.97398663\n",
      "Trained batch 836 batch loss 6.94022179 epoch total loss 6.97394657\n",
      "Trained batch 837 batch loss 7.16193628 epoch total loss 6.97417116\n",
      "Trained batch 838 batch loss 7.27162886 epoch total loss 6.97452593\n",
      "Trained batch 839 batch loss 7.16949606 epoch total loss 6.97475815\n",
      "Trained batch 840 batch loss 7.3160224 epoch total loss 6.97516441\n",
      "Trained batch 841 batch loss 7.26017857 epoch total loss 6.97550344\n",
      "Trained batch 842 batch loss 7.30102348 epoch total loss 6.97588968\n",
      "Trained batch 843 batch loss 7.17956495 epoch total loss 6.97613144\n",
      "Trained batch 844 batch loss 6.93259907 epoch total loss 6.97608\n",
      "Trained batch 845 batch loss 7.03293753 epoch total loss 6.97614717\n",
      "Trained batch 846 batch loss 7.23996067 epoch total loss 6.97645855\n",
      "Trained batch 847 batch loss 7.14504433 epoch total loss 6.97665739\n",
      "Trained batch 848 batch loss 7.1946311 epoch total loss 6.97691488\n",
      "Trained batch 849 batch loss 7.20451784 epoch total loss 6.97718287\n",
      "Trained batch 850 batch loss 7.36819267 epoch total loss 6.97764301\n",
      "Trained batch 851 batch loss 7.30566549 epoch total loss 6.9780283\n",
      "Trained batch 852 batch loss 7.06671572 epoch total loss 6.97813272\n",
      "Trained batch 853 batch loss 6.92211962 epoch total loss 6.9780674\n",
      "Trained batch 854 batch loss 6.86415148 epoch total loss 6.97793436\n",
      "Trained batch 855 batch loss 6.75154781 epoch total loss 6.97766924\n",
      "Trained batch 856 batch loss 6.99841928 epoch total loss 6.97769356\n",
      "Trained batch 857 batch loss 7.02867079 epoch total loss 6.97775316\n",
      "Trained batch 858 batch loss 6.6364913 epoch total loss 6.97735596\n",
      "Trained batch 859 batch loss 6.66374779 epoch total loss 6.9769907\n",
      "Trained batch 860 batch loss 7.27643 epoch total loss 6.97733879\n",
      "Trained batch 861 batch loss 7.2777915 epoch total loss 6.97768784\n",
      "Trained batch 862 batch loss 7.23843145 epoch total loss 6.97799\n",
      "Trained batch 863 batch loss 6.42168856 epoch total loss 6.97734547\n",
      "Trained batch 864 batch loss 6.91560221 epoch total loss 6.97727394\n",
      "Trained batch 865 batch loss 7.04302835 epoch total loss 6.97734976\n",
      "Trained batch 866 batch loss 7.16577911 epoch total loss 6.97756767\n",
      "Trained batch 867 batch loss 6.94907 epoch total loss 6.97753525\n",
      "Trained batch 868 batch loss 6.5710125 epoch total loss 6.97706652\n",
      "Trained batch 869 batch loss 6.58003902 epoch total loss 6.97660971\n",
      "Trained batch 870 batch loss 7.01294804 epoch total loss 6.97665167\n",
      "Trained batch 871 batch loss 7.03576946 epoch total loss 6.97671938\n",
      "Trained batch 872 batch loss 6.98726606 epoch total loss 6.97673178\n",
      "Trained batch 873 batch loss 7.35957623 epoch total loss 6.97717\n",
      "Trained batch 874 batch loss 6.90620804 epoch total loss 6.97708893\n",
      "Trained batch 875 batch loss 7.07066 epoch total loss 6.97719574\n",
      "Trained batch 876 batch loss 6.47029829 epoch total loss 6.97661734\n",
      "Trained batch 877 batch loss 6.85644865 epoch total loss 6.97648\n",
      "Trained batch 878 batch loss 7.03387165 epoch total loss 6.97654533\n",
      "Trained batch 879 batch loss 7.33171844 epoch total loss 6.97694921\n",
      "Trained batch 880 batch loss 7.08890438 epoch total loss 6.97707653\n",
      "Trained batch 881 batch loss 7.22534 epoch total loss 6.97735786\n",
      "Trained batch 882 batch loss 7.3451519 epoch total loss 6.9777751\n",
      "Trained batch 883 batch loss 7.23209524 epoch total loss 6.97806263\n",
      "Trained batch 884 batch loss 7.40788555 epoch total loss 6.978549\n",
      "Trained batch 885 batch loss 7.35044527 epoch total loss 6.9789691\n",
      "Trained batch 886 batch loss 7.12926292 epoch total loss 6.97913885\n",
      "Trained batch 887 batch loss 6.971 epoch total loss 6.97913\n",
      "Trained batch 888 batch loss 6.92636967 epoch total loss 6.97907066\n",
      "Trained batch 889 batch loss 7.06477 epoch total loss 6.97916698\n",
      "Trained batch 890 batch loss 6.91665173 epoch total loss 6.97909641\n",
      "Trained batch 891 batch loss 6.64533472 epoch total loss 6.9787221\n",
      "Trained batch 892 batch loss 6.8512454 epoch total loss 6.97857904\n",
      "Trained batch 893 batch loss 7.34673309 epoch total loss 6.97899151\n",
      "Trained batch 894 batch loss 7.14978933 epoch total loss 6.97918272\n",
      "Trained batch 895 batch loss 7.42636347 epoch total loss 6.97968197\n",
      "Trained batch 896 batch loss 7.28516817 epoch total loss 6.98002291\n",
      "Trained batch 897 batch loss 6.6776557 epoch total loss 6.97968578\n",
      "Trained batch 898 batch loss 6.53620911 epoch total loss 6.97919226\n",
      "Trained batch 899 batch loss 6.70760775 epoch total loss 6.97889\n",
      "Trained batch 900 batch loss 6.99502468 epoch total loss 6.97890806\n",
      "Trained batch 901 batch loss 7.00643635 epoch total loss 6.9789381\n",
      "Trained batch 902 batch loss 7.307271 epoch total loss 6.97930241\n",
      "Trained batch 903 batch loss 7.22271681 epoch total loss 6.97957182\n",
      "Trained batch 904 batch loss 7.27587 epoch total loss 6.97989941\n",
      "Trained batch 905 batch loss 7.18884611 epoch total loss 6.98013067\n",
      "Trained batch 906 batch loss 7.16915894 epoch total loss 6.98033905\n",
      "Trained batch 907 batch loss 7.30530405 epoch total loss 6.98069715\n",
      "Trained batch 908 batch loss 7.3990097 epoch total loss 6.98115778\n",
      "Trained batch 909 batch loss 7.2454114 epoch total loss 6.98144865\n",
      "Trained batch 910 batch loss 7.2638855 epoch total loss 6.98175859\n",
      "Trained batch 911 batch loss 7.11791945 epoch total loss 6.98190784\n",
      "Trained batch 912 batch loss 6.57021379 epoch total loss 6.98145676\n",
      "Trained batch 913 batch loss 7.19883728 epoch total loss 6.9816947\n",
      "Trained batch 914 batch loss 7.07491302 epoch total loss 6.98179626\n",
      "Trained batch 915 batch loss 6.38592052 epoch total loss 6.98114491\n",
      "Trained batch 916 batch loss 7.09176683 epoch total loss 6.98126554\n",
      "Trained batch 917 batch loss 6.77544641 epoch total loss 6.98104095\n",
      "Trained batch 918 batch loss 6.97061539 epoch total loss 6.98103\n",
      "Trained batch 919 batch loss 7.12558413 epoch total loss 6.98118687\n",
      "Trained batch 920 batch loss 6.94319201 epoch total loss 6.98114586\n",
      "Trained batch 921 batch loss 7.00263786 epoch total loss 6.98116922\n",
      "Trained batch 922 batch loss 6.82344532 epoch total loss 6.98099804\n",
      "Trained batch 923 batch loss 6.77387762 epoch total loss 6.98077345\n",
      "Trained batch 924 batch loss 6.91510773 epoch total loss 6.9807024\n",
      "Trained batch 925 batch loss 6.35549831 epoch total loss 6.98002625\n",
      "Trained batch 926 batch loss 7.13793373 epoch total loss 6.98019648\n",
      "Trained batch 927 batch loss 6.75201035 epoch total loss 6.97995043\n",
      "Trained batch 928 batch loss 6.84703636 epoch total loss 6.97980738\n",
      "Trained batch 929 batch loss 7.03321934 epoch total loss 6.97986507\n",
      "Trained batch 930 batch loss 6.77820158 epoch total loss 6.97964811\n",
      "Trained batch 931 batch loss 6.77810335 epoch total loss 6.97943211\n",
      "Trained batch 932 batch loss 6.75327349 epoch total loss 6.9791894\n",
      "Trained batch 933 batch loss 6.15155792 epoch total loss 6.978302\n",
      "Trained batch 934 batch loss 5.95052242 epoch total loss 6.97720194\n",
      "Trained batch 935 batch loss 6.61742115 epoch total loss 6.97681665\n",
      "Trained batch 936 batch loss 7.09027958 epoch total loss 6.97693825\n",
      "Trained batch 937 batch loss 7.03062963 epoch total loss 6.97699547\n",
      "Trained batch 938 batch loss 7.29495525 epoch total loss 6.9773345\n",
      "Trained batch 939 batch loss 6.45252323 epoch total loss 6.97677565\n",
      "Trained batch 940 batch loss 6.52086735 epoch total loss 6.9762907\n",
      "Trained batch 941 batch loss 6.74318552 epoch total loss 6.97604322\n",
      "Trained batch 942 batch loss 7.17491102 epoch total loss 6.97625399\n",
      "Trained batch 943 batch loss 7.22214127 epoch total loss 6.97651482\n",
      "Trained batch 944 batch loss 7.1789155 epoch total loss 6.97672892\n",
      "Trained batch 945 batch loss 6.95438671 epoch total loss 6.97670555\n",
      "Trained batch 946 batch loss 6.54623175 epoch total loss 6.97625065\n",
      "Trained batch 947 batch loss 6.74999332 epoch total loss 6.97601175\n",
      "Trained batch 948 batch loss 7.19845915 epoch total loss 6.97624636\n",
      "Trained batch 949 batch loss 7.01845264 epoch total loss 6.9762907\n",
      "Trained batch 950 batch loss 7.02394199 epoch total loss 6.97634077\n",
      "Trained batch 951 batch loss 7.22562742 epoch total loss 6.97660303\n",
      "Trained batch 952 batch loss 6.90063 epoch total loss 6.97652292\n",
      "Trained batch 953 batch loss 6.9946537 epoch total loss 6.976542\n",
      "Trained batch 954 batch loss 7.31722 epoch total loss 6.97689915\n",
      "Trained batch 955 batch loss 7.24189425 epoch total loss 6.97717667\n",
      "Trained batch 956 batch loss 7.41752 epoch total loss 6.97763729\n",
      "Trained batch 957 batch loss 6.99424553 epoch total loss 6.97765446\n",
      "Trained batch 958 batch loss 6.96736383 epoch total loss 6.97764349\n",
      "Trained batch 959 batch loss 7.19229746 epoch total loss 6.9778676\n",
      "Trained batch 960 batch loss 7.23542213 epoch total loss 6.97813559\n",
      "Trained batch 961 batch loss 6.87406254 epoch total loss 6.97802734\n",
      "Trained batch 962 batch loss 7.18326473 epoch total loss 6.97824049\n",
      "Trained batch 963 batch loss 7.03688717 epoch total loss 6.97830153\n",
      "Trained batch 964 batch loss 6.80174541 epoch total loss 6.97811842\n",
      "Trained batch 965 batch loss 7.30798388 epoch total loss 6.97846031\n",
      "Trained batch 966 batch loss 6.75515175 epoch total loss 6.97822952\n",
      "Trained batch 967 batch loss 7.35851812 epoch total loss 6.97862244\n",
      "Trained batch 968 batch loss 7.44158936 epoch total loss 6.9791007\n",
      "Trained batch 969 batch loss 7.44754696 epoch total loss 6.97958422\n",
      "Trained batch 970 batch loss 7.08447695 epoch total loss 6.97969246\n",
      "Trained batch 971 batch loss 6.35829782 epoch total loss 6.97905254\n",
      "Trained batch 972 batch loss 6.23523521 epoch total loss 6.9782877\n",
      "Trained batch 973 batch loss 6.59007692 epoch total loss 6.97788811\n",
      "Trained batch 974 batch loss 7.385 epoch total loss 6.97830629\n",
      "Trained batch 975 batch loss 6.83997107 epoch total loss 6.9781642\n",
      "Trained batch 976 batch loss 7.31772614 epoch total loss 6.97851229\n",
      "Trained batch 977 batch loss 6.85276031 epoch total loss 6.97838306\n",
      "Trained batch 978 batch loss 6.9694066 epoch total loss 6.978374\n",
      "Trained batch 979 batch loss 7.33271074 epoch total loss 6.97873545\n",
      "Trained batch 980 batch loss 7.40694523 epoch total loss 6.97917223\n",
      "Trained batch 981 batch loss 7.36046 epoch total loss 6.97956085\n",
      "Trained batch 982 batch loss 7.32273149 epoch total loss 6.97991037\n",
      "Trained batch 983 batch loss 6.97781 epoch total loss 6.97990847\n",
      "Trained batch 984 batch loss 7.19508171 epoch total loss 6.98012733\n",
      "Trained batch 985 batch loss 6.05546093 epoch total loss 6.97918892\n",
      "Trained batch 986 batch loss 6.64296961 epoch total loss 6.97884798\n",
      "Trained batch 987 batch loss 7.05331326 epoch total loss 6.97892332\n",
      "Trained batch 988 batch loss 7.10558701 epoch total loss 6.97905111\n",
      "Trained batch 989 batch loss 7.00010633 epoch total loss 6.97907257\n",
      "Trained batch 990 batch loss 6.68593645 epoch total loss 6.97877645\n",
      "Trained batch 991 batch loss 6.95070934 epoch total loss 6.97874832\n",
      "Trained batch 992 batch loss 6.84038544 epoch total loss 6.97860861\n",
      "Trained batch 993 batch loss 6.49214029 epoch total loss 6.9781189\n",
      "Trained batch 994 batch loss 6.98729658 epoch total loss 6.97812796\n",
      "Trained batch 995 batch loss 6.92902803 epoch total loss 6.97807884\n",
      "Trained batch 996 batch loss 6.68583775 epoch total loss 6.97778559\n",
      "Trained batch 997 batch loss 7.26614523 epoch total loss 6.97807503\n",
      "Trained batch 998 batch loss 7.12788773 epoch total loss 6.97822523\n",
      "Trained batch 999 batch loss 7.34854317 epoch total loss 6.97859573\n",
      "Trained batch 1000 batch loss 7.36110258 epoch total loss 6.97897863\n",
      "Trained batch 1001 batch loss 7.35557461 epoch total loss 6.97935486\n",
      "Trained batch 1002 batch loss 7.09381676 epoch total loss 6.97946882\n",
      "Trained batch 1003 batch loss 7.10757256 epoch total loss 6.97959614\n",
      "Trained batch 1004 batch loss 6.8556776 epoch total loss 6.97947264\n",
      "Trained batch 1005 batch loss 6.75865126 epoch total loss 6.97925329\n",
      "Trained batch 1006 batch loss 6.84230804 epoch total loss 6.97911692\n",
      "Trained batch 1007 batch loss 6.69461679 epoch total loss 6.97883463\n",
      "Trained batch 1008 batch loss 7.15130615 epoch total loss 6.97900581\n",
      "Trained batch 1009 batch loss 7.12429476 epoch total loss 6.97915\n",
      "Trained batch 1010 batch loss 7.02016878 epoch total loss 6.97919035\n",
      "Trained batch 1011 batch loss 6.70203543 epoch total loss 6.97891665\n",
      "Trained batch 1012 batch loss 6.4783392 epoch total loss 6.97842216\n",
      "Trained batch 1013 batch loss 6.87034273 epoch total loss 6.97831488\n",
      "Trained batch 1014 batch loss 6.79228449 epoch total loss 6.97813177\n",
      "Trained batch 1015 batch loss 6.96175909 epoch total loss 6.97811604\n",
      "Trained batch 1016 batch loss 6.92466784 epoch total loss 6.97806358\n",
      "Trained batch 1017 batch loss 6.52224255 epoch total loss 6.97761536\n",
      "Trained batch 1018 batch loss 6.60504723 epoch total loss 6.97724915\n",
      "Trained batch 1019 batch loss 6.59518623 epoch total loss 6.97687435\n",
      "Trained batch 1020 batch loss 6.81117487 epoch total loss 6.97671175\n",
      "Trained batch 1021 batch loss 6.83618832 epoch total loss 6.97657442\n",
      "Trained batch 1022 batch loss 7.21798229 epoch total loss 6.97681046\n",
      "Trained batch 1023 batch loss 7.17426157 epoch total loss 6.97700357\n",
      "Trained batch 1024 batch loss 7.09873867 epoch total loss 6.97712231\n",
      "Trained batch 1025 batch loss 7.14668369 epoch total loss 6.97728777\n",
      "Trained batch 1026 batch loss 7.1599474 epoch total loss 6.97746563\n",
      "Trained batch 1027 batch loss 7.24745369 epoch total loss 6.97772884\n",
      "Trained batch 1028 batch loss 7.39291286 epoch total loss 6.97813272\n",
      "Trained batch 1029 batch loss 7.24770832 epoch total loss 6.97839451\n",
      "Trained batch 1030 batch loss 7.16208553 epoch total loss 6.97857285\n",
      "Trained batch 1031 batch loss 6.88076496 epoch total loss 6.97847843\n",
      "Trained batch 1032 batch loss 5.90476751 epoch total loss 6.97743797\n",
      "Trained batch 1033 batch loss 6.30053568 epoch total loss 6.97678232\n",
      "Trained batch 1034 batch loss 6.4085989 epoch total loss 6.97623301\n",
      "Trained batch 1035 batch loss 6.49658918 epoch total loss 6.97576952\n",
      "Trained batch 1036 batch loss 6.53897333 epoch total loss 6.975348\n",
      "Trained batch 1037 batch loss 6.56116819 epoch total loss 6.97494841\n",
      "Trained batch 1038 batch loss 6.38667488 epoch total loss 6.97438192\n",
      "Trained batch 1039 batch loss 6.89781237 epoch total loss 6.97430801\n",
      "Trained batch 1040 batch loss 6.10541534 epoch total loss 6.9734726\n",
      "Trained batch 1041 batch loss 6.93149185 epoch total loss 6.97343254\n",
      "Trained batch 1042 batch loss 7.3628974 epoch total loss 6.97380638\n",
      "Trained batch 1043 batch loss 7.35111666 epoch total loss 6.97416782\n",
      "Trained batch 1044 batch loss 7.09856224 epoch total loss 6.97428703\n",
      "Trained batch 1045 batch loss 7.23985052 epoch total loss 6.97454119\n",
      "Trained batch 1046 batch loss 7.01578188 epoch total loss 6.97458029\n",
      "Trained batch 1047 batch loss 6.82492828 epoch total loss 6.97443724\n",
      "Trained batch 1048 batch loss 7.02131319 epoch total loss 6.97448206\n",
      "Trained batch 1049 batch loss 7.26687956 epoch total loss 6.97476101\n",
      "Trained batch 1050 batch loss 6.96182823 epoch total loss 6.97474909\n",
      "Trained batch 1051 batch loss 7.09919357 epoch total loss 6.97486734\n",
      "Trained batch 1052 batch loss 7.4227376 epoch total loss 6.97529316\n",
      "Trained batch 1053 batch loss 7.14508152 epoch total loss 6.97545433\n",
      "Trained batch 1054 batch loss 7.20532751 epoch total loss 6.97567272\n",
      "Trained batch 1055 batch loss 7.28407097 epoch total loss 6.97596502\n",
      "Trained batch 1056 batch loss 7.01191664 epoch total loss 6.97599888\n",
      "Trained batch 1057 batch loss 6.9740963 epoch total loss 6.97599697\n",
      "Trained batch 1058 batch loss 7.0930748 epoch total loss 6.97610807\n",
      "Trained batch 1059 batch loss 6.86210966 epoch total loss 6.97600031\n",
      "Trained batch 1060 batch loss 6.64363 epoch total loss 6.97568703\n",
      "Trained batch 1061 batch loss 6.46350574 epoch total loss 6.97520399\n",
      "Trained batch 1062 batch loss 6.15991735 epoch total loss 6.97443628\n",
      "Trained batch 1063 batch loss 6.54314756 epoch total loss 6.97403049\n",
      "Trained batch 1064 batch loss 6.84254789 epoch total loss 6.97390747\n",
      "Trained batch 1065 batch loss 6.92395973 epoch total loss 6.97386026\n",
      "Trained batch 1066 batch loss 6.59892607 epoch total loss 6.97350883\n",
      "Trained batch 1067 batch loss 6.30205917 epoch total loss 6.97287941\n",
      "Trained batch 1068 batch loss 6.72730684 epoch total loss 6.97265\n",
      "Trained batch 1069 batch loss 6.83524609 epoch total loss 6.97252131\n",
      "Trained batch 1070 batch loss 6.32745552 epoch total loss 6.97191858\n",
      "Trained batch 1071 batch loss 5.97489357 epoch total loss 6.97098827\n",
      "Trained batch 1072 batch loss 5.81781435 epoch total loss 6.96991253\n",
      "Trained batch 1073 batch loss 6.12930775 epoch total loss 6.96912909\n",
      "Trained batch 1074 batch loss 6.57988882 epoch total loss 6.96876669\n",
      "Trained batch 1075 batch loss 6.02702904 epoch total loss 6.96789074\n",
      "Trained batch 1076 batch loss 7.08523846 epoch total loss 6.968\n",
      "Trained batch 1077 batch loss 6.78208494 epoch total loss 6.96782732\n",
      "Trained batch 1078 batch loss 6.58904743 epoch total loss 6.96747589\n",
      "Trained batch 1079 batch loss 6.31719351 epoch total loss 6.96687317\n",
      "Trained batch 1080 batch loss 6.68037224 epoch total loss 6.96660805\n",
      "Trained batch 1081 batch loss 6.97663641 epoch total loss 6.96661711\n",
      "Trained batch 1082 batch loss 7.04393911 epoch total loss 6.96668863\n",
      "Trained batch 1083 batch loss 7.3167944 epoch total loss 6.96701193\n",
      "Trained batch 1084 batch loss 7.48235607 epoch total loss 6.96748734\n",
      "Trained batch 1085 batch loss 7.62923431 epoch total loss 6.96809769\n",
      "Trained batch 1086 batch loss 7.27599525 epoch total loss 6.96838093\n",
      "Trained batch 1087 batch loss 7.06683874 epoch total loss 6.96847153\n",
      "Trained batch 1088 batch loss 7.13751 epoch total loss 6.96862698\n",
      "Trained batch 1089 batch loss 6.96483707 epoch total loss 6.96862364\n",
      "Trained batch 1090 batch loss 7.20161486 epoch total loss 6.96883726\n",
      "Trained batch 1091 batch loss 7.11828375 epoch total loss 6.96897411\n",
      "Trained batch 1092 batch loss 6.99674034 epoch total loss 6.96899939\n",
      "Trained batch 1093 batch loss 7.23343658 epoch total loss 6.96924162\n",
      "Trained batch 1094 batch loss 6.82814693 epoch total loss 6.9691124\n",
      "Trained batch 1095 batch loss 6.92963552 epoch total loss 6.96907663\n",
      "Trained batch 1096 batch loss 7.09077215 epoch total loss 6.96918726\n",
      "Trained batch 1097 batch loss 7.46357059 epoch total loss 6.96963787\n",
      "Trained batch 1098 batch loss 7.05887747 epoch total loss 6.96971941\n",
      "Trained batch 1099 batch loss 7.19405317 epoch total loss 6.9699235\n",
      "Trained batch 1100 batch loss 7.33037901 epoch total loss 6.97025108\n",
      "Trained batch 1101 batch loss 7.38255787 epoch total loss 6.9706254\n",
      "Trained batch 1102 batch loss 6.94464207 epoch total loss 6.97060204\n",
      "Trained batch 1103 batch loss 6.77107859 epoch total loss 6.97042131\n",
      "Trained batch 1104 batch loss 6.54416513 epoch total loss 6.9700346\n",
      "Trained batch 1105 batch loss 6.82054234 epoch total loss 6.96989918\n",
      "Trained batch 1106 batch loss 6.9189887 epoch total loss 6.9698534\n",
      "Trained batch 1107 batch loss 6.93892956 epoch total loss 6.96982527\n",
      "Trained batch 1108 batch loss 7.11197186 epoch total loss 6.96995354\n",
      "Trained batch 1109 batch loss 6.95396185 epoch total loss 6.96993923\n",
      "Trained batch 1110 batch loss 7.14052 epoch total loss 6.97009277\n",
      "Trained batch 1111 batch loss 7.14796686 epoch total loss 6.97025299\n",
      "Trained batch 1112 batch loss 6.91573286 epoch total loss 6.97020388\n",
      "Trained batch 1113 batch loss 7.17984343 epoch total loss 6.97039223\n",
      "Trained batch 1114 batch loss 5.88950634 epoch total loss 6.96942186\n",
      "Trained batch 1115 batch loss 5.44448757 epoch total loss 6.96805429\n",
      "Trained batch 1116 batch loss 5.27471447 epoch total loss 6.966537\n",
      "Trained batch 1117 batch loss 6.52649307 epoch total loss 6.96614313\n",
      "Trained batch 1118 batch loss 7.21899748 epoch total loss 6.96636915\n",
      "Trained batch 1119 batch loss 7.1314621 epoch total loss 6.96651697\n",
      "Trained batch 1120 batch loss 6.69074 epoch total loss 6.96627045\n",
      "Trained batch 1121 batch loss 6.74295473 epoch total loss 6.96607161\n",
      "Trained batch 1122 batch loss 7.15086031 epoch total loss 6.96623611\n",
      "Trained batch 1123 batch loss 6.5232172 epoch total loss 6.96584225\n",
      "Trained batch 1124 batch loss 6.42844296 epoch total loss 6.96536398\n",
      "Trained batch 1125 batch loss 6.56542683 epoch total loss 6.96500826\n",
      "Trained batch 1126 batch loss 6.61471128 epoch total loss 6.96469736\n",
      "Trained batch 1127 batch loss 6.69033098 epoch total loss 6.9644537\n",
      "Trained batch 1128 batch loss 6.81862879 epoch total loss 6.96432495\n",
      "Trained batch 1129 batch loss 6.82231569 epoch total loss 6.96419907\n",
      "Trained batch 1130 batch loss 7.01926899 epoch total loss 6.96424723\n",
      "Trained batch 1131 batch loss 6.64678574 epoch total loss 6.96396685\n",
      "Trained batch 1132 batch loss 7.05449104 epoch total loss 6.96404696\n",
      "Trained batch 1133 batch loss 7.39881277 epoch total loss 6.96443081\n",
      "Trained batch 1134 batch loss 7.27298498 epoch total loss 6.96470308\n",
      "Trained batch 1135 batch loss 7.12396 epoch total loss 6.96484327\n",
      "Trained batch 1136 batch loss 7.29353333 epoch total loss 6.96513271\n",
      "Trained batch 1137 batch loss 7.10942554 epoch total loss 6.96525955\n",
      "Trained batch 1138 batch loss 6.87132788 epoch total loss 6.96517658\n",
      "Trained batch 1139 batch loss 6.77850246 epoch total loss 6.96501255\n",
      "Trained batch 1140 batch loss 6.43522358 epoch total loss 6.96454763\n",
      "Trained batch 1141 batch loss 6.13077831 epoch total loss 6.96381712\n",
      "Trained batch 1142 batch loss 6.28100204 epoch total loss 6.96321917\n",
      "Trained batch 1143 batch loss 6.81178761 epoch total loss 6.96308661\n",
      "Trained batch 1144 batch loss 6.70199394 epoch total loss 6.96285868\n",
      "Trained batch 1145 batch loss 6.88894415 epoch total loss 6.9627943\n",
      "Trained batch 1146 batch loss 7.13748 epoch total loss 6.96294689\n",
      "Trained batch 1147 batch loss 6.64729548 epoch total loss 6.96267176\n",
      "Trained batch 1148 batch loss 6.69929 epoch total loss 6.9624424\n",
      "Trained batch 1149 batch loss 6.87644053 epoch total loss 6.96236753\n",
      "Trained batch 1150 batch loss 6.80831623 epoch total loss 6.96223354\n",
      "Trained batch 1151 batch loss 6.92918491 epoch total loss 6.96220446\n",
      "Trained batch 1152 batch loss 6.86605072 epoch total loss 6.96212149\n",
      "Trained batch 1153 batch loss 6.80451965 epoch total loss 6.96198463\n",
      "Trained batch 1154 batch loss 7.04906464 epoch total loss 6.96206\n",
      "Trained batch 1155 batch loss 7.33796883 epoch total loss 6.96238565\n",
      "Trained batch 1156 batch loss 7.2097249 epoch total loss 6.96259975\n",
      "Trained batch 1157 batch loss 6.95345306 epoch total loss 6.96259165\n",
      "Trained batch 1158 batch loss 7.09873724 epoch total loss 6.96270943\n",
      "Trained batch 1159 batch loss 7.08859873 epoch total loss 6.96281767\n",
      "Trained batch 1160 batch loss 7.00637627 epoch total loss 6.96285534\n",
      "Trained batch 1161 batch loss 7.04056311 epoch total loss 6.9629221\n",
      "Trained batch 1162 batch loss 6.73646736 epoch total loss 6.96272707\n",
      "Trained batch 1163 batch loss 6.85206366 epoch total loss 6.96263218\n",
      "Trained batch 1164 batch loss 6.89151764 epoch total loss 6.96257114\n",
      "Trained batch 1165 batch loss 7.2710743 epoch total loss 6.96283579\n",
      "Trained batch 1166 batch loss 7.48505926 epoch total loss 6.96328354\n",
      "Trained batch 1167 batch loss 7.27592945 epoch total loss 6.96355152\n",
      "Trained batch 1168 batch loss 6.96372032 epoch total loss 6.96355152\n",
      "Trained batch 1169 batch loss 6.78527 epoch total loss 6.96339893\n",
      "Trained batch 1170 batch loss 6.86962938 epoch total loss 6.96331882\n",
      "Trained batch 1171 batch loss 6.4107132 epoch total loss 6.96284676\n",
      "Trained batch 1172 batch loss 6.40296173 epoch total loss 6.96236897\n",
      "Trained batch 1173 batch loss 6.27924156 epoch total loss 6.96178675\n",
      "Trained batch 1174 batch loss 7.09727049 epoch total loss 6.96190214\n",
      "Trained batch 1175 batch loss 7.08750725 epoch total loss 6.96200895\n",
      "Trained batch 1176 batch loss 7.03802443 epoch total loss 6.96207333\n",
      "Trained batch 1177 batch loss 7.3145628 epoch total loss 6.96237278\n",
      "Trained batch 1178 batch loss 7.14258337 epoch total loss 6.96252584\n",
      "Trained batch 1179 batch loss 6.87193727 epoch total loss 6.96244907\n",
      "Trained batch 1180 batch loss 7.45578384 epoch total loss 6.96286726\n",
      "Trained batch 1181 batch loss 7.21087217 epoch total loss 6.96307755\n",
      "Trained batch 1182 batch loss 7.28502083 epoch total loss 6.96335\n",
      "Trained batch 1183 batch loss 7.13665485 epoch total loss 6.96349669\n",
      "Trained batch 1184 batch loss 7.24327 epoch total loss 6.96373272\n",
      "Trained batch 1185 batch loss 7.32668352 epoch total loss 6.96403933\n",
      "Trained batch 1186 batch loss 7.34700775 epoch total loss 6.96436214\n",
      "Trained batch 1187 batch loss 7.1516223 epoch total loss 6.9645195\n",
      "Trained batch 1188 batch loss 7.22117329 epoch total loss 6.96473503\n",
      "Trained batch 1189 batch loss 7.20700169 epoch total loss 6.96493912\n",
      "Trained batch 1190 batch loss 7.10430479 epoch total loss 6.96505642\n",
      "Trained batch 1191 batch loss 7.34600687 epoch total loss 6.9653759\n",
      "Trained batch 1192 batch loss 7.3005724 epoch total loss 6.96565723\n",
      "Trained batch 1193 batch loss 6.85176039 epoch total loss 6.96556187\n",
      "Trained batch 1194 batch loss 7.23415041 epoch total loss 6.96578693\n",
      "Trained batch 1195 batch loss 7.28646517 epoch total loss 6.96605492\n",
      "Trained batch 1196 batch loss 7.11359453 epoch total loss 6.96617794\n",
      "Trained batch 1197 batch loss 7.40891 epoch total loss 6.96654797\n",
      "Trained batch 1198 batch loss 7.29049921 epoch total loss 6.96681786\n",
      "Trained batch 1199 batch loss 7.29593563 epoch total loss 6.96709251\n",
      "Trained batch 1200 batch loss 7.23739147 epoch total loss 6.96731758\n",
      "Trained batch 1201 batch loss 6.53617239 epoch total loss 6.96695852\n",
      "Trained batch 1202 batch loss 6.87257 epoch total loss 6.96688032\n",
      "Trained batch 1203 batch loss 7.2831831 epoch total loss 6.96714354\n",
      "Trained batch 1204 batch loss 6.85149527 epoch total loss 6.96704769\n",
      "Trained batch 1205 batch loss 6.86623907 epoch total loss 6.96696377\n",
      "Trained batch 1206 batch loss 7.25452805 epoch total loss 6.96720266\n",
      "Trained batch 1207 batch loss 6.55225468 epoch total loss 6.96685934\n",
      "Trained batch 1208 batch loss 6.94596481 epoch total loss 6.96684217\n",
      "Trained batch 1209 batch loss 7.02399731 epoch total loss 6.96689\n",
      "Trained batch 1210 batch loss 7.30527735 epoch total loss 6.96717\n",
      "Trained batch 1211 batch loss 7.36586666 epoch total loss 6.96749926\n",
      "Trained batch 1212 batch loss 7.2076478 epoch total loss 6.96769762\n",
      "Trained batch 1213 batch loss 7.34481335 epoch total loss 6.96800852\n",
      "Trained batch 1214 batch loss 7.31318903 epoch total loss 6.96829319\n",
      "Trained batch 1215 batch loss 6.9825511 epoch total loss 6.96830463\n",
      "Trained batch 1216 batch loss 6.68711138 epoch total loss 6.96807384\n",
      "Trained batch 1217 batch loss 6.82554674 epoch total loss 6.96795654\n",
      "Trained batch 1218 batch loss 6.70589399 epoch total loss 6.96774149\n",
      "Trained batch 1219 batch loss 5.86321402 epoch total loss 6.9668355\n",
      "Trained batch 1220 batch loss 6.64424229 epoch total loss 6.96657133\n",
      "Trained batch 1221 batch loss 6.43003273 epoch total loss 6.96613121\n",
      "Trained batch 1222 batch loss 6.5766263 epoch total loss 6.96581221\n",
      "Trained batch 1223 batch loss 7.15427971 epoch total loss 6.96596622\n",
      "Trained batch 1224 batch loss 7.15385628 epoch total loss 6.96612024\n",
      "Trained batch 1225 batch loss 7.2788763 epoch total loss 6.96637583\n",
      "Trained batch 1226 batch loss 7.21519136 epoch total loss 6.96657848\n",
      "Trained batch 1227 batch loss 7.20521 epoch total loss 6.96677303\n",
      "Trained batch 1228 batch loss 6.99836636 epoch total loss 6.96679831\n",
      "Trained batch 1229 batch loss 6.51909399 epoch total loss 6.96643448\n",
      "Trained batch 1230 batch loss 6.24550343 epoch total loss 6.96584797\n",
      "Trained batch 1231 batch loss 6.86259413 epoch total loss 6.96576405\n",
      "Trained batch 1232 batch loss 7.26205254 epoch total loss 6.96600437\n",
      "Trained batch 1233 batch loss 7.05464363 epoch total loss 6.9660759\n",
      "Trained batch 1234 batch loss 7.06948853 epoch total loss 6.96616\n",
      "Trained batch 1235 batch loss 7.20059061 epoch total loss 6.96634912\n",
      "Trained batch 1236 batch loss 6.82541847 epoch total loss 6.96623516\n",
      "Trained batch 1237 batch loss 7.17225933 epoch total loss 6.96640158\n",
      "Trained batch 1238 batch loss 7.21615553 epoch total loss 6.9666028\n",
      "Trained batch 1239 batch loss 7.22165346 epoch total loss 6.9668088\n",
      "Trained batch 1240 batch loss 7.22875786 epoch total loss 6.96701956\n",
      "Trained batch 1241 batch loss 7.34501791 epoch total loss 6.96732426\n",
      "Trained batch 1242 batch loss 7.08645582 epoch total loss 6.96742058\n",
      "Trained batch 1243 batch loss 7.11138916 epoch total loss 6.96753597\n",
      "Trained batch 1244 batch loss 7.20418 epoch total loss 6.96772623\n",
      "Trained batch 1245 batch loss 7.2105751 epoch total loss 6.96792173\n",
      "Trained batch 1246 batch loss 7.18581581 epoch total loss 6.96809626\n",
      "Trained batch 1247 batch loss 7.18523598 epoch total loss 6.96827078\n",
      "Trained batch 1248 batch loss 6.44392347 epoch total loss 6.96785069\n",
      "Trained batch 1249 batch loss 6.58560228 epoch total loss 6.96754503\n",
      "Trained batch 1250 batch loss 7.41101837 epoch total loss 6.9679\n",
      "Trained batch 1251 batch loss 7.29736042 epoch total loss 6.96816301\n",
      "Trained batch 1252 batch loss 7.038764 epoch total loss 6.96821976\n",
      "Trained batch 1253 batch loss 6.90268469 epoch total loss 6.96816683\n",
      "Trained batch 1254 batch loss 7.1759181 epoch total loss 6.96833277\n",
      "Trained batch 1255 batch loss 7.2761488 epoch total loss 6.96857786\n",
      "Trained batch 1256 batch loss 7.10543108 epoch total loss 6.96868706\n",
      "Trained batch 1257 batch loss 7.2412219 epoch total loss 6.96890402\n",
      "Trained batch 1258 batch loss 6.42545414 epoch total loss 6.968472\n",
      "Trained batch 1259 batch loss 5.53045559 epoch total loss 6.96733\n",
      "Trained batch 1260 batch loss 6.43268776 epoch total loss 6.96690559\n",
      "Trained batch 1261 batch loss 6.8436861 epoch total loss 6.96680784\n",
      "Trained batch 1262 batch loss 6.46790791 epoch total loss 6.96641207\n",
      "Trained batch 1263 batch loss 7.03186703 epoch total loss 6.96646452\n",
      "Trained batch 1264 batch loss 7.10355568 epoch total loss 6.96657276\n",
      "Trained batch 1265 batch loss 6.91233492 epoch total loss 6.96653\n",
      "Trained batch 1266 batch loss 7.12119102 epoch total loss 6.96665192\n",
      "Trained batch 1267 batch loss 6.94321966 epoch total loss 6.96663332\n",
      "Trained batch 1268 batch loss 7.04105616 epoch total loss 6.96669197\n",
      "Trained batch 1269 batch loss 6.67971182 epoch total loss 6.96646595\n",
      "Trained batch 1270 batch loss 6.86158419 epoch total loss 6.96638298\n",
      "Trained batch 1271 batch loss 6.94191313 epoch total loss 6.96636438\n",
      "Trained batch 1272 batch loss 6.97204971 epoch total loss 6.96636868\n",
      "Trained batch 1273 batch loss 6.69926119 epoch total loss 6.96615839\n",
      "Trained batch 1274 batch loss 6.7742033 epoch total loss 6.96600819\n",
      "Trained batch 1275 batch loss 7.03744841 epoch total loss 6.96606398\n",
      "Trained batch 1276 batch loss 7.02456427 epoch total loss 6.96610975\n",
      "Trained batch 1277 batch loss 6.85858965 epoch total loss 6.96602535\n",
      "Trained batch 1278 batch loss 6.04896832 epoch total loss 6.96530771\n",
      "Trained batch 1279 batch loss 6.0092845 epoch total loss 6.96456051\n",
      "Trained batch 1280 batch loss 7.02106524 epoch total loss 6.96460485\n",
      "Trained batch 1281 batch loss 6.95254087 epoch total loss 6.96459532\n",
      "Trained batch 1282 batch loss 6.99816 epoch total loss 6.96462154\n",
      "Trained batch 1283 batch loss 6.94208145 epoch total loss 6.9646039\n",
      "Trained batch 1284 batch loss 6.38378572 epoch total loss 6.96415186\n",
      "Trained batch 1285 batch loss 6.47247219 epoch total loss 6.96376896\n",
      "Trained batch 1286 batch loss 6.53767204 epoch total loss 6.96343803\n",
      "Trained batch 1287 batch loss 6.71411514 epoch total loss 6.96324444\n",
      "Trained batch 1288 batch loss 6.94367933 epoch total loss 6.9632287\n",
      "Trained batch 1289 batch loss 6.59494829 epoch total loss 6.96294308\n",
      "Trained batch 1290 batch loss 6.32275867 epoch total loss 6.96244717\n",
      "Trained batch 1291 batch loss 6.36555767 epoch total loss 6.96198463\n",
      "Trained batch 1292 batch loss 6.45076036 epoch total loss 6.96158886\n",
      "Trained batch 1293 batch loss 6.73375797 epoch total loss 6.96141243\n",
      "Trained batch 1294 batch loss 6.86875916 epoch total loss 6.96134138\n",
      "Trained batch 1295 batch loss 7.0586729 epoch total loss 6.96141624\n",
      "Trained batch 1296 batch loss 6.77995729 epoch total loss 6.96127653\n",
      "Trained batch 1297 batch loss 7.16176367 epoch total loss 6.9614315\n",
      "Trained batch 1298 batch loss 7.3434515 epoch total loss 6.96172619\n",
      "Trained batch 1299 batch loss 7.42521667 epoch total loss 6.96208239\n",
      "Trained batch 1300 batch loss 7.44716 epoch total loss 6.96245575\n",
      "Trained batch 1301 batch loss 7.24126291 epoch total loss 6.96267\n",
      "Trained batch 1302 batch loss 7.20153475 epoch total loss 6.96285295\n",
      "Trained batch 1303 batch loss 6.48228407 epoch total loss 6.96248436\n",
      "Trained batch 1304 batch loss 6.92699051 epoch total loss 6.96245718\n",
      "Trained batch 1305 batch loss 7.1736145 epoch total loss 6.96261883\n",
      "Trained batch 1306 batch loss 7.16944027 epoch total loss 6.96277761\n",
      "Trained batch 1307 batch loss 7.30602169 epoch total loss 6.96304\n",
      "Trained batch 1308 batch loss 7.18489838 epoch total loss 6.96320963\n",
      "Trained batch 1309 batch loss 6.7702527 epoch total loss 6.96306229\n",
      "Trained batch 1310 batch loss 6.66152811 epoch total loss 6.9628315\n",
      "Trained batch 1311 batch loss 6.84514 epoch total loss 6.96274185\n",
      "Trained batch 1312 batch loss 7.05328083 epoch total loss 6.96281099\n",
      "Trained batch 1313 batch loss 6.63845587 epoch total loss 6.96256399\n",
      "Trained batch 1314 batch loss 6.74620581 epoch total loss 6.96239948\n",
      "Trained batch 1315 batch loss 6.36456442 epoch total loss 6.96194458\n",
      "Trained batch 1316 batch loss 6.84284925 epoch total loss 6.96185398\n",
      "Trained batch 1317 batch loss 7.10501146 epoch total loss 6.96196318\n",
      "Trained batch 1318 batch loss 7.22772312 epoch total loss 6.9621644\n",
      "Trained batch 1319 batch loss 7.30197 epoch total loss 6.96242189\n",
      "Trained batch 1320 batch loss 7.25223207 epoch total loss 6.96264124\n",
      "Trained batch 1321 batch loss 6.95587301 epoch total loss 6.96263647\n",
      "Trained batch 1322 batch loss 6.60479498 epoch total loss 6.96236563\n",
      "Trained batch 1323 batch loss 6.48174143 epoch total loss 6.9620018\n",
      "Trained batch 1324 batch loss 7.04992867 epoch total loss 6.96206808\n",
      "Trained batch 1325 batch loss 6.78391457 epoch total loss 6.96193409\n",
      "Trained batch 1326 batch loss 7.30473566 epoch total loss 6.96219254\n",
      "Trained batch 1327 batch loss 6.43787813 epoch total loss 6.96179724\n",
      "Trained batch 1328 batch loss 6.37351465 epoch total loss 6.96135378\n",
      "Trained batch 1329 batch loss 6.96568632 epoch total loss 6.96135712\n",
      "Trained batch 1330 batch loss 7.14749622 epoch total loss 6.96149683\n",
      "Trained batch 1331 batch loss 7.08344507 epoch total loss 6.96158838\n",
      "Trained batch 1332 batch loss 7.24938869 epoch total loss 6.96180391\n",
      "Trained batch 1333 batch loss 6.9811368 epoch total loss 6.9618187\n",
      "Trained batch 1334 batch loss 7.009 epoch total loss 6.96185398\n",
      "Trained batch 1335 batch loss 6.76778 epoch total loss 6.96170855\n",
      "Trained batch 1336 batch loss 7.02549124 epoch total loss 6.96175623\n",
      "Trained batch 1337 batch loss 6.56429148 epoch total loss 6.96145916\n",
      "Trained batch 1338 batch loss 7.02907658 epoch total loss 6.9615097\n",
      "Trained batch 1339 batch loss 7.10913134 epoch total loss 6.96162033\n",
      "Trained batch 1340 batch loss 7.29200602 epoch total loss 6.96186686\n",
      "Trained batch 1341 batch loss 7.21709728 epoch total loss 6.96205664\n",
      "Trained batch 1342 batch loss 7.46142244 epoch total loss 6.96242857\n",
      "Trained batch 1343 batch loss 7.24772263 epoch total loss 6.96264124\n",
      "Trained batch 1344 batch loss 7.13794708 epoch total loss 6.96277142\n",
      "Trained batch 1345 batch loss 7.11183643 epoch total loss 6.96288252\n",
      "Trained batch 1346 batch loss 6.6707139 epoch total loss 6.96266556\n",
      "Trained batch 1347 batch loss 6.969 epoch total loss 6.96267033\n",
      "Trained batch 1348 batch loss 7.20792 epoch total loss 6.962852\n",
      "Trained batch 1349 batch loss 7.09993505 epoch total loss 6.96295357\n",
      "Trained batch 1350 batch loss 7.04751396 epoch total loss 6.96301651\n",
      "Trained batch 1351 batch loss 6.99814796 epoch total loss 6.96304226\n",
      "Trained batch 1352 batch loss 6.85377789 epoch total loss 6.9629612\n",
      "Trained batch 1353 batch loss 6.48178196 epoch total loss 6.96260548\n",
      "Trained batch 1354 batch loss 6.88699245 epoch total loss 6.96254969\n",
      "Trained batch 1355 batch loss 6.98029375 epoch total loss 6.96256256\n",
      "Trained batch 1356 batch loss 7.35192871 epoch total loss 6.96284962\n",
      "Trained batch 1357 batch loss 7.0824151 epoch total loss 6.96293736\n",
      "Trained batch 1358 batch loss 7.13325882 epoch total loss 6.96306229\n",
      "Trained batch 1359 batch loss 7.00586605 epoch total loss 6.96309376\n",
      "Trained batch 1360 batch loss 6.87304 epoch total loss 6.96302795\n",
      "Trained batch 1361 batch loss 6.99442148 epoch total loss 6.96305084\n",
      "Trained batch 1362 batch loss 7.19218 epoch total loss 6.96321917\n",
      "Trained batch 1363 batch loss 7.13565111 epoch total loss 6.96334553\n",
      "Trained batch 1364 batch loss 6.65724373 epoch total loss 6.96312094\n",
      "Trained batch 1365 batch loss 7.18579769 epoch total loss 6.96328402\n",
      "Trained batch 1366 batch loss 6.70745039 epoch total loss 6.96309662\n",
      "Trained batch 1367 batch loss 6.52970028 epoch total loss 6.96277905\n",
      "Trained batch 1368 batch loss 6.3385148 epoch total loss 6.96232319\n",
      "Trained batch 1369 batch loss 7.16333055 epoch total loss 6.96246958\n",
      "Trained batch 1370 batch loss 7.2784729 epoch total loss 6.96270037\n",
      "Trained batch 1371 batch loss 7.24807501 epoch total loss 6.96290827\n",
      "Trained batch 1372 batch loss 6.92744541 epoch total loss 6.962883\n",
      "Trained batch 1373 batch loss 6.38678455 epoch total loss 6.96246338\n",
      "Trained batch 1374 batch loss 6.63470697 epoch total loss 6.96222448\n",
      "Trained batch 1375 batch loss 6.50586653 epoch total loss 6.9618926\n",
      "Trained batch 1376 batch loss 6.89677477 epoch total loss 6.9618454\n",
      "Trained batch 1377 batch loss 6.94781685 epoch total loss 6.96183538\n",
      "Trained batch 1378 batch loss 6.45087528 epoch total loss 6.96146488\n",
      "Trained batch 1379 batch loss 6.73002195 epoch total loss 6.96129704\n",
      "Trained batch 1380 batch loss 6.29601717 epoch total loss 6.96081495\n",
      "Trained batch 1381 batch loss 6.58865452 epoch total loss 6.96054554\n",
      "Trained batch 1382 batch loss 6.56833029 epoch total loss 6.96026182\n",
      "Trained batch 1383 batch loss 6.8948555 epoch total loss 6.96021461\n",
      "Trained batch 1384 batch loss 6.77704525 epoch total loss 6.96008253\n",
      "Trained batch 1385 batch loss 6.9937048 epoch total loss 6.96010685\n",
      "Trained batch 1386 batch loss 7.25043917 epoch total loss 6.96031618\n",
      "Trained batch 1387 batch loss 7.29385948 epoch total loss 6.96055651\n",
      "Trained batch 1388 batch loss 6.96188688 epoch total loss 6.96055746\n",
      "Epoch 9 train loss 6.960557460784912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 03:11:03.140996: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "2025-03-19 03:11:03.141047: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "\t [[RemoteCall]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 1 batch loss 6.97380686\n",
      "Validated batch 2 batch loss 7.01448059\n",
      "Validated batch 3 batch loss 6.86617899\n",
      "Validated batch 4 batch loss 7.36465454\n",
      "Validated batch 5 batch loss 7.38807774\n",
      "Validated batch 6 batch loss 7.2293396\n",
      "Validated batch 7 batch loss 7.24631453\n",
      "Validated batch 8 batch loss 7.13011122\n",
      "Validated batch 9 batch loss 7.25526524\n",
      "Validated batch 10 batch loss 7.4480114\n",
      "Validated batch 11 batch loss 7.44421864\n",
      "Validated batch 12 batch loss 6.9859271\n",
      "Validated batch 13 batch loss 6.9065609\n",
      "Validated batch 14 batch loss 7.33400154\n",
      "Validated batch 15 batch loss 6.94438219\n",
      "Validated batch 16 batch loss 6.9456687\n",
      "Validated batch 17 batch loss 7.09445\n",
      "Validated batch 18 batch loss 6.27674961\n",
      "Validated batch 19 batch loss 6.86824179\n",
      "Validated batch 20 batch loss 7.195364\n",
      "Validated batch 21 batch loss 7.11609364\n",
      "Validated batch 22 batch loss 7.34508801\n",
      "Validated batch 23 batch loss 6.95824146\n",
      "Validated batch 24 batch loss 6.51150751\n",
      "Validated batch 25 batch loss 7.30905437\n",
      "Validated batch 26 batch loss 7.08124924\n",
      "Validated batch 27 batch loss 7.17253208\n",
      "Validated batch 28 batch loss 7.02366781\n",
      "Validated batch 29 batch loss 7.45338535\n",
      "Validated batch 30 batch loss 6.90673447\n",
      "Validated batch 31 batch loss 7.43114\n",
      "Validated batch 32 batch loss 6.78143454\n",
      "Validated batch 33 batch loss 6.96450329\n",
      "Validated batch 34 batch loss 6.92186737\n",
      "Validated batch 35 batch loss 6.45336437\n",
      "Validated batch 36 batch loss 6.4379468\n",
      "Validated batch 37 batch loss 7.4261694\n",
      "Validated batch 38 batch loss 7.23853254\n",
      "Validated batch 39 batch loss 6.72775793\n",
      "Validated batch 40 batch loss 7.27088404\n",
      "Validated batch 41 batch loss 7.25105381\n",
      "Validated batch 42 batch loss 7.22356844\n",
      "Validated batch 43 batch loss 7.42299366\n",
      "Validated batch 44 batch loss 7.29486799\n",
      "Validated batch 45 batch loss 6.9731884\n",
      "Validated batch 46 batch loss 6.66682148\n",
      "Validated batch 47 batch loss 6.74669027\n",
      "Validated batch 48 batch loss 6.95661163\n",
      "Validated batch 49 batch loss 6.58431339\n",
      "Validated batch 50 batch loss 6.45693159\n",
      "Validated batch 51 batch loss 6.61413193\n",
      "Validated batch 52 batch loss 7.06320572\n",
      "Validated batch 53 batch loss 6.89744854\n",
      "Validated batch 54 batch loss 7.0945816\n",
      "Validated batch 55 batch loss 7.23060036\n",
      "Validated batch 56 batch loss 7.2604022\n",
      "Validated batch 57 batch loss 7.15369463\n",
      "Validated batch 58 batch loss 6.61385489\n",
      "Validated batch 59 batch loss 7.15341806\n",
      "Validated batch 60 batch loss 7.13106585\n",
      "Validated batch 61 batch loss 7.14242601\n",
      "Validated batch 62 batch loss 7.42708349\n",
      "Validated batch 63 batch loss 6.85959148\n",
      "Validated batch 64 batch loss 7.37713385\n",
      "Validated batch 65 batch loss 7.25884867\n",
      "Validated batch 66 batch loss 7.10971737\n",
      "Validated batch 67 batch loss 7.08809233\n",
      "Validated batch 68 batch loss 7.22354698\n",
      "Validated batch 69 batch loss 7.33563948\n",
      "Validated batch 70 batch loss 7.2860508\n",
      "Validated batch 71 batch loss 7.02562904\n",
      "Validated batch 72 batch loss 6.9516511\n",
      "Validated batch 73 batch loss 6.69180679\n",
      "Validated batch 74 batch loss 6.69862556\n",
      "Validated batch 75 batch loss 7.06869698\n",
      "Validated batch 76 batch loss 6.82904243\n",
      "Validated batch 77 batch loss 7.202806\n",
      "Validated batch 78 batch loss 7.25803375\n",
      "Validated batch 79 batch loss 6.91283035\n",
      "Validated batch 80 batch loss 6.96627665\n",
      "Validated batch 81 batch loss 6.51683569\n",
      "Validated batch 82 batch loss 6.96041203\n",
      "Validated batch 83 batch loss 6.98078585\n",
      "Validated batch 84 batch loss 7.2581768\n",
      "Validated batch 85 batch loss 7.12821341\n",
      "Validated batch 86 batch loss 6.84068203\n",
      "Validated batch 87 batch loss 6.47089434\n",
      "Validated batch 88 batch loss 6.72708321\n",
      "Validated batch 89 batch loss 7.07091951\n",
      "Validated batch 90 batch loss 6.69649124\n",
      "Validated batch 91 batch loss 6.77665\n",
      "Validated batch 92 batch loss 6.83591127\n",
      "Validated batch 93 batch loss 7.29384804\n",
      "Validated batch 94 batch loss 7.4507966\n",
      "Validated batch 95 batch loss 6.96691608\n",
      "Validated batch 96 batch loss 6.47497225\n",
      "Validated batch 97 batch loss 7.11362028\n",
      "Validated batch 98 batch loss 6.7446928\n",
      "Validated batch 99 batch loss 6.37185574\n",
      "Validated batch 100 batch loss 7.03341866\n",
      "Validated batch 101 batch loss 6.41727638\n",
      "Validated batch 102 batch loss 7.19389915\n",
      "Validated batch 103 batch loss 6.74809361\n",
      "Validated batch 104 batch loss 6.78906536\n",
      "Validated batch 105 batch loss 6.51522541\n",
      "Validated batch 106 batch loss 7.03100872\n",
      "Validated batch 107 batch loss 6.91172218\n",
      "Validated batch 108 batch loss 7.1697154\n",
      "Validated batch 109 batch loss 6.9022646\n",
      "Validated batch 110 batch loss 7.24769402\n",
      "Validated batch 111 batch loss 6.96632481\n",
      "Validated batch 112 batch loss 6.86918926\n",
      "Validated batch 113 batch loss 7.08006811\n",
      "Validated batch 114 batch loss 6.08428335\n",
      "Validated batch 115 batch loss 7.23861742\n",
      "Validated batch 116 batch loss 7.52697563\n",
      "Validated batch 117 batch loss 7.04736233\n",
      "Validated batch 118 batch loss 7.15650177\n",
      "Validated batch 119 batch loss 7.08148861\n",
      "Validated batch 120 batch loss 7.20151711\n",
      "Validated batch 121 batch loss 7.30110502\n",
      "Validated batch 122 batch loss 7.02314854\n",
      "Validated batch 123 batch loss 6.86950588\n",
      "Validated batch 124 batch loss 6.76296377\n",
      "Validated batch 125 batch loss 6.83341217\n",
      "Validated batch 126 batch loss 7.13675\n",
      "Validated batch 127 batch loss 7.07198048\n",
      "Validated batch 128 batch loss 6.7706356\n",
      "Validated batch 129 batch loss 6.66647482\n",
      "Validated batch 130 batch loss 6.93180132\n",
      "Validated batch 131 batch loss 7.32654142\n",
      "Validated batch 132 batch loss 7.36956787\n",
      "Validated batch 133 batch loss 7.16484451\n",
      "Validated batch 134 batch loss 7.47864342\n",
      "Validated batch 135 batch loss 7.51932859\n",
      "Validated batch 136 batch loss 7.35987949\n",
      "Validated batch 137 batch loss 7.0065732\n",
      "Validated batch 138 batch loss 6.6318121\n",
      "Validated batch 139 batch loss 7.05347776\n",
      "Validated batch 140 batch loss 6.80688858\n",
      "Validated batch 141 batch loss 6.90316153\n",
      "Validated batch 142 batch loss 6.8981657\n",
      "Validated batch 143 batch loss 6.86651\n",
      "Validated batch 144 batch loss 7.16304541\n",
      "Validated batch 145 batch loss 6.87956\n",
      "Validated batch 146 batch loss 7.26023579\n",
      "Validated batch 147 batch loss 7.11031532\n",
      "Validated batch 148 batch loss 7.09177256\n",
      "Validated batch 149 batch loss 7.39762068\n",
      "Validated batch 150 batch loss 7.28252745\n",
      "Validated batch 151 batch loss 6.84239864\n",
      "Validated batch 152 batch loss 6.90625715\n",
      "Validated batch 153 batch loss 7.43787\n",
      "Validated batch 154 batch loss 6.77107525\n",
      "Validated batch 155 batch loss 7.27687645\n",
      "Validated batch 156 batch loss 6.73568058\n",
      "Validated batch 157 batch loss 7.40598345\n",
      "Validated batch 158 batch loss 6.73599195\n",
      "Validated batch 159 batch loss 6.95664787\n",
      "Validated batch 160 batch loss 6.98165655\n",
      "Validated batch 161 batch loss 7.00655031\n",
      "Validated batch 162 batch loss 7.04912615\n",
      "Validated batch 163 batch loss 6.52765799\n",
      "Validated batch 164 batch loss 7.23062515\n",
      "Validated batch 165 batch loss 7.0682826\n",
      "Validated batch 166 batch loss 6.56960392\n",
      "Validated batch 167 batch loss 7.16552305\n",
      "Validated batch 168 batch loss 7.07084656\n",
      "Validated batch 169 batch loss 7.09271717\n",
      "Validated batch 170 batch loss 7.03270245\n",
      "Validated batch 171 batch loss 7.19713259\n",
      "Validated batch 172 batch loss 6.9392767\n",
      "Validated batch 173 batch loss 6.9134531\n",
      "Validated batch 174 batch loss 7.09873199\n",
      "Validated batch 175 batch loss 7.11514902\n",
      "Validated batch 176 batch loss 7.27334118\n",
      "Validated batch 177 batch loss 7.16859913\n",
      "Validated batch 178 batch loss 6.93843126\n",
      "Validated batch 179 batch loss 6.73692036\n",
      "Validated batch 180 batch loss 7.01594877\n",
      "Validated batch 181 batch loss 7.39653873\n",
      "Validated batch 182 batch loss 7.34716225\n",
      "Validated batch 183 batch loss 7.21337223\n",
      "Validated batch 184 batch loss 7.03724527\n",
      "Validated batch 185 batch loss 3.47969198\n",
      "Epoch 9 val loss 7.002762794494629\n",
      "Start epoch 10 with learning rate 0.001\n",
      "Start distributed training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 03:11:11.667511: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "2025-03-19 03:11:11.667558: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "\t [[RemoteCall]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1 batch loss 6.62646866 epoch total loss 6.62646866\n",
      "Trained batch 2 batch loss 6.36878061 epoch total loss 6.4976244\n",
      "Trained batch 3 batch loss 7.01747322 epoch total loss 6.6709075\n",
      "Trained batch 4 batch loss 6.75254774 epoch total loss 6.69131756\n",
      "Trained batch 5 batch loss 6.66896057 epoch total loss 6.68684626\n",
      "Trained batch 6 batch loss 6.89856243 epoch total loss 6.72213221\n",
      "Trained batch 7 batch loss 7.17683554 epoch total loss 6.78709\n",
      "Trained batch 8 batch loss 6.86623669 epoch total loss 6.79698324\n",
      "Trained batch 9 batch loss 6.45325041 epoch total loss 6.75879097\n",
      "Trained batch 10 batch loss 6.06388474 epoch total loss 6.68930054\n",
      "Trained batch 11 batch loss 6.03619099 epoch total loss 6.62992716\n",
      "Trained batch 12 batch loss 6.34241629 epoch total loss 6.605968\n",
      "Trained batch 13 batch loss 6.60699844 epoch total loss 6.60604763\n",
      "Trained batch 14 batch loss 6.79887962 epoch total loss 6.61982107\n",
      "Trained batch 15 batch loss 6.96988153 epoch total loss 6.64315844\n",
      "Trained batch 16 batch loss 6.88091516 epoch total loss 6.65801811\n",
      "Trained batch 17 batch loss 7.15181732 epoch total loss 6.68706512\n",
      "Trained batch 18 batch loss 6.85662413 epoch total loss 6.69648504\n",
      "Trained batch 19 batch loss 6.52188492 epoch total loss 6.68729544\n",
      "Trained batch 20 batch loss 7.03973675 epoch total loss 6.70491695\n",
      "Trained batch 21 batch loss 7.36355686 epoch total loss 6.73628092\n",
      "Trained batch 22 batch loss 7.44586372 epoch total loss 6.76853466\n",
      "Trained batch 23 batch loss 7.38314438 epoch total loss 6.79525709\n",
      "Trained batch 24 batch loss 7.35315895 epoch total loss 6.8185029\n",
      "Trained batch 25 batch loss 7.18187571 epoch total loss 6.83303785\n",
      "Trained batch 26 batch loss 6.97270346 epoch total loss 6.83840942\n",
      "Trained batch 27 batch loss 7.02754879 epoch total loss 6.84541416\n",
      "Trained batch 28 batch loss 6.85905886 epoch total loss 6.84590149\n",
      "Trained batch 29 batch loss 7.12788486 epoch total loss 6.85562515\n",
      "Trained batch 30 batch loss 7.20133 epoch total loss 6.8671484\n",
      "Trained batch 31 batch loss 7.21031237 epoch total loss 6.87821817\n",
      "Trained batch 32 batch loss 7.35309458 epoch total loss 6.89305782\n",
      "Trained batch 33 batch loss 7.14384079 epoch total loss 6.90065765\n",
      "Trained batch 34 batch loss 7.02727509 epoch total loss 6.90438128\n",
      "Trained batch 35 batch loss 7.23730135 epoch total loss 6.91389322\n",
      "Trained batch 36 batch loss 7.29500914 epoch total loss 6.92448\n",
      "Trained batch 37 batch loss 6.87866735 epoch total loss 6.92324162\n",
      "Trained batch 38 batch loss 7.31994724 epoch total loss 6.93368149\n",
      "Trained batch 39 batch loss 6.81054354 epoch total loss 6.93052387\n",
      "Trained batch 40 batch loss 6.87829065 epoch total loss 6.92921829\n",
      "Trained batch 41 batch loss 6.93057442 epoch total loss 6.92925119\n",
      "Trained batch 42 batch loss 6.33900356 epoch total loss 6.91519737\n",
      "Trained batch 43 batch loss 5.60181379 epoch total loss 6.88465357\n",
      "Trained batch 44 batch loss 6.32363224 epoch total loss 6.87190294\n",
      "Trained batch 45 batch loss 7.03846598 epoch total loss 6.87560415\n",
      "Trained batch 46 batch loss 7.44036436 epoch total loss 6.88788176\n",
      "Trained batch 47 batch loss 6.91247082 epoch total loss 6.88840485\n",
      "Trained batch 48 batch loss 6.43979359 epoch total loss 6.87905884\n",
      "Trained batch 49 batch loss 5.98469448 epoch total loss 6.86080599\n",
      "Trained batch 50 batch loss 6.96289206 epoch total loss 6.86284781\n",
      "Trained batch 51 batch loss 7.03714895 epoch total loss 6.8662653\n",
      "Trained batch 52 batch loss 7.26029968 epoch total loss 6.87384272\n",
      "Trained batch 53 batch loss 7.34707499 epoch total loss 6.88277149\n",
      "Trained batch 54 batch loss 7.33223438 epoch total loss 6.89109516\n",
      "Trained batch 55 batch loss 7.19709921 epoch total loss 6.89665937\n",
      "Trained batch 56 batch loss 7.39019775 epoch total loss 6.90547228\n",
      "Trained batch 57 batch loss 7.43912411 epoch total loss 6.9148345\n",
      "Trained batch 58 batch loss 7.12761211 epoch total loss 6.91850328\n",
      "Trained batch 59 batch loss 7.26277542 epoch total loss 6.92433882\n",
      "Trained batch 60 batch loss 6.8206377 epoch total loss 6.92261028\n",
      "Trained batch 61 batch loss 7.33721066 epoch total loss 6.92940712\n",
      "Trained batch 62 batch loss 7.40757227 epoch total loss 6.93711948\n",
      "Trained batch 63 batch loss 7.11901903 epoch total loss 6.94000673\n",
      "Trained batch 64 batch loss 7.24100399 epoch total loss 6.94471\n",
      "Trained batch 65 batch loss 6.96996927 epoch total loss 6.9450984\n",
      "Trained batch 66 batch loss 7.02873039 epoch total loss 6.94636536\n",
      "Trained batch 67 batch loss 7.09315062 epoch total loss 6.94855595\n",
      "Trained batch 68 batch loss 6.87795 epoch total loss 6.94751787\n",
      "Trained batch 69 batch loss 7.09746647 epoch total loss 6.9496913\n",
      "Trained batch 70 batch loss 6.80896235 epoch total loss 6.94768047\n",
      "Trained batch 71 batch loss 6.37201834 epoch total loss 6.93957281\n",
      "Trained batch 72 batch loss 6.90084696 epoch total loss 6.93903494\n",
      "Trained batch 73 batch loss 6.75849819 epoch total loss 6.93656158\n",
      "Trained batch 74 batch loss 7.01771736 epoch total loss 6.93765831\n",
      "Trained batch 75 batch loss 7.07833147 epoch total loss 6.93953371\n",
      "Trained batch 76 batch loss 7.13704252 epoch total loss 6.94213247\n",
      "Trained batch 77 batch loss 6.93135214 epoch total loss 6.94199181\n",
      "Trained batch 78 batch loss 7.23068333 epoch total loss 6.94569349\n",
      "Trained batch 79 batch loss 6.90341282 epoch total loss 6.94515896\n",
      "Trained batch 80 batch loss 6.83357477 epoch total loss 6.94376373\n",
      "Trained batch 81 batch loss 6.35975266 epoch total loss 6.93655348\n",
      "Trained batch 82 batch loss 6.99022961 epoch total loss 6.93720818\n",
      "Trained batch 83 batch loss 6.93181133 epoch total loss 6.93714333\n",
      "Trained batch 84 batch loss 7.04264307 epoch total loss 6.93839931\n",
      "Trained batch 85 batch loss 7.16208 epoch total loss 6.94103146\n",
      "Trained batch 86 batch loss 7.300632 epoch total loss 6.94521332\n",
      "Trained batch 87 batch loss 7.24575186 epoch total loss 6.94866753\n",
      "Trained batch 88 batch loss 7.36185646 epoch total loss 6.95336294\n",
      "Trained batch 89 batch loss 7.19029617 epoch total loss 6.95602512\n",
      "Trained batch 90 batch loss 7.3472538 epoch total loss 6.96037197\n",
      "Trained batch 91 batch loss 7.26588154 epoch total loss 6.9637289\n",
      "Trained batch 92 batch loss 6.80851364 epoch total loss 6.96204233\n",
      "Trained batch 93 batch loss 6.80396414 epoch total loss 6.96034241\n",
      "Trained batch 94 batch loss 7.02766514 epoch total loss 6.96105814\n",
      "Trained batch 95 batch loss 7.00656223 epoch total loss 6.96153736\n",
      "Trained batch 96 batch loss 6.7234931 epoch total loss 6.95905828\n",
      "Trained batch 97 batch loss 6.9890213 epoch total loss 6.9593668\n",
      "Trained batch 98 batch loss 6.5816 epoch total loss 6.95551205\n",
      "Trained batch 99 batch loss 7.06629467 epoch total loss 6.95663118\n",
      "Trained batch 100 batch loss 6.64668369 epoch total loss 6.95353127\n",
      "Trained batch 101 batch loss 6.80071068 epoch total loss 6.95201874\n",
      "Trained batch 102 batch loss 6.92916203 epoch total loss 6.95179415\n",
      "Trained batch 103 batch loss 6.60412693 epoch total loss 6.94841862\n",
      "Trained batch 104 batch loss 6.86764622 epoch total loss 6.94764233\n",
      "Trained batch 105 batch loss 6.71546459 epoch total loss 6.94543123\n",
      "Trained batch 106 batch loss 6.6087122 epoch total loss 6.94225454\n",
      "Trained batch 107 batch loss 6.68118 epoch total loss 6.93981409\n",
      "Trained batch 108 batch loss 6.49598694 epoch total loss 6.93570471\n",
      "Trained batch 109 batch loss 7.21930838 epoch total loss 6.93830633\n",
      "Trained batch 110 batch loss 6.72729349 epoch total loss 6.93638802\n",
      "Trained batch 111 batch loss 6.6580267 epoch total loss 6.93388033\n",
      "Trained batch 112 batch loss 6.82220745 epoch total loss 6.93288326\n",
      "Trained batch 113 batch loss 7.05599546 epoch total loss 6.93397236\n",
      "Trained batch 114 batch loss 6.9826932 epoch total loss 6.9343996\n",
      "Trained batch 115 batch loss 7.06427193 epoch total loss 6.93552876\n",
      "Trained batch 116 batch loss 6.8921566 epoch total loss 6.93515491\n",
      "Trained batch 117 batch loss 7.25927544 epoch total loss 6.93792534\n",
      "Trained batch 118 batch loss 7.07878208 epoch total loss 6.93911886\n",
      "Trained batch 119 batch loss 7.23973 epoch total loss 6.94164515\n",
      "Trained batch 120 batch loss 6.88994026 epoch total loss 6.94121456\n",
      "Trained batch 121 batch loss 6.58885336 epoch total loss 6.93830252\n",
      "Trained batch 122 batch loss 6.93811417 epoch total loss 6.93830109\n",
      "Trained batch 123 batch loss 6.62353277 epoch total loss 6.9357419\n",
      "Trained batch 124 batch loss 6.79323626 epoch total loss 6.93459225\n",
      "Trained batch 125 batch loss 6.76602411 epoch total loss 6.93324423\n",
      "Trained batch 126 batch loss 6.65159893 epoch total loss 6.93100882\n",
      "Trained batch 127 batch loss 7.12577438 epoch total loss 6.9325428\n",
      "Trained batch 128 batch loss 6.73329401 epoch total loss 6.93098593\n",
      "Trained batch 129 batch loss 7.04244328 epoch total loss 6.93185\n",
      "Trained batch 130 batch loss 6.94649315 epoch total loss 6.93196201\n",
      "Trained batch 131 batch loss 7.10599232 epoch total loss 6.93329096\n",
      "Trained batch 132 batch loss 6.95060253 epoch total loss 6.93342209\n",
      "Trained batch 133 batch loss 6.39989948 epoch total loss 6.92941093\n",
      "Trained batch 134 batch loss 6.11627531 epoch total loss 6.9233427\n",
      "Trained batch 135 batch loss 6.87526131 epoch total loss 6.92298651\n",
      "Trained batch 136 batch loss 6.58222055 epoch total loss 6.92048073\n",
      "Trained batch 137 batch loss 6.26468039 epoch total loss 6.91569376\n",
      "Trained batch 138 batch loss 6.8158741 epoch total loss 6.9149704\n",
      "Trained batch 139 batch loss 6.61148119 epoch total loss 6.91278744\n",
      "Trained batch 140 batch loss 7.06405449 epoch total loss 6.91386747\n",
      "Trained batch 141 batch loss 6.93917656 epoch total loss 6.91404676\n",
      "Trained batch 142 batch loss 6.86510944 epoch total loss 6.91370249\n",
      "Trained batch 143 batch loss 7.26987362 epoch total loss 6.91619301\n",
      "Trained batch 144 batch loss 7.2699461 epoch total loss 6.91864967\n",
      "Trained batch 145 batch loss 6.40843105 epoch total loss 6.91513109\n",
      "Trained batch 146 batch loss 5.30315304 epoch total loss 6.9040904\n",
      "Trained batch 147 batch loss 6.00433874 epoch total loss 6.89796972\n",
      "Trained batch 148 batch loss 6.29714632 epoch total loss 6.89391\n",
      "Trained batch 149 batch loss 7.18507624 epoch total loss 6.89586401\n",
      "Trained batch 150 batch loss 7.36772585 epoch total loss 6.8990097\n",
      "Trained batch 151 batch loss 7.30213118 epoch total loss 6.90167904\n",
      "Trained batch 152 batch loss 7.2625 epoch total loss 6.90405273\n",
      "Trained batch 153 batch loss 7.22321081 epoch total loss 6.9061389\n",
      "Trained batch 154 batch loss 7.18562 epoch total loss 6.90795422\n",
      "Trained batch 155 batch loss 7.10376596 epoch total loss 6.90921736\n",
      "Trained batch 156 batch loss 7.15536547 epoch total loss 6.91079569\n",
      "Trained batch 157 batch loss 7.45058298 epoch total loss 6.91423368\n",
      "Trained batch 158 batch loss 7.34050512 epoch total loss 6.91693115\n",
      "Trained batch 159 batch loss 7.08254814 epoch total loss 6.91797256\n",
      "Trained batch 160 batch loss 7.0059557 epoch total loss 6.91852283\n",
      "Trained batch 161 batch loss 7.22938 epoch total loss 6.92045355\n",
      "Trained batch 162 batch loss 7.44664907 epoch total loss 6.92370176\n",
      "Trained batch 163 batch loss 7.18432522 epoch total loss 6.9253006\n",
      "Trained batch 164 batch loss 7.30607939 epoch total loss 6.92762184\n",
      "Trained batch 165 batch loss 7.29768276 epoch total loss 6.92986488\n",
      "Trained batch 166 batch loss 7.17309189 epoch total loss 6.93133\n",
      "Trained batch 167 batch loss 6.96506548 epoch total loss 6.93153238\n",
      "Trained batch 168 batch loss 7.32234144 epoch total loss 6.93385887\n",
      "Trained batch 169 batch loss 7.06298828 epoch total loss 6.93462324\n",
      "Trained batch 170 batch loss 7.0386529 epoch total loss 6.93523502\n",
      "Trained batch 171 batch loss 7.15741491 epoch total loss 6.93653488\n",
      "Trained batch 172 batch loss 6.22640657 epoch total loss 6.93240643\n",
      "Trained batch 173 batch loss 6.48474169 epoch total loss 6.92981863\n",
      "Trained batch 174 batch loss 6.69662571 epoch total loss 6.92847872\n",
      "Trained batch 175 batch loss 7.24339819 epoch total loss 6.9302783\n",
      "Trained batch 176 batch loss 7.2571 epoch total loss 6.93213511\n",
      "Trained batch 177 batch loss 7.33639526 epoch total loss 6.93441916\n",
      "Trained batch 178 batch loss 7.30086184 epoch total loss 6.93647814\n",
      "Trained batch 179 batch loss 7.06490755 epoch total loss 6.93719578\n",
      "Trained batch 180 batch loss 7.23059034 epoch total loss 6.93882561\n",
      "Trained batch 181 batch loss 6.81451082 epoch total loss 6.93813848\n",
      "Trained batch 182 batch loss 6.94617701 epoch total loss 6.93818283\n",
      "Trained batch 183 batch loss 7.24677658 epoch total loss 6.9398694\n",
      "Trained batch 184 batch loss 7.36029911 epoch total loss 6.94215441\n",
      "Trained batch 185 batch loss 7.3390646 epoch total loss 6.9443\n",
      "Trained batch 186 batch loss 7.10533 epoch total loss 6.94516611\n",
      "Trained batch 187 batch loss 7.0966568 epoch total loss 6.94597626\n",
      "Trained batch 188 batch loss 7.2719903 epoch total loss 6.94771051\n",
      "Trained batch 189 batch loss 6.89235 epoch total loss 6.94741726\n",
      "Trained batch 190 batch loss 7.42129278 epoch total loss 6.94991112\n",
      "Trained batch 191 batch loss 7.35628176 epoch total loss 6.95203924\n",
      "Trained batch 192 batch loss 7.06555128 epoch total loss 6.95263052\n",
      "Trained batch 193 batch loss 7.02934599 epoch total loss 6.95302773\n",
      "Trained batch 194 batch loss 7.15584135 epoch total loss 6.95407343\n",
      "Trained batch 195 batch loss 7.02349854 epoch total loss 6.95442963\n",
      "Trained batch 196 batch loss 6.85155201 epoch total loss 6.95390463\n",
      "Trained batch 197 batch loss 7.01765108 epoch total loss 6.9542284\n",
      "Trained batch 198 batch loss 7.18275595 epoch total loss 6.95538282\n",
      "Trained batch 199 batch loss 7.25960779 epoch total loss 6.95691156\n",
      "Trained batch 200 batch loss 7.34142208 epoch total loss 6.95883417\n",
      "Trained batch 201 batch loss 7.43488789 epoch total loss 6.9612031\n",
      "Trained batch 202 batch loss 7.20131683 epoch total loss 6.96239138\n",
      "Trained batch 203 batch loss 6.90567255 epoch total loss 6.96211195\n",
      "Trained batch 204 batch loss 6.73919725 epoch total loss 6.96101952\n",
      "Trained batch 205 batch loss 6.90581036 epoch total loss 6.96075\n",
      "Trained batch 206 batch loss 7.07075167 epoch total loss 6.96128416\n",
      "Trained batch 207 batch loss 7.20296192 epoch total loss 6.96245193\n",
      "Trained batch 208 batch loss 7.35757256 epoch total loss 6.96435118\n",
      "Trained batch 209 batch loss 7.41922808 epoch total loss 6.96652746\n",
      "Trained batch 210 batch loss 7.24500418 epoch total loss 6.96785355\n",
      "Trained batch 211 batch loss 7.37273645 epoch total loss 6.96977234\n",
      "Trained batch 212 batch loss 7.37943411 epoch total loss 6.97170448\n",
      "Trained batch 213 batch loss 7.41531515 epoch total loss 6.97378683\n",
      "Trained batch 214 batch loss 7.51092339 epoch total loss 6.9762969\n",
      "Trained batch 215 batch loss 7.44333172 epoch total loss 6.97846889\n",
      "Trained batch 216 batch loss 7.04344273 epoch total loss 6.97877\n",
      "Trained batch 217 batch loss 7.06416798 epoch total loss 6.97916365\n",
      "Trained batch 218 batch loss 6.72287607 epoch total loss 6.97798824\n",
      "Trained batch 219 batch loss 6.81848431 epoch total loss 6.97725964\n",
      "Trained batch 220 batch loss 6.68349838 epoch total loss 6.97592449\n",
      "Trained batch 221 batch loss 6.88100338 epoch total loss 6.97549486\n",
      "Trained batch 222 batch loss 6.80156279 epoch total loss 6.97471094\n",
      "Trained batch 223 batch loss 6.89386702 epoch total loss 6.97434902\n",
      "Trained batch 224 batch loss 6.63155317 epoch total loss 6.97281885\n",
      "Trained batch 225 batch loss 6.92205429 epoch total loss 6.97259283\n",
      "Trained batch 226 batch loss 6.6004734 epoch total loss 6.97094631\n",
      "Trained batch 227 batch loss 6.9809432 epoch total loss 6.97099\n",
      "Trained batch 228 batch loss 7.12553501 epoch total loss 6.97166777\n",
      "Trained batch 229 batch loss 6.45811033 epoch total loss 6.9694252\n",
      "Trained batch 230 batch loss 7.02398205 epoch total loss 6.96966219\n",
      "Trained batch 231 batch loss 7.10749817 epoch total loss 6.97025919\n",
      "Trained batch 232 batch loss 7.29073048 epoch total loss 6.97164059\n",
      "Trained batch 233 batch loss 7.21968794 epoch total loss 6.97270536\n",
      "Trained batch 234 batch loss 7.19008303 epoch total loss 6.97363424\n",
      "Trained batch 235 batch loss 6.71618176 epoch total loss 6.97253895\n",
      "Trained batch 236 batch loss 6.70161772 epoch total loss 6.9713912\n",
      "Trained batch 237 batch loss 7.11977863 epoch total loss 6.97201729\n",
      "Trained batch 238 batch loss 6.68653774 epoch total loss 6.97081757\n",
      "Trained batch 239 batch loss 6.82842445 epoch total loss 6.97022152\n",
      "Trained batch 240 batch loss 6.65204573 epoch total loss 6.96889591\n",
      "Trained batch 241 batch loss 6.92025232 epoch total loss 6.96869421\n",
      "Trained batch 242 batch loss 6.55672312 epoch total loss 6.9669919\n",
      "Trained batch 243 batch loss 7.1894722 epoch total loss 6.96790743\n",
      "Trained batch 244 batch loss 7.13786888 epoch total loss 6.96860409\n",
      "Trained batch 245 batch loss 7.36803579 epoch total loss 6.97023439\n",
      "Trained batch 246 batch loss 7.17230654 epoch total loss 6.97105598\n",
      "Trained batch 247 batch loss 7.01755619 epoch total loss 6.97124434\n",
      "Trained batch 248 batch loss 7.09793282 epoch total loss 6.97175503\n",
      "Trained batch 249 batch loss 7.17975807 epoch total loss 6.97259045\n",
      "Trained batch 250 batch loss 6.80491304 epoch total loss 6.97192\n",
      "Trained batch 251 batch loss 5.81365442 epoch total loss 6.96730518\n",
      "Trained batch 252 batch loss 6.15945768 epoch total loss 6.96409941\n",
      "Trained batch 253 batch loss 6.7109561 epoch total loss 6.96309853\n",
      "Trained batch 254 batch loss 6.63693237 epoch total loss 6.9618144\n",
      "Trained batch 255 batch loss 6.79382896 epoch total loss 6.96115589\n",
      "Trained batch 256 batch loss 7.17750692 epoch total loss 6.96200085\n",
      "Trained batch 257 batch loss 7.0637641 epoch total loss 6.96239662\n",
      "Trained batch 258 batch loss 6.53778791 epoch total loss 6.96075106\n",
      "Trained batch 259 batch loss 6.52746201 epoch total loss 6.95907831\n",
      "Trained batch 260 batch loss 6.84483862 epoch total loss 6.95863867\n",
      "Trained batch 261 batch loss 7.06715155 epoch total loss 6.95905447\n",
      "Trained batch 262 batch loss 7.223948 epoch total loss 6.96006584\n",
      "Trained batch 263 batch loss 7.21519852 epoch total loss 6.96103573\n",
      "Trained batch 264 batch loss 7.23896503 epoch total loss 6.96208906\n",
      "Trained batch 265 batch loss 7.47064352 epoch total loss 6.96400833\n",
      "Trained batch 266 batch loss 7.40382814 epoch total loss 6.96566153\n",
      "Trained batch 267 batch loss 6.80353 epoch total loss 6.96505451\n",
      "Trained batch 268 batch loss 6.60383081 epoch total loss 6.96370697\n",
      "Trained batch 269 batch loss 6.39421892 epoch total loss 6.96159\n",
      "Trained batch 270 batch loss 5.68888569 epoch total loss 6.9568758\n",
      "Trained batch 271 batch loss 6.70320034 epoch total loss 6.95594\n",
      "Trained batch 272 batch loss 6.39500713 epoch total loss 6.95387745\n",
      "Trained batch 273 batch loss 5.82959557 epoch total loss 6.94975948\n",
      "Trained batch 274 batch loss 5.68555641 epoch total loss 6.94514561\n",
      "Trained batch 275 batch loss 5.79509211 epoch total loss 6.94096327\n",
      "Trained batch 276 batch loss 6.34991837 epoch total loss 6.93882179\n",
      "Trained batch 277 batch loss 6.54850721 epoch total loss 6.93741274\n",
      "Trained batch 278 batch loss 6.85755444 epoch total loss 6.93712568\n",
      "Trained batch 279 batch loss 7.30078459 epoch total loss 6.93842888\n",
      "Trained batch 280 batch loss 7.25273466 epoch total loss 6.93955135\n",
      "Trained batch 281 batch loss 7.24970579 epoch total loss 6.94065523\n",
      "Trained batch 282 batch loss 7.3595767 epoch total loss 6.94214106\n",
      "Trained batch 283 batch loss 7.285954 epoch total loss 6.94335604\n",
      "Trained batch 284 batch loss 6.95847273 epoch total loss 6.94340944\n",
      "Trained batch 285 batch loss 7.23582554 epoch total loss 6.94443512\n",
      "Trained batch 286 batch loss 7.3140378 epoch total loss 6.94572783\n",
      "Trained batch 287 batch loss 7.30194569 epoch total loss 6.94696903\n",
      "Trained batch 288 batch loss 7.10234165 epoch total loss 6.94750834\n",
      "Trained batch 289 batch loss 7.11627817 epoch total loss 6.94809246\n",
      "Trained batch 290 batch loss 7.12618542 epoch total loss 6.9487071\n",
      "Trained batch 291 batch loss 7.30504131 epoch total loss 6.94993162\n",
      "Trained batch 292 batch loss 7.27263784 epoch total loss 6.95103645\n",
      "Trained batch 293 batch loss 6.80980825 epoch total loss 6.95055437\n",
      "Trained batch 294 batch loss 6.68675709 epoch total loss 6.94965744\n",
      "Trained batch 295 batch loss 6.87226868 epoch total loss 6.94939518\n",
      "Trained batch 296 batch loss 7.19184637 epoch total loss 6.95021439\n",
      "Trained batch 297 batch loss 7.27970314 epoch total loss 6.95132399\n",
      "Trained batch 298 batch loss 6.51811123 epoch total loss 6.94987\n",
      "Trained batch 299 batch loss 7.37524319 epoch total loss 6.95129251\n",
      "Trained batch 300 batch loss 7.05603313 epoch total loss 6.95164204\n",
      "Trained batch 301 batch loss 7.32887936 epoch total loss 6.95289564\n",
      "Trained batch 302 batch loss 7.17437553 epoch total loss 6.95362854\n",
      "Trained batch 303 batch loss 7.12696457 epoch total loss 6.95420074\n",
      "Trained batch 304 batch loss 7.10445452 epoch total loss 6.95469522\n",
      "Trained batch 305 batch loss 6.93265915 epoch total loss 6.95462275\n",
      "Trained batch 306 batch loss 6.91829491 epoch total loss 6.95450354\n",
      "Trained batch 307 batch loss 6.98720503 epoch total loss 6.95461035\n",
      "Trained batch 308 batch loss 7.14769936 epoch total loss 6.95523739\n",
      "Trained batch 309 batch loss 7.28687334 epoch total loss 6.95631075\n",
      "Trained batch 310 batch loss 7.22676897 epoch total loss 6.95718336\n",
      "Trained batch 311 batch loss 7.22655916 epoch total loss 6.9580493\n",
      "Trained batch 312 batch loss 7.19175768 epoch total loss 6.95879793\n",
      "Trained batch 313 batch loss 7.09233189 epoch total loss 6.9592247\n",
      "Trained batch 314 batch loss 7.22016335 epoch total loss 6.96005583\n",
      "Trained batch 315 batch loss 7.08275461 epoch total loss 6.9604454\n",
      "Trained batch 316 batch loss 6.99622059 epoch total loss 6.96055889\n",
      "Trained batch 317 batch loss 6.9995203 epoch total loss 6.96068192\n",
      "Trained batch 318 batch loss 7.05025291 epoch total loss 6.96096373\n",
      "Trained batch 319 batch loss 7.24257421 epoch total loss 6.96184683\n",
      "Trained batch 320 batch loss 7.3502574 epoch total loss 6.96306086\n",
      "Trained batch 321 batch loss 7.43246078 epoch total loss 6.96452284\n",
      "Trained batch 322 batch loss 7.47734499 epoch total loss 6.96611547\n",
      "Trained batch 323 batch loss 7.36859322 epoch total loss 6.96736145\n",
      "Trained batch 324 batch loss 7.18895054 epoch total loss 6.96804523\n",
      "Trained batch 325 batch loss 6.95404053 epoch total loss 6.96800232\n",
      "Trained batch 326 batch loss 6.93559456 epoch total loss 6.96790314\n",
      "Trained batch 327 batch loss 6.4431448 epoch total loss 6.9662981\n",
      "Trained batch 328 batch loss 6.21566629 epoch total loss 6.96400928\n",
      "Trained batch 329 batch loss 6.23656034 epoch total loss 6.96179819\n",
      "Trained batch 330 batch loss 6.17763376 epoch total loss 6.95942211\n",
      "Trained batch 331 batch loss 6.63426256 epoch total loss 6.95844\n",
      "Trained batch 332 batch loss 6.4853735 epoch total loss 6.95701504\n",
      "Trained batch 333 batch loss 7.22352934 epoch total loss 6.95781565\n",
      "Trained batch 334 batch loss 7.14895058 epoch total loss 6.95838785\n",
      "Trained batch 335 batch loss 7.18427086 epoch total loss 6.95906258\n",
      "Trained batch 336 batch loss 7.20817709 epoch total loss 6.95980406\n",
      "Trained batch 337 batch loss 7.10785818 epoch total loss 6.9602437\n",
      "Trained batch 338 batch loss 7.50282097 epoch total loss 6.96184921\n",
      "Trained batch 339 batch loss 7.32780075 epoch total loss 6.96292877\n",
      "Trained batch 340 batch loss 7.29833603 epoch total loss 6.96391535\n",
      "Trained batch 341 batch loss 7.14000368 epoch total loss 6.96443129\n",
      "Trained batch 342 batch loss 7.27968311 epoch total loss 6.96535349\n",
      "Trained batch 343 batch loss 7.08680534 epoch total loss 6.96570778\n",
      "Trained batch 344 batch loss 7.1011548 epoch total loss 6.96610117\n",
      "Trained batch 345 batch loss 6.8423214 epoch total loss 6.96574259\n",
      "Trained batch 346 batch loss 6.43910551 epoch total loss 6.96422052\n",
      "Trained batch 347 batch loss 5.81682539 epoch total loss 6.96091413\n",
      "Trained batch 348 batch loss 5.89048433 epoch total loss 6.95783806\n",
      "Trained batch 349 batch loss 5.89644861 epoch total loss 6.95479679\n",
      "Trained batch 350 batch loss 6.63526297 epoch total loss 6.95388412\n",
      "Trained batch 351 batch loss 6.38133478 epoch total loss 6.95225286\n",
      "Trained batch 352 batch loss 7.26413774 epoch total loss 6.95313883\n",
      "Trained batch 353 batch loss 7.08461714 epoch total loss 6.95351171\n",
      "Trained batch 354 batch loss 7.11281443 epoch total loss 6.95396137\n",
      "Trained batch 355 batch loss 7.11708927 epoch total loss 6.95442152\n",
      "Trained batch 356 batch loss 7.17144823 epoch total loss 6.95503092\n",
      "Trained batch 357 batch loss 6.8335743 epoch total loss 6.95469046\n",
      "Trained batch 358 batch loss 6.78933 epoch total loss 6.9542284\n",
      "Trained batch 359 batch loss 6.51768 epoch total loss 6.95301199\n",
      "Trained batch 360 batch loss 6.84819 epoch total loss 6.95272064\n",
      "Trained batch 361 batch loss 6.84024382 epoch total loss 6.95240927\n",
      "Trained batch 362 batch loss 6.73928213 epoch total loss 6.95182085\n",
      "Trained batch 363 batch loss 7.15544796 epoch total loss 6.95238161\n",
      "Trained batch 364 batch loss 7.00500631 epoch total loss 6.95252657\n",
      "Trained batch 365 batch loss 7.0367775 epoch total loss 6.95275784\n",
      "Trained batch 366 batch loss 6.16072845 epoch total loss 6.95059347\n",
      "Trained batch 367 batch loss 6.48310661 epoch total loss 6.94932\n",
      "Trained batch 368 batch loss 6.45907879 epoch total loss 6.94798756\n",
      "Trained batch 369 batch loss 6.19280148 epoch total loss 6.94594097\n",
      "Trained batch 370 batch loss 5.65446615 epoch total loss 6.942451\n",
      "Trained batch 371 batch loss 6.65584326 epoch total loss 6.94167805\n",
      "Trained batch 372 batch loss 6.83800173 epoch total loss 6.9413991\n",
      "Trained batch 373 batch loss 7.17292356 epoch total loss 6.94201946\n",
      "Trained batch 374 batch loss 7.24543476 epoch total loss 6.94283056\n",
      "Trained batch 375 batch loss 7.33983898 epoch total loss 6.94388914\n",
      "Trained batch 376 batch loss 7.33601236 epoch total loss 6.94493198\n",
      "Trained batch 377 batch loss 7.21240854 epoch total loss 6.94564152\n",
      "Trained batch 378 batch loss 7.37909317 epoch total loss 6.94678831\n",
      "Trained batch 379 batch loss 7.45155191 epoch total loss 6.94812059\n",
      "Trained batch 380 batch loss 7.39985228 epoch total loss 6.94930935\n",
      "Trained batch 381 batch loss 7.01495934 epoch total loss 6.94948149\n",
      "Trained batch 382 batch loss 7.22964 epoch total loss 6.95021534\n",
      "Trained batch 383 batch loss 6.79742336 epoch total loss 6.94981623\n",
      "Trained batch 384 batch loss 6.36930752 epoch total loss 6.94830465\n",
      "Trained batch 385 batch loss 6.62568665 epoch total loss 6.94746685\n",
      "Trained batch 386 batch loss 6.9184866 epoch total loss 6.94739151\n",
      "Trained batch 387 batch loss 7.25574398 epoch total loss 6.94818878\n",
      "Trained batch 388 batch loss 6.39956379 epoch total loss 6.94677496\n",
      "Trained batch 389 batch loss 6.85426569 epoch total loss 6.94653702\n",
      "Trained batch 390 batch loss 6.81462765 epoch total loss 6.94619894\n",
      "Trained batch 391 batch loss 7.07423735 epoch total loss 6.94652653\n",
      "Trained batch 392 batch loss 7.08173609 epoch total loss 6.94687128\n",
      "Trained batch 393 batch loss 6.86626911 epoch total loss 6.94666624\n",
      "Trained batch 394 batch loss 6.94076 epoch total loss 6.94665098\n",
      "Trained batch 395 batch loss 6.79871178 epoch total loss 6.94627666\n",
      "Trained batch 396 batch loss 6.86393261 epoch total loss 6.94606876\n",
      "Trained batch 397 batch loss 6.71846676 epoch total loss 6.94549561\n",
      "Trained batch 398 batch loss 6.64162302 epoch total loss 6.94473219\n",
      "Trained batch 399 batch loss 6.77843 epoch total loss 6.94431496\n",
      "Trained batch 400 batch loss 7.10207 epoch total loss 6.9447093\n",
      "Trained batch 401 batch loss 7.18343544 epoch total loss 6.94530439\n",
      "Trained batch 402 batch loss 7.33051109 epoch total loss 6.94626284\n",
      "Trained batch 403 batch loss 7.34991932 epoch total loss 6.94726419\n",
      "Trained batch 404 batch loss 7.39251471 epoch total loss 6.94836664\n",
      "Trained batch 405 batch loss 7.31219673 epoch total loss 6.949265\n",
      "Trained batch 406 batch loss 7.28020096 epoch total loss 6.95008039\n",
      "Trained batch 407 batch loss 7.0629406 epoch total loss 6.95035791\n",
      "Trained batch 408 batch loss 7.16556025 epoch total loss 6.9508853\n",
      "Trained batch 409 batch loss 7.32221842 epoch total loss 6.95179319\n",
      "Trained batch 410 batch loss 7.26499414 epoch total loss 6.95255709\n",
      "Trained batch 411 batch loss 7.35275221 epoch total loss 6.95353079\n",
      "Trained batch 412 batch loss 7.20573044 epoch total loss 6.95414305\n",
      "Trained batch 413 batch loss 7.06828451 epoch total loss 6.95441961\n",
      "Trained batch 414 batch loss 7.16849661 epoch total loss 6.9549365\n",
      "Trained batch 415 batch loss 6.91305 epoch total loss 6.95483589\n",
      "Trained batch 416 batch loss 6.68076468 epoch total loss 6.9541769\n",
      "Trained batch 417 batch loss 7.28293943 epoch total loss 6.95496511\n",
      "Trained batch 418 batch loss 6.82453632 epoch total loss 6.95465279\n",
      "Trained batch 419 batch loss 6.75906944 epoch total loss 6.95418596\n",
      "Trained batch 420 batch loss 6.89000273 epoch total loss 6.9540329\n",
      "Trained batch 421 batch loss 6.44389582 epoch total loss 6.95282125\n",
      "Trained batch 422 batch loss 6.74923325 epoch total loss 6.9523387\n",
      "Trained batch 423 batch loss 7.16278 epoch total loss 6.95283651\n",
      "Trained batch 424 batch loss 7.01066828 epoch total loss 6.95297289\n",
      "Trained batch 425 batch loss 7.37254477 epoch total loss 6.95396042\n",
      "Trained batch 426 batch loss 7.40089798 epoch total loss 6.95500946\n",
      "Trained batch 427 batch loss 7.41339064 epoch total loss 6.95608282\n",
      "Trained batch 428 batch loss 7.27738619 epoch total loss 6.95683336\n",
      "Trained batch 429 batch loss 6.88152933 epoch total loss 6.95665789\n",
      "Trained batch 430 batch loss 6.2689085 epoch total loss 6.9550581\n",
      "Trained batch 431 batch loss 6.14603472 epoch total loss 6.95318127\n",
      "Trained batch 432 batch loss 5.79419613 epoch total loss 6.9504981\n",
      "Trained batch 433 batch loss 5.98266745 epoch total loss 6.94826317\n",
      "Trained batch 434 batch loss 6.16418791 epoch total loss 6.94645691\n",
      "Trained batch 435 batch loss 5.81651735 epoch total loss 6.9438591\n",
      "Trained batch 436 batch loss 6.16663456 epoch total loss 6.94207668\n",
      "Trained batch 437 batch loss 6.89444637 epoch total loss 6.94196749\n",
      "Trained batch 438 batch loss 7.37378693 epoch total loss 6.94295359\n",
      "Trained batch 439 batch loss 7.03088427 epoch total loss 6.94315434\n",
      "Trained batch 440 batch loss 7.0719347 epoch total loss 6.94344711\n",
      "Trained batch 441 batch loss 6.49300671 epoch total loss 6.94242525\n",
      "Trained batch 442 batch loss 6.68182516 epoch total loss 6.94183588\n",
      "Trained batch 443 batch loss 7.02756071 epoch total loss 6.94202948\n",
      "Trained batch 444 batch loss 7.06705332 epoch total loss 6.94231129\n",
      "Trained batch 445 batch loss 7.41012192 epoch total loss 6.94336271\n",
      "Trained batch 446 batch loss 6.77173662 epoch total loss 6.94297791\n",
      "Trained batch 447 batch loss 6.89743042 epoch total loss 6.94287586\n",
      "Trained batch 448 batch loss 6.76242113 epoch total loss 6.94247341\n",
      "Trained batch 449 batch loss 7.00203323 epoch total loss 6.94260597\n",
      "Trained batch 450 batch loss 7.07973909 epoch total loss 6.94291067\n",
      "Trained batch 451 batch loss 7.14693213 epoch total loss 6.94336319\n",
      "Trained batch 452 batch loss 6.95700598 epoch total loss 6.94339323\n",
      "Trained batch 453 batch loss 6.95481253 epoch total loss 6.9434185\n",
      "Trained batch 454 batch loss 6.9088335 epoch total loss 6.94334269\n",
      "Trained batch 455 batch loss 7.03290796 epoch total loss 6.94353962\n",
      "Trained batch 456 batch loss 7.14056587 epoch total loss 6.94397163\n",
      "Trained batch 457 batch loss 6.70457697 epoch total loss 6.94344807\n",
      "Trained batch 458 batch loss 6.51390314 epoch total loss 6.94251\n",
      "Trained batch 459 batch loss 6.44425 epoch total loss 6.94142485\n",
      "Trained batch 460 batch loss 6.1688695 epoch total loss 6.93974543\n",
      "Trained batch 461 batch loss 7.01860952 epoch total loss 6.93991661\n",
      "Trained batch 462 batch loss 7.20922184 epoch total loss 6.94049931\n",
      "Trained batch 463 batch loss 7.37143421 epoch total loss 6.94143\n",
      "Trained batch 464 batch loss 7.26812935 epoch total loss 6.9421339\n",
      "Trained batch 465 batch loss 7.41822386 epoch total loss 6.94315767\n",
      "Trained batch 466 batch loss 7.39685774 epoch total loss 6.94413185\n",
      "Trained batch 467 batch loss 7.34012127 epoch total loss 6.94497967\n",
      "Trained batch 468 batch loss 7.08104515 epoch total loss 6.94527\n",
      "Trained batch 469 batch loss 6.89923286 epoch total loss 6.94517183\n",
      "Trained batch 470 batch loss 7.15131474 epoch total loss 6.94561052\n",
      "Trained batch 471 batch loss 7.01904392 epoch total loss 6.94576645\n",
      "Trained batch 472 batch loss 7.25795031 epoch total loss 6.9464283\n",
      "Trained batch 473 batch loss 7.28039885 epoch total loss 6.94713449\n",
      "Trained batch 474 batch loss 7.14373732 epoch total loss 6.94754934\n",
      "Trained batch 475 batch loss 7.18009663 epoch total loss 6.94803905\n",
      "Trained batch 476 batch loss 7.15093517 epoch total loss 6.94846535\n",
      "Trained batch 477 batch loss 7.2007513 epoch total loss 6.94899416\n",
      "Trained batch 478 batch loss 7.0628953 epoch total loss 6.94923258\n",
      "Trained batch 479 batch loss 6.96751 epoch total loss 6.94927073\n",
      "Trained batch 480 batch loss 6.87913132 epoch total loss 6.94912481\n",
      "Trained batch 481 batch loss 6.61738586 epoch total loss 6.94843483\n",
      "Trained batch 482 batch loss 6.92900324 epoch total loss 6.94839478\n",
      "Trained batch 483 batch loss 7.34695196 epoch total loss 6.9492197\n",
      "Trained batch 484 batch loss 7.19454908 epoch total loss 6.94972658\n",
      "Trained batch 485 batch loss 7.04299259 epoch total loss 6.94991875\n",
      "Trained batch 486 batch loss 7.29094887 epoch total loss 6.95062065\n",
      "Trained batch 487 batch loss 6.63519049 epoch total loss 6.94997311\n",
      "Trained batch 488 batch loss 7.02152634 epoch total loss 6.95012\n",
      "Trained batch 489 batch loss 7.150383 epoch total loss 6.9505291\n",
      "Trained batch 490 batch loss 6.5988555 epoch total loss 6.94981146\n",
      "Trained batch 491 batch loss 6.97685 epoch total loss 6.94986677\n",
      "Trained batch 492 batch loss 7.14569235 epoch total loss 6.95026493\n",
      "Trained batch 493 batch loss 7.40027237 epoch total loss 6.9511776\n",
      "Trained batch 494 batch loss 7.19161844 epoch total loss 6.95166445\n",
      "Trained batch 495 batch loss 7.36445761 epoch total loss 6.95249844\n",
      "Trained batch 496 batch loss 6.79430676 epoch total loss 6.95217943\n",
      "Trained batch 497 batch loss 6.8024354 epoch total loss 6.95187807\n",
      "Trained batch 498 batch loss 6.79203701 epoch total loss 6.95155716\n",
      "Trained batch 499 batch loss 6.77112484 epoch total loss 6.95119572\n",
      "Trained batch 500 batch loss 6.4235487 epoch total loss 6.95014048\n",
      "Trained batch 501 batch loss 6.70849657 epoch total loss 6.94965839\n",
      "Trained batch 502 batch loss 7.03099108 epoch total loss 6.94982052\n",
      "Trained batch 503 batch loss 7.01839304 epoch total loss 6.94995642\n",
      "Trained batch 504 batch loss 7.08694649 epoch total loss 6.95022821\n",
      "Trained batch 505 batch loss 6.74951506 epoch total loss 6.94983101\n",
      "Trained batch 506 batch loss 6.67592859 epoch total loss 6.94929\n",
      "Trained batch 507 batch loss 6.67412615 epoch total loss 6.94874668\n",
      "Trained batch 508 batch loss 6.57291603 epoch total loss 6.94800711\n",
      "Trained batch 509 batch loss 6.99751663 epoch total loss 6.94810438\n",
      "Trained batch 510 batch loss 6.80133772 epoch total loss 6.94781685\n",
      "Trained batch 511 batch loss 6.59571791 epoch total loss 6.94712734\n",
      "Trained batch 512 batch loss 6.5267725 epoch total loss 6.94630671\n",
      "Trained batch 513 batch loss 6.21490288 epoch total loss 6.94488096\n",
      "Trained batch 514 batch loss 6.70802164 epoch total loss 6.94442\n",
      "Trained batch 515 batch loss 6.79470825 epoch total loss 6.94412947\n",
      "Trained batch 516 batch loss 6.86850739 epoch total loss 6.9439826\n",
      "Trained batch 517 batch loss 6.97402859 epoch total loss 6.94404078\n",
      "Trained batch 518 batch loss 7.17752504 epoch total loss 6.94449139\n",
      "Trained batch 519 batch loss 7.11150026 epoch total loss 6.94481325\n",
      "Trained batch 520 batch loss 6.99434233 epoch total loss 6.94490862\n",
      "Trained batch 521 batch loss 7.19838953 epoch total loss 6.94539547\n",
      "Trained batch 522 batch loss 6.43511724 epoch total loss 6.94441795\n",
      "Trained batch 523 batch loss 6.6742 epoch total loss 6.94390154\n",
      "Trained batch 524 batch loss 7.00696135 epoch total loss 6.9440217\n",
      "Trained batch 525 batch loss 6.91270065 epoch total loss 6.9439621\n",
      "Trained batch 526 batch loss 6.59966564 epoch total loss 6.9433074\n",
      "Trained batch 527 batch loss 6.60259771 epoch total loss 6.94266081\n",
      "Trained batch 528 batch loss 6.67200422 epoch total loss 6.94214821\n",
      "Trained batch 529 batch loss 7.13750219 epoch total loss 6.94251776\n",
      "Trained batch 530 batch loss 6.95501375 epoch total loss 6.94254112\n",
      "Trained batch 531 batch loss 7.16269159 epoch total loss 6.94295549\n",
      "Trained batch 532 batch loss 7.12650347 epoch total loss 6.94330072\n",
      "Trained batch 533 batch loss 6.62379837 epoch total loss 6.94270134\n",
      "Trained batch 534 batch loss 6.93550396 epoch total loss 6.94268799\n",
      "Trained batch 535 batch loss 7.22174644 epoch total loss 6.94320917\n",
      "Trained batch 536 batch loss 6.96873379 epoch total loss 6.94325686\n",
      "Trained batch 537 batch loss 7.03269482 epoch total loss 6.94342327\n",
      "Trained batch 538 batch loss 7.29233122 epoch total loss 6.94407177\n",
      "Trained batch 539 batch loss 7.17754126 epoch total loss 6.94450474\n",
      "Trained batch 540 batch loss 6.71304178 epoch total loss 6.94407654\n",
      "Trained batch 541 batch loss 6.48195553 epoch total loss 6.94322205\n",
      "Trained batch 542 batch loss 7.05503511 epoch total loss 6.94342852\n",
      "Trained batch 543 batch loss 7.08347654 epoch total loss 6.94368601\n",
      "Trained batch 544 batch loss 7.39284372 epoch total loss 6.94451189\n",
      "Trained batch 545 batch loss 7.3459034 epoch total loss 6.9452486\n",
      "Trained batch 546 batch loss 7.34502411 epoch total loss 6.94598055\n",
      "Trained batch 547 batch loss 7.08218908 epoch total loss 6.94623\n",
      "Trained batch 548 batch loss 7.17277 epoch total loss 6.94664335\n",
      "Trained batch 549 batch loss 7.19111824 epoch total loss 6.94708872\n",
      "Trained batch 550 batch loss 6.56475258 epoch total loss 6.94639349\n",
      "Trained batch 551 batch loss 6.99446726 epoch total loss 6.94648027\n",
      "Trained batch 552 batch loss 6.32382584 epoch total loss 6.94535208\n",
      "Trained batch 553 batch loss 6.71195507 epoch total loss 6.94493\n",
      "Trained batch 554 batch loss 7.05622435 epoch total loss 6.94513083\n",
      "Trained batch 555 batch loss 7.00923157 epoch total loss 6.9452467\n",
      "Trained batch 556 batch loss 6.79887676 epoch total loss 6.94498301\n",
      "Trained batch 557 batch loss 6.84658432 epoch total loss 6.94480658\n",
      "Trained batch 558 batch loss 6.57246828 epoch total loss 6.94413948\n",
      "Trained batch 559 batch loss 6.32273436 epoch total loss 6.94302797\n",
      "Trained batch 560 batch loss 6.94228125 epoch total loss 6.94302654\n",
      "Trained batch 561 batch loss 7.16063404 epoch total loss 6.94341469\n",
      "Trained batch 562 batch loss 7.01684046 epoch total loss 6.94354534\n",
      "Trained batch 563 batch loss 6.91598034 epoch total loss 6.94349623\n",
      "Trained batch 564 batch loss 7.28578758 epoch total loss 6.94410372\n",
      "Trained batch 565 batch loss 6.97159672 epoch total loss 6.94415236\n",
      "Trained batch 566 batch loss 6.87877464 epoch total loss 6.94403648\n",
      "Trained batch 567 batch loss 6.93096876 epoch total loss 6.9440136\n",
      "Trained batch 568 batch loss 6.45442343 epoch total loss 6.94315147\n",
      "Trained batch 569 batch loss 6.54382515 epoch total loss 6.94244957\n",
      "Trained batch 570 batch loss 6.7571249 epoch total loss 6.94212437\n",
      "Trained batch 571 batch loss 6.24065828 epoch total loss 6.94089603\n",
      "Trained batch 572 batch loss 6.08413076 epoch total loss 6.93939829\n",
      "Trained batch 573 batch loss 5.88797712 epoch total loss 6.93756342\n",
      "Trained batch 574 batch loss 6.18331957 epoch total loss 6.93624973\n",
      "Trained batch 575 batch loss 6.69293451 epoch total loss 6.9358263\n",
      "Trained batch 576 batch loss 6.06929398 epoch total loss 6.93432188\n",
      "Trained batch 577 batch loss 6.76102 epoch total loss 6.93402147\n",
      "Trained batch 578 batch loss 6.71923065 epoch total loss 6.93365\n",
      "Trained batch 579 batch loss 6.76436567 epoch total loss 6.93335772\n",
      "Trained batch 580 batch loss 6.5292573 epoch total loss 6.93266106\n",
      "Trained batch 581 batch loss 6.43301 epoch total loss 6.93180132\n",
      "Trained batch 582 batch loss 6.72119188 epoch total loss 6.9314394\n",
      "Trained batch 583 batch loss 7.2524271 epoch total loss 6.93198967\n",
      "Trained batch 584 batch loss 7.19851828 epoch total loss 6.932446\n",
      "Trained batch 585 batch loss 7.43468952 epoch total loss 6.93330431\n",
      "Trained batch 586 batch loss 6.63670778 epoch total loss 6.93279839\n",
      "Trained batch 587 batch loss 6.88513136 epoch total loss 6.93271685\n",
      "Trained batch 588 batch loss 7.50754642 epoch total loss 6.93369484\n",
      "Trained batch 589 batch loss 7.42135668 epoch total loss 6.93452263\n",
      "Trained batch 590 batch loss 7.40969944 epoch total loss 6.93532801\n",
      "Trained batch 591 batch loss 7.25183535 epoch total loss 6.93586349\n",
      "Trained batch 592 batch loss 7.32246971 epoch total loss 6.93651581\n",
      "Trained batch 593 batch loss 7.15415859 epoch total loss 6.93688345\n",
      "Trained batch 594 batch loss 6.40802383 epoch total loss 6.93599319\n",
      "Trained batch 595 batch loss 7.0793047 epoch total loss 6.93623352\n",
      "Trained batch 596 batch loss 7.20521927 epoch total loss 6.93668461\n",
      "Trained batch 597 batch loss 7.01992798 epoch total loss 6.93682432\n",
      "Trained batch 598 batch loss 6.8002038 epoch total loss 6.93659592\n",
      "Trained batch 599 batch loss 7.06145573 epoch total loss 6.93680477\n",
      "Trained batch 600 batch loss 6.76724291 epoch total loss 6.93652201\n",
      "Trained batch 601 batch loss 7.02773857 epoch total loss 6.93667364\n",
      "Trained batch 602 batch loss 6.85772133 epoch total loss 6.93654299\n",
      "Trained batch 603 batch loss 7.55001354 epoch total loss 6.93756\n",
      "Trained batch 604 batch loss 7.19788265 epoch total loss 6.93799067\n",
      "Trained batch 605 batch loss 7.05455351 epoch total loss 6.93818378\n",
      "Trained batch 606 batch loss 7.37239695 epoch total loss 6.93890047\n",
      "Trained batch 607 batch loss 7.50273228 epoch total loss 6.93983\n",
      "Trained batch 608 batch loss 7.43198538 epoch total loss 6.9406395\n",
      "Trained batch 609 batch loss 7.07189035 epoch total loss 6.94085455\n",
      "Trained batch 610 batch loss 6.1453371 epoch total loss 6.93955088\n",
      "Trained batch 611 batch loss 5.71723413 epoch total loss 6.93755054\n",
      "Trained batch 612 batch loss 6.79439783 epoch total loss 6.93731642\n",
      "Trained batch 613 batch loss 6.95745087 epoch total loss 6.93734932\n",
      "Trained batch 614 batch loss 7.35463238 epoch total loss 6.93802881\n",
      "Trained batch 615 batch loss 6.97534513 epoch total loss 6.93809\n",
      "Trained batch 616 batch loss 6.81791592 epoch total loss 6.93789482\n",
      "Trained batch 617 batch loss 7.15926933 epoch total loss 6.9382534\n",
      "Trained batch 618 batch loss 7.03292751 epoch total loss 6.93840647\n",
      "Trained batch 619 batch loss 6.97311 epoch total loss 6.93846226\n",
      "Trained batch 620 batch loss 6.57966805 epoch total loss 6.93788338\n",
      "Trained batch 621 batch loss 6.74082518 epoch total loss 6.93756628\n",
      "Trained batch 622 batch loss 7.11269951 epoch total loss 6.93784761\n",
      "Trained batch 623 batch loss 6.94803953 epoch total loss 6.9378643\n",
      "Trained batch 624 batch loss 7.04600954 epoch total loss 6.9380374\n",
      "Trained batch 625 batch loss 7.21158838 epoch total loss 6.93847513\n",
      "Trained batch 626 batch loss 7.19267845 epoch total loss 6.9388814\n",
      "Trained batch 627 batch loss 7.24144077 epoch total loss 6.93936348\n",
      "Trained batch 628 batch loss 7.30805635 epoch total loss 6.93995094\n",
      "Trained batch 629 batch loss 7.23963404 epoch total loss 6.9404273\n",
      "Trained batch 630 batch loss 6.9724431 epoch total loss 6.94047832\n",
      "Trained batch 631 batch loss 6.90504408 epoch total loss 6.94042253\n",
      "Trained batch 632 batch loss 7.17038298 epoch total loss 6.94078684\n",
      "Trained batch 633 batch loss 7.13688564 epoch total loss 6.94109631\n",
      "Trained batch 634 batch loss 7.05096531 epoch total loss 6.9412694\n",
      "Trained batch 635 batch loss 7.22566366 epoch total loss 6.94171667\n",
      "Trained batch 636 batch loss 7.33308697 epoch total loss 6.94233227\n",
      "Trained batch 637 batch loss 7.1202364 epoch total loss 6.94261122\n",
      "Trained batch 638 batch loss 7.41401434 epoch total loss 6.94335032\n",
      "Trained batch 639 batch loss 7.36404562 epoch total loss 6.94400883\n",
      "Trained batch 640 batch loss 7.03491211 epoch total loss 6.9441514\n",
      "Trained batch 641 batch loss 6.30124378 epoch total loss 6.94314814\n",
      "Trained batch 642 batch loss 6.30803919 epoch total loss 6.94215918\n",
      "Trained batch 643 batch loss 6.11120701 epoch total loss 6.94086695\n",
      "Trained batch 644 batch loss 6.33568478 epoch total loss 6.9399271\n",
      "Trained batch 645 batch loss 6.9193778 epoch total loss 6.93989515\n",
      "Trained batch 646 batch loss 6.44626808 epoch total loss 6.93913126\n",
      "Trained batch 647 batch loss 6.67561054 epoch total loss 6.93872404\n",
      "Trained batch 648 batch loss 6.44706964 epoch total loss 6.93796587\n",
      "Trained batch 649 batch loss 6.70072269 epoch total loss 6.9376\n",
      "Trained batch 650 batch loss 6.30397606 epoch total loss 6.93662548\n",
      "Trained batch 651 batch loss 5.93039846 epoch total loss 6.93507957\n",
      "Trained batch 652 batch loss 6.23452568 epoch total loss 6.93400478\n",
      "Trained batch 653 batch loss 7.24902391 epoch total loss 6.93448734\n",
      "Trained batch 654 batch loss 7.05613947 epoch total loss 6.93467331\n",
      "Trained batch 655 batch loss 7.2498312 epoch total loss 6.93515491\n",
      "Trained batch 656 batch loss 6.97138786 epoch total loss 6.93520975\n",
      "Trained batch 657 batch loss 7.16327429 epoch total loss 6.93555641\n",
      "Trained batch 658 batch loss 6.99344 epoch total loss 6.93564463\n",
      "Trained batch 659 batch loss 7.27381325 epoch total loss 6.93615818\n",
      "Trained batch 660 batch loss 6.96321678 epoch total loss 6.93619919\n",
      "Trained batch 661 batch loss 7.03207064 epoch total loss 6.93634462\n",
      "Trained batch 662 batch loss 7.19911718 epoch total loss 6.93674183\n",
      "Trained batch 663 batch loss 7.34034348 epoch total loss 6.93735027\n",
      "Trained batch 664 batch loss 6.29272556 epoch total loss 6.93638\n",
      "Trained batch 665 batch loss 6.35471249 epoch total loss 6.93550491\n",
      "Trained batch 666 batch loss 6.38021 epoch total loss 6.9346714\n",
      "Trained batch 667 batch loss 5.94595051 epoch total loss 6.93318892\n",
      "Trained batch 668 batch loss 6.60927391 epoch total loss 6.93270397\n",
      "Trained batch 669 batch loss 6.94576025 epoch total loss 6.93272352\n",
      "Trained batch 670 batch loss 7.08944273 epoch total loss 6.93295765\n",
      "Trained batch 671 batch loss 7.26461935 epoch total loss 6.93345165\n",
      "Trained batch 672 batch loss 7.16194153 epoch total loss 6.93379211\n",
      "Trained batch 673 batch loss 7.14548206 epoch total loss 6.93410683\n",
      "Trained batch 674 batch loss 7.20038128 epoch total loss 6.93450165\n",
      "Trained batch 675 batch loss 7.16683483 epoch total loss 6.93484592\n",
      "Trained batch 676 batch loss 6.8734827 epoch total loss 6.93475533\n",
      "Trained batch 677 batch loss 7.09811258 epoch total loss 6.9349966\n",
      "Trained batch 678 batch loss 6.99851227 epoch total loss 6.93509\n",
      "Trained batch 679 batch loss 7.05978966 epoch total loss 6.93527365\n",
      "Trained batch 680 batch loss 6.20210266 epoch total loss 6.93419552\n",
      "Trained batch 681 batch loss 5.86792755 epoch total loss 6.93263\n",
      "Trained batch 682 batch loss 5.48854971 epoch total loss 6.93051291\n",
      "Trained batch 683 batch loss 6.19719172 epoch total loss 6.92943954\n",
      "Trained batch 684 batch loss 6.96785784 epoch total loss 6.92949533\n",
      "Trained batch 685 batch loss 6.84389257 epoch total loss 6.9293704\n",
      "Trained batch 686 batch loss 6.73550606 epoch total loss 6.92908764\n",
      "Trained batch 687 batch loss 6.96233368 epoch total loss 6.9291358\n",
      "Trained batch 688 batch loss 7.21031427 epoch total loss 6.92954493\n",
      "Trained batch 689 batch loss 6.8774519 epoch total loss 6.92946911\n",
      "Trained batch 690 batch loss 6.20373631 epoch total loss 6.92841721\n",
      "Trained batch 691 batch loss 7.00215292 epoch total loss 6.92852354\n",
      "Trained batch 692 batch loss 6.27224588 epoch total loss 6.92757559\n",
      "Trained batch 693 batch loss 6.1404047 epoch total loss 6.92644024\n",
      "Trained batch 694 batch loss 6.95684814 epoch total loss 6.92648411\n",
      "Trained batch 695 batch loss 6.83148241 epoch total loss 6.92634773\n",
      "Trained batch 696 batch loss 7.04509 epoch total loss 6.92651796\n",
      "Trained batch 697 batch loss 7.35978556 epoch total loss 6.92713976\n",
      "Trained batch 698 batch loss 7.34072685 epoch total loss 6.92773247\n",
      "Trained batch 699 batch loss 7.42323732 epoch total loss 6.92844152\n",
      "Trained batch 700 batch loss 7.32596207 epoch total loss 6.92900944\n",
      "Trained batch 701 batch loss 6.94329882 epoch total loss 6.92903\n",
      "Trained batch 702 batch loss 7.33727503 epoch total loss 6.92961168\n",
      "Trained batch 703 batch loss 6.3859086 epoch total loss 6.92883825\n",
      "Trained batch 704 batch loss 6.53693 epoch total loss 6.92828178\n",
      "Trained batch 705 batch loss 7.03945589 epoch total loss 6.92843962\n",
      "Trained batch 706 batch loss 7.03957176 epoch total loss 6.92859697\n",
      "Trained batch 707 batch loss 6.81459665 epoch total loss 6.92843533\n",
      "Trained batch 708 batch loss 6.51375198 epoch total loss 6.92785\n",
      "Trained batch 709 batch loss 7.0425477 epoch total loss 6.92801142\n",
      "Trained batch 710 batch loss 6.95604944 epoch total loss 6.92805052\n",
      "Trained batch 711 batch loss 6.82134724 epoch total loss 6.92790079\n",
      "Trained batch 712 batch loss 6.7059083 epoch total loss 6.92758894\n",
      "Trained batch 713 batch loss 6.66879749 epoch total loss 6.92722607\n",
      "Trained batch 714 batch loss 6.98486471 epoch total loss 6.92730713\n",
      "Trained batch 715 batch loss 7.00675058 epoch total loss 6.92741823\n",
      "Trained batch 716 batch loss 6.99881506 epoch total loss 6.92751837\n",
      "Trained batch 717 batch loss 7.00826597 epoch total loss 6.9276309\n",
      "Trained batch 718 batch loss 6.73755074 epoch total loss 6.92736626\n",
      "Trained batch 719 batch loss 7.20023823 epoch total loss 6.92774582\n",
      "Trained batch 720 batch loss 7.04207516 epoch total loss 6.92790461\n",
      "Trained batch 721 batch loss 7.18028116 epoch total loss 6.9282546\n",
      "Trained batch 722 batch loss 7.18905497 epoch total loss 6.92861557\n",
      "Trained batch 723 batch loss 7.15317965 epoch total loss 6.92892647\n",
      "Trained batch 724 batch loss 7.16082096 epoch total loss 6.92924643\n",
      "Trained batch 725 batch loss 6.43617535 epoch total loss 6.92856598\n",
      "Trained batch 726 batch loss 6.83377075 epoch total loss 6.9284358\n",
      "Trained batch 727 batch loss 7.07824612 epoch total loss 6.9286418\n",
      "Trained batch 728 batch loss 6.62161732 epoch total loss 6.92822\n",
      "Trained batch 729 batch loss 6.75748444 epoch total loss 6.92798567\n",
      "Trained batch 730 batch loss 7.42894745 epoch total loss 6.92867136\n",
      "Trained batch 731 batch loss 6.92778492 epoch total loss 6.92867041\n",
      "Trained batch 732 batch loss 7.14317274 epoch total loss 6.92896318\n",
      "Trained batch 733 batch loss 6.28747654 epoch total loss 6.92808819\n",
      "Trained batch 734 batch loss 6.75487709 epoch total loss 6.92785215\n",
      "Trained batch 735 batch loss 7.23919582 epoch total loss 6.92827559\n",
      "Trained batch 736 batch loss 7.17659903 epoch total loss 6.92861319\n",
      "Trained batch 737 batch loss 7.0638938 epoch total loss 6.92879725\n",
      "Trained batch 738 batch loss 7.11092472 epoch total loss 6.92904377\n",
      "Trained batch 739 batch loss 7.36749506 epoch total loss 6.92963743\n",
      "Trained batch 740 batch loss 7.27617 epoch total loss 6.93010569\n",
      "Trained batch 741 batch loss 7.38102722 epoch total loss 6.93071413\n",
      "Trained batch 742 batch loss 7.21422148 epoch total loss 6.93109655\n",
      "Trained batch 743 batch loss 7.3477354 epoch total loss 6.93165684\n",
      "Trained batch 744 batch loss 7.31546068 epoch total loss 6.93217278\n",
      "Trained batch 745 batch loss 7.35551 epoch total loss 6.93274117\n",
      "Trained batch 746 batch loss 7.31556702 epoch total loss 6.93325424\n",
      "Trained batch 747 batch loss 7.33901501 epoch total loss 6.93379688\n",
      "Trained batch 748 batch loss 7.34941673 epoch total loss 6.93435287\n",
      "Trained batch 749 batch loss 7.37150908 epoch total loss 6.93493652\n",
      "Trained batch 750 batch loss 7.20862818 epoch total loss 6.9353013\n",
      "Trained batch 751 batch loss 7.30799 epoch total loss 6.93579769\n",
      "Trained batch 752 batch loss 7.24468279 epoch total loss 6.93620872\n",
      "Trained batch 753 batch loss 6.91550398 epoch total loss 6.93618107\n",
      "Trained batch 754 batch loss 6.90263128 epoch total loss 6.93613672\n",
      "Trained batch 755 batch loss 7.16338825 epoch total loss 6.93643808\n",
      "Trained batch 756 batch loss 7.35507 epoch total loss 6.93699169\n",
      "Trained batch 757 batch loss 7.15599823 epoch total loss 6.93728065\n",
      "Trained batch 758 batch loss 6.6435194 epoch total loss 6.93689299\n",
      "Trained batch 759 batch loss 6.76176596 epoch total loss 6.9366622\n",
      "Trained batch 760 batch loss 7.23200655 epoch total loss 6.93705082\n",
      "Trained batch 761 batch loss 6.64725971 epoch total loss 6.9366703\n",
      "Trained batch 762 batch loss 6.44044542 epoch total loss 6.93601894\n",
      "Trained batch 763 batch loss 6.84797239 epoch total loss 6.93590403\n",
      "Trained batch 764 batch loss 6.9580946 epoch total loss 6.93593311\n",
      "Trained batch 765 batch loss 6.8906889 epoch total loss 6.93587351\n",
      "Trained batch 766 batch loss 7.14231539 epoch total loss 6.93614292\n",
      "Trained batch 767 batch loss 7.07392836 epoch total loss 6.93632221\n",
      "Trained batch 768 batch loss 6.94962 epoch total loss 6.93634\n",
      "Trained batch 769 batch loss 7.02223301 epoch total loss 6.93645191\n",
      "Trained batch 770 batch loss 6.75856876 epoch total loss 6.93622112\n",
      "Trained batch 771 batch loss 6.85503244 epoch total loss 6.93611574\n",
      "Trained batch 772 batch loss 6.84181404 epoch total loss 6.93599319\n",
      "Trained batch 773 batch loss 6.28933144 epoch total loss 6.9351573\n",
      "Trained batch 774 batch loss 6.96849394 epoch total loss 6.93519974\n",
      "Trained batch 775 batch loss 7.03593779 epoch total loss 6.93533\n",
      "Trained batch 776 batch loss 6.81845331 epoch total loss 6.93517923\n",
      "Trained batch 777 batch loss 6.80296946 epoch total loss 6.935009\n",
      "Trained batch 778 batch loss 7.16137266 epoch total loss 6.9352994\n",
      "Trained batch 779 batch loss 6.51524734 epoch total loss 6.93476\n",
      "Trained batch 780 batch loss 6.71249199 epoch total loss 6.93447495\n",
      "Trained batch 781 batch loss 6.72611904 epoch total loss 6.93420839\n",
      "Trained batch 782 batch loss 6.69551229 epoch total loss 6.93390274\n",
      "Trained batch 783 batch loss 6.85585213 epoch total loss 6.93380308\n",
      "Trained batch 784 batch loss 6.86906099 epoch total loss 6.93372059\n",
      "Trained batch 785 batch loss 7.24752474 epoch total loss 6.93412066\n",
      "Trained batch 786 batch loss 7.3273735 epoch total loss 6.93462086\n",
      "Trained batch 787 batch loss 7.34060144 epoch total loss 6.9351368\n",
      "Trained batch 788 batch loss 6.81635809 epoch total loss 6.93498611\n",
      "Trained batch 789 batch loss 6.61202049 epoch total loss 6.93457651\n",
      "Trained batch 790 batch loss 6.47042418 epoch total loss 6.93398857\n",
      "Trained batch 791 batch loss 6.86968136 epoch total loss 6.93390751\n",
      "Trained batch 792 batch loss 6.90681934 epoch total loss 6.93387318\n",
      "Trained batch 793 batch loss 7.32305241 epoch total loss 6.93436384\n",
      "Trained batch 794 batch loss 7.29799318 epoch total loss 6.93482161\n",
      "Trained batch 795 batch loss 7.27069712 epoch total loss 6.93524408\n",
      "Trained batch 796 batch loss 7.26921558 epoch total loss 6.93566322\n",
      "Trained batch 797 batch loss 7.19068909 epoch total loss 6.93598366\n",
      "Trained batch 798 batch loss 7.18606567 epoch total loss 6.93629694\n",
      "Trained batch 799 batch loss 7.33749151 epoch total loss 6.93679905\n",
      "Trained batch 800 batch loss 7.27487 epoch total loss 6.93722153\n",
      "Trained batch 801 batch loss 7.29467535 epoch total loss 6.93766785\n",
      "Trained batch 802 batch loss 7.07300329 epoch total loss 6.93783665\n",
      "Trained batch 803 batch loss 6.17491627 epoch total loss 6.93688631\n",
      "Trained batch 804 batch loss 5.7210083 epoch total loss 6.93537426\n",
      "Trained batch 805 batch loss 6.07494593 epoch total loss 6.93430519\n",
      "Trained batch 806 batch loss 6.96036625 epoch total loss 6.93433762\n",
      "Trained batch 807 batch loss 7.0045557 epoch total loss 6.9344244\n",
      "Trained batch 808 batch loss 7.06985521 epoch total loss 6.93459225\n",
      "Trained batch 809 batch loss 6.5805006 epoch total loss 6.93415451\n",
      "Trained batch 810 batch loss 6.5294 epoch total loss 6.93365479\n",
      "Trained batch 811 batch loss 6.94428396 epoch total loss 6.93366766\n",
      "Trained batch 812 batch loss 7.03382254 epoch total loss 6.93379116\n",
      "Trained batch 813 batch loss 7.31184483 epoch total loss 6.93425608\n",
      "Trained batch 814 batch loss 7.30555582 epoch total loss 6.93471241\n",
      "Trained batch 815 batch loss 7.2393856 epoch total loss 6.93508625\n",
      "Trained batch 816 batch loss 7.34867764 epoch total loss 6.93559313\n",
      "Trained batch 817 batch loss 7.04669762 epoch total loss 6.93572903\n",
      "Trained batch 818 batch loss 7.24209404 epoch total loss 6.93610382\n",
      "Trained batch 819 batch loss 7.04210329 epoch total loss 6.93623304\n",
      "Trained batch 820 batch loss 6.90120554 epoch total loss 6.93619061\n",
      "Trained batch 821 batch loss 7.39571524 epoch total loss 6.93675\n",
      "Trained batch 822 batch loss 7.16396523 epoch total loss 6.9370265\n",
      "Trained batch 823 batch loss 6.50601864 epoch total loss 6.93650246\n",
      "Trained batch 824 batch loss 6.19559 epoch total loss 6.93560362\n",
      "Trained batch 825 batch loss 6.57751083 epoch total loss 6.9351697\n",
      "Trained batch 826 batch loss 6.97864437 epoch total loss 6.93522215\n",
      "Trained batch 827 batch loss 7.02054882 epoch total loss 6.93532562\n",
      "Trained batch 828 batch loss 6.92792845 epoch total loss 6.93531609\n",
      "Trained batch 829 batch loss 6.81199217 epoch total loss 6.93516779\n",
      "Trained batch 830 batch loss 6.65777493 epoch total loss 6.93483305\n",
      "Trained batch 831 batch loss 6.63359785 epoch total loss 6.93447113\n",
      "Trained batch 832 batch loss 6.40839243 epoch total loss 6.93383837\n",
      "Trained batch 833 batch loss 6.87852478 epoch total loss 6.93377209\n",
      "Trained batch 834 batch loss 6.27011395 epoch total loss 6.93297625\n",
      "Trained batch 835 batch loss 6.79836082 epoch total loss 6.93281507\n",
      "Trained batch 836 batch loss 7.10037947 epoch total loss 6.93301535\n",
      "Trained batch 837 batch loss 7.08784485 epoch total loss 6.93320036\n",
      "Trained batch 838 batch loss 7.16015387 epoch total loss 6.9334712\n",
      "Trained batch 839 batch loss 7.15137243 epoch total loss 6.93373108\n",
      "Trained batch 840 batch loss 6.4183383 epoch total loss 6.93311787\n",
      "Trained batch 841 batch loss 7.05658817 epoch total loss 6.93326473\n",
      "Trained batch 842 batch loss 7.23760939 epoch total loss 6.93362617\n",
      "Trained batch 843 batch loss 6.68450308 epoch total loss 6.93333101\n",
      "Trained batch 844 batch loss 6.84559679 epoch total loss 6.93322706\n",
      "Trained batch 845 batch loss 6.67007494 epoch total loss 6.93291521\n",
      "Trained batch 846 batch loss 6.80823851 epoch total loss 6.93276787\n",
      "Trained batch 847 batch loss 7.12259197 epoch total loss 6.93299198\n",
      "Trained batch 848 batch loss 7.1667943 epoch total loss 6.93326807\n",
      "Trained batch 849 batch loss 6.94536686 epoch total loss 6.9332819\n",
      "Trained batch 850 batch loss 7.08376455 epoch total loss 6.93345928\n",
      "Trained batch 851 batch loss 7.14179564 epoch total loss 6.9337039\n",
      "Trained batch 852 batch loss 7.15163898 epoch total loss 6.93396\n",
      "Trained batch 853 batch loss 6.97515154 epoch total loss 6.93400812\n",
      "Trained batch 854 batch loss 6.105546 epoch total loss 6.93303823\n",
      "Trained batch 855 batch loss 6.18845081 epoch total loss 6.93216705\n",
      "Trained batch 856 batch loss 7.0300827 epoch total loss 6.93228197\n",
      "Trained batch 857 batch loss 6.85512066 epoch total loss 6.93219137\n",
      "Trained batch 858 batch loss 6.85978889 epoch total loss 6.93210745\n",
      "Trained batch 859 batch loss 7.29601288 epoch total loss 6.93253088\n",
      "Trained batch 860 batch loss 7.20672321 epoch total loss 6.93284941\n",
      "Trained batch 861 batch loss 7.22290182 epoch total loss 6.93318653\n",
      "Trained batch 862 batch loss 7.17499256 epoch total loss 6.93346691\n",
      "Trained batch 863 batch loss 6.89348412 epoch total loss 6.93342066\n",
      "Trained batch 864 batch loss 6.79245949 epoch total loss 6.93325758\n",
      "Trained batch 865 batch loss 6.79276848 epoch total loss 6.93309546\n",
      "Trained batch 866 batch loss 7.04677153 epoch total loss 6.93322659\n",
      "Trained batch 867 batch loss 7.21081829 epoch total loss 6.93354702\n",
      "Trained batch 868 batch loss 7.20497561 epoch total loss 6.93386\n",
      "Trained batch 869 batch loss 6.96864223 epoch total loss 6.9339\n",
      "Trained batch 870 batch loss 7.04484701 epoch total loss 6.93402767\n",
      "Trained batch 871 batch loss 7.19038773 epoch total loss 6.93432188\n",
      "Trained batch 872 batch loss 7.02131462 epoch total loss 6.93442202\n",
      "Trained batch 873 batch loss 6.87662315 epoch total loss 6.93435574\n",
      "Trained batch 874 batch loss 7.05104971 epoch total loss 6.93448925\n",
      "Trained batch 875 batch loss 6.86953115 epoch total loss 6.93441534\n",
      "Trained batch 876 batch loss 6.44610929 epoch total loss 6.93385792\n",
      "Trained batch 877 batch loss 6.99622059 epoch total loss 6.93392897\n",
      "Trained batch 878 batch loss 7.06236506 epoch total loss 6.93407536\n",
      "Trained batch 879 batch loss 7.13823 epoch total loss 6.93430758\n",
      "Trained batch 880 batch loss 7.12997484 epoch total loss 6.93453\n",
      "Trained batch 881 batch loss 7.39947224 epoch total loss 6.93505764\n",
      "Trained batch 882 batch loss 7.07731247 epoch total loss 6.93521881\n",
      "Trained batch 883 batch loss 6.94129086 epoch total loss 6.93522549\n",
      "Trained batch 884 batch loss 6.93828 epoch total loss 6.9352293\n",
      "Trained batch 885 batch loss 7.05657387 epoch total loss 6.93536663\n",
      "Trained batch 886 batch loss 6.96443844 epoch total loss 6.93539906\n",
      "Trained batch 887 batch loss 6.88833952 epoch total loss 6.93534613\n",
      "Trained batch 888 batch loss 6.76716375 epoch total loss 6.93515635\n",
      "Trained batch 889 batch loss 6.75014 epoch total loss 6.93494797\n",
      "Trained batch 890 batch loss 6.70857191 epoch total loss 6.93469381\n",
      "Trained batch 891 batch loss 7.09532833 epoch total loss 6.93487406\n",
      "Trained batch 892 batch loss 7.10617733 epoch total loss 6.93506575\n",
      "Trained batch 893 batch loss 7.2378912 epoch total loss 6.93540478\n",
      "Trained batch 894 batch loss 7.3623023 epoch total loss 6.93588209\n",
      "Trained batch 895 batch loss 7.41618156 epoch total loss 6.93641853\n",
      "Trained batch 896 batch loss 7.11819696 epoch total loss 6.93662167\n",
      "Trained batch 897 batch loss 7.10760593 epoch total loss 6.93681192\n",
      "Trained batch 898 batch loss 7.05839157 epoch total loss 6.93694735\n",
      "Trained batch 899 batch loss 7.34880924 epoch total loss 6.93740559\n",
      "Trained batch 900 batch loss 7.23677492 epoch total loss 6.93773794\n",
      "Trained batch 901 batch loss 7.1106 epoch total loss 6.93793\n",
      "Trained batch 902 batch loss 6.91151905 epoch total loss 6.93790102\n",
      "Trained batch 903 batch loss 6.82393599 epoch total loss 6.93777466\n",
      "Trained batch 904 batch loss 6.40618229 epoch total loss 6.93718672\n",
      "Trained batch 905 batch loss 6.72576 epoch total loss 6.93695307\n",
      "Trained batch 906 batch loss 7.34214163 epoch total loss 6.93740034\n",
      "Trained batch 907 batch loss 7.19018888 epoch total loss 6.93767929\n",
      "Trained batch 908 batch loss 6.90838385 epoch total loss 6.93764687\n",
      "Trained batch 909 batch loss 6.77632189 epoch total loss 6.93746948\n",
      "Trained batch 910 batch loss 6.58417511 epoch total loss 6.93708086\n",
      "Trained batch 911 batch loss 7.11055756 epoch total loss 6.93727112\n",
      "Trained batch 912 batch loss 7.23351812 epoch total loss 6.93759584\n",
      "Trained batch 913 batch loss 7.0433073 epoch total loss 6.93771172\n",
      "Trained batch 914 batch loss 6.7927103 epoch total loss 6.93755293\n",
      "Trained batch 915 batch loss 6.22188473 epoch total loss 6.93677044\n",
      "Trained batch 916 batch loss 6.20570898 epoch total loss 6.93597221\n",
      "Trained batch 917 batch loss 6.41394138 epoch total loss 6.93540287\n",
      "Trained batch 918 batch loss 6.36837769 epoch total loss 6.93478537\n",
      "Trained batch 919 batch loss 7.06346655 epoch total loss 6.93492508\n",
      "Trained batch 920 batch loss 7.17905807 epoch total loss 6.93519068\n",
      "Trained batch 921 batch loss 7.10475492 epoch total loss 6.93537521\n",
      "Trained batch 922 batch loss 7.04960251 epoch total loss 6.93549919\n",
      "Trained batch 923 batch loss 7.33130264 epoch total loss 6.93592834\n",
      "Trained batch 924 batch loss 7.07094765 epoch total loss 6.93607426\n",
      "Trained batch 925 batch loss 6.74414062 epoch total loss 6.93586683\n",
      "Trained batch 926 batch loss 6.98681402 epoch total loss 6.93592167\n",
      "Trained batch 927 batch loss 7.1120491 epoch total loss 6.93611145\n",
      "Trained batch 928 batch loss 6.63046932 epoch total loss 6.93578196\n",
      "Trained batch 929 batch loss 7.02561235 epoch total loss 6.93587875\n",
      "Trained batch 930 batch loss 6.86955404 epoch total loss 6.93580723\n",
      "Trained batch 931 batch loss 6.65454626 epoch total loss 6.93550539\n",
      "Trained batch 932 batch loss 6.38572884 epoch total loss 6.93491554\n",
      "Trained batch 933 batch loss 6.56002188 epoch total loss 6.93451357\n",
      "Trained batch 934 batch loss 6.62373829 epoch total loss 6.93418074\n",
      "Trained batch 935 batch loss 6.66931391 epoch total loss 6.9338975\n",
      "Trained batch 936 batch loss 6.47227573 epoch total loss 6.93340445\n",
      "Trained batch 937 batch loss 5.99698925 epoch total loss 6.93240499\n",
      "Trained batch 938 batch loss 6.28386211 epoch total loss 6.93171358\n",
      "Trained batch 939 batch loss 6.53559875 epoch total loss 6.93129158\n",
      "Trained batch 940 batch loss 6.8841548 epoch total loss 6.93124151\n",
      "Trained batch 941 batch loss 6.80280828 epoch total loss 6.93110514\n",
      "Trained batch 942 batch loss 7.23081779 epoch total loss 6.93142366\n",
      "Trained batch 943 batch loss 7.41021 epoch total loss 6.93193102\n",
      "Trained batch 944 batch loss 7.19997168 epoch total loss 6.93221521\n",
      "Trained batch 945 batch loss 7.20315886 epoch total loss 6.93250179\n",
      "Trained batch 946 batch loss 7.05948973 epoch total loss 6.93263626\n",
      "Trained batch 947 batch loss 6.53767776 epoch total loss 6.93221903\n",
      "Trained batch 948 batch loss 6.28260136 epoch total loss 6.93153381\n",
      "Trained batch 949 batch loss 6.70932817 epoch total loss 6.9313\n",
      "Trained batch 950 batch loss 7.04898214 epoch total loss 6.93142366\n",
      "Trained batch 951 batch loss 7.07045221 epoch total loss 6.93156958\n",
      "Trained batch 952 batch loss 7.11562 epoch total loss 6.93176317\n",
      "Trained batch 953 batch loss 7.19986534 epoch total loss 6.93204451\n",
      "Trained batch 954 batch loss 7.28039598 epoch total loss 6.93240929\n",
      "Trained batch 955 batch loss 7.55625963 epoch total loss 6.93306255\n",
      "Trained batch 956 batch loss 7.39507771 epoch total loss 6.93354559\n",
      "Trained batch 957 batch loss 7.13032246 epoch total loss 6.93375158\n",
      "Trained batch 958 batch loss 6.78074789 epoch total loss 6.93359184\n",
      "Trained batch 959 batch loss 6.74109316 epoch total loss 6.93339109\n",
      "Trained batch 960 batch loss 7.00414467 epoch total loss 6.93346453\n",
      "Trained batch 961 batch loss 6.47727251 epoch total loss 6.9329896\n",
      "Trained batch 962 batch loss 6.83688 epoch total loss 6.93289\n",
      "Trained batch 963 batch loss 6.47219563 epoch total loss 6.93241119\n",
      "Trained batch 964 batch loss 7.27270603 epoch total loss 6.93276453\n",
      "Trained batch 965 batch loss 7.25841045 epoch total loss 6.93310213\n",
      "Trained batch 966 batch loss 7.15507936 epoch total loss 6.93333197\n",
      "Trained batch 967 batch loss 7.13116884 epoch total loss 6.93353653\n",
      "Trained batch 968 batch loss 7.17334127 epoch total loss 6.93378448\n",
      "Trained batch 969 batch loss 7.17156315 epoch total loss 6.93402958\n",
      "Trained batch 970 batch loss 7.28867388 epoch total loss 6.93439531\n",
      "Trained batch 971 batch loss 7.22272968 epoch total loss 6.93469191\n",
      "Trained batch 972 batch loss 6.98552322 epoch total loss 6.93474436\n",
      "Trained batch 973 batch loss 6.86021423 epoch total loss 6.93466759\n",
      "Trained batch 974 batch loss 6.78903103 epoch total loss 6.93451834\n",
      "Trained batch 975 batch loss 6.91291618 epoch total loss 6.9344964\n",
      "Trained batch 976 batch loss 6.87435532 epoch total loss 6.93443489\n",
      "Trained batch 977 batch loss 7.1193347 epoch total loss 6.93462372\n",
      "Trained batch 978 batch loss 7.11240673 epoch total loss 6.93480539\n",
      "Trained batch 979 batch loss 7.14383221 epoch total loss 6.93501902\n",
      "Trained batch 980 batch loss 7.16098118 epoch total loss 6.93525\n",
      "Trained batch 981 batch loss 7.03734303 epoch total loss 6.93535376\n",
      "Trained batch 982 batch loss 7.30937 epoch total loss 6.93573475\n",
      "Trained batch 983 batch loss 7.51755 epoch total loss 6.9363265\n",
      "Trained batch 984 batch loss 7.02515459 epoch total loss 6.9364171\n",
      "Trained batch 985 batch loss 6.97281265 epoch total loss 6.93645382\n",
      "Trained batch 986 batch loss 7.24863529 epoch total loss 6.93677044\n",
      "Trained batch 987 batch loss 7.29444551 epoch total loss 6.93713284\n",
      "Trained batch 988 batch loss 7.07155466 epoch total loss 6.93726921\n",
      "Trained batch 989 batch loss 7.23741245 epoch total loss 6.93757248\n",
      "Trained batch 990 batch loss 7.02905798 epoch total loss 6.93766546\n",
      "Trained batch 991 batch loss 6.97649384 epoch total loss 6.93770456\n",
      "Trained batch 992 batch loss 7.19037104 epoch total loss 6.93795919\n",
      "Trained batch 993 batch loss 6.95574856 epoch total loss 6.93797684\n",
      "Trained batch 994 batch loss 6.82662916 epoch total loss 6.93786478\n",
      "Trained batch 995 batch loss 6.9121666 epoch total loss 6.93783903\n",
      "Trained batch 996 batch loss 7.10194 epoch total loss 6.93800402\n",
      "Trained batch 997 batch loss 7.35509443 epoch total loss 6.9384222\n",
      "Trained batch 998 batch loss 7.22450495 epoch total loss 6.93870878\n",
      "Trained batch 999 batch loss 7.35636044 epoch total loss 6.93912697\n",
      "Trained batch 1000 batch loss 7.43041563 epoch total loss 6.93961811\n",
      "Trained batch 1001 batch loss 7.32626152 epoch total loss 6.94000435\n",
      "Trained batch 1002 batch loss 7.20463037 epoch total loss 6.94026852\n",
      "Trained batch 1003 batch loss 6.99423933 epoch total loss 6.94032192\n",
      "Trained batch 1004 batch loss 7.08279705 epoch total loss 6.94046402\n",
      "Trained batch 1005 batch loss 7.19249964 epoch total loss 6.94071484\n",
      "Trained batch 1006 batch loss 7.37347174 epoch total loss 6.94114494\n",
      "Trained batch 1007 batch loss 7.26294422 epoch total loss 6.9414649\n",
      "Trained batch 1008 batch loss 7.36774111 epoch total loss 6.94188786\n",
      "Trained batch 1009 batch loss 7.28661776 epoch total loss 6.94222927\n",
      "Trained batch 1010 batch loss 7.27065563 epoch total loss 6.94255447\n",
      "Trained batch 1011 batch loss 7.24914026 epoch total loss 6.94285774\n",
      "Trained batch 1012 batch loss 6.97834873 epoch total loss 6.94289303\n",
      "Trained batch 1013 batch loss 6.63879967 epoch total loss 6.94259262\n",
      "Trained batch 1014 batch loss 7.18522882 epoch total loss 6.94283152\n",
      "Trained batch 1015 batch loss 7.34134817 epoch total loss 6.94322443\n",
      "Trained batch 1016 batch loss 7.36044884 epoch total loss 6.94363499\n",
      "Trained batch 1017 batch loss 7.27013159 epoch total loss 6.9439559\n",
      "Trained batch 1018 batch loss 7.25414133 epoch total loss 6.94426\n",
      "Trained batch 1019 batch loss 6.96474838 epoch total loss 6.94428\n",
      "Trained batch 1020 batch loss 7.04783392 epoch total loss 6.94438171\n",
      "Trained batch 1021 batch loss 7.02998734 epoch total loss 6.94446564\n",
      "Trained batch 1022 batch loss 6.86401129 epoch total loss 6.94438648\n",
      "Trained batch 1023 batch loss 6.82298326 epoch total loss 6.94426775\n",
      "Trained batch 1024 batch loss 6.45375729 epoch total loss 6.94378853\n",
      "Trained batch 1025 batch loss 6.50071383 epoch total loss 6.94335604\n",
      "Trained batch 1026 batch loss 6.7796793 epoch total loss 6.94319677\n",
      "Trained batch 1027 batch loss 6.51844692 epoch total loss 6.94278336\n",
      "Trained batch 1028 batch loss 6.97050905 epoch total loss 6.94281\n",
      "Trained batch 1029 batch loss 6.3162446 epoch total loss 6.94220161\n",
      "Trained batch 1030 batch loss 6.6790719 epoch total loss 6.94194603\n",
      "Trained batch 1031 batch loss 6.41035748 epoch total loss 6.94143057\n",
      "Trained batch 1032 batch loss 6.19978333 epoch total loss 6.9407115\n",
      "Trained batch 1033 batch loss 6.58949661 epoch total loss 6.94037151\n",
      "Trained batch 1034 batch loss 7.02471352 epoch total loss 6.94045353\n",
      "Trained batch 1035 batch loss 6.98960876 epoch total loss 6.94050074\n",
      "Trained batch 1036 batch loss 6.97267675 epoch total loss 6.94053173\n",
      "Trained batch 1037 batch loss 7.21728659 epoch total loss 6.94079876\n",
      "Trained batch 1038 batch loss 7.31888628 epoch total loss 6.94116306\n",
      "Trained batch 1039 batch loss 7.49118948 epoch total loss 6.94169235\n",
      "Trained batch 1040 batch loss 7.34909 epoch total loss 6.94208431\n",
      "Trained batch 1041 batch loss 7.27849865 epoch total loss 6.94240713\n",
      "Trained batch 1042 batch loss 7.27189684 epoch total loss 6.94272327\n",
      "Trained batch 1043 batch loss 6.90373039 epoch total loss 6.94268608\n",
      "Trained batch 1044 batch loss 7.16401386 epoch total loss 6.94289827\n",
      "Trained batch 1045 batch loss 7.02106571 epoch total loss 6.94297314\n",
      "Trained batch 1046 batch loss 7.0918889 epoch total loss 6.94311523\n",
      "Trained batch 1047 batch loss 7.09571791 epoch total loss 6.94326115\n",
      "Trained batch 1048 batch loss 6.89922237 epoch total loss 6.94321918\n",
      "Trained batch 1049 batch loss 7.09062767 epoch total loss 6.94336\n",
      "Trained batch 1050 batch loss 7.01702261 epoch total loss 6.94343\n",
      "Trained batch 1051 batch loss 7.01550913 epoch total loss 6.94349861\n",
      "Trained batch 1052 batch loss 7.23132753 epoch total loss 6.94377232\n",
      "Trained batch 1053 batch loss 6.7673974 epoch total loss 6.94360495\n",
      "Trained batch 1054 batch loss 7.04431248 epoch total loss 6.94370079\n",
      "Trained batch 1055 batch loss 7.21002054 epoch total loss 6.94395304\n",
      "Trained batch 1056 batch loss 7.35458851 epoch total loss 6.94434214\n",
      "Trained batch 1057 batch loss 7.39149332 epoch total loss 6.94476509\n",
      "Trained batch 1058 batch loss 7.39703846 epoch total loss 6.94519234\n",
      "Trained batch 1059 batch loss 7.03250456 epoch total loss 6.94527531\n",
      "Trained batch 1060 batch loss 6.91951799 epoch total loss 6.94525099\n",
      "Trained batch 1061 batch loss 7.04904938 epoch total loss 6.94534826\n",
      "Trained batch 1062 batch loss 7.26775312 epoch total loss 6.94565201\n",
      "Trained batch 1063 batch loss 7.22070312 epoch total loss 6.94591045\n",
      "Trained batch 1064 batch loss 7.32146406 epoch total loss 6.94626331\n",
      "Trained batch 1065 batch loss 7.33073425 epoch total loss 6.94662428\n",
      "Trained batch 1066 batch loss 7.06851959 epoch total loss 6.94673824\n",
      "Trained batch 1067 batch loss 6.72219658 epoch total loss 6.94652796\n",
      "Trained batch 1068 batch loss 6.8726759 epoch total loss 6.94645882\n",
      "Trained batch 1069 batch loss 6.76983929 epoch total loss 6.94629383\n",
      "Trained batch 1070 batch loss 7.1335187 epoch total loss 6.94646835\n",
      "Trained batch 1071 batch loss 6.99520588 epoch total loss 6.94651365\n",
      "Trained batch 1072 batch loss 7.08412743 epoch total loss 6.94664192\n",
      "Trained batch 1073 batch loss 6.82491922 epoch total loss 6.94652843\n",
      "Trained batch 1074 batch loss 6.74138737 epoch total loss 6.94633722\n",
      "Trained batch 1075 batch loss 7.23039627 epoch total loss 6.94660139\n",
      "Trained batch 1076 batch loss 7.21068716 epoch total loss 6.94684696\n",
      "Trained batch 1077 batch loss 7.08760166 epoch total loss 6.94697714\n",
      "Trained batch 1078 batch loss 7.34317493 epoch total loss 6.94734478\n",
      "Trained batch 1079 batch loss 7.28916597 epoch total loss 6.9476614\n",
      "Trained batch 1080 batch loss 7.10786676 epoch total loss 6.94781\n",
      "Trained batch 1081 batch loss 7.15608215 epoch total loss 6.94800282\n",
      "Trained batch 1082 batch loss 7.17594671 epoch total loss 6.9482131\n",
      "Trained batch 1083 batch loss 7.35603523 epoch total loss 6.94859\n",
      "Trained batch 1084 batch loss 7.24014378 epoch total loss 6.94885874\n",
      "Trained batch 1085 batch loss 7.3521 epoch total loss 6.94923067\n",
      "Trained batch 1086 batch loss 6.91262674 epoch total loss 6.94919682\n",
      "Trained batch 1087 batch loss 6.98647213 epoch total loss 6.94923067\n",
      "Trained batch 1088 batch loss 7.4302659 epoch total loss 6.9496727\n",
      "Trained batch 1089 batch loss 7.20058537 epoch total loss 6.94990349\n",
      "Trained batch 1090 batch loss 6.9435854 epoch total loss 6.94989729\n",
      "Trained batch 1091 batch loss 7.23398304 epoch total loss 6.95015764\n",
      "Trained batch 1092 batch loss 7.15923071 epoch total loss 6.95034933\n",
      "Trained batch 1093 batch loss 7.08165932 epoch total loss 6.95046902\n",
      "Trained batch 1094 batch loss 7.19586372 epoch total loss 6.95069361\n",
      "Trained batch 1095 batch loss 7.27973604 epoch total loss 6.95099401\n",
      "Trained batch 1096 batch loss 7.47862864 epoch total loss 6.95147514\n",
      "Trained batch 1097 batch loss 7.4030757 epoch total loss 6.95188665\n",
      "Trained batch 1098 batch loss 7.3573103 epoch total loss 6.9522562\n",
      "Trained batch 1099 batch loss 7.23135 epoch total loss 6.95251\n",
      "Trained batch 1100 batch loss 7.3528285 epoch total loss 6.95287418\n",
      "Trained batch 1101 batch loss 7.2563591 epoch total loss 6.95315\n",
      "Trained batch 1102 batch loss 7.06554461 epoch total loss 6.95325184\n",
      "Trained batch 1103 batch loss 7.05423069 epoch total loss 6.95334339\n",
      "Trained batch 1104 batch loss 7.21304941 epoch total loss 6.95357847\n",
      "Trained batch 1105 batch loss 7.13466311 epoch total loss 6.9537425\n",
      "Trained batch 1106 batch loss 7.17402172 epoch total loss 6.95394135\n",
      "Trained batch 1107 batch loss 7.41179609 epoch total loss 6.95435476\n",
      "Trained batch 1108 batch loss 6.86144733 epoch total loss 6.95427084\n",
      "Trained batch 1109 batch loss 6.81036282 epoch total loss 6.95414114\n",
      "Trained batch 1110 batch loss 7.25160456 epoch total loss 6.95440912\n",
      "Trained batch 1111 batch loss 7.28448 epoch total loss 6.95470619\n",
      "Trained batch 1112 batch loss 7.19594431 epoch total loss 6.95492315\n",
      "Trained batch 1113 batch loss 7.10736036 epoch total loss 6.95506\n",
      "Trained batch 1114 batch loss 6.91816044 epoch total loss 6.95502663\n",
      "Trained batch 1115 batch loss 6.76832151 epoch total loss 6.95485973\n",
      "Trained batch 1116 batch loss 6.04794025 epoch total loss 6.95404673\n",
      "Trained batch 1117 batch loss 6.03518581 epoch total loss 6.95322418\n",
      "Trained batch 1118 batch loss 6.56983137 epoch total loss 6.95288134\n",
      "Trained batch 1119 batch loss 6.01132202 epoch total loss 6.95203972\n",
      "Trained batch 1120 batch loss 5.99651814 epoch total loss 6.95118666\n",
      "Trained batch 1121 batch loss 5.79138279 epoch total loss 6.9501524\n",
      "Trained batch 1122 batch loss 5.81110764 epoch total loss 6.94913673\n",
      "Trained batch 1123 batch loss 6.4882226 epoch total loss 6.94872665\n",
      "Trained batch 1124 batch loss 7.18997431 epoch total loss 6.94894123\n",
      "Trained batch 1125 batch loss 7.25172853 epoch total loss 6.94921064\n",
      "Trained batch 1126 batch loss 6.98486948 epoch total loss 6.94924212\n",
      "Trained batch 1127 batch loss 6.90565872 epoch total loss 6.94920349\n",
      "Trained batch 1128 batch loss 7.06131315 epoch total loss 6.94930315\n",
      "Trained batch 1129 batch loss 7.09140062 epoch total loss 6.94942904\n",
      "Trained batch 1130 batch loss 7.19842815 epoch total loss 6.94964933\n",
      "Trained batch 1131 batch loss 7.33888817 epoch total loss 6.94999313\n",
      "Trained batch 1132 batch loss 7.24859858 epoch total loss 6.95025682\n",
      "Trained batch 1133 batch loss 7.3133359 epoch total loss 6.95057774\n",
      "Trained batch 1134 batch loss 7.12213707 epoch total loss 6.95072889\n",
      "Trained batch 1135 batch loss 6.73355293 epoch total loss 6.9505372\n",
      "Trained batch 1136 batch loss 6.93016481 epoch total loss 6.95051956\n",
      "Trained batch 1137 batch loss 7.01411915 epoch total loss 6.95057535\n",
      "Trained batch 1138 batch loss 7.02579784 epoch total loss 6.95064163\n",
      "Trained batch 1139 batch loss 7.02873564 epoch total loss 6.9507103\n",
      "Trained batch 1140 batch loss 6.74343395 epoch total loss 6.95052862\n",
      "Trained batch 1141 batch loss 6.69067621 epoch total loss 6.95030117\n",
      "Trained batch 1142 batch loss 7.06436491 epoch total loss 6.95040083\n",
      "Trained batch 1143 batch loss 6.99174833 epoch total loss 6.95043707\n",
      "Trained batch 1144 batch loss 7.11784267 epoch total loss 6.95058346\n",
      "Trained batch 1145 batch loss 6.85917616 epoch total loss 6.95050383\n",
      "Trained batch 1146 batch loss 6.85189342 epoch total loss 6.95041752\n",
      "Trained batch 1147 batch loss 7.17155552 epoch total loss 6.95061\n",
      "Trained batch 1148 batch loss 6.87705517 epoch total loss 6.95054626\n",
      "Trained batch 1149 batch loss 6.54449892 epoch total loss 6.95019293\n",
      "Trained batch 1150 batch loss 7.3378334 epoch total loss 6.95053\n",
      "Trained batch 1151 batch loss 6.72808504 epoch total loss 6.95033646\n",
      "Trained batch 1152 batch loss 7.13251066 epoch total loss 6.95049477\n",
      "Trained batch 1153 batch loss 6.98277235 epoch total loss 6.9505229\n",
      "Trained batch 1154 batch loss 6.7118082 epoch total loss 6.95031595\n",
      "Trained batch 1155 batch loss 6.59923077 epoch total loss 6.95001173\n",
      "Trained batch 1156 batch loss 6.82796669 epoch total loss 6.94990635\n",
      "Trained batch 1157 batch loss 6.98511124 epoch total loss 6.94993687\n",
      "Trained batch 1158 batch loss 7.22688293 epoch total loss 6.95017624\n",
      "Trained batch 1159 batch loss 7.11030149 epoch total loss 6.95031452\n",
      "Trained batch 1160 batch loss 6.929039 epoch total loss 6.9502964\n",
      "Trained batch 1161 batch loss 6.86353779 epoch total loss 6.95022202\n",
      "Trained batch 1162 batch loss 6.84162617 epoch total loss 6.95012856\n",
      "Trained batch 1163 batch loss 6.62555885 epoch total loss 6.94984913\n",
      "Trained batch 1164 batch loss 6.78105402 epoch total loss 6.94970465\n",
      "Trained batch 1165 batch loss 7.26511669 epoch total loss 6.94997549\n",
      "Trained batch 1166 batch loss 7.23725033 epoch total loss 6.95022154\n",
      "Trained batch 1167 batch loss 7.35303926 epoch total loss 6.95056677\n",
      "Trained batch 1168 batch loss 7.39426708 epoch total loss 6.95094633\n",
      "Trained batch 1169 batch loss 7.27139616 epoch total loss 6.95122051\n",
      "Trained batch 1170 batch loss 7.31533718 epoch total loss 6.95153189\n",
      "Trained batch 1171 batch loss 6.83978558 epoch total loss 6.95143652\n",
      "Trained batch 1172 batch loss 7.00297928 epoch total loss 6.95148039\n",
      "Trained batch 1173 batch loss 6.57564068 epoch total loss 6.95116043\n",
      "Trained batch 1174 batch loss 6.98568583 epoch total loss 6.95119\n",
      "Trained batch 1175 batch loss 6.9923892 epoch total loss 6.9512248\n",
      "Trained batch 1176 batch loss 7.20791292 epoch total loss 6.9514432\n",
      "Trained batch 1177 batch loss 7.27594805 epoch total loss 6.95171881\n",
      "Trained batch 1178 batch loss 7.09897327 epoch total loss 6.95184374\n",
      "Trained batch 1179 batch loss 6.7282505 epoch total loss 6.95165396\n",
      "Trained batch 1180 batch loss 6.86548328 epoch total loss 6.95158052\n",
      "Trained batch 1181 batch loss 6.81406879 epoch total loss 6.95146465\n",
      "Trained batch 1182 batch loss 7.07498789 epoch total loss 6.95156908\n",
      "Trained batch 1183 batch loss 6.73820972 epoch total loss 6.95138884\n",
      "Trained batch 1184 batch loss 6.7652359 epoch total loss 6.95123196\n",
      "Trained batch 1185 batch loss 6.13776827 epoch total loss 6.95054579\n",
      "Trained batch 1186 batch loss 6.81655359 epoch total loss 6.9504323\n",
      "Trained batch 1187 batch loss 7.07359028 epoch total loss 6.95053577\n",
      "Trained batch 1188 batch loss 7.1972127 epoch total loss 6.95074368\n",
      "Trained batch 1189 batch loss 7.34774208 epoch total loss 6.95107746\n",
      "Trained batch 1190 batch loss 7.32022429 epoch total loss 6.95138788\n",
      "Trained batch 1191 batch loss 7.1686182 epoch total loss 6.95157051\n",
      "Trained batch 1192 batch loss 6.22708893 epoch total loss 6.95096302\n",
      "Trained batch 1193 batch loss 6.82411146 epoch total loss 6.95085669\n",
      "Trained batch 1194 batch loss 6.14628887 epoch total loss 6.95018291\n",
      "Trained batch 1195 batch loss 6.9569 epoch total loss 6.95018864\n",
      "Trained batch 1196 batch loss 7.14998198 epoch total loss 6.95035601\n",
      "Trained batch 1197 batch loss 7.08901072 epoch total loss 6.95047188\n",
      "Trained batch 1198 batch loss 7.15967417 epoch total loss 6.95064688\n",
      "Trained batch 1199 batch loss 7.17387962 epoch total loss 6.95083284\n",
      "Trained batch 1200 batch loss 6.66247654 epoch total loss 6.95059252\n",
      "Trained batch 1201 batch loss 6.99058056 epoch total loss 6.95062542\n",
      "Trained batch 1202 batch loss 6.52680588 epoch total loss 6.95027256\n",
      "Trained batch 1203 batch loss 6.95696688 epoch total loss 6.95027828\n",
      "Trained batch 1204 batch loss 7.13649178 epoch total loss 6.95043278\n",
      "Trained batch 1205 batch loss 7.21728706 epoch total loss 6.95065498\n",
      "Trained batch 1206 batch loss 7.41219091 epoch total loss 6.95103741\n",
      "Trained batch 1207 batch loss 7.26034927 epoch total loss 6.95129395\n",
      "Trained batch 1208 batch loss 7.30202389 epoch total loss 6.95158434\n",
      "Trained batch 1209 batch loss 7.21115398 epoch total loss 6.95179892\n",
      "Trained batch 1210 batch loss 7.29756927 epoch total loss 6.95208454\n",
      "Trained batch 1211 batch loss 7.11320543 epoch total loss 6.95221758\n",
      "Trained batch 1212 batch loss 6.90720749 epoch total loss 6.95218086\n",
      "Trained batch 1213 batch loss 6.98557472 epoch total loss 6.95220804\n",
      "Trained batch 1214 batch loss 7.21813869 epoch total loss 6.95242691\n",
      "Trained batch 1215 batch loss 7.29642725 epoch total loss 6.95271\n",
      "Trained batch 1216 batch loss 7.35006285 epoch total loss 6.95303679\n",
      "Trained batch 1217 batch loss 7.20236921 epoch total loss 6.95324135\n",
      "Trained batch 1218 batch loss 6.99817085 epoch total loss 6.95327806\n",
      "Trained batch 1219 batch loss 7.12007809 epoch total loss 6.95341492\n",
      "Trained batch 1220 batch loss 6.89032698 epoch total loss 6.95336342\n",
      "Trained batch 1221 batch loss 7.15836477 epoch total loss 6.95353127\n",
      "Trained batch 1222 batch loss 7.19636059 epoch total loss 6.95373\n",
      "Trained batch 1223 batch loss 6.65245581 epoch total loss 6.95348358\n",
      "Trained batch 1224 batch loss 6.93272686 epoch total loss 6.95346642\n",
      "Trained batch 1225 batch loss 7.01157665 epoch total loss 6.9535141\n",
      "Trained batch 1226 batch loss 7.27837372 epoch total loss 6.95377874\n",
      "Trained batch 1227 batch loss 7.33175135 epoch total loss 6.95408726\n",
      "Trained batch 1228 batch loss 7.22314024 epoch total loss 6.95430613\n",
      "Trained batch 1229 batch loss 7.29968262 epoch total loss 6.95458698\n",
      "Trained batch 1230 batch loss 7.18578625 epoch total loss 6.95477486\n",
      "Trained batch 1231 batch loss 7.34585905 epoch total loss 6.95509243\n",
      "Trained batch 1232 batch loss 7.0321722 epoch total loss 6.9551549\n",
      "Trained batch 1233 batch loss 6.48047924 epoch total loss 6.95477\n",
      "Trained batch 1234 batch loss 6.60616636 epoch total loss 6.9544878\n",
      "Trained batch 1235 batch loss 6.72012281 epoch total loss 6.95429754\n",
      "Trained batch 1236 batch loss 6.64904881 epoch total loss 6.95405102\n",
      "Trained batch 1237 batch loss 6.50012302 epoch total loss 6.95368385\n",
      "Trained batch 1238 batch loss 7.10719299 epoch total loss 6.95380831\n",
      "Trained batch 1239 batch loss 7.18477726 epoch total loss 6.95399427\n",
      "Trained batch 1240 batch loss 6.67217445 epoch total loss 6.95376682\n",
      "Trained batch 1241 batch loss 7.03257465 epoch total loss 6.95383024\n",
      "Trained batch 1242 batch loss 7.15398216 epoch total loss 6.95399141\n",
      "Trained batch 1243 batch loss 7.10042095 epoch total loss 6.95410919\n",
      "Trained batch 1244 batch loss 7.30133629 epoch total loss 6.9543891\n",
      "Trained batch 1245 batch loss 7.07909346 epoch total loss 6.95448923\n",
      "Trained batch 1246 batch loss 7.03909874 epoch total loss 6.95455694\n",
      "Trained batch 1247 batch loss 6.60800409 epoch total loss 6.95427942\n",
      "Trained batch 1248 batch loss 6.44645119 epoch total loss 6.9538722\n",
      "Trained batch 1249 batch loss 7.10608339 epoch total loss 6.95399427\n",
      "Trained batch 1250 batch loss 7.10265827 epoch total loss 6.95411348\n",
      "Trained batch 1251 batch loss 7.13411093 epoch total loss 6.95425701\n",
      "Trained batch 1252 batch loss 7.116714 epoch total loss 6.95438719\n",
      "Trained batch 1253 batch loss 6.7053237 epoch total loss 6.95418787\n",
      "Trained batch 1254 batch loss 6.93551207 epoch total loss 6.95417309\n",
      "Trained batch 1255 batch loss 7.06679773 epoch total loss 6.95426273\n",
      "Trained batch 1256 batch loss 7.01669931 epoch total loss 6.95431232\n",
      "Trained batch 1257 batch loss 7.23062181 epoch total loss 6.95453215\n",
      "Trained batch 1258 batch loss 6.28039074 epoch total loss 6.95399618\n",
      "Trained batch 1259 batch loss 6.45082808 epoch total loss 6.95359659\n",
      "Trained batch 1260 batch loss 6.32336378 epoch total loss 6.95309639\n",
      "Trained batch 1261 batch loss 7.09440136 epoch total loss 6.95320845\n",
      "Trained batch 1262 batch loss 7.09974 epoch total loss 6.95332479\n",
      "Trained batch 1263 batch loss 7.05760288 epoch total loss 6.95340729\n",
      "Trained batch 1264 batch loss 7.22313833 epoch total loss 6.95362043\n",
      "Trained batch 1265 batch loss 7.08796215 epoch total loss 6.95372629\n",
      "Trained batch 1266 batch loss 7.16686 epoch total loss 6.95389462\n",
      "Trained batch 1267 batch loss 7.42478323 epoch total loss 6.95426655\n",
      "Trained batch 1268 batch loss 7.01733208 epoch total loss 6.95431662\n",
      "Trained batch 1269 batch loss 7.36661959 epoch total loss 6.95464087\n",
      "Trained batch 1270 batch loss 7.11844444 epoch total loss 6.95476961\n",
      "Trained batch 1271 batch loss 6.86312628 epoch total loss 6.95469761\n",
      "Trained batch 1272 batch loss 6.72999 epoch total loss 6.95452166\n",
      "Trained batch 1273 batch loss 6.7551403 epoch total loss 6.95436478\n",
      "Trained batch 1274 batch loss 7.41866 epoch total loss 6.95472956\n",
      "Trained batch 1275 batch loss 7.2723155 epoch total loss 6.95497847\n",
      "Trained batch 1276 batch loss 6.48124599 epoch total loss 6.95460749\n",
      "Trained batch 1277 batch loss 6.83610344 epoch total loss 6.9545145\n",
      "Trained batch 1278 batch loss 6.38599586 epoch total loss 6.95406961\n",
      "Trained batch 1279 batch loss 6.7030468 epoch total loss 6.95387316\n",
      "Trained batch 1280 batch loss 7.23674822 epoch total loss 6.95409393\n",
      "Trained batch 1281 batch loss 6.55555344 epoch total loss 6.95378304\n",
      "Trained batch 1282 batch loss 7.19937801 epoch total loss 6.95397425\n",
      "Trained batch 1283 batch loss 6.91371441 epoch total loss 6.95394325\n",
      "Trained batch 1284 batch loss 6.87385702 epoch total loss 6.95388079\n",
      "Trained batch 1285 batch loss 6.95482111 epoch total loss 6.95388174\n",
      "Trained batch 1286 batch loss 6.99280357 epoch total loss 6.95391226\n",
      "Trained batch 1287 batch loss 6.76802206 epoch total loss 6.95376778\n",
      "Trained batch 1288 batch loss 6.68956614 epoch total loss 6.95356226\n",
      "Trained batch 1289 batch loss 7.19436646 epoch total loss 6.95374918\n",
      "Trained batch 1290 batch loss 6.54311514 epoch total loss 6.95343065\n",
      "Trained batch 1291 batch loss 6.84320211 epoch total loss 6.9533453\n",
      "Trained batch 1292 batch loss 7.18636 epoch total loss 6.95352554\n",
      "Trained batch 1293 batch loss 7.23910427 epoch total loss 6.9537468\n",
      "Trained batch 1294 batch loss 7.27887249 epoch total loss 6.95399809\n",
      "Trained batch 1295 batch loss 7.25561666 epoch total loss 6.95423126\n",
      "Trained batch 1296 batch loss 7.09285164 epoch total loss 6.95433807\n",
      "Trained batch 1297 batch loss 7.05787039 epoch total loss 6.95441771\n",
      "Trained batch 1298 batch loss 6.91638613 epoch total loss 6.95438814\n",
      "Trained batch 1299 batch loss 7.24110222 epoch total loss 6.95460892\n",
      "Trained batch 1300 batch loss 6.28812933 epoch total loss 6.95409632\n",
      "Trained batch 1301 batch loss 6.59878397 epoch total loss 6.95382309\n",
      "Trained batch 1302 batch loss 7.13183594 epoch total loss 6.95396\n",
      "Trained batch 1303 batch loss 7.09951591 epoch total loss 6.95407152\n",
      "Trained batch 1304 batch loss 6.08502 epoch total loss 6.9534049\n",
      "Trained batch 1305 batch loss 7.28651714 epoch total loss 6.95366\n",
      "Trained batch 1306 batch loss 6.79115438 epoch total loss 6.95353556\n",
      "Trained batch 1307 batch loss 6.18888569 epoch total loss 6.95295\n",
      "Trained batch 1308 batch loss 6.92445421 epoch total loss 6.95292854\n",
      "Trained batch 1309 batch loss 6.92778158 epoch total loss 6.95290947\n",
      "Trained batch 1310 batch loss 7.30142355 epoch total loss 6.95317554\n",
      "Trained batch 1311 batch loss 7.22703362 epoch total loss 6.9533844\n",
      "Trained batch 1312 batch loss 7.10547352 epoch total loss 6.95350027\n",
      "Trained batch 1313 batch loss 7.18094778 epoch total loss 6.95367336\n",
      "Trained batch 1314 batch loss 7.21254253 epoch total loss 6.9538703\n",
      "Trained batch 1315 batch loss 7.11246538 epoch total loss 6.95399094\n",
      "Trained batch 1316 batch loss 6.54805231 epoch total loss 6.95368242\n",
      "Trained batch 1317 batch loss 7.02645779 epoch total loss 6.95373726\n",
      "Trained batch 1318 batch loss 7.02409792 epoch total loss 6.95379114\n",
      "Trained batch 1319 batch loss 7.09979486 epoch total loss 6.95390177\n",
      "Trained batch 1320 batch loss 7.056355 epoch total loss 6.95397949\n",
      "Trained batch 1321 batch loss 7.01843262 epoch total loss 6.95402861\n",
      "Trained batch 1322 batch loss 6.54553604 epoch total loss 6.95371962\n",
      "Trained batch 1323 batch loss 6.86486 epoch total loss 6.95365286\n",
      "Trained batch 1324 batch loss 7.09652328 epoch total loss 6.95376062\n",
      "Trained batch 1325 batch loss 7.31044102 epoch total loss 6.95403\n",
      "Trained batch 1326 batch loss 6.96472645 epoch total loss 6.95403814\n",
      "Trained batch 1327 batch loss 7.03848934 epoch total loss 6.95410156\n",
      "Trained batch 1328 batch loss 7.13910484 epoch total loss 6.95424032\n",
      "Trained batch 1329 batch loss 7.14683104 epoch total loss 6.95438528\n",
      "Trained batch 1330 batch loss 6.86307478 epoch total loss 6.95431662\n",
      "Trained batch 1331 batch loss 7.07725763 epoch total loss 6.95440912\n",
      "Trained batch 1332 batch loss 7.06473303 epoch total loss 6.95449162\n",
      "Trained batch 1333 batch loss 6.87146187 epoch total loss 6.95442915\n",
      "Trained batch 1334 batch loss 7.10589504 epoch total loss 6.95454216\n",
      "Trained batch 1335 batch loss 6.82945919 epoch total loss 6.95444822\n",
      "Trained batch 1336 batch loss 7.2879405 epoch total loss 6.95469809\n",
      "Trained batch 1337 batch loss 7.43348932 epoch total loss 6.95505619\n",
      "Trained batch 1338 batch loss 7.41671753 epoch total loss 6.95540142\n",
      "Trained batch 1339 batch loss 6.51566696 epoch total loss 6.95507288\n",
      "Trained batch 1340 batch loss 6.25786734 epoch total loss 6.95455265\n",
      "Trained batch 1341 batch loss 5.94906235 epoch total loss 6.95380306\n",
      "Trained batch 1342 batch loss 7.29480505 epoch total loss 6.95405722\n",
      "Trained batch 1343 batch loss 7.21712446 epoch total loss 6.95425272\n",
      "Trained batch 1344 batch loss 7.03789663 epoch total loss 6.95431519\n",
      "Trained batch 1345 batch loss 6.67691946 epoch total loss 6.95410872\n",
      "Trained batch 1346 batch loss 6.5772 epoch total loss 6.95382881\n",
      "Trained batch 1347 batch loss 6.47872782 epoch total loss 6.95347595\n",
      "Trained batch 1348 batch loss 7.00346041 epoch total loss 6.95351315\n",
      "Trained batch 1349 batch loss 6.85347176 epoch total loss 6.95343924\n",
      "Trained batch 1350 batch loss 7.18733263 epoch total loss 6.95361233\n",
      "Trained batch 1351 batch loss 7.26516533 epoch total loss 6.95384359\n",
      "Trained batch 1352 batch loss 7.12291336 epoch total loss 6.95396852\n",
      "Trained batch 1353 batch loss 7.1963582 epoch total loss 6.95414782\n",
      "Trained batch 1354 batch loss 7.27921867 epoch total loss 6.95438766\n",
      "Trained batch 1355 batch loss 7.34621143 epoch total loss 6.95467758\n",
      "Trained batch 1356 batch loss 7.22869825 epoch total loss 6.95487928\n",
      "Trained batch 1357 batch loss 7.0668726 epoch total loss 6.95496178\n",
      "Trained batch 1358 batch loss 7.3067112 epoch total loss 6.9552207\n",
      "Trained batch 1359 batch loss 7.17027903 epoch total loss 6.95537853\n",
      "Trained batch 1360 batch loss 7.25435591 epoch total loss 6.95559788\n",
      "Trained batch 1361 batch loss 7.15695477 epoch total loss 6.95574617\n",
      "Trained batch 1362 batch loss 7.15699816 epoch total loss 6.95589399\n",
      "Trained batch 1363 batch loss 7.17995501 epoch total loss 6.9560585\n",
      "Trained batch 1364 batch loss 7.06642294 epoch total loss 6.95613909\n",
      "Trained batch 1365 batch loss 7.38354301 epoch total loss 6.95645237\n",
      "Trained batch 1366 batch loss 7.26903296 epoch total loss 6.95668077\n",
      "Trained batch 1367 batch loss 7.02295732 epoch total loss 6.95673\n",
      "Trained batch 1368 batch loss 6.59790611 epoch total loss 6.95646715\n",
      "Trained batch 1369 batch loss 6.71903038 epoch total loss 6.95629358\n",
      "Trained batch 1370 batch loss 6.53852224 epoch total loss 6.95598841\n",
      "Trained batch 1371 batch loss 6.86794758 epoch total loss 6.95592451\n",
      "Trained batch 1372 batch loss 6.36515903 epoch total loss 6.95549393\n",
      "Trained batch 1373 batch loss 6.16850662 epoch total loss 6.95492077\n",
      "Trained batch 1374 batch loss 6.1977458 epoch total loss 6.95436954\n",
      "Trained batch 1375 batch loss 6.38458347 epoch total loss 6.95395517\n",
      "Trained batch 1376 batch loss 6.9948349 epoch total loss 6.95398521\n",
      "Trained batch 1377 batch loss 7.05163383 epoch total loss 6.95405626\n",
      "Trained batch 1378 batch loss 6.98737669 epoch total loss 6.95408\n",
      "Trained batch 1379 batch loss 6.5346632 epoch total loss 6.95377588\n",
      "Trained batch 1380 batch loss 6.72929668 epoch total loss 6.95361328\n",
      "Trained batch 1381 batch loss 6.70581341 epoch total loss 6.95343399\n",
      "Trained batch 1382 batch loss 7.14126587 epoch total loss 6.95357037\n",
      "Trained batch 1383 batch loss 6.78030062 epoch total loss 6.95344496\n",
      "Trained batch 1384 batch loss 6.81314468 epoch total loss 6.95334387\n",
      "Trained batch 1385 batch loss 7.06542587 epoch total loss 6.95342445\n",
      "Trained batch 1386 batch loss 6.92800379 epoch total loss 6.95340633\n",
      "Trained batch 1387 batch loss 7.07124233 epoch total loss 6.95349121\n",
      "Trained batch 1388 batch loss 7.15292025 epoch total loss 6.95363522\n",
      "Epoch 10 train loss 6.953635215759277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 03:12:11.183887: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "2025-03-19 03:12:11.183947: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "\t [[RemoteCall]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 1 batch loss 6.36331034\n",
      "Validated batch 2 batch loss 7.36480379\n",
      "Validated batch 3 batch loss 7.06789541\n",
      "Validated batch 4 batch loss 7.1231885\n",
      "Validated batch 5 batch loss 6.98414898\n",
      "Validated batch 6 batch loss 7.43786669\n",
      "Validated batch 7 batch loss 6.93752956\n",
      "Validated batch 8 batch loss 7.43136024\n",
      "Validated batch 9 batch loss 6.70616722\n",
      "Validated batch 10 batch loss 7.09163952\n",
      "Validated batch 11 batch loss 6.83634329\n",
      "Validated batch 12 batch loss 6.32692862\n",
      "Validated batch 13 batch loss 6.56861353\n",
      "Validated batch 14 batch loss 7.40496302\n",
      "Validated batch 15 batch loss 7.1499486\n",
      "Validated batch 16 batch loss 6.68382502\n",
      "Validated batch 17 batch loss 7.24091434\n",
      "Validated batch 18 batch loss 7.20008564\n",
      "Validated batch 19 batch loss 7.12234783\n",
      "Validated batch 20 batch loss 7.41875362\n",
      "Validated batch 21 batch loss 7.21044254\n",
      "Validated batch 22 batch loss 6.87454891\n",
      "Validated batch 23 batch loss 6.57778549\n",
      "Validated batch 24 batch loss 6.90729237\n",
      "Validated batch 25 batch loss 7.07845879\n",
      "Validated batch 26 batch loss 6.5644412\n",
      "Validated batch 27 batch loss 6.94126081\n",
      "Validated batch 28 batch loss 7.03081512\n",
      "Validated batch 29 batch loss 7.11424398\n",
      "Validated batch 30 batch loss 6.92272091\n",
      "Validated batch 31 batch loss 6.99467564\n",
      "Validated batch 32 batch loss 6.9677434\n",
      "Validated batch 33 batch loss 7.32431459\n",
      "Validated batch 34 batch loss 7.39652395\n",
      "Validated batch 35 batch loss 7.10602045\n",
      "Validated batch 36 batch loss 7.0110836\n",
      "Validated batch 37 batch loss 6.88310862\n",
      "Validated batch 38 batch loss 7.12418699\n",
      "Validated batch 39 batch loss 6.94735909\n",
      "Validated batch 40 batch loss 6.97373962\n",
      "Validated batch 41 batch loss 6.99731159\n",
      "Validated batch 42 batch loss 6.93478107\n",
      "Validated batch 43 batch loss 7.16304493\n",
      "Validated batch 44 batch loss 6.69288588\n",
      "Validated batch 45 batch loss 7.08891153\n",
      "Validated batch 46 batch loss 7.08899\n",
      "Validated batch 47 batch loss 6.87579823\n",
      "Validated batch 48 batch loss 6.99941349\n",
      "Validated batch 49 batch loss 6.82204151\n",
      "Validated batch 50 batch loss 7.26864529\n",
      "Validated batch 51 batch loss 7.37135029\n",
      "Validated batch 52 batch loss 7.14973068\n",
      "Validated batch 53 batch loss 7.20127296\n",
      "Validated batch 54 batch loss 7.04188204\n",
      "Validated batch 55 batch loss 7.26268959\n",
      "Validated batch 56 batch loss 7.32916451\n",
      "Validated batch 57 batch loss 7.42175293\n",
      "Validated batch 58 batch loss 7.20569658\n",
      "Validated batch 59 batch loss 6.72823668\n",
      "Validated batch 60 batch loss 7.22347736\n",
      "Validated batch 61 batch loss 6.93655062\n",
      "Validated batch 62 batch loss 6.91165161\n",
      "Validated batch 63 batch loss 7.2550807\n",
      "Validated batch 64 batch loss 6.11514854\n",
      "Validated batch 65 batch loss 6.90663481\n",
      "Validated batch 66 batch loss 7.28539085\n",
      "Validated batch 67 batch loss 6.93749475\n",
      "Validated batch 68 batch loss 7.37494326\n",
      "Validated batch 69 batch loss 7.05865717\n",
      "Validated batch 70 batch loss 7.27119923\n",
      "Validated batch 71 batch loss 7.33418894\n",
      "Validated batch 72 batch loss 6.92908478\n",
      "Validated batch 73 batch loss 6.27697611\n",
      "Validated batch 74 batch loss 7.13971519\n",
      "Validated batch 75 batch loss 6.71261597\n",
      "Validated batch 76 batch loss 6.43435383\n",
      "Validated batch 77 batch loss 6.87987\n",
      "Validated batch 78 batch loss 6.53090525\n",
      "Validated batch 79 batch loss 7.27426815\n",
      "Validated batch 80 batch loss 6.57207394\n",
      "Validated batch 81 batch loss 6.5154314\n",
      "Validated batch 82 batch loss 6.82130241\n",
      "Validated batch 83 batch loss 6.97717619\n",
      "Validated batch 84 batch loss 6.91063833\n",
      "Validated batch 85 batch loss 7.11157656\n",
      "Validated batch 86 batch loss 6.95568562\n",
      "Validated batch 87 batch loss 7.22005\n",
      "Validated batch 88 batch loss 6.90994167\n",
      "Validated batch 89 batch loss 6.71081209\n",
      "Validated batch 90 batch loss 7.19101954\n",
      "Validated batch 91 batch loss 5.964746\n",
      "Validated batch 92 batch loss 7.32062864\n",
      "Validated batch 93 batch loss 7.49075\n",
      "Validated batch 94 batch loss 7.01879072\n",
      "Validated batch 95 batch loss 7.15012121\n",
      "Validated batch 96 batch loss 7.10007095\n",
      "Validated batch 97 batch loss 7.13837528\n",
      "Validated batch 98 batch loss 7.26904154\n",
      "Validated batch 99 batch loss 6.99414682\n",
      "Validated batch 100 batch loss 6.72484\n",
      "Validated batch 101 batch loss 6.80995\n",
      "Validated batch 102 batch loss 6.83770418\n",
      "Validated batch 103 batch loss 7.15838385\n",
      "Validated batch 104 batch loss 7.03885794\n",
      "Validated batch 105 batch loss 6.66371\n",
      "Validated batch 106 batch loss 6.63342524\n",
      "Validated batch 107 batch loss 6.92325497\n",
      "Validated batch 108 batch loss 7.31188297\n",
      "Validated batch 109 batch loss 7.2789669\n",
      "Validated batch 110 batch loss 7.12272167\n",
      "Validated batch 111 batch loss 7.47892666\n",
      "Validated batch 112 batch loss 7.44321\n",
      "Validated batch 113 batch loss 7.41964722\n",
      "Validated batch 114 batch loss 6.87449265\n",
      "Validated batch 115 batch loss 6.64612913\n",
      "Validated batch 116 batch loss 6.59338\n",
      "Validated batch 117 batch loss 6.59187937\n",
      "Validated batch 118 batch loss 7.07657766\n",
      "Validated batch 119 batch loss 7.09465456\n",
      "Validated batch 120 batch loss 6.61657333\n",
      "Validated batch 121 batch loss 7.22140074\n",
      "Validated batch 122 batch loss 7.04541063\n",
      "Validated batch 123 batch loss 7.10474062\n",
      "Validated batch 124 batch loss 6.99237633\n",
      "Validated batch 125 batch loss 7.01864338\n",
      "Validated batch 126 batch loss 6.9223156\n",
      "Validated batch 127 batch loss 6.97075081\n",
      "Validated batch 128 batch loss 6.88199282\n",
      "Validated batch 129 batch loss 7.26147509\n",
      "Validated batch 130 batch loss 7.28581619\n",
      "Validated batch 131 batch loss 7.06854057\n",
      "Validated batch 132 batch loss 6.98522568\n",
      "Validated batch 133 batch loss 6.52886581\n",
      "Validated batch 134 batch loss 7.13596153\n",
      "Validated batch 135 batch loss 7.29621363\n",
      "Validated batch 136 batch loss 7.18694\n",
      "Validated batch 137 batch loss 7.25866175\n",
      "Validated batch 138 batch loss 6.81490469\n",
      "Validated batch 139 batch loss 6.99342823\n",
      "Validated batch 140 batch loss 6.91929436\n",
      "Validated batch 141 batch loss 6.73065519\n",
      "Validated batch 142 batch loss 6.18643856\n",
      "Validated batch 143 batch loss 6.87999\n",
      "Validated batch 144 batch loss 7.09545279\n",
      "Validated batch 145 batch loss 6.52898026\n",
      "Validated batch 146 batch loss 7.10836267\n",
      "Validated batch 147 batch loss 7.19040728\n",
      "Validated batch 148 batch loss 7.23532391\n",
      "Validated batch 149 batch loss 7.27946901\n",
      "Validated batch 150 batch loss 7.02131462\n",
      "Validated batch 151 batch loss 6.71454191\n",
      "Validated batch 152 batch loss 7.11859417\n",
      "Validated batch 153 batch loss 7.15229797\n",
      "Validated batch 154 batch loss 7.34550285\n",
      "Validated batch 155 batch loss 7.02251291\n",
      "Validated batch 156 batch loss 7.22617579\n",
      "Validated batch 157 batch loss 7.38924408\n",
      "Validated batch 158 batch loss 6.93619394\n",
      "Validated batch 159 batch loss 7.21108627\n",
      "Validated batch 160 batch loss 7.1425643\n",
      "Validated batch 161 batch loss 7.22536469\n",
      "Validated batch 162 batch loss 7.19575405\n",
      "Validated batch 163 batch loss 7.03624868\n",
      "Validated batch 164 batch loss 7.05517673\n",
      "Validated batch 165 batch loss 6.8835206\n",
      "Validated batch 166 batch loss 6.64950562\n",
      "Validated batch 167 batch loss 7.04120398\n",
      "Validated batch 168 batch loss 6.7086978\n",
      "Validated batch 169 batch loss 7.00123453\n",
      "Validated batch 170 batch loss 7.42487621\n",
      "Validated batch 171 batch loss 7.10537624\n",
      "Validated batch 172 batch loss 6.81529951\n",
      "Validated batch 173 batch loss 6.85763168\n",
      "Validated batch 174 batch loss 6.56728125\n",
      "Validated batch 175 batch loss 7.16539669\n",
      "Validated batch 176 batch loss 7.01715612\n",
      "Validated batch 177 batch loss 7.0672574\n",
      "Validated batch 178 batch loss 7.01107359\n",
      "Validated batch 179 batch loss 6.52620888\n",
      "Validated batch 180 batch loss 6.46994257\n",
      "Validated batch 181 batch loss 6.9329381\n",
      "Validated batch 182 batch loss 6.94403696\n",
      "Validated batch 183 batch loss 6.3990407\n",
      "Validated batch 184 batch loss 6.99953175\n",
      "Validated batch 185 batch loss 3.57394338\n",
      "Epoch 10 val loss 6.976254940032959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 03:12:19.506058: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "2025-03-19 03:12:19.506108: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "\t [[RemoteCall]]\n"
     ]
    }
   ],
   "source": [
    "# 학습 실행 (history 객체에 에포크별 손실 기록 저장)\n",
    "history_sb = train_sb(EPOCHS, LEARNING_RATE, NUM_HEATMAP, BATCH_SIZE, train_tfrecords, val_tfrecords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 에포크 수: 각 모델의 history 길이에 따라 설정 (예시로 두 모델 모두 동일한 에포크 수라고 가정)\n",
    "epochs_hg = range(1, len(history[\"train_loss\"]) + 1)    # 모래시계 모델의 history\n",
    "epochs_sb = range(1, len(history_sb[\"train_loss\"]) + 1)   # SimpleBaseline 모델의 history_sb\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# 학습 손실 비교 (Train Loss)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_hg, history[\"train_loss\"], label=\"Hourglass Train Loss\", marker=\"o\")\n",
    "plt.plot(epochs_sb, history_sb[\"train_loss\"], label=\"SimpleBaseline Train Loss\", marker=\"o\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Train Loss\")\n",
    "plt.title(\"Training Loss Comparison\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# 검증 손실 비교 (Validation Loss)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_hg, history[\"val_loss\"], label=\"Hourglass Val Loss\", marker=\"o\")\n",
    "plt.plot(epochs_sb, history_sb[\"val_loss\"], label=\"SimpleBaseline Val Loss\", marker=\"o\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Validation Loss\")\n",
    "plt.title(\"Validation Loss Comparison\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 에포크 수 설정\n",
    "epochs_hg = range(1, len(history[\"train_loss\"]) + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs_hg, history[\"train_loss\"], label=\"Train Loss\", marker=\"o\")\n",
    "plt.plot(epochs_hg, history[\"val_loss\"], label=\"Validation Loss\", marker=\"o\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Stacked Hourglass Model Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 에포크 수 설정\n",
    "epochs_sb = range(1, len(history_sb[\"train_loss\"]) + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs_sb, history_sb[\"train_loss\"], label=\"Train Loss\", marker=\"o\")\n",
    "plt.plot(epochs_sb, history_sb[\"val_loss\"], label=\"Validation Loss\", marker=\"o\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"SimpleBaseline Model Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 에포크로도 과대적합을 판단하기에는 애매했다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "R_ANKLE = 0\n",
    "R_KNEE = 1\n",
    "R_HIP = 2\n",
    "L_HIP = 3\n",
    "L_KNEE = 4\n",
    "L_ANKLE = 5\n",
    "PELVIS = 6\n",
    "THORAX = 7\n",
    "UPPER_NECK = 8\n",
    "HEAD_TOP = 9\n",
    "R_WRIST = 10\n",
    "R_ELBOW = 11\n",
    "R_SHOULDER = 12\n",
    "L_SHOULDER = 13\n",
    "L_ELBOW = 14\n",
    "L_WRIST = 15\n",
    "\n",
    "MPII_BONES = [\n",
    "    [R_ANKLE, R_KNEE],\n",
    "    [R_KNEE, R_HIP],\n",
    "    [R_HIP, PELVIS],\n",
    "    [L_HIP, PELVIS],\n",
    "    [L_HIP, L_KNEE],\n",
    "    [L_KNEE, L_ANKLE],\n",
    "    [PELVIS, THORAX],\n",
    "    [THORAX, UPPER_NECK],\n",
    "    [UPPER_NECK, HEAD_TOP],\n",
    "    [R_WRIST, R_ELBOW],\n",
    "    [R_ELBOW, R_SHOULDER],\n",
    "    [THORAX, R_SHOULDER],\n",
    "    [THORAX, L_SHOULDER],\n",
    "    [L_SHOULDER, L_ELBOW],\n",
    "    [L_ELBOW, L_WRIST]\n",
    "]\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def find_max_coordinates(heatmaps):\n",
    "    flatten_heatmaps = tf.reshape(heatmaps, (-1, 16))\n",
    "    indices = tf.math.argmax(flatten_heatmaps, axis=0)\n",
    "    y = tf.cast(indices / 64, dtype=tf.int64)\n",
    "    x = indices - 64 * y\n",
    "    return tf.stack([x, y], axis=1).numpy()\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def extract_keypoints_from_heatmap(heatmaps):\n",
    "    max_keypoints = find_max_coordinates(heatmaps)\n",
    "\n",
    "    padded_heatmap = np.pad(heatmaps, [[1,1],[1,1],[0,0]], mode='constant')\n",
    "    adjusted_keypoints = []\n",
    "    for i, keypoint in enumerate(max_keypoints):\n",
    "        max_y = keypoint[1]+1\n",
    "        max_x = keypoint[0]+1\n",
    "        \n",
    "        patch = padded_heatmap[max_y-1:max_y+2, max_x-1:max_x+2, i]\n",
    "        patch[1][1] = 0\n",
    "        \n",
    "        index = np.argmax(patch)\n",
    "        \n",
    "        next_y = index // 3\n",
    "        next_x = index - next_y * 3\n",
    "        delta_y = (next_y - 1) / 4\n",
    "        delta_x = (next_x - 1) / 4\n",
    "        \n",
    "        adjusted_keypoint_x = keypoint[0] + delta_x\n",
    "        adjusted_keypoint_y = keypoint[1] + delta_y\n",
    "        adjusted_keypoints.append((adjusted_keypoint_x, adjusted_keypoint_y))\n",
    "        \n",
    "    adjusted_keypoints = np.clip(adjusted_keypoints, 0, 64)\n",
    "    normalized_keypoints = adjusted_keypoints / 64\n",
    "    return normalized_keypoints\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def predict(model, image_path):\n",
    "    encoded = tf.io.read_file(image_path)\n",
    "    image = tf.io.decode_jpeg(encoded)\n",
    "    inputs = tf.image.resize(image, (256, 256))\n",
    "    inputs = tf.cast(inputs, tf.float32) / 127.5 - 1\n",
    "    inputs = tf.expand_dims(inputs, 0)\n",
    "    outputs = model(inputs, training=False)\n",
    "    if type(outputs) != list:\n",
    "        outputs = [outputs]\n",
    "    heatmap = tf.squeeze(outputs[-1], axis=0).numpy()\n",
    "    kp = extract_keypoints_from_heatmap(heatmap)\n",
    "    return image, kp\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def draw_keypoints_on_image(image, keypoints, index=None):\n",
    "    fig,ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "    joints = []\n",
    "    for i, joint in enumerate(keypoints):\n",
    "        joint_x = joint[0] * image.shape[1]\n",
    "        joint_y = joint[1] * image.shape[0]\n",
    "        if index is not None and index != i:\n",
    "            continue\n",
    "        plt.scatter(joint_x, joint_y, s=10, c='red', marker='o')\n",
    "    plt.show()\n",
    "\n",
    "def draw_skeleton_on_image(image, keypoints, index=None):\n",
    "    fig,ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "    joints = []\n",
    "    for i, joint in enumerate(keypoints):\n",
    "        joint_x = joint[0] * image.shape[1]\n",
    "        joint_y = joint[1] * image.shape[0]\n",
    "        joints.append((joint_x, joint_y))\n",
    "    \n",
    "    for bone in MPII_BONES:\n",
    "        joint_1 = joints[bone[0]]\n",
    "        joint_2 = joints[bone[1]]\n",
    "        plt.plot([joint_1[0], joint_2[0]], [joint_1[1], joint_2[1]], linewidth=5, alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m test_image = os.path.join(PROJECT_PATH, \u001b[33m'\u001b[39m\u001b[33mtest_image.jpg\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m image, keypoints = predict(\u001b[43mmodel\u001b[49m, test_image)\n\u001b[32m      4\u001b[39m draw_keypoints_on_image(image, keypoints)\n\u001b[32m      5\u001b[39m draw_skeleton_on_image(image, keypoints)\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "test_image = os.path.join(PROJECT_PATH, 'test_image.jpg')\n",
    "\n",
    "image, keypoints = predict(model, test_image)\n",
    "draw_keypoints_on_image(image, keypoints)\n",
    "draw_skeleton_on_image(image, keypoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 회고\n",
    "\n",
    "1. SimpleBaseline 모델이 구조가 좀 더 간단해서 그런지 수렴도 오래걸리고 StacekdHourglass 모델에 비해 성능도 잘 안나왔다.\n",
    "2. 논문에서 언긊한 에포크가 거의 200가까이 였던 것 같아서, 학습을 더 돌려보고 싶었으나 쉽지 않았다.\n",
    "3. 정성적인 평가를 진행하지 못해서 아쉬웠다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
